{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses\n",
    "from Inference.GeNVI_PAC_method import GeNPAC, GeNetEns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Experiments.foong import Setup\n",
    "setup=Setup(device, layerwidth=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target density #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "loss= setup.loss\n",
    "logprior=setup.logprior\n",
    "n_data_samples=setup.n_train_samples\n",
    "param_count=setup.param_count\n",
    "model=setup._model\n",
    "print(n_data_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activation = nn.Tanh()#nn.ReLU()\n",
    "init_b = .001\n",
    "lat_dim=5\n",
    "GeN = GeNetEns(1, lat_dim, 50, param_count, activation, 0.2, init_b, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20000], Bound: 1.168597936630249, Entropy: 57.88224411010742, Temp: 1.0, KL: 113.99787902832031, Loss: 0.7403866648674011, Learning Rate: 0.03\n",
      "Epoch [1/20000], Bound: 1.215516209602356, Entropy: 80.47090148925781, Temp: 0.9936903715133667, KL: 101.28401184082031, Loss: 0.9180778861045837, Learning Rate: 0.03\n",
      "Epoch [2/20000], Bound: 1.2061084508895874, Entropy: 108.00324249267578, Temp: 0.9874571561813354, KL: 89.31916046142578, Loss: 0.947492778301239, Learning Rate: 0.03\n",
      "Epoch [3/20000], Bound: 1.2015223503112793, Entropy: 129.00759887695312, Temp: 0.9812135696411133, KL: 86.94535827636719, Loss: 0.9420843124389648, Learning Rate: 0.03\n",
      "Epoch [4/20000], Bound: 1.183053970336914, Entropy: 148.82772827148438, Temp: 0.9749624729156494, KL: 80.40480041503906, Loss: 0.9242614507675171, Learning Rate: 0.03\n",
      "Epoch [5/20000], Bound: 1.2154054641723633, Entropy: 160.03274536132812, Temp: 0.9687147736549377, KL: 93.21176147460938, Loss: 0.9341996312141418, Learning Rate: 0.03\n",
      "Epoch [6/20000], Bound: 1.2336947917938232, Entropy: 171.55276489257812, Temp: 0.9624941349029541, KL: 98.83346557617188, Loss: 0.9472514390945435, Learning Rate: 0.03\n",
      "Epoch [7/20000], Bound: 1.2275148630142212, Entropy: 177.75881958007812, Temp: 0.9563072323799133, KL: 93.33233642578125, Loss: 0.9529261589050293, Learning Rate: 0.03\n",
      "Epoch [8/20000], Bound: 1.2495925426483154, Entropy: 180.35166931152344, Temp: 0.9501438140869141, KL: 105.72276306152344, Loss: 0.9407129287719727, Learning Rate: 0.03\n",
      "Epoch [9/20000], Bound: 1.2438095808029175, Entropy: 181.4188690185547, Temp: 0.9440380334854126, KL: 95.80015563964844, Loss: 0.9703133702278137, Learning Rate: 0.03\n",
      "Epoch [10/20000], Bound: 1.2511564493179321, Entropy: 182.8519744873047, Temp: 0.9379526972770691, KL: 100.29731750488281, Loss: 0.959394633769989, Learning Rate: 0.03\n",
      "Epoch [11/20000], Bound: 1.2286410331726074, Entropy: 182.06956481933594, Temp: 0.9319075345993042, KL: 88.55424499511719, Loss: 0.9563677906990051, Learning Rate: 0.03\n",
      "Epoch [12/20000], Bound: 1.2347869873046875, Entropy: 179.11366271972656, Temp: 0.9258815050125122, KL: 92.55720520019531, Loss: 0.9443095922470093, Learning Rate: 0.03\n",
      "Epoch [13/20000], Bound: 1.2607877254486084, Entropy: 174.60910034179688, Temp: 0.9198950529098511, KL: 98.51492309570312, Loss: 0.9737412333488464, Learning Rate: 0.03\n",
      "Epoch [14/20000], Bound: 1.2433927059173584, Entropy: 172.85250854492188, Temp: 0.9139481782913208, KL: 90.85824584960938, Loss: 0.9628525972366333, Learning Rate: 0.03\n",
      "Epoch [15/20000], Bound: 1.2079148292541504, Entropy: 169.64187622070312, Temp: 0.9080315828323364, KL: 80.05584716796875, Loss: 0.929097056388855, Learning Rate: 0.03\n",
      "Epoch [16/20000], Bound: 1.2474297285079956, Entropy: 165.68844604492188, Temp: 0.9021409749984741, KL: 92.10720825195312, Loss: 0.9533354043960571, Learning Rate: 0.03\n",
      "Epoch [17/20000], Bound: 1.2388181686401367, Entropy: 162.5987091064453, Temp: 0.8962982296943665, KL: 89.86383056640625, Loss: 0.9378159642219543, Learning Rate: 0.03\n",
      "Epoch [18/20000], Bound: 1.2118512392044067, Entropy: 155.158935546875, Temp: 0.8905078172683716, KL: 76.88301086425781, Loss: 0.940175473690033, Learning Rate: 0.03\n",
      "Epoch [19/20000], Bound: 1.2440810203552246, Entropy: 155.7452850341797, Temp: 0.884734570980072, KL: 85.32057189941406, Loss: 0.9639893174171448, Learning Rate: 0.03\n",
      "Epoch [20/20000], Bound: 1.2397059202194214, Entropy: 150.4095916748047, Temp: 0.8789947032928467, KL: 85.40766906738281, Loss: 0.9467434287071228, Learning Rate: 0.03\n",
      "Epoch [21/20000], Bound: 1.2272284030914307, Entropy: 148.63331604003906, Temp: 0.8733018040657043, KL: 79.04594421386719, Loss: 0.9476146697998047, Learning Rate: 0.03\n",
      "Epoch [22/20000], Bound: 1.2231584787368774, Entropy: 144.21104431152344, Temp: 0.867639422416687, KL: 76.74606323242188, Loss: 0.9459490776062012, Learning Rate: 0.03\n",
      "Epoch [23/20000], Bound: 1.2189126014709473, Entropy: 142.21043395996094, Temp: 0.8620058298110962, KL: 70.33219909667969, Loss: 0.9681451320648193, Learning Rate: 0.03\n",
      "Epoch [24/20000], Bound: 1.2348490953445435, Entropy: 140.33432006835938, Temp: 0.8563746809959412, KL: 81.50630187988281, Loss: 0.9342494606971741, Learning Rate: 0.03\n",
      "Epoch [25/20000], Bound: 1.2429274320602417, Entropy: 140.06581115722656, Temp: 0.8508071303367615, KL: 79.29747009277344, Loss: 0.9602651596069336, Learning Rate: 0.03\n",
      "Epoch [26/20000], Bound: 1.2631289958953857, Entropy: 140.16488647460938, Temp: 0.8452815413475037, KL: 85.74136352539062, Loss: 0.9642410278320312, Learning Rate: 0.03\n",
      "Epoch [27/20000], Bound: 1.2503852844238281, Entropy: 138.98358154296875, Temp: 0.839819610118866, KL: 78.84176635742188, Loss: 0.9688313007354736, Learning Rate: 0.03\n",
      "Epoch [28/20000], Bound: 1.2377123832702637, Entropy: 137.49887084960938, Temp: 0.8343966603279114, KL: 78.3587646484375, Loss: 0.936897873878479, Learning Rate: 0.03\n",
      "Epoch [29/20000], Bound: 1.2469555139541626, Entropy: 138.4630889892578, Temp: 0.8290333151817322, KL: 78.08522033691406, Loss: 0.9539817571640015, Learning Rate: 0.03\n",
      "Epoch [30/20000], Bound: 1.2272770404815674, Entropy: 138.7364044189453, Temp: 0.8237197399139404, KL: 75.3045654296875, Loss: 0.9213729500770569, Learning Rate: 0.03\n",
      "Epoch [31/20000], Bound: 1.2499324083328247, Entropy: 135.63796997070312, Temp: 0.818467378616333, KL: 78.63027954101562, Loss: 0.9461628794670105, Learning Rate: 0.03\n",
      "Epoch [32/20000], Bound: 1.2207541465759277, Entropy: 139.237060546875, Temp: 0.8132745623588562, KL: 69.33731079101562, Loss: 0.9337486028671265, Learning Rate: 0.03\n",
      "Epoch [33/20000], Bound: 1.2251344919204712, Entropy: 141.28790283203125, Temp: 0.8081145882606506, KL: 68.3177490234375, Loss: 0.9445828199386597, Learning Rate: 0.03\n",
      "Epoch [34/20000], Bound: 1.2333028316497803, Entropy: 143.29092407226562, Temp: 0.8029817342758179, KL: 70.23136901855469, Loss: 0.9453164935112, Learning Rate: 0.03\n",
      "Epoch [35/20000], Bound: 1.2483644485473633, Entropy: 146.69541931152344, Temp: 0.7978874444961548, KL: 75.36770629882812, Loss: 0.9407792687416077, Learning Rate: 0.03\n",
      "Epoch [36/20000], Bound: 1.289416790008545, Entropy: 148.61642456054688, Temp: 0.7928585410118103, KL: 86.9974365234375, Loss: 0.9549387693405151, Learning Rate: 0.03\n",
      "Epoch [37/20000], Bound: 1.2879987955093384, Entropy: 153.5685577392578, Temp: 0.7879330515861511, KL: 86.2547607421875, Loss: 0.9499484300613403, Learning Rate: 0.03\n",
      "Epoch [38/20000], Bound: 1.3012974262237549, Entropy: 154.9145050048828, Temp: 0.7831079363822937, KL: 88.99797058105469, Loss: 0.9572457671165466, Learning Rate: 0.03\n",
      "Epoch [39/20000], Bound: 1.2890987396240234, Entropy: 157.6227569580078, Temp: 0.7783868908882141, KL: 85.11700439453125, Loss: 0.9474137425422668, Learning Rate: 0.03\n",
      "Epoch [40/20000], Bound: 1.2830805778503418, Entropy: 156.78866577148438, Temp: 0.7737570405006409, KL: 78.82659912109375, Loss: 0.9684178829193115, Learning Rate: 0.03\n",
      "Epoch [41/20000], Bound: 1.2797129154205322, Entropy: 154.87449645996094, Temp: 0.7691761255264282, KL: 79.66453552246094, Loss: 0.949826180934906, Learning Rate: 0.03\n",
      "Epoch [42/20000], Bound: 1.2829581499099731, Entropy: 154.8102569580078, Temp: 0.7646617889404297, KL: 76.01531982421875, Loss: 0.9753420948982239, Learning Rate: 0.03\n",
      "Epoch [43/20000], Bound: 1.267323613166809, Entropy: 156.8190460205078, Temp: 0.7601823210716248, KL: 74.34434509277344, Loss: 0.9467990398406982, Learning Rate: 0.03\n",
      "Epoch [44/20000], Bound: 1.2710827589035034, Entropy: 153.95494079589844, Temp: 0.7557516694068909, KL: 73.75724792480469, Loss: 0.9536425471305847, Learning Rate: 0.03\n",
      "Epoch [45/20000], Bound: 1.29746413230896, Entropy: 150.5643768310547, Temp: 0.7513651847839355, KL: 79.15127563476562, Loss: 0.9703340530395508, Learning Rate: 0.03\n",
      "Epoch [46/20000], Bound: 1.3006446361541748, Entropy: 153.6542205810547, Temp: 0.7470400333404541, KL: 77.7088623046875, Loss: 0.9814460277557373, Learning Rate: 0.03\n",
      "Epoch [47/20000], Bound: 1.2718780040740967, Entropy: 150.01084899902344, Temp: 0.7427635788917542, KL: 70.20835876464844, Loss: 0.9640302658081055, Learning Rate: 0.03\n",
      "Epoch [48/20000], Bound: 1.2780613899230957, Entropy: 150.17572021484375, Temp: 0.7385129928588867, KL: 68.88241577148438, Loss: 0.9812036156654358, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/20000], Bound: 1.3082199096679688, Entropy: 147.8010711669922, Temp: 0.7342754602432251, KL: 79.37892150878906, Loss: 0.9700837731361389, Learning Rate: 0.03\n",
      "Epoch [50/20000], Bound: 1.2989915609359741, Entropy: 145.3416748046875, Temp: 0.7301163077354431, KL: 77.60505676269531, Loss: 0.9564087986946106, Learning Rate: 0.03\n",
      "Epoch [51/20000], Bound: 1.3006813526153564, Entropy: 143.63072204589844, Temp: 0.7260348200798035, KL: 75.46122741699219, Loss: 0.96945720911026, Learning Rate: 0.03\n",
      "Epoch [52/20000], Bound: 1.3250102996826172, Entropy: 143.65487670898438, Temp: 0.7220110893249512, KL: 80.96902465820312, Loss: 0.9794031977653503, Learning Rate: 0.03\n",
      "Epoch [53/20000], Bound: 1.3299152851104736, Entropy: 136.5740966796875, Temp: 0.7180680632591248, KL: 87.84710693359375, Loss: 0.9367568492889404, Learning Rate: 0.03\n",
      "Epoch [54/20000], Bound: 1.313698410987854, Entropy: 137.14797973632812, Temp: 0.7142691016197205, KL: 81.44725036621094, Loss: 0.9401054382324219, Learning Rate: 0.03\n",
      "Epoch [55/20000], Bound: 1.3214819431304932, Entropy: 137.8284454345703, Temp: 0.7105709314346313, KL: 86.72149658203125, Loss: 0.9146353006362915, Learning Rate: 0.03\n",
      "Epoch [56/20000], Bound: 1.2890926599502563, Entropy: 135.10629272460938, Temp: 0.7070161700248718, KL: 73.71340942382812, Loss: 0.9326452612876892, Learning Rate: 0.03\n",
      "Epoch [57/20000], Bound: 1.303528070449829, Entropy: 133.33106994628906, Temp: 0.7035131454467773, KL: 77.94459533691406, Loss: 0.9281303286552429, Learning Rate: 0.03\n",
      "Epoch [58/20000], Bound: 1.3369455337524414, Entropy: 132.27085876464844, Temp: 0.7000898718833923, KL: 84.01144409179688, Loss: 0.951801598072052, Learning Rate: 0.03\n",
      "Epoch [59/20000], Bound: 1.289402723312378, Entropy: 134.20863342285156, Temp: 0.696763277053833, KL: 69.82218933105469, Loss: 0.9480503797531128, Learning Rate: 0.03\n",
      "Epoch [60/20000], Bound: 1.284180998802185, Entropy: 132.8978729248047, Temp: 0.6934506297111511, KL: 72.10765075683594, Loss: 0.9168511629104614, Learning Rate: 0.03\n",
      "Epoch [61/20000], Bound: 1.3106331825256348, Entropy: 137.63011169433594, Temp: 0.6901912093162537, KL: 75.31886291503906, Loss: 0.9435528516769409, Learning Rate: 0.03\n",
      "Epoch [62/20000], Bound: 1.3151434659957886, Entropy: 140.5033416748047, Temp: 0.6869871020317078, KL: 77.40740966796875, Loss: 0.9333071708679199, Learning Rate: 0.03\n",
      "Epoch [63/20000], Bound: 1.318640112876892, Entropy: 140.6001434326172, Temp: 0.6838587522506714, KL: 75.53511047363281, Loss: 0.9498270750045776, Learning Rate: 0.03\n",
      "Epoch [64/20000], Bound: 1.3012624979019165, Entropy: 142.52584838867188, Temp: 0.6807820200920105, KL: 68.38214111328125, Loss: 0.9623053669929504, Learning Rate: 0.03\n",
      "Epoch [65/20000], Bound: 1.3011794090270996, Entropy: 144.44659423828125, Temp: 0.6777052879333496, KL: 69.69537353515625, Loss: 0.9484624862670898, Learning Rate: 0.03\n",
      "Epoch [66/20000], Bound: 1.3152837753295898, Entropy: 147.81649780273438, Temp: 0.6746518611907959, KL: 72.33836364746094, Loss: 0.9535073637962341, Learning Rate: 0.03\n",
      "Epoch [67/20000], Bound: 1.32432222366333, Entropy: 147.9365997314453, Temp: 0.6716391444206238, KL: 75.45768737792969, Loss: 0.9446712136268616, Learning Rate: 0.03\n",
      "Epoch [68/20000], Bound: 1.3387829065322876, Entropy: 149.30361938476562, Temp: 0.6686949133872986, KL: 79.96551513671875, Loss: 0.936747670173645, Learning Rate: 0.03\n",
      "Epoch [69/20000], Bound: 1.3256665468215942, Entropy: 147.95545959472656, Temp: 0.6658528447151184, KL: 72.98455810546875, Loss: 0.9575628638267517, Learning Rate: 0.03\n",
      "Epoch [70/20000], Bound: 1.2978920936584473, Entropy: 149.7396697998047, Temp: 0.6630496382713318, KL: 68.51559448242188, Loss: 0.9314565658569336, Learning Rate: 0.03\n",
      "Epoch [71/20000], Bound: 1.3217400312423706, Entropy: 146.88421630859375, Temp: 0.6602745652198792, KL: 72.32212829589844, Loss: 0.9465453624725342, Learning Rate: 0.03\n",
      "Epoch [72/20000], Bound: 1.3617358207702637, Entropy: 147.255859375, Temp: 0.65754634141922, KL: 83.626953125, Loss: 0.9396241307258606, Learning Rate: 0.03\n",
      "Epoch [73/20000], Bound: 1.3133389949798584, Entropy: 147.15841674804688, Temp: 0.6549462676048279, KL: 69.08903503417969, Loss: 0.946781575679779, Learning Rate: 0.03\n",
      "Epoch [74/20000], Bound: 1.3363630771636963, Entropy: 145.06483459472656, Temp: 0.652364194393158, KL: 73.34196472167969, Loss: 0.956940233707428, Learning Rate: 0.03\n",
      "Epoch [75/20000], Bound: 1.3989990949630737, Entropy: 140.8451690673828, Temp: 0.6498265862464905, KL: 92.86897277832031, Loss: 0.935919463634491, Learning Rate: 0.03\n",
      "Epoch [76/20000], Bound: 1.3258180618286133, Entropy: 140.7814483642578, Temp: 0.6474806666374207, KL: 71.3267822265625, Loss: 0.9439202547073364, Learning Rate: 0.03\n",
      "Epoch [77/20000], Bound: 1.3375229835510254, Entropy: 140.19674682617188, Temp: 0.645160973072052, KL: 74.93711853027344, Loss: 0.9360722303390503, Learning Rate: 0.03\n",
      "Epoch [78/20000], Bound: 1.3253575563430786, Entropy: 141.55218505859375, Temp: 0.6429010033607483, KL: 71.22314453125, Loss: 0.9371335506439209, Learning Rate: 0.03\n",
      "Epoch [79/20000], Bound: 1.3285866975784302, Entropy: 136.97900390625, Temp: 0.6406724452972412, KL: 72.14079284667969, Loss: 0.9331175684928894, Learning Rate: 0.03\n",
      "Epoch [80/20000], Bound: 1.372877836227417, Entropy: 136.33218383789062, Temp: 0.6384866833686829, KL: 81.12980651855469, Loss: 0.949785590171814, Learning Rate: 0.03\n",
      "Epoch [81/20000], Bound: 1.329006552696228, Entropy: 136.1158905029297, Temp: 0.6363981366157532, KL: 72.82321166992188, Loss: 0.9222196936607361, Learning Rate: 0.03\n",
      "Epoch [82/20000], Bound: 1.3360708951950073, Entropy: 138.32308959960938, Temp: 0.6343607306480408, KL: 76.46733093261719, Loss: 0.904413104057312, Learning Rate: 0.03\n",
      "Epoch [83/20000], Bound: 1.353148341178894, Entropy: 135.63558959960938, Temp: 0.6324142813682556, KL: 76.15579223632812, Loss: 0.938037097454071, Learning Rate: 0.03\n",
      "Epoch [84/20000], Bound: 1.3081786632537842, Entropy: 139.36679077148438, Temp: 0.6305282711982727, KL: 67.42269897460938, Loss: 0.9158765077590942, Learning Rate: 0.03\n",
      "Epoch [85/20000], Bound: 1.346459150314331, Entropy: 142.46839904785156, Temp: 0.6286498308181763, KL: 71.13764953613281, Loss: 0.9583989977836609, Learning Rate: 0.03\n",
      "Epoch [86/20000], Bound: 1.3721524477005005, Entropy: 143.1692352294922, Temp: 0.6267819404602051, KL: 78.20722961425781, Loss: 0.9511289596557617, Learning Rate: 0.03\n",
      "Epoch [87/20000], Bound: 1.3771978616714478, Entropy: 142.6997833251953, Temp: 0.6249873042106628, KL: 80.39219665527344, Loss: 0.940942108631134, Learning Rate: 0.03\n",
      "Epoch [88/20000], Bound: 1.3797920942306519, Entropy: 145.20591735839844, Temp: 0.6232872009277344, KL: 77.63078308105469, Loss: 0.9654330015182495, Learning Rate: 0.03\n",
      "Epoch [89/20000], Bound: 1.3879365921020508, Entropy: 144.3517608642578, Temp: 0.6216384172439575, KL: 81.82662963867188, Loss: 0.9456777572631836, Learning Rate: 0.03\n",
      "Epoch [90/20000], Bound: 1.3283473253250122, Entropy: 144.1991424560547, Temp: 0.6200865507125854, KL: 64.44172668457031, Loss: 0.9637109041213989, Learning Rate: 0.03\n",
      "Epoch [91/20000], Bound: 1.397660732269287, Entropy: 148.11961364746094, Temp: 0.6184748411178589, KL: 79.04780578613281, Loss: 0.9826123714447021, Learning Rate: 0.03\n",
      "Epoch [92/20000], Bound: 1.399481177330017, Entropy: 142.90887451171875, Temp: 0.616917610168457, KL: 81.09666442871094, Loss: 0.9669578075408936, Learning Rate: 0.03\n",
      "Epoch [93/20000], Bound: 1.3446440696716309, Entropy: 144.6264190673828, Temp: 0.6154406070709229, KL: 70.62501525878906, Loss: 0.9383615851402283, Learning Rate: 0.03\n",
      "Epoch [94/20000], Bound: 1.3345946073532104, Entropy: 142.53445434570312, Temp: 0.613976001739502, KL: 69.92008972167969, Loss: 0.9223188757896423, Learning Rate: 0.03\n",
      "Epoch [95/20000], Bound: 1.350806474685669, Entropy: 141.28746032714844, Temp: 0.6125317811965942, KL: 72.08409118652344, Loss: 0.9339260458946228, Learning Rate: 0.03\n",
      "Epoch [96/20000], Bound: 1.3620468378067017, Entropy: 140.99392700195312, Temp: 0.6111194491386414, KL: 73.15864562988281, Loss: 0.9450047016143799, Learning Rate: 0.03\n",
      "Epoch [97/20000], Bound: 1.3772388696670532, Entropy: 139.18983459472656, Temp: 0.6097406148910522, KL: 80.60240173339844, Loss: 0.9120247960090637, Learning Rate: 0.03\n",
      "Epoch [98/20000], Bound: 1.3559329509735107, Entropy: 138.79344177246094, Temp: 0.6084812879562378, KL: 72.52688598632812, Loss: 0.9337713122367859, Learning Rate: 0.03\n",
      "Epoch [99/20000], Bound: 1.353631615638733, Entropy: 137.16677856445312, Temp: 0.6072501540184021, KL: 70.02696228027344, Loss: 0.947834312915802, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/20000], Bound: 1.3390555381774902, Entropy: 139.84413146972656, Temp: 0.6060159802436829, KL: 68.46250915527344, Loss: 0.9305958151817322, Learning Rate: 0.03\n",
      "Epoch [101/20000], Bound: 1.3697788715362549, Entropy: 135.9608154296875, Temp: 0.6047810316085815, KL: 76.59159851074219, Loss: 0.9213116765022278, Learning Rate: 0.03\n",
      "Epoch [102/20000], Bound: 1.3563363552093506, Entropy: 135.58099365234375, Temp: 0.6036259531974792, KL: 76.33126831054688, Loss: 0.8950509428977966, Learning Rate: 0.03\n",
      "Epoch [103/20000], Bound: 1.415830135345459, Entropy: 136.0960235595703, Temp: 0.6025635004043579, KL: 88.129150390625, Loss: 0.9153020977973938, Learning Rate: 0.03\n",
      "Epoch [104/20000], Bound: 1.405161738395691, Entropy: 138.7372589111328, Temp: 0.601672351360321, KL: 79.58015441894531, Loss: 0.9623808860778809, Learning Rate: 0.03\n",
      "Epoch [105/20000], Bound: 1.400496244430542, Entropy: 141.21604919433594, Temp: 0.6008322238922119, KL: 79.78619384765625, Loss: 0.949490487575531, Learning Rate: 0.03\n",
      "Epoch [106/20000], Bound: 1.378117322921753, Entropy: 139.2346954345703, Temp: 0.6000519394874573, KL: 79.13787841796875, Loss: 0.9083069562911987, Learning Rate: 0.03\n",
      "Epoch [107/20000], Bound: 1.402334213256836, Entropy: 140.32122802734375, Temp: 0.5993532538414001, KL: 81.16197204589844, Loss: 0.9389581680297852, Learning Rate: 0.03\n",
      "Epoch [108/20000], Bound: 1.3800523281097412, Entropy: 146.30784606933594, Temp: 0.5987249612808228, KL: 77.56558227539062, Loss: 0.9228655695915222, Learning Rate: 0.03\n",
      "Epoch [109/20000], Bound: 1.4338841438293457, Entropy: 144.21461486816406, Temp: 0.5981429815292358, KL: 85.62928771972656, Loss: 0.9649533629417419, Learning Rate: 0.03\n",
      "Epoch [110/20000], Bound: 1.376570224761963, Entropy: 145.65476989746094, Temp: 0.5976431965827942, KL: 77.36073303222656, Loss: 0.9157231450080872, Learning Rate: 0.03\n",
      "Epoch [111/20000], Bound: 1.3049285411834717, Entropy: 144.453857421875, Temp: 0.5971841812133789, KL: 63.893280029296875, Loss: 0.8917585611343384, Learning Rate: 0.03\n",
      "Epoch [112/20000], Bound: 1.3750468492507935, Entropy: 146.76187133789062, Temp: 0.5966556072235107, KL: 73.96749877929688, Loss: 0.9393784999847412, Learning Rate: 0.03\n",
      "Epoch [113/20000], Bound: 1.4350985288619995, Entropy: 149.86111450195312, Temp: 0.5961247682571411, KL: 86.18434143066406, Loss: 0.9586155414581299, Learning Rate: 0.03\n",
      "Epoch [114/20000], Bound: 1.357034683227539, Entropy: 153.97152709960938, Temp: 0.5956883430480957, KL: 66.90286254882812, Loss: 0.9619078040122986, Learning Rate: 0.03\n",
      "Epoch [115/20000], Bound: 1.3998388051986694, Entropy: 150.94168090820312, Temp: 0.5951598882675171, KL: 81.99557495117188, Loss: 0.9187892079353333, Learning Rate: 0.03\n",
      "Epoch [116/20000], Bound: 1.380662202835083, Entropy: 152.20608520507812, Temp: 0.5947228670120239, KL: 72.18650817871094, Loss: 0.9620419144630432, Learning Rate: 0.03\n",
      "Epoch [117/20000], Bound: 1.4001532793045044, Entropy: 148.96514892578125, Temp: 0.5942462682723999, KL: 80.91191101074219, Loss: 0.9267686009407043, Learning Rate: 0.03\n",
      "Epoch [118/20000], Bound: 1.4096825122833252, Entropy: 150.68370056152344, Temp: 0.5938434600830078, KL: 81.07196044921875, Loss: 0.944018542766571, Learning Rate: 0.03\n",
      "Epoch [119/20000], Bound: 1.4099795818328857, Entropy: 150.86312866210938, Temp: 0.5934967398643494, KL: 85.01493835449219, Loss: 0.9107258915901184, Learning Rate: 0.03\n",
      "Epoch [120/20000], Bound: 1.3762547969818115, Entropy: 147.9116973876953, Temp: 0.5932642221450806, KL: 73.39021301269531, Loss: 0.9406754970550537, Learning Rate: 0.03\n",
      "Epoch [121/20000], Bound: 1.421211838722229, Entropy: 147.56504821777344, Temp: 0.5930037498474121, KL: 87.41648864746094, Loss: 0.9125824570655823, Learning Rate: 0.03\n",
      "Epoch [122/20000], Bound: 1.3620808124542236, Entropy: 145.68606567382812, Temp: 0.5928720235824585, KL: 72.84982299804688, Loss: 0.9169593453407288, Learning Rate: 0.03\n",
      "Epoch [123/20000], Bound: 1.3889312744140625, Entropy: 144.15574645996094, Temp: 0.592716634273529, KL: 78.16471862792969, Loss: 0.9244932532310486, Learning Rate: 0.03\n",
      "Epoch [124/20000], Bound: 1.3498648405075073, Entropy: 141.92709350585938, Temp: 0.5925859212875366, KL: 71.78359985351562, Loss: 0.9020596146583557, Learning Rate: 0.03\n",
      "Epoch [125/20000], Bound: 1.3706958293914795, Entropy: 143.11212158203125, Temp: 0.5924335718154907, KL: 74.8607177734375, Loss: 0.915943443775177, Learning Rate: 0.03\n",
      "Epoch [126/20000], Bound: 1.3678677082061768, Entropy: 143.96946716308594, Temp: 0.5922815203666687, KL: 72.48271179199219, Loss: 0.93025141954422, Learning Rate: 0.03\n",
      "Epoch [127/20000], Bound: 1.4136152267456055, Entropy: 144.77610778808594, Temp: 0.5920949578285217, KL: 84.58280944824219, Loss: 0.9189683198928833, Learning Rate: 0.03\n",
      "Epoch [128/20000], Bound: 1.453076958656311, Entropy: 142.5888214111328, Temp: 0.5920042395591736, KL: 91.38148498535156, Loss: 0.9441967010498047, Learning Rate: 0.03\n",
      "Epoch [129/20000], Bound: 1.4120683670043945, Entropy: 143.0466766357422, Temp: 0.5920427441596985, KL: 84.78585815429688, Loss: 0.9139814376831055, Learning Rate: 0.03\n",
      "Epoch [130/20000], Bound: 1.397751808166504, Entropy: 140.71592712402344, Temp: 0.5921621322631836, KL: 79.73234558105469, Loss: 0.9278651475906372, Learning Rate: 0.03\n",
      "Epoch [131/20000], Bound: 1.381410837173462, Entropy: 140.43663024902344, Temp: 0.5922942757606506, KL: 76.98005676269531, Loss: 0.918810248374939, Learning Rate: 0.03\n",
      "Epoch [132/20000], Bound: 1.4463974237442017, Entropy: 135.63284301757812, Temp: 0.5924182534217834, KL: 95.48603820800781, Loss: 0.8961765170097351, Learning Rate: 0.03\n",
      "Epoch [133/20000], Bound: 1.3818479776382446, Entropy: 131.80589294433594, Temp: 0.5927290916442871, KL: 80.08033752441406, Loss: 0.8943116664886475, Learning Rate: 0.03\n",
      "Epoch [134/20000], Bound: 1.3936151266098022, Entropy: 132.44061279296875, Temp: 0.5930641293525696, KL: 81.438720703125, Loss: 0.9068942070007324, Learning Rate: 0.03\n",
      "Epoch [135/20000], Bound: 1.4052560329437256, Entropy: 130.27325439453125, Temp: 0.5934227705001831, KL: 85.59713745117188, Loss: 0.8960424661636353, Learning Rate: 0.03\n",
      "Epoch [136/20000], Bound: 1.3601417541503906, Entropy: 124.59441375732422, Temp: 0.5938514471054077, KL: 75.48859405517578, Loss: 0.8926642537117004, Learning Rate: 0.03\n",
      "Epoch [137/20000], Bound: 1.369374394416809, Entropy: 125.68632507324219, Temp: 0.5942448973655701, KL: 79.91862487792969, Loss: 0.8739855289459229, Learning Rate: 0.03\n",
      "Epoch [138/20000], Bound: 1.384655475616455, Entropy: 125.58880615234375, Temp: 0.5946658253669739, KL: 80.31967163085938, Loss: 0.9014748334884644, Learning Rate: 0.03\n",
      "Epoch [139/20000], Bound: 1.3361552953720093, Entropy: 129.57247924804688, Temp: 0.5950906872749329, KL: 68.3685302734375, Loss: 0.9089066982269287, Learning Rate: 0.03\n",
      "Epoch [140/20000], Bound: 1.3698452711105347, Entropy: 131.2411346435547, Temp: 0.5953877568244934, KL: 76.20062255859375, Loss: 0.9082072377204895, Learning Rate: 0.03\n",
      "Epoch [141/20000], Bound: 1.3888412714004517, Entropy: 132.36659240722656, Temp: 0.5956505537033081, KL: 79.76004028320312, Loss: 0.9163652062416077, Learning Rate: 0.03\n",
      "Epoch [142/20000], Bound: 1.3638278245925903, Entropy: 135.40081787109375, Temp: 0.595910906791687, KL: 76.34901428222656, Loss: 0.896152913570404, Learning Rate: 0.03\n",
      "Epoch [143/20000], Bound: 1.3519741296768188, Entropy: 139.18663024902344, Temp: 0.5961505174636841, KL: 74.80680847167969, Loss: 0.8866226077079773, Learning Rate: 0.03\n",
      "Epoch [144/20000], Bound: 1.4034664630889893, Entropy: 144.18800354003906, Temp: 0.5963630080223083, KL: 83.00337219238281, Loss: 0.920051097869873, Learning Rate: 0.03\n",
      "Epoch [145/20000], Bound: 1.3527966737747192, Entropy: 143.72039794921875, Temp: 0.5966050028800964, KL: 73.70330810546875, Loss: 0.8982192277908325, Learning Rate: 0.03\n",
      "Epoch [146/20000], Bound: 1.4211841821670532, Entropy: 144.5845489501953, Temp: 0.5967960357666016, KL: 83.76170349121094, Loss: 0.9510785937309265, Learning Rate: 0.03\n",
      "Epoch [147/20000], Bound: 1.3997665643692017, Entropy: 148.1396942138672, Temp: 0.5969977974891663, KL: 81.41569519042969, Loss: 0.9270632863044739, Learning Rate: 0.03\n",
      "Epoch [148/20000], Bound: 1.3094193935394287, Entropy: 147.75726318359375, Temp: 0.5972058773040771, KL: 66.9698486328125, Loss: 0.8742338418960571, Learning Rate: 0.03\n",
      "Epoch [149/20000], Bound: 1.3691656589508057, Entropy: 147.1513214111328, Temp: 0.5973114371299744, KL: 77.89193725585938, Loss: 0.8961100578308105, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/20000], Bound: 1.4169172048568726, Entropy: 148.36965942382812, Temp: 0.5974226593971252, KL: 86.49797058105469, Loss: 0.920572817325592, Learning Rate: 0.03\n",
      "Epoch [151/20000], Bound: 1.3495595455169678, Entropy: 148.29037475585938, Temp: 0.597605288028717, KL: 70.89093017578125, Loss: 0.9172127842903137, Learning Rate: 0.03\n",
      "Epoch [152/20000], Bound: 1.3874530792236328, Entropy: 146.01365661621094, Temp: 0.597690761089325, KL: 84.34677124023438, Loss: 0.8790278434753418, Learning Rate: 0.03\n",
      "Epoch [153/20000], Bound: 1.3813896179199219, Entropy: 145.32472229003906, Temp: 0.5978657007217407, KL: 85.83377075195312, Loss: 0.8548222780227661, Learning Rate: 0.03\n",
      "Epoch [154/20000], Bound: 1.3807357549667358, Entropy: 147.70787048339844, Temp: 0.5981590747833252, KL: 82.41865539550781, Loss: 0.882634162902832, Learning Rate: 0.03\n",
      "Epoch [155/20000], Bound: 1.4091973304748535, Entropy: 146.5028839111328, Temp: 0.5984973907470703, KL: 87.42919921875, Loss: 0.8990264534950256, Learning Rate: 0.03\n",
      "Epoch [156/20000], Bound: 1.3930082321166992, Entropy: 142.56039428710938, Temp: 0.5989119410514832, KL: 82.96151733398438, Loss: 0.9041317701339722, Learning Rate: 0.03\n",
      "Epoch [157/20000], Bound: 1.3481471538543701, Entropy: 144.67138671875, Temp: 0.599342942237854, KL: 71.533447265625, Loss: 0.9119652509689331, Learning Rate: 0.03\n",
      "Epoch [158/20000], Bound: 1.4147413969039917, Entropy: 143.19712829589844, Temp: 0.5996565818786621, KL: 86.418212890625, Loss: 0.9212986826896667, Learning Rate: 0.03\n",
      "Epoch [159/20000], Bound: 1.3878321647644043, Entropy: 140.8730010986328, Temp: 0.6000139713287354, KL: 77.70697021484375, Loss: 0.9395907521247864, Learning Rate: 0.03\n",
      "Epoch [160/20000], Bound: 1.3697673082351685, Entropy: 141.87408447265625, Temp: 0.6003004908561707, KL: 75.12294006347656, Loss: 0.9256725311279297, Learning Rate: 0.03\n",
      "Epoch [161/20000], Bound: 1.3733609914779663, Entropy: 139.90611267089844, Temp: 0.6005064249038696, KL: 74.63038635253906, Loss: 0.937226414680481, Learning Rate: 0.03\n",
      "Epoch [162/20000], Bound: 1.4519981145858765, Entropy: 142.81964111328125, Temp: 0.6006223559379578, KL: 100.25083923339844, Loss: 0.8875234127044678, Learning Rate: 0.03\n",
      "Epoch [163/20000], Bound: 1.3644583225250244, Entropy: 142.33058166503906, Temp: 0.6009708046913147, KL: 77.00088500976562, Loss: 0.9007781147956848, Learning Rate: 0.03\n",
      "Epoch [164/20000], Bound: 1.424359917640686, Entropy: 141.13548278808594, Temp: 0.6012735962867737, KL: 90.50865173339844, Loss: 0.9107454419136047, Learning Rate: 0.03\n",
      "Epoch [165/20000], Bound: 1.3865416049957275, Entropy: 140.39828491210938, Temp: 0.6016689538955688, KL: 77.95065307617188, Loss: 0.9379895329475403, Learning Rate: 0.03\n",
      "Epoch [166/20000], Bound: 1.3368390798568726, Entropy: 139.1612548828125, Temp: 0.6019866466522217, KL: 66.8248291015625, Loss: 0.9337409138679504, Learning Rate: 0.03\n",
      "Epoch [167/20000], Bound: 1.4281508922576904, Entropy: 137.14369201660156, Temp: 0.6021104454994202, KL: 85.33387756347656, Loss: 0.963524580001831, Learning Rate: 0.03\n",
      "Epoch [168/20000], Bound: 1.3448349237442017, Entropy: 137.69049072265625, Temp: 0.602236807346344, KL: 68.998291015625, Loss: 0.9313467741012573, Learning Rate: 0.03\n",
      "Epoch [169/20000], Bound: 1.347089171409607, Entropy: 141.29010009765625, Temp: 0.6022135615348816, KL: 68.85115051269531, Loss: 0.9368619918823242, Learning Rate: 0.03\n",
      "Epoch [170/20000], Bound: 1.3501477241516113, Entropy: 139.7330780029297, Temp: 0.6020480990409851, KL: 68.570068359375, Loss: 0.9448269605636597, Learning Rate: 0.03\n",
      "Epoch [171/20000], Bound: 1.3746920824050903, Entropy: 141.92181396484375, Temp: 0.6017439365386963, KL: 77.16024780273438, Loss: 0.9209984540939331, Learning Rate: 0.03\n",
      "Epoch [172/20000], Bound: 1.3476576805114746, Entropy: 141.42185974121094, Temp: 0.6014364361763, KL: 71.1954345703125, Loss: 0.9172367453575134, Learning Rate: 0.03\n",
      "Epoch [173/20000], Bound: 1.3261359930038452, Entropy: 141.87513732910156, Temp: 0.6010623574256897, KL: 70.95451354980469, Loss: 0.8777878284454346, Learning Rate: 0.03\n",
      "Epoch [174/20000], Bound: 1.3688346147537231, Entropy: 142.64276123046875, Temp: 0.6006647348403931, KL: 76.1876220703125, Loss: 0.9156053066253662, Learning Rate: 0.03\n",
      "Epoch [175/20000], Bound: 1.343995213508606, Entropy: 142.49383544921875, Temp: 0.6002711653709412, KL: 71.05479431152344, Loss: 0.9095081090927124, Learning Rate: 0.03\n",
      "Epoch [176/20000], Bound: 1.350846529006958, Entropy: 141.12588500976562, Temp: 0.5998291373252869, KL: 76.44758605957031, Loss: 0.8769824504852295, Learning Rate: 0.03\n",
      "Epoch [177/20000], Bound: 1.3432780504226685, Entropy: 141.22833251953125, Temp: 0.5994395613670349, KL: 71.49589538574219, Loss: 0.9031226634979248, Learning Rate: 0.03\n",
      "Epoch [178/20000], Bound: 1.3663039207458496, Entropy: 141.89588928222656, Temp: 0.5990151762962341, KL: 79.43489074707031, Loss: 0.8806513547897339, Learning Rate: 0.03\n",
      "Epoch [179/20000], Bound: 1.333152174949646, Entropy: 140.47474670410156, Temp: 0.5986757278442383, KL: 72.30487060546875, Loss: 0.8759504556655884, Learning Rate: 0.03\n",
      "Epoch [180/20000], Bound: 1.3308526277542114, Entropy: 139.8310089111328, Temp: 0.5983356833457947, KL: 71.04421997070312, Loss: 0.8816168904304504, Learning Rate: 0.03\n",
      "Epoch [181/20000], Bound: 1.3498256206512451, Entropy: 138.2823944091797, Temp: 0.597975492477417, KL: 79.43136596679688, Loss: 0.8469141721725464, Learning Rate: 0.03\n",
      "Epoch [182/20000], Bound: 1.3293086290359497, Entropy: 141.83230590820312, Temp: 0.5977327227592468, KL: 71.53564453125, Loss: 0.8736600875854492, Learning Rate: 0.03\n",
      "Epoch [183/20000], Bound: 1.3201417922973633, Entropy: 138.66683959960938, Temp: 0.5974763631820679, KL: 72.81881713867188, Loss: 0.8454394340515137, Learning Rate: 0.03\n",
      "Epoch [184/20000], Bound: 1.372490644454956, Entropy: 139.80441284179688, Temp: 0.597252607345581, KL: 75.62960815429688, Loss: 0.9214707612991333, Learning Rate: 0.03\n",
      "Epoch [185/20000], Bound: 1.3878636360168457, Entropy: 139.39193725585938, Temp: 0.5970165729522705, KL: 86.42886352539062, Loss: 0.8611118197441101, Learning Rate: 0.03\n",
      "Epoch [186/20000], Bound: 1.326622724533081, Entropy: 139.79759216308594, Temp: 0.596956729888916, KL: 69.63896179199219, Loss: 0.8832994699478149, Learning Rate: 0.03\n",
      "Epoch [187/20000], Bound: 1.3282973766326904, Entropy: 139.4906005859375, Temp: 0.596835196018219, KL: 68.22923278808594, Loss: 0.8980473279953003, Learning Rate: 0.03\n",
      "Epoch [188/20000], Bound: 1.3207288980484009, Entropy: 139.53807067871094, Temp: 0.5966260433197021, KL: 68.81489562988281, Loss: 0.8787384033203125, Learning Rate: 0.03\n",
      "Epoch [189/20000], Bound: 1.348307490348816, Entropy: 140.8291778564453, Temp: 0.5963649749755859, KL: 76.06465148925781, Loss: 0.8694221377372742, Learning Rate: 0.03\n",
      "Epoch [190/20000], Bound: 1.3909709453582764, Entropy: 137.4288787841797, Temp: 0.5961565971374512, KL: 80.88816833496094, Loss: 0.9121197462081909, Learning Rate: 0.03\n",
      "Epoch [191/20000], Bound: 1.304950475692749, Entropy: 138.0365447998047, Temp: 0.5960104465484619, KL: 66.385009765625, Loss: 0.8692297339439392, Learning Rate: 0.03\n",
      "Epoch [192/20000], Bound: 1.3304436206817627, Entropy: 138.33116149902344, Temp: 0.5957870483398438, KL: 77.2169189453125, Loss: 0.8250190615653992, Learning Rate: 0.03\n",
      "Epoch [193/20000], Bound: 1.291016697883606, Entropy: 138.4116973876953, Temp: 0.5956755876541138, KL: 74.27218627929688, Loss: 0.7773901224136353, Learning Rate: 0.03\n",
      "Epoch [194/20000], Bound: 1.3528318405151367, Entropy: 138.2777099609375, Temp: 0.5956804156303406, KL: 76.14326477050781, Loss: 0.8762525916099548, Learning Rate: 0.03\n",
      "Epoch [195/20000], Bound: 1.367964267730713, Entropy: 141.80319213867188, Temp: 0.5957090258598328, KL: 73.57452392578125, Loss: 0.9271409511566162, Learning Rate: 0.03\n",
      "Epoch [196/20000], Bound: 1.3710964918136597, Entropy: 138.44720458984375, Temp: 0.5956752896308899, KL: 77.51031494140625, Loss: 0.9001713395118713, Learning Rate: 0.03\n",
      "Epoch [197/20000], Bound: 1.428670048713684, Entropy: 137.34341430664062, Temp: 0.5956612825393677, KL: 89.24974060058594, Loss: 0.9183467030525208, Learning Rate: 0.03\n",
      "Epoch [198/20000], Bound: 1.4081677198410034, Entropy: 140.93734741210938, Temp: 0.5957853198051453, KL: 82.95106506347656, Loss: 0.9289543032646179, Learning Rate: 0.03\n",
      "Epoch [199/20000], Bound: 1.3721576929092407, Entropy: 141.30711364746094, Temp: 0.5959489941596985, KL: 71.27619934082031, Loss: 0.9550449252128601, Learning Rate: 0.03\n",
      "Epoch [200/20000], Bound: 1.3527569770812988, Entropy: 141.22732543945312, Temp: 0.5959787368774414, KL: 70.42613220214844, Loss: 0.9245851039886475, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [201/20000], Bound: 1.3876832723617554, Entropy: 141.23587036132812, Temp: 0.5959070324897766, KL: 81.51576232910156, Loss: 0.8997986912727356, Learning Rate: 0.03\n",
      "Epoch [202/20000], Bound: 1.3848716020584106, Entropy: 141.781982421875, Temp: 0.5959072113037109, KL: 80.50593566894531, Loss: 0.9026625156402588, Learning Rate: 0.03\n",
      "Epoch [203/20000], Bound: 1.3522672653198242, Entropy: 142.17710876464844, Temp: 0.5959571003913879, KL: 70.26841735839844, Loss: 0.9249340891838074, Learning Rate: 0.03\n",
      "Epoch [204/20000], Bound: 1.3732351064682007, Entropy: 141.877197265625, Temp: 0.5959005355834961, KL: 75.92724609375, Loss: 0.9180524945259094, Learning Rate: 0.03\n",
      "Epoch [205/20000], Bound: 1.368871808052063, Entropy: 142.42347717285156, Temp: 0.5958266854286194, KL: 74.63493347167969, Loss: 0.9202173948287964, Learning Rate: 0.03\n",
      "Epoch [206/20000], Bound: 1.2994495630264282, Entropy: 143.0340576171875, Temp: 0.5957192182540894, KL: 66.21757507324219, Loss: 0.8602373600006104, Learning Rate: 0.03\n",
      "Epoch [207/20000], Bound: 1.3399797677993774, Entropy: 140.56224060058594, Temp: 0.5955353379249573, KL: 66.85678100585938, Loss: 0.9295057654380798, Learning Rate: 0.03\n",
      "Epoch [208/20000], Bound: 1.345668077468872, Entropy: 137.87339782714844, Temp: 0.5952195525169373, KL: 73.63137817382812, Loss: 0.8828874230384827, Learning Rate: 0.03\n",
      "Epoch [209/20000], Bound: 1.3790345191955566, Entropy: 139.656494140625, Temp: 0.5949225425720215, KL: 80.27488708496094, Loss: 0.8911939263343811, Learning Rate: 0.03\n",
      "Epoch [210/20000], Bound: 1.3407158851623535, Entropy: 141.2884979248047, Temp: 0.5947190523147583, KL: 71.29597473144531, Loss: 0.8923078179359436, Learning Rate: 0.03\n",
      "Epoch [211/20000], Bound: 1.3745530843734741, Entropy: 140.49929809570312, Temp: 0.5944852828979492, KL: 81.32402038574219, Loss: 0.8727384805679321, Learning Rate: 0.03\n",
      "Epoch [212/20000], Bound: 1.4057039022445679, Entropy: 138.4974822998047, Temp: 0.5943741202354431, KL: 83.88499450683594, Loss: 0.9132729768753052, Learning Rate: 0.03\n",
      "Epoch [213/20000], Bound: 1.349183440208435, Entropy: 139.55030822753906, Temp: 0.5943623781204224, KL: 75.09512329101562, Loss: 0.8758370280265808, Learning Rate: 0.03\n",
      "Epoch [214/20000], Bound: 1.3776395320892334, Entropy: 136.89901733398438, Temp: 0.5943697690963745, KL: 83.0872802734375, Loss: 0.8637608885765076, Learning Rate: 0.03\n",
      "Epoch [215/20000], Bound: 1.3188495635986328, Entropy: 139.11947631835938, Temp: 0.5945096015930176, KL: 73.57752990722656, Loss: 0.8319807052612305, Learning Rate: 0.03\n",
      "Epoch [216/20000], Bound: 1.3788347244262695, Entropy: 137.37139892578125, Temp: 0.5946816205978394, KL: 83.41845703125, Loss: 0.863922655582428, Learning Rate: 0.03\n",
      "Epoch [217/20000], Bound: 1.3254610300064087, Entropy: 135.7274627685547, Temp: 0.5949730277061462, KL: 73.73579406738281, Loss: 0.8436286449432373, Learning Rate: 0.03\n",
      "Epoch [218/20000], Bound: 1.3452870845794678, Entropy: 133.73977661132812, Temp: 0.5952694416046143, KL: 74.76148986816406, Loss: 0.8727535605430603, Learning Rate: 0.03\n",
      "Epoch [219/20000], Bound: 1.3378759622573853, Entropy: 135.80892944335938, Temp: 0.5955501794815063, KL: 79.61701965332031, Loss: 0.8184277415275574, Learning Rate: 0.03\n",
      "Epoch [220/20000], Bound: 1.3257818222045898, Entropy: 138.28428649902344, Temp: 0.5959398150444031, KL: 75.01734924316406, Loss: 0.8350297212600708, Learning Rate: 0.03\n",
      "Epoch [221/20000], Bound: 1.3834857940673828, Entropy: 135.17420959472656, Temp: 0.5963475108146667, KL: 85.13493347167969, Loss: 0.8619152903556824, Learning Rate: 0.03\n",
      "Epoch [222/20000], Bound: 1.3752259016036987, Entropy: 141.5519256591797, Temp: 0.5968698263168335, KL: 82.19137573242188, Loss: 0.871213972568512, Learning Rate: 0.03\n",
      "Epoch [223/20000], Bound: 1.372581958770752, Entropy: 138.89529418945312, Temp: 0.5974457263946533, KL: 79.15489196777344, Loss: 0.8924878239631653, Learning Rate: 0.03\n",
      "Epoch [224/20000], Bound: 1.3714303970336914, Entropy: 137.11305236816406, Temp: 0.5980050563812256, KL: 80.29747009277344, Loss: 0.8816801309585571, Learning Rate: 0.03\n",
      "Epoch [225/20000], Bound: 1.3924369812011719, Entropy: 138.98057556152344, Temp: 0.5985736846923828, KL: 84.9825439453125, Loss: 0.8854464292526245, Learning Rate: 0.03\n",
      "Epoch [226/20000], Bound: 1.358945369720459, Entropy: 141.55661010742188, Temp: 0.5992038249969482, KL: 74.43348693847656, Loss: 0.9083741307258606, Learning Rate: 0.03\n",
      "Epoch [227/20000], Bound: 1.3013315200805664, Entropy: 140.15199279785156, Temp: 0.5997244119644165, KL: 67.65544128417969, Loss: 0.8574211597442627, Learning Rate: 0.03\n",
      "Epoch [228/20000], Bound: 1.3865901231765747, Entropy: 137.64620971679688, Temp: 0.6001084446907043, KL: 80.56318664550781, Loss: 0.9134690165519714, Learning Rate: 0.03\n",
      "Epoch [229/20000], Bound: 1.4323704242706299, Entropy: 138.58966064453125, Temp: 0.6004772186279297, KL: 90.88838195800781, Loss: 0.9228442907333374, Learning Rate: 0.03\n",
      "Epoch [230/20000], Bound: 1.3997281789779663, Entropy: 136.22976684570312, Temp: 0.600949764251709, KL: 87.53768920898438, Loss: 0.8836491107940674, Learning Rate: 0.03\n",
      "Epoch [231/20000], Bound: 1.3479186296463013, Entropy: 135.89279174804688, Temp: 0.6015176773071289, KL: 72.0673828125, Loss: 0.9106219410896301, Learning Rate: 0.03\n",
      "Epoch [232/20000], Bound: 1.2774794101715088, Entropy: 136.11557006835938, Temp: 0.6019362807273865, KL: 63.17546081542969, Loss: 0.8548524379730225, Learning Rate: 0.03\n",
      "Epoch [233/20000], Bound: 1.3634490966796875, Entropy: 134.12521362304688, Temp: 0.6021569967269897, KL: 74.38485717773438, Loss: 0.9225897192955017, Learning Rate: 0.03\n",
      "Epoch [234/20000], Bound: 1.3253304958343506, Entropy: 134.37596130371094, Temp: 0.6022765636444092, KL: 73.9554443359375, Loss: 0.8532509207725525, Learning Rate: 0.03\n",
      "Epoch [235/20000], Bound: 1.346810221672058, Entropy: 130.6333465576172, Temp: 0.6023778915405273, KL: 75.01004028320312, Loss: 0.8854634165763855, Learning Rate: 0.03\n",
      "Epoch [236/20000], Bound: 1.3684425354003906, Entropy: 134.8184356689453, Temp: 0.602439284324646, KL: 80.61805725097656, Loss: 0.8811594843864441, Learning Rate: 0.03\n",
      "Epoch [237/20000], Bound: 1.3348559141159058, Entropy: 137.6797637939453, Temp: 0.6025441288948059, KL: 72.84822082519531, Loss: 0.8808408379554749, Learning Rate: 0.03\n",
      "Epoch [238/20000], Bound: 1.351430058479309, Entropy: 137.18174743652344, Temp: 0.6025835275650024, KL: 72.24468994140625, Loss: 0.9176632761955261, Learning Rate: 0.03\n",
      "Epoch [239/20000], Bound: 1.3694918155670166, Entropy: 140.42576599121094, Temp: 0.6025128364562988, KL: 80.18807983398438, Loss: 0.8869320750236511, Learning Rate: 0.03\n",
      "Epoch [240/20000], Bound: 1.3705273866653442, Entropy: 141.98516845703125, Temp: 0.6024860143661499, KL: 76.68751525878906, Loss: 0.9179819226264954, Learning Rate: 0.03\n",
      "Epoch [241/20000], Bound: 1.3458136320114136, Entropy: 141.2731475830078, Temp: 0.6024158000946045, KL: 74.59246826171875, Loss: 0.8870767951011658, Learning Rate: 0.03\n",
      "Epoch [242/20000], Bound: 1.3580901622772217, Entropy: 140.08053588867188, Temp: 0.6023142337799072, KL: 75.23680114746094, Loss: 0.905310332775116, Learning Rate: 0.03\n",
      "Epoch [243/20000], Bound: 1.3662761449813843, Entropy: 142.13536071777344, Temp: 0.6021720767021179, KL: 75.96270751953125, Loss: 0.9150652885437012, Learning Rate: 0.03\n",
      "Epoch [244/20000], Bound: 1.3499760627746582, Entropy: 140.4794464111328, Temp: 0.601992666721344, KL: 73.57496643066406, Loss: 0.9028381109237671, Learning Rate: 0.03\n",
      "Epoch [245/20000], Bound: 1.3521305322647095, Entropy: 139.52813720703125, Temp: 0.6017616987228394, KL: 70.16438293457031, Loss: 0.9349547624588013, Learning Rate: 0.03\n",
      "Epoch [246/20000], Bound: 1.358482837677002, Entropy: 142.4891815185547, Temp: 0.6014004945755005, KL: 76.94757080078125, Loss: 0.8902974724769592, Learning Rate: 0.03\n",
      "Epoch [247/20000], Bound: 1.418157935142517, Entropy: 140.85635375976562, Temp: 0.601069450378418, KL: 84.60032653808594, Loss: 0.946435272693634, Learning Rate: 0.03\n",
      "Epoch [248/20000], Bound: 1.3261750936508179, Entropy: 143.73123168945312, Temp: 0.6008043885231018, KL: 72.62049865722656, Loss: 0.8635945916175842, Learning Rate: 0.03\n",
      "Epoch [249/20000], Bound: 1.3180752992630005, Entropy: 141.72824096679688, Temp: 0.600534200668335, KL: 71.89979553222656, Loss: 0.8540612459182739, Learning Rate: 0.03\n",
      "Epoch [250/20000], Bound: 1.3567874431610107, Entropy: 138.5124969482422, Temp: 0.6002619862556458, KL: 75.14344787597656, Loss: 0.900066077709198, Learning Rate: 0.03\n",
      "Epoch [251/20000], Bound: 1.3031888008117676, Entropy: 136.9108123779297, Temp: 0.5999799370765686, KL: 68.46066284179688, Loss: 0.8544682860374451, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [252/20000], Bound: 1.359553575515747, Entropy: 138.28836059570312, Temp: 0.5996493697166443, KL: 78.915771484375, Loss: 0.8729388117790222, Learning Rate: 0.03\n",
      "Epoch [253/20000], Bound: 1.3957829475402832, Entropy: 139.97743225097656, Temp: 0.5994026064872742, KL: 83.72171020507812, Loss: 0.9043546319007874, Learning Rate: 0.03\n",
      "Epoch [254/20000], Bound: 1.3338594436645508, Entropy: 134.31411743164062, Temp: 0.599260687828064, KL: 70.54010009765625, Loss: 0.8929440975189209, Learning Rate: 0.03\n",
      "Epoch [255/20000], Bound: 1.4130525588989258, Entropy: 136.6324920654297, Temp: 0.5990435481071472, KL: 81.6705322265625, Loss: 0.9561716318130493, Learning Rate: 0.03\n",
      "Epoch [256/20000], Bound: 1.3207509517669678, Entropy: 135.74972534179688, Temp: 0.5988409519195557, KL: 67.760009765625, Loss: 0.8909599781036377, Learning Rate: 0.03\n",
      "Epoch [257/20000], Bound: 1.360120415687561, Entropy: 137.49044799804688, Temp: 0.598532497882843, KL: 74.36138916015625, Loss: 0.9101163744926453, Learning Rate: 0.03\n",
      "Epoch [258/20000], Bound: 1.3184924125671387, Entropy: 136.9474334716797, Temp: 0.5982027649879456, KL: 65.98629760742188, Loss: 0.9006364941596985, Learning Rate: 0.03\n",
      "Epoch [259/20000], Bound: 1.319676160812378, Entropy: 141.89663696289062, Temp: 0.5977444052696228, KL: 67.62281799316406, Loss: 0.8884626030921936, Learning Rate: 0.03\n",
      "Epoch [260/20000], Bound: 1.429086446762085, Entropy: 143.84457397460938, Temp: 0.5972108840942383, KL: 87.29571533203125, Loss: 0.9389002919197083, Learning Rate: 0.03\n",
      "Epoch [261/20000], Bound: 1.3630579710006714, Entropy: 145.6284942626953, Temp: 0.5968296527862549, KL: 74.12095642089844, Loss: 0.9149268865585327, Learning Rate: 0.03\n",
      "Epoch [262/20000], Bound: 1.4132730960845947, Entropy: 143.40679931640625, Temp: 0.596433162689209, KL: 83.89772033691406, Loss: 0.9328001141548157, Learning Rate: 0.03\n",
      "Epoch [263/20000], Bound: 1.360457420349121, Entropy: 142.3134002685547, Temp: 0.5961410403251648, KL: 74.73258972167969, Loss: 0.9035743474960327, Learning Rate: 0.03\n",
      "Epoch [264/20000], Bound: 1.3182339668273926, Entropy: 138.43785095214844, Temp: 0.5958504676818848, KL: 64.60072326660156, Loss: 0.9083065390586853, Learning Rate: 0.03\n",
      "Epoch [265/20000], Bound: 1.329905390739441, Entropy: 141.6048583984375, Temp: 0.5954069495201111, KL: 66.84896850585938, Loss: 0.9104437232017517, Learning Rate: 0.03\n",
      "Epoch [266/20000], Bound: 1.3745794296264648, Entropy: 145.28988647460938, Temp: 0.5948587656021118, KL: 80.16581726074219, Loss: 0.88321852684021, Learning Rate: 0.03\n",
      "Epoch [267/20000], Bound: 1.3307075500488281, Entropy: 140.84133911132812, Temp: 0.5944464802742004, KL: 65.69169616699219, Loss: 0.9202104210853577, Learning Rate: 0.03\n",
      "Epoch [268/20000], Bound: 1.4094027280807495, Entropy: 142.05430603027344, Temp: 0.5939017534255981, KL: 85.05599975585938, Loss: 0.9100193381309509, Learning Rate: 0.03\n",
      "Epoch [269/20000], Bound: 1.3300355672836304, Entropy: 145.3832550048828, Temp: 0.5935337543487549, KL: 65.38748168945312, Loss: 0.9201384782791138, Learning Rate: 0.03\n",
      "Epoch [270/20000], Bound: 1.3801512718200684, Entropy: 143.95018005371094, Temp: 0.5930280685424805, KL: 81.97193908691406, Loss: 0.8755717873573303, Learning Rate: 0.03\n",
      "Epoch [271/20000], Bound: 1.3560975790023804, Entropy: 141.8680877685547, Temp: 0.5926992893218994, KL: 74.22454833984375, Loss: 0.8935601115226746, Learning Rate: 0.03\n",
      "Epoch [272/20000], Bound: 1.3089171648025513, Entropy: 143.7992706298828, Temp: 0.5923973321914673, KL: 64.38468933105469, Loss: 0.888036847114563, Learning Rate: 0.03\n",
      "Epoch [273/20000], Bound: 1.3149586915969849, Entropy: 144.69998168945312, Temp: 0.5919783711433411, KL: 67.40153503417969, Loss: 0.872966468334198, Learning Rate: 0.03\n",
      "Epoch [274/20000], Bound: 1.3389606475830078, Entropy: 143.23837280273438, Temp: 0.5915206670761108, KL: 67.15591430664062, Loss: 0.9188186526298523, Learning Rate: 0.03\n",
      "Epoch [275/20000], Bound: 1.353219985961914, Entropy: 145.21188354492188, Temp: 0.5909713506698608, KL: 76.41348266601562, Loss: 0.8665952086448669, Learning Rate: 0.03\n",
      "Epoch [276/20000], Bound: 1.3672585487365723, Entropy: 145.5311279296875, Temp: 0.5905449986457825, KL: 75.69277954101562, Loss: 0.8989092111587524, Learning Rate: 0.03\n",
      "Epoch [277/20000], Bound: 1.3748501539230347, Entropy: 144.67724609375, Temp: 0.5901813507080078, KL: 76.58810424804688, Loss: 0.9054296612739563, Learning Rate: 0.03\n",
      "Epoch [278/20000], Bound: 1.4321633577346802, Entropy: 141.03717041015625, Temp: 0.5898813605308533, KL: 90.27423095703125, Loss: 0.9045080542564392, Learning Rate: 0.03\n",
      "Epoch [279/20000], Bound: 1.3256988525390625, Entropy: 142.27899169921875, Temp: 0.5898377895355225, KL: 64.21929931640625, Loss: 0.916423499584198, Learning Rate: 0.03\n",
      "Epoch [280/20000], Bound: 1.3702746629714966, Entropy: 138.30520629882812, Temp: 0.5896256566047668, KL: 79.44447326660156, Loss: 0.8713104128837585, Learning Rate: 0.03\n",
      "Epoch [281/20000], Bound: 1.2840522527694702, Entropy: 136.62991333007812, Temp: 0.5895498394966125, KL: 57.16246032714844, Loss: 0.9006951451301575, Learning Rate: 0.03\n",
      "Epoch [282/20000], Bound: 1.349256157875061, Entropy: 139.0072784423828, Temp: 0.589214563369751, KL: 73.34819030761719, Loss: 0.8820365071296692, Learning Rate: 0.03\n",
      "Epoch [283/20000], Bound: 1.4128764867782593, Entropy: 133.5092315673828, Temp: 0.588925302028656, KL: 84.62081909179688, Loss: 0.9106811881065369, Learning Rate: 0.03\n",
      "Epoch [284/20000], Bound: 1.3388950824737549, Entropy: 135.9345245361328, Temp: 0.5888106822967529, KL: 66.48793029785156, Loss: 0.920124888420105, Learning Rate: 0.03\n",
      "Epoch [285/20000], Bound: 1.4041117429733276, Entropy: 138.74937438964844, Temp: 0.5885696411132812, KL: 85.678466796875, Loss: 0.8832280039787292, Learning Rate: 0.03\n",
      "Epoch [286/20000], Bound: 1.4213368892669678, Entropy: 134.56272888183594, Temp: 0.5885504484176636, KL: 86.68966674804688, Loss: 0.9096444249153137, Learning Rate: 0.03\n",
      "Epoch [287/20000], Bound: 1.336666464805603, Entropy: 138.80596923828125, Temp: 0.5887130498886108, KL: 71.46978759765625, Loss: 0.8734997510910034, Learning Rate: 0.03\n",
      "Epoch [288/20000], Bound: 1.3714591264724731, Entropy: 138.58079528808594, Temp: 0.5888570547103882, KL: 79.9398193359375, Loss: 0.8679924607276917, Learning Rate: 0.03\n",
      "Epoch [289/20000], Bound: 1.3462356328964233, Entropy: 138.19195556640625, Temp: 0.5891193747520447, KL: 77.74710083007812, Loss: 0.8388351202011108, Learning Rate: 0.03\n",
      "Epoch [290/20000], Bound: 1.3612220287322998, Entropy: 136.91481018066406, Temp: 0.589491605758667, KL: 79.91885375976562, Loss: 0.8495798110961914, Learning Rate: 0.03\n",
      "Epoch [291/20000], Bound: 1.3737455606460571, Entropy: 141.29991149902344, Temp: 0.58998042345047, KL: 80.47404479980469, Loss: 0.8699820041656494, Learning Rate: 0.03\n",
      "Epoch [292/20000], Bound: 1.2843880653381348, Entropy: 136.8960418701172, Temp: 0.5905542373657227, KL: 63.993438720703125, Loss: 0.8447519540786743, Learning Rate: 0.03\n",
      "Epoch [293/20000], Bound: 1.2885934114456177, Entropy: 139.35203552246094, Temp: 0.5909768342971802, KL: 59.72235107421875, Loss: 0.8889297246932983, Learning Rate: 0.03\n",
      "Epoch [294/20000], Bound: 1.3560799360275269, Entropy: 139.34397888183594, Temp: 0.5911349654197693, KL: 77.13923645019531, Loss: 0.866200864315033, Learning Rate: 0.03\n",
      "Epoch [295/20000], Bound: 1.3276848793029785, Entropy: 142.53939819335938, Temp: 0.591359555721283, KL: 69.59400939941406, Loss: 0.8769227266311646, Learning Rate: 0.03\n",
      "Epoch [296/20000], Bound: 1.3148823976516724, Entropy: 144.42593383789062, Temp: 0.5915118455886841, KL: 69.56179809570312, Loss: 0.8538642525672913, Learning Rate: 0.03\n",
      "Epoch [297/20000], Bound: 1.368079662322998, Entropy: 146.66046142578125, Temp: 0.5916269421577454, KL: 75.97636413574219, Loss: 0.9000127911567688, Learning Rate: 0.03\n",
      "Epoch [298/20000], Bound: 1.327435851097107, Entropy: 147.42979431152344, Temp: 0.591749370098114, KL: 70.286376953125, Loss: 0.8712220788002014, Learning Rate: 0.03\n",
      "Epoch [299/20000], Bound: 1.2890851497650146, Entropy: 146.84803771972656, Temp: 0.591825544834137, KL: 63.64274597167969, Loss: 0.8578169345855713, Learning Rate: 0.03\n",
      "Epoch [300/20000], Bound: 1.3900468349456787, Entropy: 147.87673950195312, Temp: 0.5917690396308899, KL: 80.44622802734375, Loss: 0.9056727290153503, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [301/20000], Bound: 1.3933860063552856, Entropy: 150.91903686523438, Temp: 0.5917971134185791, KL: 83.90742492675781, Loss: 0.883144199848175, Learning Rate: 0.03\n",
      "Epoch [302/20000], Bound: 1.388218641281128, Entropy: 151.9895477294922, Temp: 0.5919826030731201, KL: 79.06231689453125, Loss: 0.9141297936439514, Learning Rate: 0.03\n",
      "Epoch [303/20000], Bound: 1.409690499305725, Entropy: 146.40115356445312, Temp: 0.5921962261199951, KL: 82.71501159667969, Loss: 0.9269211292266846, Learning Rate: 0.03\n",
      "Epoch [304/20000], Bound: 1.3946938514709473, Entropy: 144.00518798828125, Temp: 0.592473030090332, KL: 81.19313049316406, Loss: 0.9099923372268677, Learning Rate: 0.03\n",
      "Epoch [305/20000], Bound: 1.3338656425476074, Entropy: 145.58534240722656, Temp: 0.5928042531013489, KL: 69.35391235351562, Loss: 0.8927290439605713, Learning Rate: 0.03\n",
      "Epoch [306/20000], Bound: 1.4230304956436157, Entropy: 140.80331420898438, Temp: 0.593019962310791, KL: 86.55148315429688, Loss: 0.9236805438995361, Learning Rate: 0.03\n",
      "Epoch [307/20000], Bound: 1.3820189237594604, Entropy: 140.46240234375, Temp: 0.5933557748794556, KL: 76.96568298339844, Loss: 0.9220637083053589, Learning Rate: 0.03\n",
      "Epoch [308/20000], Bound: 1.3733948469161987, Entropy: 141.75613403320312, Temp: 0.5936554074287415, KL: 75.27253723144531, Loss: 0.9198970794677734, Learning Rate: 0.03\n",
      "Epoch [309/20000], Bound: 1.350403070449829, Entropy: 140.96511840820312, Temp: 0.5938969254493713, KL: 69.77845764160156, Loss: 0.9221339225769043, Learning Rate: 0.03\n",
      "Epoch [310/20000], Bound: 1.388995885848999, Entropy: 138.12039184570312, Temp: 0.5939944386482239, KL: 81.34140014648438, Loss: 0.9002557396888733, Learning Rate: 0.03\n",
      "Epoch [311/20000], Bound: 1.3441503047943115, Entropy: 137.0906982421875, Temp: 0.5941714644432068, KL: 71.56573486328125, Loss: 0.8956477046012878, Learning Rate: 0.03\n",
      "Epoch [312/20000], Bound: 1.378943920135498, Entropy: 137.2126922607422, Temp: 0.5942719578742981, KL: 75.71580505371094, Loss: 0.9281684160232544, Learning Rate: 0.03\n",
      "Epoch [313/20000], Bound: 1.3809894323349, Entropy: 138.5140838623047, Temp: 0.5943264961242676, KL: 81.812744140625, Loss: 0.8810134530067444, Learning Rate: 0.03\n",
      "Epoch [314/20000], Bound: 1.336864948272705, Entropy: 143.1515350341797, Temp: 0.5944963097572327, KL: 72.82400512695312, Loss: 0.8718389868736267, Learning Rate: 0.03\n",
      "Epoch [315/20000], Bound: 1.377004623413086, Entropy: 138.99452209472656, Temp: 0.5946398973464966, KL: 73.69180297851562, Loss: 0.9420216679573059, Learning Rate: 0.03\n",
      "Epoch [316/20000], Bound: 1.3652349710464478, Entropy: 137.9733123779297, Temp: 0.5946810841560364, KL: 76.13473510742188, Loss: 0.8985326886177063, Learning Rate: 0.03\n",
      "Epoch [317/20000], Bound: 1.3084322214126587, Entropy: 139.02783203125, Temp: 0.5947252511978149, KL: 67.25491333007812, Loss: 0.8663797378540039, Learning Rate: 0.03\n",
      "Epoch [318/20000], Bound: 1.3663140535354614, Entropy: 136.7267608642578, Temp: 0.5946700572967529, KL: 77.71342468261719, Loss: 0.8873374462127686, Learning Rate: 0.03\n",
      "Epoch [319/20000], Bound: 1.3440030813217163, Entropy: 138.84336853027344, Temp: 0.594667375087738, KL: 72.36639404296875, Loss: 0.889445424079895, Learning Rate: 0.03\n",
      "Epoch [320/20000], Bound: 1.2655459642410278, Entropy: 140.57034301757812, Temp: 0.5946235656738281, KL: 57.72235107421875, Loss: 0.8699714541435242, Learning Rate: 0.03\n",
      "Epoch [321/20000], Bound: 1.3601210117340088, Entropy: 139.17478942871094, Temp: 0.5943209528923035, KL: 78.60087585449219, Loss: 0.8672532439231873, Learning Rate: 0.03\n",
      "Epoch [322/20000], Bound: 1.3195308446884155, Entropy: 139.3787841796875, Temp: 0.5941386818885803, KL: 66.64599609375, Loss: 0.8909775614738464, Learning Rate: 0.03\n",
      "Epoch [323/20000], Bound: 1.3606621026992798, Entropy: 140.58164978027344, Temp: 0.5938388109207153, KL: 84.50138854980469, Loss: 0.8177602291107178, Learning Rate: 0.03\n",
      "Epoch [324/20000], Bound: 1.3662792444229126, Entropy: 142.27581787109375, Temp: 0.5938231945037842, KL: 82.04902648925781, Loss: 0.8492565751075745, Learning Rate: 0.03\n",
      "Epoch [325/20000], Bound: 1.3300493955612183, Entropy: 141.39141845703125, Temp: 0.593981921672821, KL: 77.48365783691406, Loss: 0.8190171718597412, Learning Rate: 0.03\n",
      "Epoch [326/20000], Bound: 1.3380844593048096, Entropy: 141.8754119873047, Temp: 0.5942667722702026, KL: 73.38864135742188, Loss: 0.8690077662467957, Learning Rate: 0.03\n",
      "Epoch [327/20000], Bound: 1.3544450998306274, Entropy: 137.8781280517578, Temp: 0.5945281982421875, KL: 80.13655090332031, Loss: 0.8437826633453369, Learning Rate: 0.03\n",
      "Epoch [328/20000], Bound: 1.3886317014694214, Entropy: 137.49176025390625, Temp: 0.5949112176895142, KL: 86.6649169921875, Loss: 0.8565279245376587, Learning Rate: 0.03\n",
      "Epoch [329/20000], Bound: 1.3386377096176147, Entropy: 135.37747192382812, Temp: 0.5954863429069519, KL: 74.51057434082031, Loss: 0.8626304268836975, Learning Rate: 0.03\n",
      "Epoch [330/20000], Bound: 1.3146380186080933, Entropy: 133.93772888183594, Temp: 0.5960304141044617, KL: 70.70407104492188, Loss: 0.8507453799247742, Learning Rate: 0.03\n",
      "Epoch [331/20000], Bound: 1.293281078338623, Entropy: 129.65675354003906, Temp: 0.5964968204498291, KL: 62.11933898925781, Loss: 0.8845834732055664, Learning Rate: 0.03\n",
      "Epoch [332/20000], Bound: 1.3543895483016968, Entropy: 130.1990966796875, Temp: 0.5966976881027222, KL: 77.95132446289062, Loss: 0.8658407330513, Learning Rate: 0.03\n",
      "Epoch [333/20000], Bound: 1.3019039630889893, Entropy: 132.80711364746094, Temp: 0.5969491600990295, KL: 67.55952453613281, Loss: 0.8552172780036926, Learning Rate: 0.03\n",
      "Epoch [334/20000], Bound: 1.4121112823486328, Entropy: 132.22731018066406, Temp: 0.597087025642395, KL: 89.98434448242188, Loss: 0.8807470798492432, Learning Rate: 0.03\n",
      "Epoch [335/20000], Bound: 1.3931809663772583, Entropy: 130.73501586914062, Temp: 0.5974476933479309, KL: 86.81271362304688, Loss: 0.8694312572479248, Learning Rate: 0.03\n",
      "Epoch [336/20000], Bound: 1.310704231262207, Entropy: 131.95156860351562, Temp: 0.5979748368263245, KL: 69.94819641113281, Loss: 0.8528165817260742, Learning Rate: 0.03\n",
      "Epoch [337/20000], Bound: 1.360886573791504, Entropy: 130.54884338378906, Temp: 0.5983996391296387, KL: 79.55131530761719, Loss: 0.8680135011672974, Learning Rate: 0.03\n",
      "Epoch [338/20000], Bound: 1.3536170721054077, Entropy: 129.71096801757812, Temp: 0.598866879940033, KL: 77.17597961425781, Loss: 0.8745965361595154, Learning Rate: 0.03\n",
      "Epoch [339/20000], Bound: 1.3436756134033203, Entropy: 137.11224365234375, Temp: 0.5993223190307617, KL: 72.36349487304688, Loss: 0.8964539170265198, Learning Rate: 0.03\n",
      "Epoch [340/20000], Bound: 1.347785234451294, Entropy: 135.70057678222656, Temp: 0.5996543765068054, KL: 74.16496276855469, Loss: 0.8898360729217529, Learning Rate: 0.03\n",
      "Epoch [341/20000], Bound: 1.3642791509628296, Entropy: 136.88381958007812, Temp: 0.5999124646186829, KL: 82.1064453125, Loss: 0.8560258150100708, Learning Rate: 0.03\n",
      "Epoch [342/20000], Bound: 1.3835883140563965, Entropy: 138.55029296875, Temp: 0.600279688835144, KL: 89.55001831054688, Loss: 0.8329117298126221, Learning Rate: 0.03\n",
      "Epoch [343/20000], Bound: 1.3577978610992432, Entropy: 138.21592712402344, Temp: 0.6008943319320679, KL: 79.01469421386719, Loss: 0.870890736579895, Learning Rate: 0.03\n",
      "Epoch [344/20000], Bound: 1.360998272895813, Entropy: 141.0740203857422, Temp: 0.6015067100524902, KL: 79.60110473632812, Loss: 0.873328685760498, Learning Rate: 0.03\n",
      "Epoch [345/20000], Bound: 1.3526699542999268, Entropy: 141.3087921142578, Temp: 0.6021199822425842, KL: 77.12257385253906, Loss: 0.8787957429885864, Learning Rate: 0.03\n",
      "Epoch [346/20000], Bound: 1.357653021812439, Entropy: 140.34202575683594, Temp: 0.6026821136474609, KL: 82.14985656738281, Loss: 0.8477306962013245, Learning Rate: 0.03\n",
      "Epoch [347/20000], Bound: 1.3118138313293457, Entropy: 139.75132751464844, Temp: 0.6033217906951904, KL: 76.76524353027344, Loss: 0.8064239025115967, Learning Rate: 0.03\n",
      "Epoch [348/20000], Bound: 1.306401252746582, Entropy: 142.8719024658203, Temp: 0.604000985622406, KL: 71.30976867675781, Loss: 0.8426715135574341, Learning Rate: 0.03\n",
      "Epoch [349/20000], Bound: 1.3396700620651245, Entropy: 136.67144775390625, Temp: 0.6045666933059692, KL: 79.19729614257812, Loss: 0.8407508730888367, Learning Rate: 0.03\n",
      "Epoch [350/20000], Bound: 1.295048713684082, Entropy: 137.9053497314453, Temp: 0.6051617860794067, KL: 73.75645446777344, Loss: 0.8034003376960754, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/20000], Bound: 1.2790721654891968, Entropy: 138.35232543945312, Temp: 0.6057449579238892, KL: 68.21903991699219, Loss: 0.8211280703544617, Learning Rate: 0.03\n",
      "Epoch [352/20000], Bound: 1.2823024988174438, Entropy: 139.49510192871094, Temp: 0.6061925888061523, KL: 69.12684631347656, Loss: 0.8200623393058777, Learning Rate: 0.03\n",
      "Epoch [353/20000], Bound: 1.3660027980804443, Entropy: 138.70460510253906, Temp: 0.6065326929092407, KL: 81.86318969726562, Loss: 0.873418390750885, Learning Rate: 0.03\n",
      "Epoch [354/20000], Bound: 1.2993425130844116, Entropy: 140.0345916748047, Temp: 0.6069087386131287, KL: 67.13107299804688, Loss: 0.8684676885604858, Learning Rate: 0.03\n",
      "Epoch [355/20000], Bound: 1.3395991325378418, Entropy: 136.56130981445312, Temp: 0.6070737838745117, KL: 81.39729309082031, Loss: 0.8267378807067871, Learning Rate: 0.03\n",
      "Epoch [356/20000], Bound: 1.3654813766479492, Entropy: 143.5681610107422, Temp: 0.6073513627052307, KL: 88.06605529785156, Loss: 0.822794497013092, Learning Rate: 0.03\n",
      "Epoch [357/20000], Bound: 1.35318922996521, Entropy: 141.851318359375, Temp: 0.6078423261642456, KL: 78.02839660644531, Loss: 0.8821008205413818, Learning Rate: 0.03\n",
      "Epoch [358/20000], Bound: 1.283416986465454, Entropy: 140.2954864501953, Temp: 0.6082713603973389, KL: 69.55972290039062, Loss: 0.8214460611343384, Learning Rate: 0.03\n",
      "Epoch [359/20000], Bound: 1.3359285593032837, Entropy: 138.27171325683594, Temp: 0.6085881590843201, KL: 75.47244262695312, Loss: 0.8709657192230225, Learning Rate: 0.03\n",
      "Epoch [360/20000], Bound: 1.280356764793396, Entropy: 137.0756072998047, Temp: 0.6088292598724365, KL: 69.79283142089844, Loss: 0.8148037791252136, Learning Rate: 0.03\n",
      "Epoch [361/20000], Bound: 1.3222020864486694, Entropy: 136.62655639648438, Temp: 0.6089873909950256, KL: 78.54922485351562, Loss: 0.8202473521232605, Learning Rate: 0.03\n",
      "Epoch [362/20000], Bound: 1.302485704421997, Entropy: 137.2411651611328, Temp: 0.6092115640640259, KL: 72.09150695800781, Loss: 0.8368165493011475, Learning Rate: 0.03\n",
      "Epoch [363/20000], Bound: 1.3199204206466675, Entropy: 137.44981384277344, Temp: 0.6093587279319763, KL: 79.79377746582031, Loss: 0.8063380718231201, Learning Rate: 0.03\n",
      "Epoch [364/20000], Bound: 1.2985609769821167, Entropy: 135.64923095703125, Temp: 0.6096135973930359, KL: 76.81019592285156, Loss: 0.7914829254150391, Learning Rate: 0.03\n",
      "Epoch [365/20000], Bound: 1.3000668287277222, Entropy: 137.41656494140625, Temp: 0.6099371314048767, KL: 73.29660034179688, Loss: 0.8235570192337036, Learning Rate: 0.03\n",
      "Epoch [366/20000], Bound: 1.349427342414856, Entropy: 135.51019287109375, Temp: 0.6102107763290405, KL: 86.08467102050781, Loss: 0.812770664691925, Learning Rate: 0.03\n",
      "Epoch [367/20000], Bound: 1.3330905437469482, Entropy: 136.63966369628906, Temp: 0.6106693148612976, KL: 78.62962341308594, Loss: 0.8430448770523071, Learning Rate: 0.03\n",
      "Epoch [368/20000], Bound: 1.3542053699493408, Entropy: 134.8383331298828, Temp: 0.6111215353012085, KL: 80.98045349121094, Loss: 0.8655311465263367, Learning Rate: 0.03\n",
      "Epoch [369/20000], Bound: 1.3415101766586304, Entropy: 135.22471618652344, Temp: 0.6115704774856567, KL: 78.63496398925781, Loss: 0.8607154488563538, Learning Rate: 0.03\n",
      "Epoch [370/20000], Bound: 1.3330519199371338, Entropy: 139.35089111328125, Temp: 0.6119819283485413, KL: 79.91006469726562, Loss: 0.8346762657165527, Learning Rate: 0.03\n",
      "Epoch [371/20000], Bound: 1.3545763492584229, Entropy: 139.28955078125, Temp: 0.6124188899993896, KL: 87.65736389160156, Loss: 0.8140068650245667, Learning Rate: 0.03\n",
      "Epoch [372/20000], Bound: 1.3712239265441895, Entropy: 138.25985717773438, Temp: 0.613037109375, KL: 84.4984130859375, Loss: 0.8740567564964294, Learning Rate: 0.03\n",
      "Epoch [373/20000], Bound: 1.3612300157546997, Entropy: 137.34820556640625, Temp: 0.6136695742607117, KL: 80.20468139648438, Loss: 0.8901974558830261, Learning Rate: 0.03\n",
      "Epoch [374/20000], Bound: 1.3472821712493896, Entropy: 140.26516723632812, Temp: 0.6142151951789856, KL: 79.82608032226562, Loss: 0.8666958212852478, Learning Rate: 0.03\n",
      "Epoch [375/20000], Bound: 1.338344931602478, Entropy: 138.71551513671875, Temp: 0.6147091388702393, KL: 75.06730651855469, Loss: 0.8888270854949951, Learning Rate: 0.03\n",
      "Epoch [376/20000], Bound: 1.3496828079223633, Entropy: 141.00421142578125, Temp: 0.6150381565093994, KL: 82.07963562011719, Loss: 0.8544837236404419, Learning Rate: 0.03\n",
      "Epoch [377/20000], Bound: 1.3162715435028076, Entropy: 144.70587158203125, Temp: 0.6153886914253235, KL: 72.37127685546875, Loss: 0.8695562481880188, Learning Rate: 0.03\n",
      "Epoch [378/20000], Bound: 1.3674354553222656, Entropy: 145.13926696777344, Temp: 0.6155669093132019, KL: 84.35769653320312, Loss: 0.8721992373466492, Learning Rate: 0.03\n",
      "Epoch [379/20000], Bound: 1.3592160940170288, Entropy: 147.90834045410156, Temp: 0.6157882809638977, KL: 83.74700927734375, Loss: 0.8611135482788086, Learning Rate: 0.03\n",
      "Epoch [380/20000], Bound: 1.3349827527999878, Entropy: 147.88958740234375, Temp: 0.6160547137260437, KL: 74.41908264160156, Loss: 0.8897349238395691, Learning Rate: 0.03\n",
      "Epoch [381/20000], Bound: 1.3048101663589478, Entropy: 151.09625244140625, Temp: 0.6161571145057678, KL: 68.00126647949219, Loss: 0.8846360445022583, Learning Rate: 0.03\n",
      "Epoch [382/20000], Bound: 1.3624858856201172, Entropy: 147.0377960205078, Temp: 0.6160051822662354, KL: 77.58018493652344, Loss: 0.9180794358253479, Learning Rate: 0.03\n",
      "Epoch [383/20000], Bound: 1.348181128501892, Entropy: 145.82444763183594, Temp: 0.6157410740852356, KL: 82.61187744140625, Loss: 0.8484271168708801, Learning Rate: 0.03\n",
      "Epoch [384/20000], Bound: 1.3757268190383911, Entropy: 145.44578552246094, Temp: 0.6155717968940735, KL: 82.98100280761719, Loss: 0.9001457095146179, Learning Rate: 0.03\n",
      "Epoch [385/20000], Bound: 1.3527982234954834, Entropy: 143.87440490722656, Temp: 0.6154134273529053, KL: 80.91517639160156, Loss: 0.8707333207130432, Learning Rate: 0.03\n",
      "Epoch [386/20000], Bound: 1.353482961654663, Entropy: 140.9127655029297, Temp: 0.6152773499488831, KL: 80.52000427246094, Loss: 0.8750621676445007, Learning Rate: 0.03\n",
      "Epoch [387/20000], Bound: 1.2880123853683472, Entropy: 138.8025360107422, Temp: 0.6151486039161682, KL: 67.61656188964844, Loss: 0.8552993535995483, Learning Rate: 0.03\n",
      "Epoch [388/20000], Bound: 1.280760407447815, Entropy: 140.67581176757812, Temp: 0.6148312091827393, KL: 64.69778442382812, Loss: 0.8653880953788757, Learning Rate: 0.03\n",
      "Epoch [389/20000], Bound: 1.315582275390625, Entropy: 136.391845703125, Temp: 0.614276111125946, KL: 73.49333190917969, Loss: 0.8574375510215759, Learning Rate: 0.03\n",
      "Epoch [390/20000], Bound: 1.3556861877441406, Entropy: 136.61021423339844, Temp: 0.6136816740036011, KL: 82.71286010742188, Loss: 0.858791172504425, Learning Rate: 0.03\n",
      "Epoch [391/20000], Bound: 1.3331702947616577, Entropy: 136.89801025390625, Temp: 0.6132131814956665, KL: 80.35760498046875, Loss: 0.8333040475845337, Learning Rate: 0.03\n",
      "Epoch [392/20000], Bound: 1.2984554767608643, Entropy: 139.7823944091797, Temp: 0.6128615736961365, KL: 70.986572265625, Loss: 0.8438010215759277, Learning Rate: 0.03\n",
      "Epoch [393/20000], Bound: 1.3927992582321167, Entropy: 139.12928771972656, Temp: 0.6124351024627686, KL: 89.52203369140625, Loss: 0.8758919835090637, Learning Rate: 0.03\n",
      "Epoch [394/20000], Bound: 1.3016021251678467, Entropy: 143.1425018310547, Temp: 0.6122124791145325, KL: 75.07829284667969, Loss: 0.8152502179145813, Learning Rate: 0.03\n",
      "Epoch [395/20000], Bound: 1.3483506441116333, Entropy: 144.51034545898438, Temp: 0.6120250821113586, KL: 84.74777221679688, Loss: 0.8248473405838013, Learning Rate: 0.03\n",
      "Epoch [396/20000], Bound: 1.3438559770584106, Entropy: 143.38487243652344, Temp: 0.612023651599884, KL: 83.02180480957031, Loss: 0.8301870822906494, Learning Rate: 0.03\n",
      "Epoch [397/20000], Bound: 1.398244857788086, Entropy: 144.80551147460938, Temp: 0.6121519207954407, KL: 96.0955810546875, Loss: 0.8329199552536011, Learning Rate: 0.03\n",
      "Epoch [398/20000], Bound: 1.3567460775375366, Entropy: 142.6651153564453, Temp: 0.612609326839447, KL: 78.671630859375, Loss: 0.891975462436676, Learning Rate: 0.03\n",
      "Epoch [399/20000], Bound: 1.3898435831069946, Entropy: 144.4252471923828, Temp: 0.6129732131958008, KL: 88.62933349609375, Loss: 0.8781384229660034, Learning Rate: 0.03\n",
      "Epoch [400/20000], Bound: 1.3849762678146362, Entropy: 141.97000122070312, Temp: 0.6134423017501831, KL: 87.97148132324219, Loss: 0.8744166493415833, Learning Rate: 0.03\n",
      "Epoch [401/20000], Bound: 1.3283319473266602, Entropy: 141.84608459472656, Temp: 0.6139987707138062, KL: 74.93162536621094, Loss: 0.8695159554481506, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [402/20000], Bound: 1.2951916456222534, Entropy: 140.91952514648438, Temp: 0.6144126653671265, KL: 66.83203125, Loss: 0.8738415241241455, Learning Rate: 0.03\n",
      "Epoch [403/20000], Bound: 1.3689663410186768, Entropy: 137.489013671875, Temp: 0.6145403385162354, KL: 84.97663879394531, Loss: 0.8683750629425049, Learning Rate: 0.03\n",
      "Epoch [404/20000], Bound: 1.336665391921997, Entropy: 136.23013305664062, Temp: 0.6147413849830627, KL: 80.71699523925781, Loss: 0.8396751284599304, Learning Rate: 0.03\n",
      "Epoch [405/20000], Bound: 1.320920705795288, Entropy: 135.67269897460938, Temp: 0.6149811148643494, KL: 78.72280883789062, Loss: 0.8261063098907471, Learning Rate: 0.03\n",
      "Epoch [406/20000], Bound: 1.3428194522857666, Entropy: 135.6289825439453, Temp: 0.6152414083480835, KL: 83.25257873535156, Loss: 0.8318641185760498, Learning Rate: 0.03\n",
      "Epoch [407/20000], Bound: 1.335069179534912, Entropy: 134.037841796875, Temp: 0.615588903427124, KL: 79.93917846679688, Loss: 0.8443260192871094, Learning Rate: 0.03\n",
      "Epoch [408/20000], Bound: 1.4094889163970947, Entropy: 129.84646606445312, Temp: 0.6159341931343079, KL: 100.01803588867188, Loss: 0.8326141834259033, Learning Rate: 0.03\n",
      "Epoch [409/20000], Bound: 1.342754602432251, Entropy: 129.27536010742188, Temp: 0.6166319847106934, KL: 86.47506713867188, Loss: 0.8080084323883057, Learning Rate: 0.03\n",
      "Epoch [410/20000], Bound: 1.2517989873886108, Entropy: 128.89529418945312, Temp: 0.6174618005752563, KL: 66.43423461914062, Loss: 0.8029446005821228, Learning Rate: 0.03\n",
      "Epoch [411/20000], Bound: 1.3325221538543701, Entropy: 124.68427276611328, Temp: 0.6180526614189148, KL: 83.81722259521484, Loss: 0.8121062517166138, Learning Rate: 0.03\n",
      "Epoch [412/20000], Bound: 1.3543317317962646, Entropy: 123.55885314941406, Temp: 0.6187249422073364, KL: 83.04751586914062, Loss: 0.8622466325759888, Learning Rate: 0.03\n",
      "Epoch [413/20000], Bound: 1.3428399562835693, Entropy: 125.98561096191406, Temp: 0.6193687319755554, KL: 81.70071411132812, Loss: 0.85154128074646, Learning Rate: 0.03\n",
      "Epoch [414/20000], Bound: 1.315953016281128, Entropy: 132.984619140625, Temp: 0.6199771165847778, KL: 75.63699340820312, Loss: 0.8495280146598816, Learning Rate: 0.03\n",
      "Epoch [415/20000], Bound: 1.3298298120498657, Entropy: 133.51516723632812, Temp: 0.6204453110694885, KL: 75.340576171875, Loss: 0.8792580366134644, Learning Rate: 0.03\n",
      "Epoch [416/20000], Bound: 1.3459655046463013, Entropy: 137.0299530029297, Temp: 0.6207290887832642, KL: 74.95539855957031, Loss: 0.9143357872962952, Learning Rate: 0.03\n",
      "Epoch [417/20000], Bound: 1.410570740699768, Entropy: 138.8922119140625, Temp: 0.6207801699638367, KL: 89.51968383789062, Loss: 0.929928719997406, Learning Rate: 0.03\n",
      "Epoch [418/20000], Bound: 1.375990867614746, Entropy: 140.46434020996094, Temp: 0.6208486557006836, KL: 83.70292663574219, Loss: 0.9044546484947205, Learning Rate: 0.03\n",
      "Epoch [419/20000], Bound: 1.328229546546936, Entropy: 144.7027130126953, Temp: 0.6208752989768982, KL: 75.41465759277344, Loss: 0.8762441873550415, Learning Rate: 0.03\n",
      "Epoch [420/20000], Bound: 1.384968638420105, Entropy: 144.37513732910156, Temp: 0.6207639575004578, KL: 82.79904174804688, Loss: 0.9300791621208191, Learning Rate: 0.03\n",
      "Epoch [421/20000], Bound: 1.35541570186615, Entropy: 145.62075805664062, Temp: 0.6205710172653198, KL: 78.09666442871094, Loss: 0.9075228571891785, Learning Rate: 0.03\n",
      "Epoch [422/20000], Bound: 1.392249345779419, Entropy: 141.6686553955078, Temp: 0.6202598810195923, KL: 88.22059631347656, Loss: 0.9005950689315796, Learning Rate: 0.03\n",
      "Epoch [423/20000], Bound: 1.3376853466033936, Entropy: 145.74716186523438, Temp: 0.6200326681137085, KL: 74.24513244628906, Loss: 0.9026987552642822, Learning Rate: 0.03\n",
      "Epoch [424/20000], Bound: 1.341795802116394, Entropy: 142.25611877441406, Temp: 0.6196319460868835, KL: 79.62564086914062, Loss: 0.8666837215423584, Learning Rate: 0.03\n",
      "Epoch [425/20000], Bound: 1.3599112033843994, Entropy: 143.17239379882812, Temp: 0.6192347407341003, KL: 78.43838500976562, Loss: 0.9114912152290344, Learning Rate: 0.03\n",
      "Epoch [426/20000], Bound: 1.2913683652877808, Entropy: 144.86729431152344, Temp: 0.618747353553772, KL: 64.62692260742188, Loss: 0.8905950784683228, Learning Rate: 0.03\n",
      "Epoch [427/20000], Bound: 1.3639594316482544, Entropy: 141.2219696044922, Temp: 0.6179590225219727, KL: 82.77450561523438, Loss: 0.8823434114456177, Learning Rate: 0.03\n",
      "Epoch [428/20000], Bound: 1.4190834760665894, Entropy: 141.80926513671875, Temp: 0.617253303527832, KL: 90.37095642089844, Loss: 0.9341450929641724, Learning Rate: 0.03\n",
      "Epoch [429/20000], Bound: 1.320233941078186, Entropy: 138.22901916503906, Temp: 0.6166719794273376, KL: 69.7364501953125, Loss: 0.9003790020942688, Learning Rate: 0.03\n",
      "Epoch [430/20000], Bound: 1.3481308221817017, Entropy: 135.2804718017578, Temp: 0.6158924698829651, KL: 74.70298767089844, Loss: 0.912797212600708, Learning Rate: 0.03\n",
      "Epoch [431/20000], Bound: 1.316810131072998, Entropy: 134.71327209472656, Temp: 0.6150116920471191, KL: 72.77226257324219, Loss: 0.8667435050010681, Learning Rate: 0.03\n",
      "Epoch [432/20000], Bound: 1.3385276794433594, Entropy: 134.87010192871094, Temp: 0.6140855550765991, KL: 77.03367614746094, Loss: 0.8721667528152466, Learning Rate: 0.03\n",
      "Epoch [433/20000], Bound: 1.3694911003112793, Entropy: 134.3668670654297, Temp: 0.6131940484046936, KL: 82.84822082519531, Loss: 0.88431715965271, Learning Rate: 0.03\n",
      "Epoch [434/20000], Bound: 1.3749393224716187, Entropy: 135.5114288330078, Temp: 0.612424910068512, KL: 81.19636535644531, Loss: 0.907376766204834, Learning Rate: 0.03\n",
      "Epoch [435/20000], Bound: 1.3359800577163696, Entropy: 135.23812866210938, Temp: 0.6117029190063477, KL: 71.54710388183594, Loss: 0.9081950187683105, Learning Rate: 0.03\n",
      "Epoch [436/20000], Bound: 1.4039303064346313, Entropy: 135.63987731933594, Temp: 0.6108484268188477, KL: 88.38392639160156, Loss: 0.9051603674888611, Learning Rate: 0.03\n",
      "Epoch [437/20000], Bound: 1.33994460105896, Entropy: 137.2794952392578, Temp: 0.6101915836334229, KL: 76.04217529296875, Loss: 0.8766273260116577, Learning Rate: 0.03\n",
      "Epoch [438/20000], Bound: 1.3569964170455933, Entropy: 140.77642822265625, Temp: 0.6095414757728577, KL: 76.39134216308594, Loss: 0.9058996438980103, Learning Rate: 0.03\n",
      "Epoch [439/20000], Bound: 1.3767962455749512, Entropy: 137.3584747314453, Temp: 0.608859658241272, KL: 80.63734436035156, Loss: 0.909250795841217, Learning Rate: 0.03\n",
      "Epoch [440/20000], Bound: 1.3494315147399902, Entropy: 139.14881896972656, Temp: 0.6082265377044678, KL: 77.61773681640625, Loss: 0.8788058161735535, Learning Rate: 0.03\n",
      "Epoch [441/20000], Bound: 1.3723819255828857, Entropy: 143.72645568847656, Temp: 0.6076361536979675, KL: 78.68377685546875, Loss: 0.9142634272575378, Learning Rate: 0.03\n",
      "Epoch [442/20000], Bound: 1.3074121475219727, Entropy: 141.1208038330078, Temp: 0.6070491671562195, KL: 69.564697265625, Loss: 0.8634886145591736, Learning Rate: 0.03\n",
      "Epoch [443/20000], Bound: 1.3722589015960693, Entropy: 144.58966064453125, Temp: 0.6063793897628784, KL: 78.5931396484375, Loss: 0.91253262758255, Learning Rate: 0.03\n",
      "Epoch [444/20000], Bound: 1.353722333908081, Entropy: 145.94869995117188, Temp: 0.6057304739952087, KL: 74.26188659667969, Loss: 0.9106158018112183, Learning Rate: 0.03\n",
      "Epoch [445/20000], Bound: 1.3992533683776855, Entropy: 144.33143615722656, Temp: 0.605026364326477, KL: 82.88720703125, Loss: 0.9292434453964233, Learning Rate: 0.03\n",
      "Epoch [446/20000], Bound: 1.3961708545684814, Entropy: 143.14393615722656, Temp: 0.6044074892997742, KL: 82.17819213867188, Loss: 0.9275960326194763, Learning Rate: 0.03\n",
      "Epoch [447/20000], Bound: 1.3466079235076904, Entropy: 147.58526611328125, Temp: 0.6038593649864197, KL: 72.37275695800781, Loss: 0.9093731641769409, Learning Rate: 0.03\n",
      "Epoch [448/20000], Bound: 1.3544925451278687, Entropy: 144.07997131347656, Temp: 0.6032235622406006, KL: 74.44869995117188, Loss: 0.9063822031021118, Learning Rate: 0.03\n",
      "Epoch [449/20000], Bound: 1.3706088066101074, Entropy: 143.65975952148438, Temp: 0.6025577783584595, KL: 75.63833618164062, Loss: 0.9269749522209167, Learning Rate: 0.03\n",
      "Epoch [450/20000], Bound: 1.4031822681427002, Entropy: 141.83302307128906, Temp: 0.6018584966659546, KL: 84.30976867675781, Loss: 0.919369637966156, Learning Rate: 0.03\n",
      "Epoch [451/20000], Bound: 1.334688663482666, Entropy: 140.2985382080078, Temp: 0.6013083457946777, KL: 69.86798095703125, Loss: 0.9033238291740417, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [452/20000], Bound: 1.3790326118469238, Entropy: 138.95213317871094, Temp: 0.6006477475166321, KL: 75.22419738769531, Loss: 0.9437918066978455, Learning Rate: 0.03\n",
      "Epoch [453/20000], Bound: 1.3953335285186768, Entropy: 137.9037322998047, Temp: 0.5999308824539185, KL: 79.929931640625, Loss: 0.9360687732696533, Learning Rate: 0.03\n",
      "Epoch [454/20000], Bound: 1.39438796043396, Entropy: 137.03927612304688, Temp: 0.5992699265480042, KL: 78.459228515625, Loss: 0.9451762437820435, Learning Rate: 0.03\n",
      "Epoch [455/20000], Bound: 1.3337494134902954, Entropy: 138.74757385253906, Temp: 0.5986213684082031, KL: 70.17555236816406, Loss: 0.8947737216949463, Learning Rate: 0.03\n",
      "Epoch [456/20000], Bound: 1.3394798040390015, Entropy: 138.59384155273438, Temp: 0.5979089736938477, KL: 72.23582458496094, Loss: 0.8872482180595398, Learning Rate: 0.03\n",
      "Epoch [457/20000], Bound: 1.3356659412384033, Entropy: 139.0625762939453, Temp: 0.597196638584137, KL: 67.81846618652344, Loss: 0.9158746004104614, Learning Rate: 0.03\n",
      "Epoch [458/20000], Bound: 1.3447353839874268, Entropy: 139.9274139404297, Temp: 0.5963549017906189, KL: 70.10432434082031, Loss: 0.9125688076019287, Learning Rate: 0.03\n",
      "Epoch [459/20000], Bound: 1.4219403266906738, Entropy: 136.35414123535156, Temp: 0.5954532027244568, KL: 91.23440551757812, Loss: 0.8871679306030273, Learning Rate: 0.03\n",
      "Epoch [460/20000], Bound: 1.353434443473816, Entropy: 136.0126953125, Temp: 0.5949467420578003, KL: 73.60983276367188, Loss: 0.89743971824646, Learning Rate: 0.03\n",
      "Epoch [461/20000], Bound: 1.4092940092086792, Entropy: 138.22921752929688, Temp: 0.5944497585296631, KL: 82.63987731933594, Loss: 0.9312257766723633, Learning Rate: 0.03\n",
      "Epoch [462/20000], Bound: 1.3541754484176636, Entropy: 137.9377899169922, Temp: 0.5940839052200317, KL: 72.62005615234375, Loss: 0.905735969543457, Learning Rate: 0.03\n",
      "Epoch [463/20000], Bound: 1.3716778755187988, Entropy: 137.47283935546875, Temp: 0.5936862230300903, KL: 71.11648559570312, Loss: 0.9515948295593262, Learning Rate: 0.03\n",
      "Epoch [464/20000], Bound: 1.3866651058197021, Entropy: 138.55471801757812, Temp: 0.5931592583656311, KL: 82.01361083984375, Loss: 0.8883631825447083, Learning Rate: 0.03\n",
      "Epoch [465/20000], Bound: 1.344282865524292, Entropy: 142.86277770996094, Temp: 0.5928340554237366, KL: 69.77214050292969, Loss: 0.9088412523269653, Learning Rate: 0.03\n",
      "Epoch [466/20000], Bound: 1.2829413414001465, Entropy: 138.10263061523438, Temp: 0.5924187898635864, KL: 58.77549743652344, Loss: 0.8888169527053833, Learning Rate: 0.03\n",
      "Epoch [467/20000], Bound: 1.384497880935669, Entropy: 142.28961181640625, Temp: 0.5917280912399292, KL: 78.8125, Loss: 0.9083898067474365, Learning Rate: 0.03\n",
      "Epoch [468/20000], Bound: 1.3813270330429077, Entropy: 139.74761962890625, Temp: 0.5911712050437927, KL: 79.481689453125, Loss: 0.8954414129257202, Learning Rate: 0.03\n",
      "Epoch [469/20000], Bound: 1.3324161767959595, Entropy: 138.3358612060547, Temp: 0.5907728672027588, KL: 63.64030456542969, Loss: 0.9351765513420105, Learning Rate: 0.03\n",
      "Epoch [470/20000], Bound: 1.328703761100769, Entropy: 142.8767547607422, Temp: 0.5901371836662292, KL: 66.80453491210938, Loss: 0.9005237221717834, Learning Rate: 0.03\n",
      "Epoch [471/20000], Bound: 1.3544753789901733, Entropy: 144.34507751464844, Temp: 0.589412271976471, KL: 74.83409118652344, Loss: 0.8796741962432861, Learning Rate: 0.03\n",
      "Epoch [472/20000], Bound: 1.3669390678405762, Entropy: 143.3639678955078, Temp: 0.5888093113899231, KL: 79.91484069824219, Loss: 0.8593726754188538, Learning Rate: 0.03\n",
      "Epoch [473/20000], Bound: 1.3302834033966064, Entropy: 143.8270263671875, Temp: 0.5884542465209961, KL: 70.11335754394531, Loss: 0.8727445006370544, Learning Rate: 0.03\n",
      "Epoch [474/20000], Bound: 1.3265522718429565, Entropy: 140.61769104003906, Temp: 0.5881060361862183, KL: 72.98750305175781, Loss: 0.8408639430999756, Learning Rate: 0.03\n",
      "Epoch [475/20000], Bound: 1.3525606393814087, Entropy: 137.28692626953125, Temp: 0.5878777503967285, KL: 77.25028991699219, Loss: 0.8528472781181335, Learning Rate: 0.03\n",
      "Epoch [476/20000], Bound: 1.3609910011291504, Entropy: 140.89198303222656, Temp: 0.5878251791000366, KL: 78.08879089355469, Loss: 0.8616808652877808, Learning Rate: 0.03\n",
      "Epoch [477/20000], Bound: 1.3274857997894287, Entropy: 140.17767333984375, Temp: 0.5879335403442383, KL: 71.84231567382812, Loss: 0.8520443439483643, Learning Rate: 0.03\n",
      "Epoch [478/20000], Bound: 1.3261700868606567, Entropy: 135.16738891601562, Temp: 0.5880758762359619, KL: 72.42793273925781, Loss: 0.8448686003684998, Learning Rate: 0.03\n",
      "Epoch [479/20000], Bound: 1.3665637969970703, Entropy: 138.06814575195312, Temp: 0.5882720947265625, KL: 78.83503723144531, Loss: 0.8668437004089355, Learning Rate: 0.03\n",
      "Epoch [480/20000], Bound: 1.3190927505493164, Entropy: 141.07957458496094, Temp: 0.5886083245277405, KL: 68.88581848144531, Loss: 0.8628191351890564, Learning Rate: 0.03\n",
      "Epoch [481/20000], Bound: 1.3920705318450928, Entropy: 139.0087432861328, Temp: 0.5888725519180298, KL: 82.93157958984375, Loss: 0.8830760717391968, Learning Rate: 0.03\n",
      "Epoch [482/20000], Bound: 1.337762475013733, Entropy: 138.8284912109375, Temp: 0.5893199443817139, KL: 69.57524108886719, Loss: 0.8926070928573608, Learning Rate: 0.03\n",
      "Epoch [483/20000], Bound: 1.407922387123108, Entropy: 138.75843811035156, Temp: 0.5896448493003845, KL: 86.69979858398438, Loss: 0.8844501972198486, Learning Rate: 0.03\n",
      "Epoch [484/20000], Bound: 1.4257242679595947, Entropy: 137.7470703125, Temp: 0.5902122259140015, KL: 85.80015563964844, Loss: 0.9297353625297546, Learning Rate: 0.03\n",
      "Epoch [485/20000], Bound: 1.3578956127166748, Entropy: 138.3932342529297, Temp: 0.5909018516540527, KL: 68.84437561035156, Loss: 0.9394550919532776, Learning Rate: 0.03\n",
      "Epoch [486/20000], Bound: 1.356064796447754, Entropy: 133.8465118408203, Temp: 0.5913426876068115, KL: 72.85121154785156, Loss: 0.9027920365333557, Learning Rate: 0.03\n",
      "Epoch [487/20000], Bound: 1.4280558824539185, Entropy: 132.99398803710938, Temp: 0.5916985869407654, KL: 92.83226013183594, Loss: 0.878279983997345, Learning Rate: 0.03\n",
      "Epoch [488/20000], Bound: 1.4035457372665405, Entropy: 134.7752227783203, Temp: 0.5924065113067627, KL: 86.19596862792969, Loss: 0.8854711055755615, Learning Rate: 0.03\n",
      "Epoch [489/20000], Bound: 1.3056007623672485, Entropy: 127.88052368164062, Temp: 0.5932897925376892, KL: 69.86669921875, Loss: 0.8370978832244873, Learning Rate: 0.03\n",
      "Epoch [490/20000], Bound: 1.3087843656539917, Entropy: 129.261962890625, Temp: 0.59408038854599, KL: 71.452880859375, Loss: 0.830735445022583, Learning Rate: 0.03\n",
      "Epoch [491/20000], Bound: 1.2970975637435913, Entropy: 130.1625518798828, Temp: 0.5948262214660645, KL: 72.45068359375, Loss: 0.8023132681846619, Learning Rate: 0.03\n",
      "Epoch [492/20000], Bound: 1.3041778802871704, Entropy: 127.84825897216797, Temp: 0.5955972671508789, KL: 72.71312713623047, Loss: 0.8141001462936401, Learning Rate: 0.03\n",
      "Epoch [493/20000], Bound: 1.3425525426864624, Entropy: 129.7328643798828, Temp: 0.5963709354400635, KL: 79.41438293457031, Loss: 0.8303918838500977, Learning Rate: 0.03\n",
      "Epoch [494/20000], Bound: 1.265671730041504, Entropy: 136.48941040039062, Temp: 0.5972496271133423, KL: 70.46121215820312, Loss: 0.7668207883834839, Learning Rate: 0.03\n",
      "Epoch [495/20000], Bound: 1.2695811986923218, Entropy: 137.56161499023438, Temp: 0.5981472730636597, KL: 71.84317016601562, Loss: 0.7633978128433228, Learning Rate: 0.03\n",
      "Epoch [496/20000], Bound: 1.3964248895645142, Entropy: 133.3344268798828, Temp: 0.5990917682647705, KL: 93.60202026367188, Loss: 0.8225926756858826, Learning Rate: 0.03\n",
      "Epoch [497/20000], Bound: 1.3437272310256958, Entropy: 135.51004028320312, Temp: 0.6003946661949158, KL: 82.8861083984375, Loss: 0.8106649518013, Learning Rate: 0.03\n",
      "Epoch [498/20000], Bound: 1.3766906261444092, Entropy: 138.6285858154297, Temp: 0.6018292307853699, KL: 85.64317321777344, Loss: 0.8546476364135742, Learning Rate: 0.03\n",
      "Epoch [499/20000], Bound: 1.344316005706787, Entropy: 139.739990234375, Temp: 0.6033479571342468, KL: 72.63577270507812, Loss: 0.9019556045532227, Learning Rate: 0.03\n",
      "Epoch [500/20000], Bound: 1.3092540502548218, Entropy: 135.15049743652344, Temp: 0.6045893430709839, KL: 79.593994140625, Loss: 0.780310332775116, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [501/20000], Bound: 1.3766303062438965, Entropy: 134.88002014160156, Temp: 0.6059311628341675, KL: 85.95307922363281, Loss: 0.8597363829612732, Learning Rate: 0.03\n",
      "Epoch [502/20000], Bound: 1.3159259557724, Entropy: 137.39608764648438, Temp: 0.6069133877754211, KL: 72.60142517089844, Loss: 0.8540988564491272, Learning Rate: 0.03\n",
      "Epoch [503/20000], Bound: 1.3342684507369995, Entropy: 135.63540649414062, Temp: 0.6077498197555542, KL: 76.23994445800781, Loss: 0.8601152896881104, Learning Rate: 0.03\n",
      "Epoch [504/20000], Bound: 1.3640040159225464, Entropy: 139.53282165527344, Temp: 0.60849529504776, KL: 82.06475830078125, Loss: 0.8713294267654419, Learning Rate: 0.03\n",
      "Epoch [505/20000], Bound: 1.3728269338607788, Entropy: 139.8990478515625, Temp: 0.6092221736907959, KL: 79.71211242675781, Loss: 0.9095274209976196, Learning Rate: 0.03\n",
      "Epoch [506/20000], Bound: 1.409515142440796, Entropy: 138.72293090820312, Temp: 0.6098484396934509, KL: 89.51542663574219, Loss: 0.9055689573287964, Learning Rate: 0.03\n",
      "Epoch [507/20000], Bound: 1.374966025352478, Entropy: 140.61941528320312, Temp: 0.6105183362960815, KL: 78.80979919433594, Loss: 0.923531174659729, Learning Rate: 0.03\n",
      "Epoch [508/20000], Bound: 1.3541849851608276, Entropy: 142.135498046875, Temp: 0.6110565066337585, KL: 74.33969116210938, Loss: 0.9197164177894592, Learning Rate: 0.03\n",
      "Epoch [509/20000], Bound: 1.3653388023376465, Entropy: 143.5847930908203, Temp: 0.6114144921302795, KL: 77.45071411132812, Loss: 0.916941225528717, Learning Rate: 0.03\n",
      "Epoch [510/20000], Bound: 1.4048690795898438, Entropy: 145.18055725097656, Temp: 0.6116556525230408, KL: 86.37193298339844, Loss: 0.9251868724822998, Learning Rate: 0.03\n",
      "Epoch [511/20000], Bound: 1.301274299621582, Entropy: 147.900146484375, Temp: 0.6119025349617004, KL: 69.50593566894531, Loss: 0.8597053289413452, Learning Rate: 0.03\n",
      "Epoch [512/20000], Bound: 1.3791648149490356, Entropy: 146.4279327392578, Temp: 0.611998438835144, KL: 85.68663024902344, Loss: 0.8784708380699158, Learning Rate: 0.03\n",
      "Epoch [513/20000], Bound: 1.3557610511779785, Entropy: 146.34066772460938, Temp: 0.6121630668640137, KL: 74.11921691894531, Loss: 0.9264497756958008, Learning Rate: 0.03\n",
      "Epoch [514/20000], Bound: 1.3782060146331787, Entropy: 150.20391845703125, Temp: 0.6121665239334106, KL: 80.09233093261719, Loss: 0.922534704208374, Learning Rate: 0.03\n",
      "Epoch [515/20000], Bound: 1.3202509880065918, Entropy: 147.58399963378906, Temp: 0.612113893032074, KL: 70.09341430664062, Loss: 0.89070063829422, Learning Rate: 0.03\n",
      "Epoch [516/20000], Bound: 1.3672761917114258, Entropy: 147.80093383789062, Temp: 0.6119077801704407, KL: 82.68505859375, Loss: 0.8788904547691345, Learning Rate: 0.03\n",
      "Epoch [517/20000], Bound: 1.3751842975616455, Entropy: 145.55613708496094, Temp: 0.6117591261863708, KL: 78.17915344238281, Loss: 0.9313291907310486, Learning Rate: 0.03\n",
      "Epoch [518/20000], Bound: 1.4345407485961914, Entropy: 146.28286743164062, Temp: 0.6115335822105408, KL: 91.89540100097656, Loss: 0.9431202411651611, Learning Rate: 0.03\n",
      "Epoch [519/20000], Bound: 1.408400297164917, Entropy: 149.2174530029297, Temp: 0.6114116907119751, KL: 89.2696533203125, Loss: 0.9084164500236511, Learning Rate: 0.03\n",
      "Epoch [520/20000], Bound: 1.3415420055389404, Entropy: 147.0414581298828, Temp: 0.6113935708999634, KL: 76.61436462402344, Loss: 0.8770055770874023, Learning Rate: 0.03\n",
      "Epoch [521/20000], Bound: 1.3613181114196777, Entropy: 145.8419189453125, Temp: 0.6113336682319641, KL: 76.1539306640625, Loss: 0.9194173812866211, Learning Rate: 0.03\n",
      "Epoch [522/20000], Bound: 1.3700050115585327, Entropy: 140.54058837890625, Temp: 0.6111758947372437, KL: 84.45553588867188, Loss: 0.8685439229011536, Learning Rate: 0.03\n",
      "Epoch [523/20000], Bound: 1.3817752599716187, Entropy: 140.88485717773438, Temp: 0.6111128330230713, KL: 81.58544921875, Loss: 0.9156574010848999, Learning Rate: 0.03\n",
      "Epoch [524/20000], Bound: 1.341641902923584, Entropy: 142.0098419189453, Temp: 0.6110349297523499, KL: 76.13949584960938, Loss: 0.8804937601089478, Learning Rate: 0.03\n",
      "Epoch [525/20000], Bound: 1.3611398935317993, Entropy: 142.0199432373047, Temp: 0.6109114289283752, KL: 78.93382263183594, Loss: 0.8955953121185303, Learning Rate: 0.03\n",
      "Epoch [526/20000], Bound: 1.307159185409546, Entropy: 143.25889587402344, Temp: 0.61076819896698, KL: 67.80548095703125, Loss: 0.8828871250152588, Learning Rate: 0.03\n",
      "Epoch [527/20000], Bound: 1.3323757648468018, Entropy: 142.1057891845703, Temp: 0.6104606986045837, KL: 68.11424255371094, Loss: 0.927458643913269, Learning Rate: 0.03\n",
      "Epoch [528/20000], Bound: 1.3277041912078857, Entropy: 144.6058807373047, Temp: 0.6099556088447571, KL: 73.98284912109375, Loss: 0.8696820139884949, Learning Rate: 0.03\n",
      "Epoch [529/20000], Bound: 1.3295689821243286, Entropy: 146.40536499023438, Temp: 0.6094350814819336, KL: 71.9921875, Loss: 0.8887392282485962, Learning Rate: 0.03\n",
      "Epoch [530/20000], Bound: 1.3763319253921509, Entropy: 144.23287963867188, Temp: 0.6088495254516602, KL: 77.2354736328125, Loss: 0.9362350106239319, Learning Rate: 0.03\n",
      "Epoch [531/20000], Bound: 1.358144998550415, Entropy: 148.8563995361328, Temp: 0.6082247495651245, KL: 75.765625, Loss: 0.9110691547393799, Learning Rate: 0.03\n",
      "Epoch [532/20000], Bound: 1.3408979177474976, Entropy: 151.51596069335938, Temp: 0.6075782179832458, KL: 70.14167785644531, Loss: 0.9227285385131836, Learning Rate: 0.03\n",
      "Epoch [533/20000], Bound: 1.3206613063812256, Entropy: 148.72056579589844, Temp: 0.6068180203437805, KL: 67.15495300292969, Loss: 0.9076983332633972, Learning Rate: 0.03\n",
      "Epoch [534/20000], Bound: 1.3952438831329346, Entropy: 149.04400634765625, Temp: 0.6059327125549316, KL: 82.93803405761719, Loss: 0.922330915927887, Learning Rate: 0.03\n",
      "Epoch [535/20000], Bound: 1.4164056777954102, Entropy: 148.69448852539062, Temp: 0.6051523089408875, KL: 89.8406982421875, Loss: 0.9076979756355286, Learning Rate: 0.03\n",
      "Epoch [536/20000], Bound: 1.3834657669067383, Entropy: 149.17958068847656, Temp: 0.6045850515365601, KL: 82.44148254394531, Loss: 0.8999493718147278, Learning Rate: 0.03\n",
      "Epoch [537/20000], Bound: 1.3874866962432861, Entropy: 146.05726623535156, Temp: 0.6041193008422852, KL: 82.39710998535156, Loss: 0.9075637459754944, Learning Rate: 0.03\n",
      "Epoch [538/20000], Bound: 1.360964298248291, Entropy: 141.64503479003906, Temp: 0.6037371754646301, KL: 79.34597778320312, Loss: 0.8793161511421204, Learning Rate: 0.03\n",
      "Epoch [539/20000], Bound: 1.3782436847686768, Entropy: 140.59532165527344, Temp: 0.6034243702888489, KL: 81.87213134765625, Loss: 0.8920225501060486, Learning Rate: 0.03\n",
      "Epoch [540/20000], Bound: 1.3297117948532104, Entropy: 140.2058868408203, Temp: 0.6031960844993591, KL: 72.69430541992188, Loss: 0.8734194040298462, Learning Rate: 0.03\n",
      "Epoch [541/20000], Bound: 1.3437987565994263, Entropy: 135.5301971435547, Temp: 0.6029337048530579, KL: 76.96247863769531, Loss: 0.8644111156463623, Learning Rate: 0.03\n",
      "Epoch [542/20000], Bound: 1.3776273727416992, Entropy: 132.94273376464844, Temp: 0.6027174592018127, KL: 82.59207153320312, Loss: 0.8835127949714661, Learning Rate: 0.03\n",
      "Epoch [543/20000], Bound: 1.3181787729263306, Entropy: 134.43069458007812, Temp: 0.6026011109352112, KL: 63.46826171875, Loss: 0.9274135231971741, Learning Rate: 0.03\n",
      "Epoch [544/20000], Bound: 1.4107577800750732, Entropy: 134.04847717285156, Temp: 0.6022324562072754, KL: 92.57640075683594, Loss: 0.8671290874481201, Learning Rate: 0.03\n",
      "Epoch [545/20000], Bound: 1.3378838300704956, Entropy: 133.67478942871094, Temp: 0.6021435856819153, KL: 73.23542785644531, Loss: 0.8827357292175293, Learning Rate: 0.03\n",
      "Epoch [546/20000], Bound: 1.3451424837112427, Entropy: 134.4666290283203, Temp: 0.6020081639289856, KL: 77.11221313476562, Loss: 0.8641860485076904, Learning Rate: 0.03\n",
      "Epoch [547/20000], Bound: 1.3528398275375366, Entropy: 139.95220947265625, Temp: 0.6019134521484375, KL: 73.84233093261719, Loss: 0.9060197472572327, Learning Rate: 0.03\n",
      "Epoch [548/20000], Bound: 1.3085838556289673, Entropy: 136.65150451660156, Temp: 0.6017533540725708, KL: 64.08612060546875, Loss: 0.9033158421516418, Learning Rate: 0.03\n",
      "Epoch [549/20000], Bound: 1.361656665802002, Entropy: 137.8558807373047, Temp: 0.6013879776000977, KL: 88.18521118164062, Loss: 0.8030350208282471, Learning Rate: 0.03\n",
      "Epoch [550/20000], Bound: 1.3240224123001099, Entropy: 140.7069854736328, Temp: 0.6013320088386536, KL: 67.40811157226562, Loss: 0.9037362337112427, Learning Rate: 0.03\n",
      "Epoch [551/20000], Bound: 1.349115014076233, Entropy: 142.1446990966797, Temp: 0.60111403465271, KL: 78.98066711425781, Loss: 0.85476154088974, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [552/20000], Bound: 1.3557027578353882, Entropy: 142.9054718017578, Temp: 0.6009897589683533, KL: 73.1053466796875, Loss: 0.9161536693572998, Learning Rate: 0.03\n",
      "Epoch [553/20000], Bound: 1.3451597690582275, Entropy: 143.64857482910156, Temp: 0.6007834672927856, KL: 75.66131591796875, Loss: 0.8742227554321289, Learning Rate: 0.03\n",
      "Epoch [554/20000], Bound: 1.4422283172607422, Entropy: 142.5390625, Temp: 0.6005966663360596, KL: 98.94577026367188, Loss: 0.8771370649337769, Learning Rate: 0.03\n",
      "Epoch [555/20000], Bound: 1.3654145002365112, Entropy: 141.5489044189453, Temp: 0.6007558703422546, KL: 78.606201171875, Loss: 0.8889157772064209, Learning Rate: 0.03\n",
      "Epoch [556/20000], Bound: 1.337958574295044, Entropy: 141.20326232910156, Temp: 0.6009235978126526, KL: 76.27464294433594, Loss: 0.8556161522865295, Learning Rate: 0.03\n",
      "Epoch [557/20000], Bound: 1.4009013175964355, Entropy: 137.4226531982422, Temp: 0.6011064648628235, KL: 89.77580261230469, Loss: 0.8677464723587036, Learning Rate: 0.03\n",
      "Epoch [558/20000], Bound: 1.3759111166000366, Entropy: 136.3935089111328, Temp: 0.6014831066131592, KL: 81.0323486328125, Loss: 0.8907680511474609, Learning Rate: 0.03\n",
      "Epoch [559/20000], Bound: 1.3303097486495972, Entropy: 134.42611694335938, Temp: 0.6018763780593872, KL: 74.39665222167969, Loss: 0.8583118319511414, Learning Rate: 0.03\n",
      "Epoch [560/20000], Bound: 1.3082913160324097, Entropy: 133.4228515625, Temp: 0.6022259593009949, KL: 73.39683532714844, Loss: 0.8261442184448242, Learning Rate: 0.03\n",
      "Epoch [561/20000], Bound: 1.3163801431655884, Entropy: 133.03514099121094, Temp: 0.6025612950325012, KL: 72.85780334472656, Loss: 0.846096932888031, Learning Rate: 0.03\n",
      "Epoch [562/20000], Bound: 1.3647668361663818, Entropy: 132.95912170410156, Temp: 0.6028474569320679, KL: 80.55010986328125, Loss: 0.8752190470695496, Learning Rate: 0.03\n",
      "Epoch [563/20000], Bound: 1.3845832347869873, Entropy: 135.6920623779297, Temp: 0.6031656265258789, KL: 86.51914978027344, Loss: 0.865740180015564, Learning Rate: 0.03\n",
      "Epoch [564/20000], Bound: 1.4127309322357178, Entropy: 135.5355682373047, Temp: 0.6036108732223511, KL: 87.43060302734375, Loss: 0.9167664647102356, Learning Rate: 0.03\n",
      "Epoch [565/20000], Bound: 1.4316951036453247, Entropy: 137.3871612548828, Temp: 0.6041136384010315, KL: 93.60520935058594, Loss: 0.9067729711532593, Learning Rate: 0.03\n",
      "Epoch [566/20000], Bound: 1.3029508590698242, Entropy: 141.15663146972656, Temp: 0.6047657132148743, KL: 65.23538208007812, Loss: 0.8876962065696716, Learning Rate: 0.03\n",
      "Epoch [567/20000], Bound: 1.3780993223190308, Entropy: 140.4423370361328, Temp: 0.6051514148712158, KL: 81.83695983886719, Loss: 0.8952114582061768, Learning Rate: 0.03\n",
      "Epoch [568/20000], Bound: 1.307606816291809, Entropy: 142.50868225097656, Temp: 0.605539858341217, KL: 66.94172668457031, Loss: 0.8832770586013794, Learning Rate: 0.03\n",
      "Epoch [569/20000], Bound: 1.4223798513412476, Entropy: 139.83322143554688, Temp: 0.6057166457176208, KL: 90.15577697753906, Loss: 0.9188878536224365, Learning Rate: 0.03\n",
      "Epoch [570/20000], Bound: 1.3953372240066528, Entropy: 143.1734161376953, Temp: 0.6060022115707397, KL: 85.88255310058594, Loss: 0.8983607292175293, Learning Rate: 0.03\n",
      "Epoch [571/20000], Bound: 1.3152084350585938, Entropy: 139.4786376953125, Temp: 0.6063507199287415, KL: 71.17510986328125, Loss: 0.8636552095413208, Learning Rate: 0.03\n",
      "Epoch [572/20000], Bound: 1.3364523649215698, Entropy: 142.33888244628906, Temp: 0.6065794229507446, KL: 77.04833984375, Loss: 0.8557096123695374, Learning Rate: 0.03\n",
      "Epoch [573/20000], Bound: 1.290734887123108, Entropy: 139.70960998535156, Temp: 0.6068001985549927, KL: 65.29444885253906, Loss: 0.8677417039871216, Learning Rate: 0.03\n",
      "Epoch [574/20000], Bound: 1.3101990222930908, Entropy: 147.38360595703125, Temp: 0.6068127155303955, KL: 67.01124572753906, Loss: 0.8893433809280396, Learning Rate: 0.03\n",
      "Epoch [575/20000], Bound: 1.4042394161224365, Entropy: 142.81768798828125, Temp: 0.6066361665725708, KL: 92.05149841308594, Loss: 0.867120623588562, Learning Rate: 0.03\n",
      "Epoch [576/20000], Bound: 1.3639568090438843, Entropy: 145.6171875, Temp: 0.6066960096359253, KL: 78.24992370605469, Loss: 0.8994452953338623, Learning Rate: 0.03\n",
      "Epoch [577/20000], Bound: 1.3397095203399658, Entropy: 145.9677276611328, Temp: 0.6067235469818115, KL: 70.23075866699219, Loss: 0.9183707237243652, Learning Rate: 0.03\n",
      "Epoch [578/20000], Bound: 1.3982110023498535, Entropy: 144.67544555664062, Temp: 0.6065735220909119, KL: 85.09161376953125, Loss: 0.9119016528129578, Learning Rate: 0.03\n",
      "Epoch [579/20000], Bound: 1.3935352563858032, Entropy: 144.8306427001953, Temp: 0.6064974069595337, KL: 81.32682800292969, Loss: 0.9331908226013184, Learning Rate: 0.03\n",
      "Epoch [580/20000], Bound: 1.406590461730957, Entropy: 145.54745483398438, Temp: 0.6064047813415527, KL: 86.38671875, Loss: 0.9182347059249878, Learning Rate: 0.03\n",
      "Epoch [581/20000], Bound: 1.4348323345184326, Entropy: 143.25881958007812, Temp: 0.6063916683197021, KL: 94.13761901855469, Loss: 0.9141204357147217, Learning Rate: 0.03\n",
      "Epoch [582/20000], Bound: 1.4146863222122192, Entropy: 142.69090270996094, Temp: 0.6065663695335388, KL: 87.57548522949219, Loss: 0.9256905913352966, Learning Rate: 0.03\n",
      "Epoch [583/20000], Bound: 1.3717025518417358, Entropy: 139.8828887939453, Temp: 0.6068007349967957, KL: 77.85612487792969, Loss: 0.9182442426681519, Learning Rate: 0.03\n",
      "Epoch [584/20000], Bound: 1.3790208101272583, Entropy: 138.27818298339844, Temp: 0.6069536805152893, KL: 80.39236450195312, Loss: 0.9122808575630188, Learning Rate: 0.03\n",
      "Epoch [585/20000], Bound: 1.4161368608474731, Entropy: 140.09974670410156, Temp: 0.6070786118507385, KL: 90.60470581054688, Loss: 0.9048405289649963, Learning Rate: 0.03\n",
      "Epoch [586/20000], Bound: 1.386953592300415, Entropy: 136.42416381835938, Temp: 0.6073371767997742, KL: 78.67802429199219, Loss: 0.943141758441925, Learning Rate: 0.03\n",
      "Epoch [587/20000], Bound: 1.350034475326538, Entropy: 137.88246154785156, Temp: 0.6074883937835693, KL: 73.40151977539062, Loss: 0.91343092918396, Learning Rate: 0.03\n",
      "Epoch [588/20000], Bound: 1.38420832157135, Entropy: 137.16946411132812, Temp: 0.6075001955032349, KL: 84.76832580566406, Loss: 0.8877390623092651, Learning Rate: 0.03\n",
      "Epoch [589/20000], Bound: 1.337152361869812, Entropy: 135.95550537109375, Temp: 0.6075933575630188, KL: 76.64787292480469, Loss: 0.8620191812515259, Learning Rate: 0.03\n",
      "Epoch [590/20000], Bound: 1.336173415184021, Entropy: 139.96371459960938, Temp: 0.6076720356941223, KL: 77.78450012207031, Loss: 0.850921630859375, Learning Rate: 0.03\n",
      "Epoch [591/20000], Bound: 1.3482637405395508, Entropy: 139.8881378173828, Temp: 0.6077700257301331, KL: 75.2542724609375, Loss: 0.895207941532135, Learning Rate: 0.03\n",
      "Epoch [592/20000], Bound: 1.4076564311981201, Entropy: 142.18751525878906, Temp: 0.6077852249145508, KL: 86.77262878417969, Loss: 0.9200356602668762, Learning Rate: 0.03\n",
      "Epoch [593/20000], Bound: 1.3861007690429688, Entropy: 138.81866455078125, Temp: 0.6078656315803528, KL: 82.09339904785156, Loss: 0.9142767786979675, Learning Rate: 0.03\n",
      "Epoch [594/20000], Bound: 1.4042000770568848, Entropy: 139.8969268798828, Temp: 0.6079429388046265, KL: 86.24822998046875, Loss: 0.9174609184265137, Learning Rate: 0.03\n",
      "Epoch [595/20000], Bound: 1.4088921546936035, Entropy: 135.76292419433594, Temp: 0.608074426651001, KL: 88.88523864746094, Loss: 0.9058260917663574, Learning Rate: 0.03\n",
      "Epoch [596/20000], Bound: 1.3261568546295166, Entropy: 140.90164184570312, Temp: 0.6083086133003235, KL: 65.94854736328125, Loss: 0.9301847815513611, Learning Rate: 0.03\n",
      "Epoch [597/20000], Bound: 1.3810877799987793, Entropy: 143.4144744873047, Temp: 0.608248233795166, KL: 85.25392150878906, Loss: 0.8788443803787231, Learning Rate: 0.03\n",
      "Epoch [598/20000], Bound: 1.3836370706558228, Entropy: 143.00840759277344, Temp: 0.6082925796508789, KL: 83.59776306152344, Loss: 0.8977030515670776, Learning Rate: 0.03\n",
      "Epoch [599/20000], Bound: 1.3108302354812622, Entropy: 138.91563415527344, Temp: 0.6083802580833435, KL: 69.74362182617188, Loss: 0.8703277111053467, Learning Rate: 0.03\n",
      "Epoch [600/20000], Bound: 1.4027222394943237, Entropy: 142.34194946289062, Temp: 0.6083288192749023, KL: 89.9598388671875, Loss: 0.8846464157104492, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [601/20000], Bound: 1.372441053390503, Entropy: 140.84580993652344, Temp: 0.6084421873092651, KL: 82.77803039550781, Loss: 0.8821665644645691, Learning Rate: 0.03\n",
      "Epoch [602/20000], Bound: 1.3165581226348877, Entropy: 139.4628448486328, Temp: 0.6086007356643677, KL: 73.06292724609375, Loss: 0.8540814518928528, Learning Rate: 0.03\n",
      "Epoch [603/20000], Bound: 1.357848882675171, Entropy: 139.17152404785156, Temp: 0.6086872816085815, KL: 78.47085571289062, Loss: 0.8890466094017029, Learning Rate: 0.03\n",
      "Epoch [604/20000], Bound: 1.3555017709732056, Entropy: 138.01016235351562, Temp: 0.6087448000907898, KL: 74.1978759765625, Loss: 0.9196328520774841, Learning Rate: 0.03\n",
      "Epoch [605/20000], Bound: 1.3147943019866943, Entropy: 137.34628295898438, Temp: 0.6086674332618713, KL: 67.95913696289062, Loss: 0.892808735370636, Learning Rate: 0.03\n",
      "Epoch [606/20000], Bound: 1.3806411027908325, Entropy: 135.45614624023438, Temp: 0.6084052920341492, KL: 86.23255920410156, Loss: 0.8701956272125244, Learning Rate: 0.03\n",
      "Epoch [607/20000], Bound: 1.3844279050827026, Entropy: 135.94239807128906, Temp: 0.6082947254180908, KL: 86.0384521484375, Loss: 0.8792496919631958, Learning Rate: 0.03\n",
      "Epoch [608/20000], Bound: 1.3572381734848022, Entropy: 137.0478515625, Temp: 0.6083056926727295, KL: 77.10142517089844, Loss: 0.8984439373016357, Learning Rate: 0.03\n",
      "Epoch [609/20000], Bound: 1.3519929647445679, Entropy: 135.13421630859375, Temp: 0.6082630157470703, KL: 75.70841979980469, Loss: 0.899553656578064, Learning Rate: 0.03\n",
      "Epoch [610/20000], Bound: 1.345202922821045, Entropy: 138.30279541015625, Temp: 0.6081485748291016, KL: 76.97123718261719, Loss: 0.8757834434509277, Learning Rate: 0.03\n",
      "Epoch [611/20000], Bound: 1.3781565427780151, Entropy: 136.70501708984375, Temp: 0.608022928237915, KL: 87.70579528808594, Loss: 0.8523440361022949, Learning Rate: 0.03\n",
      "Epoch [612/20000], Bound: 1.3919603824615479, Entropy: 137.42120361328125, Temp: 0.6080854535102844, KL: 86.02078247070312, Loss: 0.8943510055541992, Learning Rate: 0.03\n",
      "Epoch [613/20000], Bound: 1.331992745399475, Entropy: 141.7500762939453, Temp: 0.6082327961921692, KL: 70.98124694824219, Loss: 0.8997892737388611, Learning Rate: 0.03\n",
      "Epoch [614/20000], Bound: 1.423475742340088, Entropy: 136.49472045898438, Temp: 0.6082134246826172, KL: 87.77055358886719, Loss: 0.9460787177085876, Learning Rate: 0.03\n",
      "Epoch [615/20000], Bound: 1.3748893737792969, Entropy: 139.15097045898438, Temp: 0.6082406044006348, KL: 77.16061401367188, Loss: 0.9328795671463013, Learning Rate: 0.03\n",
      "Epoch [616/20000], Bound: 1.4490647315979004, Entropy: 141.2882537841797, Temp: 0.6081660985946655, KL: 93.87791442871094, Loss: 0.9512550830841064, Learning Rate: 0.03\n",
      "Epoch [617/20000], Bound: 1.4527089595794678, Entropy: 142.36492919921875, Temp: 0.6082257032394409, KL: 97.36808776855469, Loss: 0.9307556748390198, Learning Rate: 0.03\n",
      "Epoch [618/20000], Bound: 1.4192701578140259, Entropy: 139.68386840820312, Temp: 0.6084846258163452, KL: 85.75566101074219, Loss: 0.9542492628097534, Learning Rate: 0.03\n",
      "Epoch [619/20000], Bound: 1.3647520542144775, Entropy: 140.17108154296875, Temp: 0.6087197065353394, KL: 76.92535400390625, Loss: 0.915428876876831, Learning Rate: 0.03\n",
      "Epoch [620/20000], Bound: 1.3703575134277344, Entropy: 139.87387084960938, Temp: 0.6088496446609497, KL: 77.37265014648438, Loss: 0.92313551902771, Learning Rate: 0.03\n",
      "Epoch [621/20000], Bound: 1.339084267616272, Entropy: 139.4970703125, Temp: 0.6088805198669434, KL: 71.70475769042969, Loss: 0.9084393382072449, Learning Rate: 0.03\n",
      "Epoch [622/20000], Bound: 1.3647985458374023, Entropy: 141.496826171875, Temp: 0.6087515354156494, KL: 81.68421936035156, Loss: 0.8764889240264893, Learning Rate: 0.03\n",
      "Epoch [623/20000], Bound: 1.3718137741088867, Entropy: 139.7829132080078, Temp: 0.6086821556091309, KL: 75.94485473632812, Loss: 0.9374810457229614, Learning Rate: 0.03\n",
      "Epoch [624/20000], Bound: 1.2959133386611938, Entropy: 140.00271606445312, Temp: 0.6084917783737183, KL: 65.27275085449219, Loss: 0.8796986937522888, Learning Rate: 0.03\n",
      "Epoch [625/20000], Bound: 1.4143346548080444, Entropy: 140.92945861816406, Temp: 0.6080986857414246, KL: 87.14218139648438, Loss: 0.9316344857215881, Learning Rate: 0.03\n",
      "Epoch [626/20000], Bound: 1.3658652305603027, Entropy: 139.37583923339844, Temp: 0.607801079750061, KL: 74.56864929199219, Loss: 0.9354361295700073, Learning Rate: 0.03\n",
      "Epoch [627/20000], Bound: 1.3866099119186401, Entropy: 141.7413787841797, Temp: 0.607390820980072, KL: 76.41079711914062, Loss: 0.9612044095993042, Learning Rate: 0.03\n",
      "Epoch [628/20000], Bound: 1.3727587461471558, Entropy: 144.4032440185547, Temp: 0.6068757176399231, KL: 77.72821044921875, Loss: 0.9215379357337952, Learning Rate: 0.03\n",
      "Epoch [629/20000], Bound: 1.3541258573532104, Entropy: 146.26576232910156, Temp: 0.6063442230224609, KL: 73.6580810546875, Loss: 0.9174028038978577, Learning Rate: 0.03\n",
      "Epoch [630/20000], Bound: 1.332651972770691, Entropy: 145.17381286621094, Temp: 0.6057414412498474, KL: 66.4588623046875, Loss: 0.9344926476478577, Learning Rate: 0.03\n",
      "Epoch [631/20000], Bound: 1.3770707845687866, Entropy: 145.21083068847656, Temp: 0.6049365401268005, KL: 74.7750244140625, Loss: 0.9511252641677856, Learning Rate: 0.03\n",
      "Epoch [632/20000], Bound: 1.3944180011749268, Entropy: 144.6885528564453, Temp: 0.604067862033844, KL: 78.761474609375, Loss: 0.9516474604606628, Learning Rate: 0.03\n",
      "Epoch [633/20000], Bound: 1.3825960159301758, Entropy: 147.59927368164062, Temp: 0.6032092571258545, KL: 78.21865844726562, Loss: 0.9306300282478333, Learning Rate: 0.03\n",
      "Epoch [634/20000], Bound: 1.384579062461853, Entropy: 144.419921875, Temp: 0.6023848056793213, KL: 79.19358825683594, Loss: 0.9250264763832092, Learning Rate: 0.03\n",
      "Epoch [635/20000], Bound: 1.3766603469848633, Entropy: 144.10752868652344, Temp: 0.6016189455986023, KL: 77.37196350097656, Loss: 0.9229287505149841, Learning Rate: 0.03\n",
      "Epoch [636/20000], Bound: 1.39059317111969, Entropy: 142.02023315429688, Temp: 0.600883960723877, KL: 82.94265747070312, Loss: 0.9031862616539001, Learning Rate: 0.03\n",
      "Epoch [637/20000], Bound: 1.3561824560165405, Entropy: 140.65975952148438, Temp: 0.600296676158905, KL: 75.31858825683594, Loss: 0.8974928259849548, Learning Rate: 0.03\n",
      "Epoch [638/20000], Bound: 1.386834979057312, Entropy: 140.25125122070312, Temp: 0.5997314453125, KL: 80.71830749511719, Loss: 0.9119641780853271, Learning Rate: 0.03\n",
      "Epoch [639/20000], Bound: 1.4122896194458008, Entropy: 139.88002014160156, Temp: 0.5992562174797058, KL: 82.27403259277344, Loss: 0.9499755501747131, Learning Rate: 0.03\n",
      "Epoch [640/20000], Bound: 1.3838716745376587, Entropy: 135.2270050048828, Temp: 0.5988372564315796, KL: 77.92596435546875, Loss: 0.9276742339134216, Learning Rate: 0.03\n",
      "Epoch [641/20000], Bound: 1.3210406303405762, Entropy: 140.22596740722656, Temp: 0.5984326601028442, KL: 63.4599609375, Loss: 0.9268105626106262, Learning Rate: 0.03\n",
      "Epoch [642/20000], Bound: 1.3670636415481567, Entropy: 140.0634307861328, Temp: 0.5978029370307922, KL: 71.48619079589844, Loss: 0.9464463591575623, Learning Rate: 0.03\n",
      "Epoch [643/20000], Bound: 1.3460631370544434, Entropy: 143.05764770507812, Temp: 0.5970839858055115, KL: 70.06207275390625, Loss: 0.9166234135627747, Learning Rate: 0.03\n",
      "Epoch [644/20000], Bound: 1.3393175601959229, Entropy: 141.5399169921875, Temp: 0.5963051319122314, KL: 75.27076721191406, Loss: 0.858894944190979, Learning Rate: 0.03\n",
      "Epoch [645/20000], Bound: 1.3316559791564941, Entropy: 141.38870239257812, Temp: 0.5956428647041321, KL: 62.8565673828125, Loss: 0.9475918412208557, Learning Rate: 0.03\n",
      "Epoch [646/20000], Bound: 1.3743350505828857, Entropy: 145.6933135986328, Temp: 0.5947585105895996, KL: 73.05191040039062, Loss: 0.942359209060669, Learning Rate: 0.03\n",
      "Epoch [647/20000], Bound: 1.3633159399032593, Entropy: 147.033203125, Temp: 0.5938593745231628, KL: 75.52166748046875, Loss: 0.8985331654548645, Learning Rate: 0.03\n",
      "Epoch [648/20000], Bound: 1.344832420349121, Entropy: 148.9683074951172, Temp: 0.5930518507957458, KL: 73.45777893066406, Loss: 0.8791587352752686, Learning Rate: 0.03\n",
      "Epoch [649/20000], Bound: 1.3857896327972412, Entropy: 151.3675537109375, Temp: 0.5923237800598145, KL: 78.75306701660156, Loss: 0.9125566482543945, Learning Rate: 0.03\n",
      "Epoch [650/20000], Bound: 1.3611112833023071, Entropy: 149.21615600585938, Temp: 0.5917129516601562, KL: 74.79118347167969, Loss: 0.8967120051383972, Learning Rate: 0.03\n",
      "Epoch [651/20000], Bound: 1.420790672302246, Entropy: 146.84776306152344, Temp: 0.5911674499511719, KL: 85.88691711425781, Loss: 0.9207930564880371, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [652/20000], Bound: 1.4290482997894287, Entropy: 146.39376831054688, Temp: 0.5908309817314148, KL: 83.80471801757812, Loss: 0.9548307657241821, Learning Rate: 0.03\n",
      "Epoch [653/20000], Bound: 1.3259048461914062, Entropy: 147.4999542236328, Temp: 0.5906055569648743, KL: 69.5284423828125, Loss: 0.8730016350746155, Learning Rate: 0.03\n",
      "Epoch [654/20000], Bound: 1.4218488931655884, Entropy: 144.55224609375, Temp: 0.5903562903404236, KL: 84.189453125, Loss: 0.9356644153594971, Learning Rate: 0.03\n",
      "Epoch [655/20000], Bound: 1.3773739337921143, Entropy: 143.98956298828125, Temp: 0.5902443528175354, KL: 79.99118041992188, Loss: 0.8816471099853516, Learning Rate: 0.03\n",
      "Epoch [656/20000], Bound: 1.4181890487670898, Entropy: 143.60723876953125, Temp: 0.5902635455131531, KL: 86.41014099121094, Loss: 0.9091285467147827, Learning Rate: 0.03\n",
      "Epoch [657/20000], Bound: 1.3659688234329224, Entropy: 141.77862548828125, Temp: 0.5904659032821655, KL: 74.82394409179688, Loss: 0.9036329388618469, Learning Rate: 0.03\n",
      "Epoch [658/20000], Bound: 1.393864631652832, Entropy: 138.0951690673828, Temp: 0.5906509757041931, KL: 80.45504760742188, Loss: 0.9110787510871887, Learning Rate: 0.03\n",
      "Epoch [659/20000], Bound: 1.3627078533172607, Entropy: 139.76437377929688, Temp: 0.5909024477005005, KL: 72.9228515625, Loss: 0.9141897559165955, Learning Rate: 0.03\n",
      "Epoch [660/20000], Bound: 1.3847413063049316, Entropy: 139.3128204345703, Temp: 0.591082751750946, KL: 78.558349609375, Loss: 0.9098214507102966, Learning Rate: 0.03\n",
      "Epoch [661/20000], Bound: 1.3910224437713623, Entropy: 136.77142333984375, Temp: 0.5912981033325195, KL: 77.87969970703125, Loss: 0.928423285484314, Learning Rate: 0.03\n",
      "Epoch [662/20000], Bound: 1.3423900604248047, Entropy: 136.72781372070312, Temp: 0.5915071368217468, KL: 70.75094604492188, Loss: 0.8948582410812378, Learning Rate: 0.03\n",
      "Epoch [663/20000], Bound: 1.3686212301254272, Entropy: 137.87652587890625, Temp: 0.5916348695755005, KL: 80.11476135253906, Loss: 0.8661038279533386, Learning Rate: 0.03\n",
      "Epoch [664/20000], Bound: 1.3899978399276733, Entropy: 138.665771484375, Temp: 0.5918866991996765, KL: 79.5753173828125, Loss: 0.9131559729576111, Learning Rate: 0.03\n",
      "Epoch [665/20000], Bound: 1.3831753730773926, Entropy: 138.29931640625, Temp: 0.5921741127967834, KL: 76.11076354980469, Loss: 0.9294145107269287, Learning Rate: 0.03\n",
      "Epoch [666/20000], Bound: 1.4276152849197388, Entropy: 138.61175537109375, Temp: 0.5924121737480164, KL: 87.44046020507812, Loss: 0.9244391322135925, Learning Rate: 0.03\n",
      "Epoch [667/20000], Bound: 1.465920329093933, Entropy: 140.62928771972656, Temp: 0.5927954912185669, KL: 98.92561340332031, Loss: 0.9102662205696106, Learning Rate: 0.03\n",
      "Epoch [668/20000], Bound: 1.3448762893676758, Entropy: 138.3419647216797, Temp: 0.5935040712356567, KL: 69.83932495117188, Loss: 0.910477876663208, Learning Rate: 0.03\n",
      "Epoch [669/20000], Bound: 1.4212031364440918, Entropy: 135.8450164794922, Temp: 0.5940331816673279, KL: 89.86863708496094, Loss: 0.8940821886062622, Learning Rate: 0.03\n",
      "Epoch [670/20000], Bound: 1.3806350231170654, Entropy: 135.25653076171875, Temp: 0.5947505235671997, KL: 76.04582214355469, Loss: 0.9295914173126221, Learning Rate: 0.03\n",
      "Epoch [671/20000], Bound: 1.4142661094665527, Entropy: 132.8197479248047, Temp: 0.5953596234321594, KL: 89.32479858398438, Loss: 0.8871081471443176, Learning Rate: 0.03\n",
      "Epoch [672/20000], Bound: 1.424123764038086, Entropy: 127.8797607421875, Temp: 0.5961427688598633, KL: 87.9832763671875, Loss: 0.9204729199409485, Learning Rate: 0.03\n",
      "Epoch [673/20000], Bound: 1.3676797151565552, Entropy: 129.38726806640625, Temp: 0.5970099568367004, KL: 80.22311401367188, Loss: 0.8731405735015869, Learning Rate: 0.03\n",
      "Epoch [674/20000], Bound: 1.3312311172485352, Entropy: 126.45597076416016, Temp: 0.5978901982307434, KL: 71.6209945678711, Loss: 0.8768005967140198, Learning Rate: 0.03\n",
      "Epoch [675/20000], Bound: 1.4523180723190308, Entropy: 128.420166015625, Temp: 0.5986267924308777, KL: 96.91494750976562, Loss: 0.9113427996635437, Learning Rate: 0.03\n",
      "Epoch [676/20000], Bound: 1.3917405605316162, Entropy: 133.40232849121094, Temp: 0.5995877981185913, KL: 84.67378234863281, Loss: 0.8885897397994995, Learning Rate: 0.03\n",
      "Epoch [677/20000], Bound: 1.362023949623108, Entropy: 130.54075622558594, Temp: 0.6005880236625671, KL: 81.18826293945312, Loss: 0.8604891896247864, Learning Rate: 0.03\n",
      "Epoch [678/20000], Bound: 1.388744831085205, Entropy: 131.0441131591797, Temp: 0.60160231590271, KL: 80.37608337402344, Loss: 0.9221522212028503, Learning Rate: 0.03\n",
      "Epoch [679/20000], Bound: 1.359144687652588, Entropy: 133.302978515625, Temp: 0.6025206446647644, KL: 75.90541076660156, Loss: 0.902169406414032, Learning Rate: 0.03\n",
      "Epoch [680/20000], Bound: 1.3067283630371094, Entropy: 137.73477172851562, Temp: 0.6033012270927429, KL: 63.093505859375, Loss: 0.9103143811225891, Learning Rate: 0.03\n",
      "Epoch [681/20000], Bound: 1.3832508325576782, Entropy: 141.8929443359375, Temp: 0.6037185192108154, KL: 79.70181274414062, Loss: 0.9205862283706665, Learning Rate: 0.03\n",
      "Epoch [682/20000], Bound: 1.3720546960830688, Entropy: 141.9610137939453, Temp: 0.604076623916626, KL: 81.53732299804688, Loss: 0.8836597204208374, Learning Rate: 0.03\n",
      "Epoch [683/20000], Bound: 1.3617143630981445, Entropy: 148.41429138183594, Temp: 0.6044631600379944, KL: 77.82991027832031, Loss: 0.8946056962013245, Learning Rate: 0.03\n",
      "Epoch [684/20000], Bound: 1.3875480890274048, Entropy: 145.26182556152344, Temp: 0.604795515537262, KL: 80.75596618652344, Loss: 0.9225283861160278, Learning Rate: 0.03\n",
      "Epoch [685/20000], Bound: 1.3963128328323364, Entropy: 148.60598754882812, Temp: 0.6050850749015808, KL: 82.0513916015625, Loss: 0.9302262663841248, Learning Rate: 0.03\n",
      "Epoch [686/20000], Bound: 1.3405702114105225, Entropy: 152.2103729248047, Temp: 0.6053442358970642, KL: 69.34263610839844, Loss: 0.9251912236213684, Learning Rate: 0.03\n",
      "Epoch [687/20000], Bound: 1.4110318422317505, Entropy: 151.17092895507812, Temp: 0.6053676605224609, KL: 91.55238342285156, Loss: 0.88274747133255, Learning Rate: 0.03\n",
      "Epoch [688/20000], Bound: 1.4336392879486084, Entropy: 151.00836181640625, Temp: 0.6056070327758789, KL: 93.67573547363281, Loss: 0.91363525390625, Learning Rate: 0.03\n",
      "Epoch [689/20000], Bound: 1.4017658233642578, Entropy: 147.2500762939453, Temp: 0.6060264110565186, KL: 84.42277526855469, Loss: 0.9236825108528137, Learning Rate: 0.03\n",
      "Epoch [690/20000], Bound: 1.296923279762268, Entropy: 148.7877960205078, Temp: 0.6064448356628418, KL: 68.94659423828125, Loss: 0.8484108448028564, Learning Rate: 0.03\n",
      "Epoch [691/20000], Bound: 1.3739181756973267, Entropy: 145.9524688720703, Temp: 0.6067101359367371, KL: 73.66043090820312, Loss: 0.9570850133895874, Learning Rate: 0.03\n",
      "Epoch [692/20000], Bound: 1.3838540315628052, Entropy: 142.67308044433594, Temp: 0.6067590117454529, KL: 82.77261352539062, Loss: 0.9020626544952393, Learning Rate: 0.03\n",
      "Epoch [693/20000], Bound: 1.3865827322006226, Entropy: 138.18020629882812, Temp: 0.6068441867828369, KL: 80.053466796875, Loss: 0.9301572442054749, Learning Rate: 0.03\n",
      "Epoch [694/20000], Bound: 1.3248363733291626, Entropy: 135.4745330810547, Temp: 0.6068755984306335, KL: 71.13346862792969, Loss: 0.8828637599945068, Learning Rate: 0.03\n",
      "Epoch [695/20000], Bound: 1.398212194442749, Entropy: 136.30142211914062, Temp: 0.6067764163017273, KL: 85.63737487792969, Loss: 0.9078017473220825, Learning Rate: 0.03\n",
      "Epoch [696/20000], Bound: 1.3815685510635376, Entropy: 134.81076049804688, Temp: 0.6067663431167603, KL: 80.4501953125, Loss: 0.9165961742401123, Learning Rate: 0.03\n",
      "Epoch [697/20000], Bound: 1.3568166494369507, Entropy: 129.90179443359375, Temp: 0.6067387461662292, KL: 78.01710510253906, Loss: 0.8873956203460693, Learning Rate: 0.03\n",
      "Epoch [698/20000], Bound: 1.4030877351760864, Entropy: 133.85813903808594, Temp: 0.6066980957984924, KL: 84.52046203613281, Loss: 0.9269256591796875, Learning Rate: 0.03\n",
      "Epoch [699/20000], Bound: 1.3580892086029053, Entropy: 132.52056884765625, Temp: 0.6066949367523193, KL: 79.37762451171875, Loss: 0.8786014914512634, Learning Rate: 0.03\n",
      "Epoch [700/20000], Bound: 1.4167014360427856, Entropy: 137.0047149658203, Temp: 0.6067126989364624, KL: 85.19619750976562, Loss: 0.9498406648635864, Learning Rate: 0.03\n",
      "Epoch [701/20000], Bound: 1.3709821701049805, Entropy: 136.01634216308594, Temp: 0.6067391037940979, KL: 80.41206359863281, Loss: 0.8956370949745178, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [702/20000], Bound: 1.3754948377609253, Entropy: 139.25624084472656, Temp: 0.606775164604187, KL: 78.23573303222656, Loss: 0.9226493835449219, Learning Rate: 0.03\n",
      "Epoch [703/20000], Bound: 1.3706252574920654, Entropy: 141.09910583496094, Temp: 0.6067429184913635, KL: 83.56983947753906, Loss: 0.8689112067222595, Learning Rate: 0.03\n",
      "Epoch [704/20000], Bound: 1.3749676942825317, Entropy: 137.4356231689453, Temp: 0.6068187952041626, KL: 81.64871215820312, Loss: 0.8935497999191284, Learning Rate: 0.03\n",
      "Epoch [705/20000], Bound: 1.3621817827224731, Entropy: 140.56692504882812, Temp: 0.6069226264953613, KL: 77.43107604980469, Loss: 0.9030836224555969, Learning Rate: 0.03\n",
      "Epoch [706/20000], Bound: 1.3661994934082031, Entropy: 139.23912048339844, Temp: 0.6069657802581787, KL: 81.261474609375, Loss: 0.8795446753501892, Learning Rate: 0.03\n",
      "Epoch [707/20000], Bound: 1.3657052516937256, Entropy: 141.4318084716797, Temp: 0.6070539951324463, KL: 78.40541076660156, Loss: 0.9022476673126221, Learning Rate: 0.03\n",
      "Epoch [708/20000], Bound: 1.3893074989318848, Entropy: 141.16561889648438, Temp: 0.6071000099182129, KL: 83.85781860351562, Loss: 0.9048400521278381, Learning Rate: 0.03\n",
      "Epoch [709/20000], Bound: 1.3920753002166748, Entropy: 141.40371704101562, Temp: 0.6071949005126953, KL: 87.82369995117188, Loss: 0.8780149817466736, Learning Rate: 0.03\n",
      "Epoch [710/20000], Bound: 1.3781414031982422, Entropy: 140.9815673828125, Temp: 0.6074391007423401, KL: 83.02098083496094, Loss: 0.8897574543952942, Learning Rate: 0.03\n",
      "Epoch [711/20000], Bound: 1.404510498046875, Entropy: 139.566650390625, Temp: 0.6077197194099426, KL: 87.51866149902344, Loss: 0.9072118997573853, Learning Rate: 0.03\n",
      "Epoch [712/20000], Bound: 1.3615844249725342, Entropy: 140.6512451171875, Temp: 0.6080787181854248, KL: 73.99160766601562, Loss: 0.9321823120117188, Learning Rate: 0.03\n",
      "Epoch [713/20000], Bound: 1.3631800413131714, Entropy: 139.09617614746094, Temp: 0.6082419753074646, KL: 74.57508850097656, Loss: 0.930812656879425, Learning Rate: 0.03\n",
      "Epoch [714/20000], Bound: 1.3995808362960815, Entropy: 140.91763305664062, Temp: 0.6082398891448975, KL: 85.55958557128906, Loss: 0.9141278862953186, Learning Rate: 0.03\n",
      "Epoch [715/20000], Bound: 1.3832581043243408, Entropy: 140.40536499023438, Temp: 0.6082985401153564, KL: 78.7923583984375, Loss: 0.9364446997642517, Learning Rate: 0.03\n",
      "Epoch [716/20000], Bound: 1.3480024337768555, Entropy: 143.97764587402344, Temp: 0.6082655191421509, KL: 71.98902893066406, Loss: 0.9223617315292358, Learning Rate: 0.03\n",
      "Epoch [717/20000], Bound: 1.4273202419281006, Entropy: 145.4285888671875, Temp: 0.6080535650253296, KL: 88.34011840820312, Loss: 0.9492798447608948, Learning Rate: 0.03\n",
      "Epoch [718/20000], Bound: 1.3753774166107178, Entropy: 146.99172973632812, Temp: 0.6079167723655701, KL: 77.02677917480469, Loss: 0.9343886971473694, Learning Rate: 0.03\n",
      "Epoch [719/20000], Bound: 1.3652619123458862, Entropy: 146.56214904785156, Temp: 0.6076827645301819, KL: 76.72894287109375, Loss: 0.9162646532058716, Learning Rate: 0.03\n",
      "Epoch [720/20000], Bound: 1.3209638595581055, Entropy: 148.34619140625, Temp: 0.607384443283081, KL: 74.54145812988281, Loss: 0.8482963442802429, Learning Rate: 0.03\n",
      "Epoch [721/20000], Bound: 1.350714087486267, Entropy: 147.01077270507812, Temp: 0.6070954203605652, KL: 73.30194091796875, Loss: 0.9149287343025208, Learning Rate: 0.03\n",
      "Epoch [722/20000], Bound: 1.3931679725646973, Entropy: 144.77333068847656, Temp: 0.6066935062408447, KL: 82.09413146972656, Loss: 0.9264848232269287, Learning Rate: 0.03\n",
      "Epoch [723/20000], Bound: 1.4029828310012817, Entropy: 142.5319061279297, Temp: 0.6063259840011597, KL: 83.31254577636719, Loss: 0.9359408617019653, Learning Rate: 0.03\n",
      "Epoch [724/20000], Bound: 1.3257077932357788, Entropy: 143.4269256591797, Temp: 0.6059978604316711, KL: 66.30250549316406, Loss: 0.9230138659477234, Learning Rate: 0.03\n",
      "Epoch [725/20000], Bound: 1.345231294631958, Entropy: 139.4815216064453, Temp: 0.605430006980896, KL: 75.16067504882812, Loss: 0.8862389922142029, Learning Rate: 0.03\n",
      "Epoch [726/20000], Bound: 1.3722991943359375, Entropy: 144.05552673339844, Temp: 0.604862630367279, KL: 77.21232604980469, Loss: 0.9213292002677917, Learning Rate: 0.03\n",
      "Epoch [727/20000], Bound: 1.3842546939849854, Entropy: 139.79864501953125, Temp: 0.604282021522522, KL: 78.51849365234375, Loss: 0.9334323406219482, Learning Rate: 0.03\n",
      "Epoch [728/20000], Bound: 1.3408408164978027, Entropy: 139.6105499267578, Temp: 0.603697657585144, KL: 71.24005126953125, Loss: 0.9074187874794006, Learning Rate: 0.03\n",
      "Epoch [729/20000], Bound: 1.3887134790420532, Entropy: 138.26417541503906, Temp: 0.603024423122406, KL: 79.36592102050781, Loss: 0.933114767074585, Learning Rate: 0.03\n",
      "Epoch [730/20000], Bound: 1.404619574546814, Entropy: 139.37498474121094, Temp: 0.6023795008659363, KL: 86.61929321289062, Loss: 0.9041839241981506, Learning Rate: 0.03\n",
      "Epoch [731/20000], Bound: 1.372934341430664, Entropy: 138.7391357421875, Temp: 0.6019302606582642, KL: 74.70639038085938, Loss: 0.9382257461547852, Learning Rate: 0.03\n",
      "Epoch [732/20000], Bound: 1.3401918411254883, Entropy: 141.14022827148438, Temp: 0.6014050841331482, KL: 69.25592041015625, Loss: 0.9190198183059692, Learning Rate: 0.03\n",
      "Epoch [733/20000], Bound: 1.415250301361084, Entropy: 140.50634765625, Temp: 0.600745677947998, KL: 87.15391540527344, Loss: 0.918458104133606, Learning Rate: 0.03\n",
      "Epoch [734/20000], Bound: 1.3982958793640137, Entropy: 142.3367462158203, Temp: 0.6002813577651978, KL: 78.78860473632812, Loss: 0.9522629976272583, Learning Rate: 0.03\n",
      "Epoch [735/20000], Bound: 1.4106470346450806, Entropy: 140.76487731933594, Temp: 0.5998031497001648, KL: 84.33299255371094, Loss: 0.9304924011230469, Learning Rate: 0.03\n",
      "Epoch [736/20000], Bound: 1.4105767011642456, Entropy: 135.56353759765625, Temp: 0.5994428396224976, KL: 83.32716369628906, Loss: 0.9380173087120056, Learning Rate: 0.03\n",
      "Epoch [737/20000], Bound: 1.3713479042053223, Entropy: 135.26275634765625, Temp: 0.5991623401641846, KL: 80.1776123046875, Loss: 0.8846226334571838, Learning Rate: 0.03\n",
      "Epoch [738/20000], Bound: 1.3760825395584106, Entropy: 135.58154296875, Temp: 0.5989820957183838, KL: 75.65068054199219, Loss: 0.9314340949058533, Learning Rate: 0.03\n",
      "Epoch [739/20000], Bound: 1.398414969444275, Entropy: 137.66819763183594, Temp: 0.5987434387207031, KL: 79.04916381835938, Loss: 0.947437584400177, Learning Rate: 0.03\n",
      "Epoch [740/20000], Bound: 1.4116413593292236, Entropy: 136.75051879882812, Temp: 0.5984897017478943, KL: 85.75495910644531, Loss: 0.918039083480835, Learning Rate: 0.03\n",
      "Epoch [741/20000], Bound: 1.3943666219711304, Entropy: 138.19302368164062, Temp: 0.5983828902244568, KL: 81.31471252441406, Loss: 0.9196194410324097, Learning Rate: 0.03\n",
      "Epoch [742/20000], Bound: 1.3834837675094604, Entropy: 141.05059814453125, Temp: 0.5983308553695679, KL: 78.55549621582031, Loss: 0.9207151532173157, Learning Rate: 0.03\n",
      "Epoch [743/20000], Bound: 1.3500198125839233, Entropy: 140.33309936523438, Temp: 0.5982787609100342, KL: 74.61764526367188, Loss: 0.8880459666252136, Learning Rate: 0.03\n",
      "Epoch [744/20000], Bound: 1.3506102561950684, Entropy: 141.59085083007812, Temp: 0.5982061624526978, KL: 73.00816345214844, Loss: 0.9025097489356995, Learning Rate: 0.03\n",
      "Epoch [745/20000], Bound: 1.3568679094314575, Entropy: 144.81253051757812, Temp: 0.5980646014213562, KL: 75.27301025390625, Loss: 0.89539635181427, Learning Rate: 0.03\n",
      "Epoch [746/20000], Bound: 1.3212660551071167, Entropy: 147.1910858154297, Temp: 0.597913384437561, KL: 74.13531494140625, Loss: 0.8372056484222412, Learning Rate: 0.03\n",
      "Epoch [747/20000], Bound: 1.3959873914718628, Entropy: 146.13800048828125, Temp: 0.5978227853775024, KL: 85.308837890625, Loss: 0.8884242177009583, Learning Rate: 0.03\n",
      "Epoch [748/20000], Bound: 1.4031344652175903, Entropy: 147.4569549560547, Temp: 0.597905695438385, KL: 88.15776062011719, Loss: 0.8792967796325684, Learning Rate: 0.03\n",
      "Epoch [749/20000], Bound: 1.3312475681304932, Entropy: 150.13623046875, Temp: 0.5982068181037903, KL: 72.69517517089844, Loss: 0.8683560490608215, Learning Rate: 0.03\n",
      "Epoch [750/20000], Bound: 1.3062241077423096, Entropy: 144.9427947998047, Temp: 0.598447859287262, KL: 66.49453735351562, Loss: 0.8741793036460876, Learning Rate: 0.03\n",
      "Epoch [751/20000], Bound: 1.3356835842132568, Entropy: 144.9390869140625, Temp: 0.5985091328620911, KL: 75.95356750488281, Loss: 0.8499761819839478, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [752/20000], Bound: 1.3034353256225586, Entropy: 144.75706481933594, Temp: 0.5986196398735046, KL: 67.6121826171875, Loss: 0.8600034713745117, Learning Rate: 0.03\n",
      "Epoch [753/20000], Bound: 1.3802746534347534, Entropy: 145.28013610839844, Temp: 0.5986048579216003, KL: 79.09327697753906, Loss: 0.9103296995162964, Learning Rate: 0.03\n",
      "Epoch [754/20000], Bound: 1.355290174484253, Entropy: 143.88491821289062, Temp: 0.5986095666885376, KL: 76.00343322753906, Loss: 0.8871752619743347, Learning Rate: 0.03\n",
      "Epoch [755/20000], Bound: 1.3448090553283691, Entropy: 143.77426147460938, Temp: 0.5986123085021973, KL: 77.90321350097656, Loss: 0.8511880040168762, Learning Rate: 0.03\n",
      "Epoch [756/20000], Bound: 1.3930637836456299, Entropy: 146.12571716308594, Temp: 0.5987030267715454, KL: 83.86732482910156, Loss: 0.8962775468826294, Learning Rate: 0.03\n",
      "Epoch [757/20000], Bound: 1.3574225902557373, Entropy: 146.64111328125, Temp: 0.5989077091217041, KL: 82.54435729980469, Loss: 0.8372046947479248, Learning Rate: 0.03\n",
      "Epoch [758/20000], Bound: 1.3358154296875, Entropy: 145.31654357910156, Temp: 0.5992832779884338, KL: 75.71186828613281, Loss: 0.8535246253013611, Learning Rate: 0.03\n",
      "Epoch [759/20000], Bound: 1.441208004951477, Entropy: 144.8534393310547, Temp: 0.5996628403663635, KL: 93.92851257324219, Loss: 0.9146132469177246, Learning Rate: 0.03\n",
      "Epoch [760/20000], Bound: 1.3873616456985474, Entropy: 145.8856658935547, Temp: 0.6002633571624756, KL: 83.12849426269531, Loss: 0.8939411640167236, Learning Rate: 0.03\n",
      "Epoch [761/20000], Bound: 1.3465499877929688, Entropy: 142.53575134277344, Temp: 0.6009088158607483, KL: 71.54557800292969, Loss: 0.9113433361053467, Learning Rate: 0.03\n",
      "Epoch [762/20000], Bound: 1.3815839290618896, Entropy: 143.65234375, Temp: 0.6013562083244324, KL: 82.39756774902344, Loss: 0.8904956579208374, Learning Rate: 0.03\n",
      "Epoch [763/20000], Bound: 1.3605371713638306, Entropy: 137.89730834960938, Temp: 0.6018491983413696, KL: 73.98753356933594, Loss: 0.9196698665618896, Learning Rate: 0.03\n",
      "Epoch [764/20000], Bound: 1.3410941362380981, Entropy: 139.22496032714844, Temp: 0.602185070514679, KL: 67.60317993164062, Loss: 0.9356884360313416, Learning Rate: 0.03\n",
      "Epoch [765/20000], Bound: 1.3940367698669434, Entropy: 140.33993530273438, Temp: 0.6022348403930664, KL: 82.95498657226562, Loss: 0.9126498103141785, Learning Rate: 0.03\n",
      "Epoch [766/20000], Bound: 1.3748193979263306, Entropy: 137.55142211914062, Temp: 0.6023392081260681, KL: 76.63371276855469, Loss: 0.9266794919967651, Learning Rate: 0.03\n",
      "Epoch [767/20000], Bound: 1.4507975578308105, Entropy: 139.7709503173828, Temp: 0.6023585796356201, KL: 93.66310119628906, Loss: 0.9436944127082825, Learning Rate: 0.03\n",
      "Epoch [768/20000], Bound: 1.3695194721221924, Entropy: 139.9545440673828, Temp: 0.6025682091712952, KL: 73.76744079589844, Loss: 0.940363883972168, Learning Rate: 0.03\n",
      "Epoch [769/20000], Bound: 1.4145591259002686, Entropy: 141.7234649658203, Temp: 0.6026080846786499, KL: 83.53684997558594, Loss: 0.9508411884307861, Learning Rate: 0.03\n",
      "Epoch [770/20000], Bound: 1.351841926574707, Entropy: 142.557861328125, Temp: 0.602652370929718, KL: 68.16584777832031, Loss: 0.9524134397506714, Learning Rate: 0.03\n",
      "Epoch [771/20000], Bound: 1.3421489000320435, Entropy: 140.25790405273438, Temp: 0.6024209856987, KL: 67.17575073242188, Loss: 0.941617488861084, Learning Rate: 0.03\n",
      "Epoch [772/20000], Bound: 1.398476004600525, Entropy: 140.30239868164062, Temp: 0.601940393447876, KL: 79.54013061523438, Loss: 0.9495008587837219, Learning Rate: 0.03\n",
      "Epoch [773/20000], Bound: 1.375122308731079, Entropy: 144.67630004882812, Temp: 0.6014524698257446, KL: 72.91738891601562, Loss: 0.9566065073013306, Learning Rate: 0.03\n",
      "Epoch [774/20000], Bound: 1.3962433338165283, Entropy: 144.03488159179688, Temp: 0.6008312702178955, KL: 77.73806762695312, Loss: 0.957855761051178, Learning Rate: 0.03\n",
      "Epoch [775/20000], Bound: 1.4309271574020386, Entropy: 141.9569091796875, Temp: 0.6001793742179871, KL: 85.84213256835938, Loss: 0.9611721634864807, Learning Rate: 0.03\n",
      "Epoch [776/20000], Bound: 1.3241071701049805, Entropy: 142.77392578125, Temp: 0.5996412038803101, KL: 63.72882080078125, Loss: 0.9320303201675415, Learning Rate: 0.03\n",
      "Epoch [777/20000], Bound: 1.4077588319778442, Entropy: 142.95947265625, Temp: 0.5988500714302063, KL: 88.27157592773438, Loss: 0.8897498846054077, Learning Rate: 0.03\n",
      "Epoch [778/20000], Bound: 1.3327136039733887, Entropy: 142.49058532714844, Temp: 0.5983487367630005, KL: 69.41444396972656, Loss: 0.8987549543380737, Learning Rate: 0.03\n",
      "Epoch [779/20000], Bound: 1.369320034980774, Entropy: 141.68051147460938, Temp: 0.59775710105896, KL: 76.93637084960938, Loss: 0.9051997661590576, Learning Rate: 0.03\n",
      "Epoch [780/20000], Bound: 1.385925054550171, Entropy: 141.52029418945312, Temp: 0.5972173810005188, KL: 79.50447082519531, Loss: 0.9155991673469543, Learning Rate: 0.03\n",
      "Epoch [781/20000], Bound: 1.347138524055481, Entropy: 140.93252563476562, Temp: 0.5967583656311035, KL: 72.17848205566406, Loss: 0.9004184007644653, Learning Rate: 0.03\n",
      "Epoch [782/20000], Bound: 1.3816721439361572, Entropy: 138.98501586914062, Temp: 0.5962638258934021, KL: 80.2464599609375, Loss: 0.8991425037384033, Learning Rate: 0.03\n",
      "Epoch [783/20000], Bound: 1.350098729133606, Entropy: 143.62014770507812, Temp: 0.5958904027938843, KL: 72.54930114746094, Loss: 0.9015337824821472, Learning Rate: 0.03\n",
      "Epoch [784/20000], Bound: 1.3459465503692627, Entropy: 140.1989288330078, Temp: 0.5954835414886475, KL: 70.75788879394531, Loss: 0.9079828262329102, Learning Rate: 0.03\n",
      "Epoch [785/20000], Bound: 1.368791937828064, Entropy: 137.38824462890625, Temp: 0.595005452632904, KL: 73.70599365234375, Loss: 0.9264388680458069, Learning Rate: 0.03\n",
      "Epoch [786/20000], Bound: 1.3845620155334473, Entropy: 139.9184112548828, Temp: 0.5944934487342834, KL: 78.54705810546875, Loss: 0.9158751964569092, Learning Rate: 0.03\n",
      "Epoch [787/20000], Bound: 1.4111173152923584, Entropy: 135.10337829589844, Temp: 0.5940595865249634, KL: 86.80868530273438, Loss: 0.8990963697433472, Learning Rate: 0.03\n",
      "Epoch [788/20000], Bound: 1.4135085344314575, Entropy: 134.39527893066406, Temp: 0.5938724279403687, KL: 86.74201965332031, Loss: 0.9041794538497925, Learning Rate: 0.03\n",
      "Epoch [789/20000], Bound: 1.4373782873153687, Entropy: 133.49903869628906, Temp: 0.5938997864723206, KL: 95.5882568359375, Loss: 0.8795343637466431, Learning Rate: 0.03\n",
      "Epoch [790/20000], Bound: 1.3688424825668335, Entropy: 135.1574249267578, Temp: 0.5943108201026917, KL: 74.81678771972656, Loss: 0.9159900546073914, Learning Rate: 0.03\n",
      "Epoch [791/20000], Bound: 1.3895701169967651, Entropy: 133.9747772216797, Temp: 0.5946406126022339, KL: 78.67875671386719, Loss: 0.9250192046165466, Learning Rate: 0.03\n",
      "Epoch [792/20000], Bound: 1.423667073249817, Entropy: 135.30938720703125, Temp: 0.5949521660804749, KL: 86.78947448730469, Loss: 0.9270428419113159, Learning Rate: 0.03\n",
      "Epoch [793/20000], Bound: 1.3448435068130493, Entropy: 138.932373046875, Temp: 0.5953872203826904, KL: 69.0655517578125, Loss: 0.9199419021606445, Learning Rate: 0.03\n",
      "Epoch [794/20000], Bound: 1.4598907232284546, Entropy: 138.93878173828125, Temp: 0.5956166982650757, KL: 91.21087646484375, Loss: 0.9686563014984131, Learning Rate: 0.03\n",
      "Epoch [795/20000], Bound: 1.3648710250854492, Entropy: 139.73980712890625, Temp: 0.5959854125976562, KL: 72.40229797363281, Loss: 0.931422233581543, Learning Rate: 0.03\n",
      "Epoch [796/20000], Bound: 1.3970568180084229, Entropy: 139.7337188720703, Temp: 0.5961970090866089, KL: 78.42124938964844, Loss: 0.94514399766922, Learning Rate: 0.03\n",
      "Epoch [797/20000], Bound: 1.3476073741912842, Entropy: 140.12353515625, Temp: 0.5963562726974487, KL: 70.14642333984375, Loss: 0.9176903963088989, Learning Rate: 0.03\n",
      "Epoch [798/20000], Bound: 1.3901375532150269, Entropy: 140.75062561035156, Temp: 0.5963547229766846, KL: 79.68409729003906, Loss: 0.9209180474281311, Learning Rate: 0.03\n",
      "Epoch [799/20000], Bound: 1.4154411554336548, Entropy: 144.652099609375, Temp: 0.596381425857544, KL: 86.72262573242188, Loss: 0.9134909510612488, Learning Rate: 0.03\n",
      "Epoch [800/20000], Bound: 1.4260766506195068, Entropy: 143.34237670898438, Temp: 0.5965707302093506, KL: 84.99072265625, Loss: 0.9505450129508972, Learning Rate: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [801/20000], Bound: 1.3794211149215698, Entropy: 144.51536560058594, Temp: 0.5968173146247864, KL: 77.33515930175781, Loss: 0.9200927019119263, Learning Rate: 0.03\n",
      "Epoch [802/20000], Bound: 1.423020601272583, Entropy: 138.765869140625, Temp: 0.5970224738121033, KL: 86.08590698242188, Loss: 0.9359080195426941, Learning Rate: 0.03\n",
      "Epoch [803/20000], Bound: 1.3573880195617676, Entropy: 138.92677307128906, Temp: 0.597322404384613, KL: 71.35635375976562, Loss: 0.9279184341430664, Learning Rate: 0.03\n",
      "Epoch [804/20000], Bound: 1.3997060060501099, Entropy: 139.65402221679688, Temp: 0.5974481105804443, KL: 84.88088989257812, Loss: 0.8988084197044373, Learning Rate: 0.03\n",
      "Epoch [805/20000], Bound: 1.330750823020935, Entropy: 140.62437438964844, Temp: 0.5977109670639038, KL: 71.50738525390625, Loss: 0.8765650987625122, Learning Rate: 0.03\n",
      "Epoch [806/20000], Bound: 1.3842616081237793, Entropy: 136.60780334472656, Temp: 0.5978835225105286, KL: 81.68684387207031, Loss: 0.895260751247406, Learning Rate: 0.03\n",
      "Epoch [807/20000], Bound: 1.3557384014129639, Entropy: 133.57826232910156, Temp: 0.5981340408325195, KL: 76.82853698730469, Loss: 0.8803291320800781, Learning Rate: 0.03\n",
      "Epoch [808/20000], Bound: 1.3732284307479858, Entropy: 134.72998046875, Temp: 0.5983875393867493, KL: 78.77638244628906, Loss: 0.8986279368400574, Learning Rate: 0.03\n",
      "Epoch [809/20000], Bound: 1.3788918256759644, Entropy: 133.25523376464844, Temp: 0.5986491441726685, KL: 78.94169616699219, Loss: 0.9089283347129822, Learning Rate: 0.03\n",
      "Epoch [810/20000], Bound: 1.3459815979003906, Entropy: 135.9903106689453, Temp: 0.5989030003547668, KL: 74.83050537109375, Loss: 0.8795776963233948, Learning Rate: 0.03\n",
      "Epoch [811/20000], Bound: 1.33832848072052, Entropy: 134.68499755859375, Temp: 0.5991182327270508, KL: 78.80018615722656, Loss: 0.8322353959083557, Learning Rate: 0.03\n",
      "Epoch [812/20000], Bound: 1.4224021434783936, Entropy: 138.98611450195312, Temp: 0.5994486808776855, KL: 87.12246704101562, Loss: 0.9309890270233154, Learning Rate: 0.03\n",
      "Epoch [813/20000], Bound: 1.36862313747406, Entropy: 142.6006622314453, Temp: 0.5998719930648804, KL: 80.05879211425781, Loss: 0.8815361857414246, Learning Rate: 0.03\n",
      "Epoch [814/20000], Bound: 1.376596450805664, Entropy: 141.26171875, Temp: 0.6003283262252808, KL: 76.62556457519531, Loss: 0.9267117381095886, Learning Rate: 0.03\n",
      "Epoch [815/20000], Bound: 1.4214658737182617, Entropy: 147.43882751464844, Temp: 0.6006754636764526, KL: 90.46632385253906, Loss: 0.9037379026412964, Learning Rate: 0.03\n",
      "Epoch [816/20000], Bound: 1.309363842010498, Entropy: 146.74551391601562, Temp: 0.6012083888053894, KL: 62.78741455078125, Loss: 0.9147794842720032, Learning Rate: 0.03\n",
      "Epoch [817/20000], Bound: 1.3723561763763428, Entropy: 148.73460388183594, Temp: 0.6013696789741516, KL: 83.40591430664062, Loss: 0.8637763857841492, Learning Rate: 0.03\n",
      "Epoch [818/20000], Bound: 1.4181348085403442, Entropy: 151.7907257080078, Temp: 0.6016703844070435, KL: 86.73219299316406, Loss: 0.9298867583274841, Learning Rate: 0.03\n",
      "Epoch [819/20000], Bound: 1.3517961502075195, Entropy: 152.00416564941406, Temp: 0.6020470857620239, KL: 72.51997375488281, Loss: 0.9152053594589233, Learning Rate: 0.03\n",
      "Epoch [820/20000], Bound: 1.359076976776123, Entropy: 146.44595336914062, Temp: 0.6022513508796692, KL: 72.06375122070312, Loss: 0.9334703087806702, Learning Rate: 0.03\n",
      "Epoch [821/20000], Bound: 1.3413770198822021, Entropy: 152.72207641601562, Temp: 0.6022614240646362, KL: 71.74943542480469, Loss: 0.9019247889518738, Learning Rate: 0.03\n",
      "Epoch [822/20000], Bound: 1.3746131658554077, Entropy: 147.6251983642578, Temp: 0.6021403074264526, KL: 77.13143920898438, Loss: 0.9217851758003235, Learning Rate: 0.020999999999999998\n",
      "Epoch [823/20000], Bound: 1.4217932224273682, Entropy: 146.03985595703125, Temp: 0.6019720435142517, KL: 86.80027770996094, Loss: 0.937623143196106, Learning Rate: 0.020999999999999998\n",
      "Epoch [824/20000], Bound: 1.3973324298858643, Entropy: 146.1245880126953, Temp: 0.6019132137298584, KL: 85.18978881835938, Loss: 0.9001830816268921, Learning Rate: 0.020999999999999998\n",
      "Epoch [825/20000], Bound: 1.3687465190887451, Entropy: 147.18618774414062, Temp: 0.6019853353500366, KL: 74.70339965820312, Loss: 0.9300662279129028, Learning Rate: 0.020999999999999998\n",
      "Epoch [826/20000], Bound: 1.3917955160140991, Entropy: 143.10079956054688, Temp: 0.601932942867279, KL: 83.490234375, Loss: 0.9030745625495911, Learning Rate: 0.020999999999999998\n",
      "Epoch [827/20000], Bound: 1.3923262357711792, Entropy: 143.84326171875, Temp: 0.6019755005836487, KL: 87.92832946777344, Loss: 0.8673698902130127, Learning Rate: 0.020999999999999998\n",
      "Epoch [828/20000], Bound: 1.373032569885254, Entropy: 142.3642578125, Temp: 0.6022422909736633, KL: 77.94778442382812, Loss: 0.9120513200759888, Learning Rate: 0.020999999999999998\n",
      "Epoch [829/20000], Bound: 1.3884947299957275, Entropy: 141.85435485839844, Temp: 0.602453351020813, KL: 81.76963806152344, Loss: 0.9116678237915039, Learning Rate: 0.020999999999999998\n",
      "Epoch [830/20000], Bound: 1.3907924890518188, Entropy: 141.3807830810547, Temp: 0.6026843786239624, KL: 82.15776062011719, Loss: 0.9135343432426453, Learning Rate: 0.020999999999999998\n",
      "Epoch [831/20000], Bound: 1.35972261428833, Entropy: 143.2365264892578, Temp: 0.6029359698295593, KL: 78.9888916015625, Loss: 0.8784377574920654, Learning Rate: 0.020999999999999998\n",
      "Epoch [832/20000], Bound: 1.3696144819259644, Entropy: 139.79345703125, Temp: 0.6032031774520874, KL: 77.85235595703125, Loss: 0.9077779650688171, Learning Rate: 0.020999999999999998\n",
      "Epoch [833/20000], Bound: 1.3418587446212769, Entropy: 139.7030487060547, Temp: 0.603413462638855, KL: 70.93667602539062, Loss: 0.9114287495613098, Learning Rate: 0.020999999999999998\n",
      "Epoch [834/20000], Bound: 1.4111328125, Entropy: 141.28634643554688, Temp: 0.6034327149391174, KL: 86.07368469238281, Loss: 0.92431640625, Learning Rate: 0.020999999999999998\n",
      "Epoch [835/20000], Bound: 1.377231478691101, Entropy: 142.74203491210938, Temp: 0.603542149066925, KL: 82.30854797363281, Loss: 0.8866022229194641, Learning Rate: 0.020999999999999998\n",
      "Epoch [836/20000], Bound: 1.3865615129470825, Entropy: 141.32968139648438, Temp: 0.6037255525588989, KL: 82.60649108886719, Loss: 0.9032179117202759, Learning Rate: 0.020999999999999998\n",
      "Epoch [837/20000], Bound: 1.3761762380599976, Entropy: 143.8916015625, Temp: 0.6039526462554932, KL: 78.9710693359375, Loss: 0.9128844738006592, Learning Rate: 0.020999999999999998\n",
      "Epoch [838/20000], Bound: 1.3804062604904175, Entropy: 144.2306671142578, Temp: 0.6041340231895447, KL: 83.46464538574219, Loss: 0.884486973285675, Learning Rate: 0.020999999999999998\n",
      "Epoch [839/20000], Bound: 1.3836424350738525, Entropy: 140.69985961914062, Temp: 0.604403555393219, KL: 85.15281677246094, Loss: 0.8775358200073242, Learning Rate: 0.020999999999999998\n",
      "Epoch [840/20000], Bound: 1.297995686531067, Entropy: 139.36746215820312, Temp: 0.6047931909561157, KL: 63.864837646484375, Loss: 0.8899931311607361, Learning Rate: 0.020999999999999998\n",
      "Epoch [841/20000], Bound: 1.3951325416564941, Entropy: 137.90884399414062, Temp: 0.6048582792282104, KL: 84.55169677734375, Loss: 0.9067094326019287, Learning Rate: 0.020999999999999998\n",
      "Epoch [842/20000], Bound: 1.3476065397262573, Entropy: 135.61546325683594, Temp: 0.6050009727478027, KL: 74.27685546875, Loss: 0.8974169492721558, Learning Rate: 0.020999999999999998\n",
      "Epoch [843/20000], Bound: 1.3375613689422607, Entropy: 135.00885009765625, Temp: 0.6050354838371277, KL: 78.18035888671875, Loss: 0.8459173440933228, Learning Rate: 0.020999999999999998\n",
      "Epoch [844/20000], Bound: 1.3288441896438599, Entropy: 132.01963806152344, Temp: 0.6051331162452698, KL: 72.65313720703125, Loss: 0.8751816153526306, Learning Rate: 0.020999999999999998\n",
      "Epoch [845/20000], Bound: 1.2846529483795166, Entropy: 135.21055603027344, Temp: 0.6051314473152161, KL: 64.97084045410156, Loss: 0.8571261167526245, Learning Rate: 0.020999999999999998\n",
      "Epoch [846/20000], Bound: 1.3301141262054443, Entropy: 135.2807159423828, Temp: 0.6049174666404724, KL: 73.36045837402344, Loss: 0.8713981509208679, Learning Rate: 0.020999999999999998\n",
      "Epoch [847/20000], Bound: 1.3830803632736206, Entropy: 140.0910186767578, Temp: 0.604656457901001, KL: 84.26670837402344, Loss: 0.8842130899429321, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [848/20000], Bound: 1.306915044784546, Entropy: 136.8024139404297, Temp: 0.6045398116111755, KL: 70.01542663574219, Loss: 0.8551340103149414, Learning Rate: 0.020999999999999998\n",
      "Epoch [849/20000], Bound: 1.3079848289489746, Entropy: 138.73045349121094, Temp: 0.6043302416801453, KL: 62.73699951171875, Loss: 0.9170141220092773, Learning Rate: 0.020999999999999998\n",
      "Epoch [850/20000], Bound: 1.3388582468032837, Entropy: 140.38409423828125, Temp: 0.6037906408309937, KL: 83.60073852539062, Loss: 0.8014183044433594, Learning Rate: 0.020999999999999998\n",
      "Epoch [851/20000], Bound: 1.302032232284546, Entropy: 143.411376953125, Temp: 0.6035590767860413, KL: 69.46034240722656, Loss: 0.8493080139160156, Learning Rate: 0.020999999999999998\n",
      "Epoch [852/20000], Bound: 1.3255140781402588, Entropy: 141.52272033691406, Temp: 0.6032509207725525, KL: 67.17680358886719, Loss: 0.9113306999206543, Learning Rate: 0.020999999999999998\n",
      "Epoch [853/20000], Bound: 1.385924220085144, Entropy: 142.74424743652344, Temp: 0.6027281880378723, KL: 80.20271301269531, Loss: 0.9199941754341125, Learning Rate: 0.020999999999999998\n",
      "Epoch [854/20000], Bound: 1.331915020942688, Entropy: 142.10891723632812, Temp: 0.6022541522979736, KL: 74.61712646484375, Loss: 0.8601219058036804, Learning Rate: 0.020999999999999998\n",
      "Epoch [855/20000], Bound: 1.3443031311035156, Entropy: 141.901611328125, Temp: 0.6018195748329163, KL: 74.39279174804688, Loss: 0.8848509192466736, Learning Rate: 0.020999999999999998\n",
      "Epoch [856/20000], Bound: 1.3306695222854614, Entropy: 145.6216278076172, Temp: 0.6013778448104858, KL: 76.15327453613281, Loss: 0.8435804843902588, Learning Rate: 0.020999999999999998\n",
      "Epoch [857/20000], Bound: 1.3411273956298828, Entropy: 142.9774627685547, Temp: 0.6010358929634094, KL: 80.82554626464844, Loss: 0.8239760994911194, Learning Rate: 0.020999999999999998\n",
      "Epoch [858/20000], Bound: 1.269005537033081, Entropy: 140.62725830078125, Temp: 0.6009095311164856, KL: 67.16426086425781, Loss: 0.8053017854690552, Learning Rate: 0.020999999999999998\n",
      "Epoch [859/20000], Bound: 1.3191856145858765, Entropy: 136.7579345703125, Temp: 0.6007404923439026, KL: 77.818603515625, Loss: 0.8071816563606262, Learning Rate: 0.020999999999999998\n",
      "Epoch [860/20000], Bound: 1.309357762336731, Entropy: 136.47567749023438, Temp: 0.6007431149482727, KL: 71.79283142089844, Loss: 0.8391634821891785, Learning Rate: 0.020999999999999998\n",
      "Epoch [861/20000], Bound: 1.3869739770889282, Entropy: 138.34848022460938, Temp: 0.6007270812988281, KL: 83.35292053222656, Loss: 0.8921748399734497, Learning Rate: 0.020999999999999998\n",
      "Epoch [862/20000], Bound: 1.3196768760681152, Entropy: 132.03770446777344, Temp: 0.6008285284042358, KL: 72.456787109375, Loss: 0.8528600335121155, Learning Rate: 0.020999999999999998\n",
      "Epoch [863/20000], Bound: 1.3732672929763794, Entropy: 133.9615020751953, Temp: 0.6008910536766052, KL: 82.82176208496094, Loss: 0.8695504665374756, Learning Rate: 0.020999999999999998\n",
      "Epoch [864/20000], Bound: 1.33838951587677, Entropy: 131.38119506835938, Temp: 0.6010903716087341, KL: 75.73751831054688, Loss: 0.8611798882484436, Learning Rate: 0.020999999999999998\n",
      "Epoch [865/20000], Bound: 1.311963677406311, Entropy: 130.5614776611328, Temp: 0.6012901067733765, KL: 74.56076049804688, Loss: 0.8217853903770447, Learning Rate: 0.020999999999999998\n",
      "Epoch [866/20000], Bound: 1.2922475337982178, Entropy: 132.4318389892578, Temp: 0.6015328764915466, KL: 68.92958068847656, Loss: 0.8329593539237976, Learning Rate: 0.020999999999999998\n",
      "Epoch [867/20000], Bound: 1.2877687215805054, Entropy: 130.99658203125, Temp: 0.6016808152198792, KL: 70.47789001464844, Loss: 0.8122317790985107, Learning Rate: 0.020999999999999998\n",
      "Epoch [868/20000], Bound: 1.3092656135559082, Entropy: 126.57402038574219, Temp: 0.6018092632293701, KL: 71.41412353515625, Loss: 0.8437671661376953, Learning Rate: 0.020999999999999998\n",
      "Epoch [869/20000], Bound: 1.2773535251617432, Entropy: 127.76337432861328, Temp: 0.6018842458724976, KL: 69.30281829833984, Loss: 0.8036575317382812, Learning Rate: 0.020999999999999998\n",
      "Epoch [870/20000], Bound: 1.3944629430770874, Entropy: 128.8642578125, Temp: 0.6019365787506104, KL: 90.21867370605469, Loss: 0.8526086807250977, Learning Rate: 0.020999999999999998\n",
      "Epoch [871/20000], Bound: 1.3423219919204712, Entropy: 131.61083984375, Temp: 0.6022872924804688, KL: 79.90962219238281, Loss: 0.8360296487808228, Learning Rate: 0.020999999999999998\n",
      "Epoch [872/20000], Bound: 1.2830984592437744, Entropy: 129.5613555908203, Temp: 0.6027398705482483, KL: 69.26461791992188, Loss: 0.8154482841491699, Learning Rate: 0.020999999999999998\n",
      "Epoch [873/20000], Bound: 1.3308178186416626, Entropy: 133.91741943359375, Temp: 0.6031060218811035, KL: 73.81448364257812, Loss: 0.8660793304443359, Learning Rate: 0.020999999999999998\n",
      "Epoch [874/20000], Bound: 1.324163794517517, Entropy: 130.13267517089844, Temp: 0.6033963561058044, KL: 76.93165588378906, Loss: 0.828179657459259, Learning Rate: 0.020999999999999998\n",
      "Epoch [875/20000], Bound: 1.3215243816375732, Entropy: 134.12020874023438, Temp: 0.603743314743042, KL: 74.56813049316406, Loss: 0.8433699607849121, Learning Rate: 0.020999999999999998\n",
      "Epoch [876/20000], Bound: 1.3607851266860962, Entropy: 133.6437530517578, Temp: 0.604066014289856, KL: 81.04420471191406, Loss: 0.8654868602752686, Learning Rate: 0.020999999999999998\n",
      "Epoch [877/20000], Bound: 1.3752409219741821, Entropy: 133.30210876464844, Temp: 0.6044528484344482, KL: 87.85093688964844, Loss: 0.8384639024734497, Learning Rate: 0.020999999999999998\n",
      "Epoch [878/20000], Bound: 1.312238335609436, Entropy: 132.57481384277344, Temp: 0.6050703525543213, KL: 77.67955017089844, Loss: 0.8024395704269409, Learning Rate: 0.020999999999999998\n",
      "Epoch [879/20000], Bound: 1.30672025680542, Entropy: 137.0878448486328, Temp: 0.6057616472244263, KL: 67.61080932617188, Loss: 0.8764367699623108, Learning Rate: 0.020999999999999998\n",
      "Epoch [880/20000], Bound: 1.3763344287872314, Entropy: 133.56210327148438, Temp: 0.6061845421791077, KL: 81.78515625, Loss: 0.8940014243125916, Learning Rate: 0.020999999999999998\n",
      "Epoch [881/20000], Bound: 1.3474657535552979, Entropy: 133.98373413085938, Temp: 0.6066123247146606, KL: 74.78042602539062, Loss: 0.8956476449966431, Learning Rate: 0.020999999999999998\n",
      "Epoch [882/20000], Bound: 1.4049320220947266, Entropy: 138.364013671875, Temp: 0.6069031357765198, KL: 82.63670349121094, Loss: 0.9466738700866699, Learning Rate: 0.020999999999999998\n",
      "Epoch [883/20000], Bound: 1.3680040836334229, Entropy: 140.98435974121094, Temp: 0.6071334481239319, KL: 78.18717956542969, Loss: 0.9087435007095337, Learning Rate: 0.020999999999999998\n",
      "Epoch [884/20000], Bound: 1.3622318506240845, Entropy: 138.8618621826172, Temp: 0.607286810874939, KL: 80.30094909667969, Loss: 0.8801831007003784, Learning Rate: 0.020999999999999998\n",
      "Epoch [885/20000], Bound: 1.2908252477645874, Entropy: 142.0857391357422, Temp: 0.6074596643447876, KL: 62.3404541015625, Loss: 0.8931285738945007, Learning Rate: 0.020999999999999998\n",
      "Epoch [886/20000], Bound: 1.3356329202651978, Entropy: 139.94500732421875, Temp: 0.6072655916213989, KL: 77.88124084472656, Loss: 0.848417341709137, Learning Rate: 0.020999999999999998\n",
      "Epoch [887/20000], Bound: 1.2979886531829834, Entropy: 145.19073486328125, Temp: 0.6071341633796692, KL: 67.66334533691406, Loss: 0.8619238138198853, Learning Rate: 0.020999999999999998\n",
      "Epoch [888/20000], Bound: 1.3522917032241821, Entropy: 141.73074340820312, Temp: 0.6068317294120789, KL: 77.97186279296875, Loss: 0.8790891766548157, Learning Rate: 0.020999999999999998\n",
      "Epoch [889/20000], Bound: 1.4141379594802856, Entropy: 143.90670776367188, Temp: 0.6065539717674255, KL: 91.61610412597656, Loss: 0.8912050724029541, Learning Rate: 0.020999999999999998\n",
      "Epoch [890/20000], Bound: 1.365562081336975, Entropy: 146.54110717773438, Temp: 0.6065367460250854, KL: 79.23115539550781, Loss: 0.894250750541687, Learning Rate: 0.020999999999999998\n",
      "Epoch [891/20000], Bound: 1.3795419931411743, Entropy: 146.43331909179688, Temp: 0.6065161228179932, KL: 80.83924865722656, Loss: 0.9088485240936279, Learning Rate: 0.020999999999999998\n",
      "Epoch [892/20000], Bound: 1.3826545476913452, Entropy: 145.75, Temp: 0.606498658657074, KL: 80.77699279785156, Loss: 0.915603518486023, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [893/20000], Bound: 1.41128408908844, Entropy: 144.0939178466797, Temp: 0.6064712405204773, KL: 87.399658203125, Loss: 0.9198102951049805, Learning Rate: 0.020999999999999998\n",
      "Epoch [894/20000], Bound: 1.3871721029281616, Entropy: 144.4044647216797, Temp: 0.6065530180931091, KL: 83.10610961914062, Loss: 0.9056541323661804, Learning Rate: 0.020999999999999998\n",
      "Epoch [895/20000], Bound: 1.4226969480514526, Entropy: 140.91204833984375, Temp: 0.606676459312439, KL: 92.28193664550781, Loss: 0.9040576219558716, Learning Rate: 0.020999999999999998\n",
      "Epoch [896/20000], Bound: 1.4236980676651, Entropy: 139.55276489257812, Temp: 0.6070101261138916, KL: 89.51519775390625, Loss: 0.9296889305114746, Learning Rate: 0.020999999999999998\n",
      "Epoch [897/20000], Bound: 1.3698559999465942, Entropy: 135.0487060546875, Temp: 0.6074357628822327, KL: 75.21897888183594, Loss: 0.9373900890350342, Learning Rate: 0.020999999999999998\n",
      "Epoch [898/20000], Bound: 1.3446334600448608, Entropy: 134.659912109375, Temp: 0.6076552867889404, KL: 73.38885498046875, Loss: 0.9033352732658386, Learning Rate: 0.020999999999999998\n",
      "Epoch [899/20000], Bound: 1.3060896396636963, Entropy: 134.19369506835938, Temp: 0.6077091097831726, KL: 69.13291931152344, Loss: 0.8655662536621094, Learning Rate: 0.020999999999999998\n",
      "Epoch [900/20000], Bound: 1.3770471811294556, Entropy: 128.62677001953125, Temp: 0.607591986656189, KL: 82.05705261230469, Loss: 0.8957727551460266, Learning Rate: 0.020999999999999998\n",
      "Epoch [901/20000], Bound: 1.4086309671401978, Entropy: 125.90443420410156, Temp: 0.6075261831283569, KL: 90.19024658203125, Loss: 0.8934246897697449, Learning Rate: 0.020999999999999998\n",
      "Epoch [902/20000], Bound: 1.35378098487854, Entropy: 130.5887451171875, Temp: 0.6076644659042358, KL: 78.46282958984375, Loss: 0.8793782591819763, Learning Rate: 0.020999999999999998\n",
      "Epoch [903/20000], Bound: 1.3314876556396484, Entropy: 128.6531219482422, Temp: 0.607787013053894, KL: 70.8212890625, Loss: 0.8994500637054443, Learning Rate: 0.020999999999999998\n",
      "Epoch [904/20000], Bound: 1.30777108669281, Entropy: 125.53701782226562, Temp: 0.6077068448066711, KL: 70.69169616699219, Loss: 0.8558505177497864, Learning Rate: 0.020999999999999998\n",
      "Epoch [905/20000], Bound: 1.3884795904159546, Entropy: 128.54774475097656, Temp: 0.607517659664154, KL: 84.5181884765625, Loss: 0.8985111713409424, Learning Rate: 0.020999999999999998\n",
      "Epoch [906/20000], Bound: 1.3620134592056274, Entropy: 131.2242889404297, Temp: 0.6074298620223999, KL: 76.24433898925781, Loss: 0.9133965969085693, Learning Rate: 0.020999999999999998\n",
      "Epoch [907/20000], Bound: 1.3990871906280518, Entropy: 128.9835662841797, Temp: 0.6072473526000977, KL: 88.19430541992188, Loss: 0.8894733786582947, Learning Rate: 0.020999999999999998\n",
      "Epoch [908/20000], Bound: 1.3447948694229126, Entropy: 132.02757263183594, Temp: 0.6072532534599304, KL: 76.46356201171875, Loss: 0.8776781558990479, Learning Rate: 0.020999999999999998\n",
      "Epoch [909/20000], Bound: 1.358731985092163, Entropy: 136.78431701660156, Temp: 0.6072227358818054, KL: 75.27618408203125, Loss: 0.9145554900169373, Learning Rate: 0.020999999999999998\n",
      "Epoch [910/20000], Bound: 1.3676024675369263, Entropy: 142.33163452148438, Temp: 0.6070717573165894, KL: 78.97801208496094, Loss: 0.9013239145278931, Learning Rate: 0.020999999999999998\n",
      "Epoch [911/20000], Bound: 1.3669184446334839, Entropy: 143.63778686523438, Temp: 0.6069096326828003, KL: 76.36357116699219, Loss: 0.9212197661399841, Learning Rate: 0.020999999999999998\n",
      "Epoch [912/20000], Bound: 1.3065886497497559, Entropy: 146.05093383789062, Temp: 0.6066523790359497, KL: 70.43800354003906, Loss: 0.8541829586029053, Learning Rate: 0.020999999999999998\n",
      "Epoch [913/20000], Bound: 1.3625385761260986, Entropy: 145.70176696777344, Temp: 0.6063078045845032, KL: 76.75436401367188, Loss: 0.9083030223846436, Learning Rate: 0.020999999999999998\n",
      "Epoch [914/20000], Bound: 1.365159034729004, Entropy: 144.35260009765625, Temp: 0.6059203743934631, KL: 78.90887451171875, Loss: 0.8950251340866089, Learning Rate: 0.020999999999999998\n",
      "Epoch [915/20000], Bound: 1.347998023033142, Entropy: 148.65489196777344, Temp: 0.6055632829666138, KL: 77.31002807617188, Loss: 0.8740570545196533, Learning Rate: 0.020999999999999998\n",
      "Epoch [916/20000], Bound: 1.3584657907485962, Entropy: 146.59988403320312, Temp: 0.6052405834197998, KL: 78.45414733886719, Loss: 0.8844277858734131, Learning Rate: 0.020999999999999998\n",
      "Epoch [917/20000], Bound: 1.4295945167541504, Entropy: 149.54307556152344, Temp: 0.6049559116363525, KL: 89.42692565917969, Loss: 0.9386562705039978, Learning Rate: 0.020999999999999998\n",
      "Epoch [918/20000], Bound: 1.4023869037628174, Entropy: 147.70408630371094, Temp: 0.6048243045806885, KL: 87.34992980957031, Loss: 0.8984166383743286, Learning Rate: 0.020999999999999998\n",
      "Epoch [919/20000], Bound: 1.3633232116699219, Entropy: 145.56866455078125, Temp: 0.6048630475997925, KL: 75.34632873535156, Loss: 0.9189946055412292, Learning Rate: 0.020999999999999998\n",
      "Epoch [920/20000], Bound: 1.3592137098312378, Entropy: 143.64315795898438, Temp: 0.6047839522361755, KL: 71.60496520996094, Loss: 0.9417235255241394, Learning Rate: 0.020999999999999998\n",
      "Epoch [921/20000], Bound: 1.3130981922149658, Entropy: 140.70042419433594, Temp: 0.604484498500824, KL: 68.0504150390625, Loss: 0.8827427625656128, Learning Rate: 0.020999999999999998\n",
      "Epoch [922/20000], Bound: 1.3812793493270874, Entropy: 140.11276245117188, Temp: 0.6040160655975342, KL: 80.20559692382812, Loss: 0.9129967093467712, Learning Rate: 0.020999999999999998\n",
      "Epoch [923/20000], Bound: 1.4167732000350952, Entropy: 141.1981964111328, Temp: 0.6035935282707214, KL: 88.26475524902344, Loss: 0.9182770848274231, Learning Rate: 0.020999999999999998\n",
      "Epoch [924/20000], Bound: 1.3566745519638062, Entropy: 142.0595245361328, Temp: 0.6033622026443481, KL: 79.87667846679688, Loss: 0.8658790588378906, Learning Rate: 0.020999999999999998\n",
      "Epoch [925/20000], Bound: 1.378614068031311, Entropy: 137.37091064453125, Temp: 0.6032341718673706, KL: 81.81562805175781, Loss: 0.892879843711853, Learning Rate: 0.020999999999999998\n",
      "Epoch [926/20000], Bound: 1.351887822151184, Entropy: 138.64730834960938, Temp: 0.6031908392906189, KL: 73.04759216308594, Loss: 0.9128889441490173, Learning Rate: 0.020999999999999998\n",
      "Epoch [927/20000], Bound: 1.3937952518463135, Entropy: 139.6039581298828, Temp: 0.60301274061203, KL: 84.44013977050781, Loss: 0.9013338088989258, Learning Rate: 0.020999999999999998\n",
      "Epoch [928/20000], Bound: 1.404563069343567, Entropy: 137.19198608398438, Temp: 0.6029625535011292, KL: 85.19500732421875, Loss: 0.9170447587966919, Learning Rate: 0.020999999999999998\n",
      "Epoch [929/20000], Bound: 1.339925765991211, Entropy: 136.8525390625, Temp: 0.6030148267745972, KL: 77.39553833007812, Loss: 0.8535436987876892, Learning Rate: 0.020999999999999998\n",
      "Epoch [930/20000], Bound: 1.389617681503296, Entropy: 136.0555419921875, Temp: 0.6031169891357422, KL: 84.1177978515625, Loss: 0.8957226872444153, Learning Rate: 0.020999999999999998\n",
      "Epoch [931/20000], Bound: 1.385927438735962, Entropy: 134.26461791992188, Temp: 0.6033222079277039, KL: 78.55224609375, Loss: 0.9347779750823975, Learning Rate: 0.020999999999999998\n",
      "Epoch [932/20000], Bound: 1.3885048627853394, Entropy: 132.28872680664062, Temp: 0.6034406423568726, KL: 84.08120727539062, Loss: 0.8943912386894226, Learning Rate: 0.020999999999999998\n",
      "Epoch [933/20000], Bound: 1.3406544923782349, Entropy: 132.53961181640625, Temp: 0.6036599278450012, KL: 73.47663879394531, Loss: 0.8884770274162292, Learning Rate: 0.020999999999999998\n",
      "Epoch [934/20000], Bound: 1.362610936164856, Entropy: 131.6748046875, Temp: 0.6037659049034119, KL: 80.16221618652344, Loss: 0.8758361339569092, Learning Rate: 0.020999999999999998\n",
      "Epoch [935/20000], Bound: 1.3531450033187866, Entropy: 133.76268005371094, Temp: 0.6039273738861084, KL: 77.25642395019531, Loss: 0.8816962242126465, Learning Rate: 0.020999999999999998\n",
      "Epoch [936/20000], Bound: 1.3546277284622192, Entropy: 132.77220153808594, Temp: 0.6040686368942261, KL: 80.40602111816406, Loss: 0.8587502241134644, Learning Rate: 0.020999999999999998\n",
      "Epoch [937/20000], Bound: 1.3353673219680786, Entropy: 132.30181884765625, Temp: 0.6042954325675964, KL: 69.62518310546875, Loss: 0.9112827181816101, Learning Rate: 0.020999999999999998\n",
      "Epoch [938/20000], Bound: 1.394870638847351, Entropy: 136.0906982421875, Temp: 0.6042830944061279, KL: 80.28755187988281, Loss: 0.9403437376022339, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [939/20000], Bound: 1.281040906906128, Entropy: 138.01451110839844, Temp: 0.6042230129241943, KL: 59.29034423828125, Loss: 0.8964097499847412, Learning Rate: 0.020999999999999998\n",
      "Epoch [940/20000], Bound: 1.3771188259124756, Entropy: 138.6004180908203, Temp: 0.6037554144859314, KL: 78.87860107421875, Loss: 0.915177047252655, Learning Rate: 0.020999999999999998\n",
      "Epoch [941/20000], Bound: 1.3346714973449707, Entropy: 137.7705535888672, Temp: 0.6033052206039429, KL: 76.84193420410156, Loss: 0.8486102223396301, Learning Rate: 0.020999999999999998\n",
      "Epoch [942/20000], Bound: 1.313456654548645, Entropy: 137.89088439941406, Temp: 0.6029508113861084, KL: 77.32832336425781, Loss: 0.8042039275169373, Learning Rate: 0.020999999999999998\n",
      "Epoch [943/20000], Bound: 1.376806378364563, Entropy: 141.28431701660156, Temp: 0.6027763485908508, KL: 82.91278076171875, Loss: 0.8793242573738098, Learning Rate: 0.020999999999999998\n",
      "Epoch [944/20000], Bound: 1.3692368268966675, Entropy: 141.83938598632812, Temp: 0.6027413606643677, KL: 81.91790771484375, Loss: 0.8724900484085083, Learning Rate: 0.020999999999999998\n",
      "Epoch [945/20000], Bound: 1.3153156042099, Entropy: 139.72596740722656, Temp: 0.6028246879577637, KL: 71.22453308105469, Loss: 0.8580753803253174, Learning Rate: 0.020999999999999998\n",
      "Epoch [946/20000], Bound: 1.3039997816085815, Entropy: 140.3700408935547, Temp: 0.6028202176094055, KL: 73.47573852539062, Loss: 0.8185188174247742, Learning Rate: 0.020999999999999998\n",
      "Epoch [947/20000], Bound: 1.304650902748108, Entropy: 142.6552734375, Temp: 0.602855384349823, KL: 75.67198181152344, Loss: 0.8015522360801697, Learning Rate: 0.020999999999999998\n",
      "Epoch [948/20000], Bound: 1.3349313735961914, Entropy: 140.99229431152344, Temp: 0.6030033826828003, KL: 75.70266723632812, Loss: 0.8580508232116699, Learning Rate: 0.020999999999999998\n",
      "Epoch [949/20000], Bound: 1.2604583501815796, Entropy: 141.85598754882812, Temp: 0.6031495332717896, KL: 65.14596557617188, Loss: 0.8100765943527222, Learning Rate: 0.020999999999999998\n",
      "Epoch [950/20000], Bound: 1.3132356405258179, Entropy: 138.9034881591797, Temp: 0.6031546592712402, KL: 77.11614990234375, Loss: 0.8058807849884033, Learning Rate: 0.020999999999999998\n",
      "Epoch [951/20000], Bound: 1.3163981437683105, Entropy: 139.77554321289062, Temp: 0.6032958030700684, KL: 73.94337463378906, Loss: 0.8382742404937744, Learning Rate: 0.020999999999999998\n",
      "Epoch [952/20000], Bound: 1.3646502494812012, Entropy: 137.41490173339844, Temp: 0.6034329533576965, KL: 83.27191162109375, Loss: 0.8534846305847168, Learning Rate: 0.020999999999999998\n",
      "Epoch [953/20000], Bound: 1.3123372793197632, Entropy: 138.37901306152344, Temp: 0.6037287712097168, KL: 74.19273376464844, Loss: 0.8293468356132507, Learning Rate: 0.020999999999999998\n",
      "Epoch [954/20000], Bound: 1.3382790088653564, Entropy: 138.0616455078125, Temp: 0.6040239334106445, KL: 77.13299560546875, Loss: 0.8542606234550476, Learning Rate: 0.020999999999999998\n",
      "Epoch [955/20000], Bound: 1.3375388383865356, Entropy: 138.10617065429688, Temp: 0.6043319702148438, KL: 76.587646484375, Loss: 0.8578724265098572, Learning Rate: 0.020999999999999998\n",
      "Epoch [956/20000], Bound: 1.3140625953674316, Entropy: 134.50624084472656, Temp: 0.604631781578064, KL: 73.47886657714844, Loss: 0.8398604989051819, Learning Rate: 0.020999999999999998\n",
      "Epoch [957/20000], Bound: 1.3442044258117676, Entropy: 134.67608642578125, Temp: 0.6048899292945862, KL: 83.74578857421875, Loss: 0.812404215335846, Learning Rate: 0.020999999999999998\n",
      "Epoch [958/20000], Bound: 1.3384995460510254, Entropy: 132.1347198486328, Temp: 0.6053710579872131, KL: 76.68748474121094, Loss: 0.860602855682373, Learning Rate: 0.020999999999999998\n",
      "Epoch [959/20000], Bound: 1.3036272525787354, Entropy: 135.00112915039062, Temp: 0.6058167219161987, KL: 72.3546142578125, Loss: 0.8316646218299866, Learning Rate: 0.020999999999999998\n",
      "Epoch [960/20000], Bound: 1.2846163511276245, Entropy: 134.3227081298828, Temp: 0.606189489364624, KL: 78.93511962890625, Loss: 0.74332195520401, Learning Rate: 0.020999999999999998\n",
      "Epoch [961/20000], Bound: 1.2813197374343872, Entropy: 134.10267639160156, Temp: 0.6067997217178345, KL: 74.60563659667969, Loss: 0.7740073800086975, Learning Rate: 0.020999999999999998\n",
      "Epoch [962/20000], Bound: 1.312567114830017, Entropy: 133.63156127929688, Temp: 0.6074705123901367, KL: 79.06878662109375, Loss: 0.7954554557800293, Learning Rate: 0.020999999999999998\n",
      "Epoch [963/20000], Bound: 1.2878761291503906, Entropy: 130.80113220214844, Temp: 0.6082435250282288, KL: 71.87918090820312, Loss: 0.8104063868522644, Learning Rate: 0.020999999999999998\n",
      "Epoch [964/20000], Bound: 1.2916303873062134, Entropy: 132.89309692382812, Temp: 0.6089245080947876, KL: 76.11349487304688, Loss: 0.7834582328796387, Learning Rate: 0.020999999999999998\n",
      "Epoch [965/20000], Bound: 1.3200045824050903, Entropy: 130.06480407714844, Temp: 0.6096585988998413, KL: 83.91816711425781, Loss: 0.7731618285179138, Learning Rate: 0.020999999999999998\n",
      "Epoch [966/20000], Bound: 1.2670847177505493, Entropy: 133.21417236328125, Temp: 0.6106159090995789, KL: 72.37452697753906, Loss: 0.7724131345748901, Learning Rate: 0.020999999999999998\n",
      "Epoch [967/20000], Bound: 1.3398065567016602, Entropy: 132.16493225097656, Temp: 0.6115304827690125, KL: 83.35237121582031, Loss: 0.818781852722168, Learning Rate: 0.020999999999999998\n",
      "Epoch [968/20000], Bound: 1.2967185974121094, Entropy: 131.6283721923828, Temp: 0.6125384569168091, KL: 77.06735229492188, Loss: 0.7904923558235168, Learning Rate: 0.020999999999999998\n",
      "Epoch [969/20000], Bound: 1.2971646785736084, Entropy: 130.68360900878906, Temp: 0.6135498285293579, KL: 85.14222717285156, Loss: 0.7270610928535461, Learning Rate: 0.020999999999999998\n",
      "Epoch [970/20000], Bound: 1.2556332349777222, Entropy: 132.14505004882812, Temp: 0.6148478984832764, KL: 70.87338256835938, Loss: 0.7702354192733765, Learning Rate: 0.020999999999999998\n",
      "Epoch [971/20000], Bound: 1.2896469831466675, Entropy: 129.08425903320312, Temp: 0.6160141825675964, KL: 79.32829284667969, Loss: 0.764430582523346, Learning Rate: 0.020999999999999998\n",
      "Epoch [972/20000], Bound: 1.217111587524414, Entropy: 133.6476287841797, Temp: 0.617243230342865, KL: 64.99627685546875, Loss: 0.754314661026001, Learning Rate: 0.020999999999999998\n",
      "Epoch [973/20000], Bound: 1.3164821863174438, Entropy: 133.26487731933594, Temp: 0.6182346343994141, KL: 78.07075500488281, Loss: 0.8281561732292175, Learning Rate: 0.020999999999999998\n",
      "Epoch [974/20000], Bound: 1.3137942552566528, Entropy: 130.21243286132812, Temp: 0.6191398501396179, KL: 79.14955139160156, Loss: 0.815774142742157, Learning Rate: 0.020999999999999998\n",
      "Epoch [975/20000], Bound: 1.3273085355758667, Entropy: 132.9345245361328, Temp: 0.6200071573257446, KL: 82.85334777832031, Loss: 0.813115119934082, Learning Rate: 0.020999999999999998\n",
      "Epoch [976/20000], Bound: 1.3595446348190308, Entropy: 134.88638305664062, Temp: 0.6209147572517395, KL: 85.9271240234375, Loss: 0.85331130027771, Learning Rate: 0.020999999999999998\n",
      "Epoch [977/20000], Bound: 1.277587652206421, Entropy: 133.87538146972656, Temp: 0.6218347549438477, KL: 73.74172973632812, Loss: 0.7961140871047974, Learning Rate: 0.020999999999999998\n",
      "Epoch [978/20000], Bound: 1.2733790874481201, Entropy: 133.57862854003906, Temp: 0.6226232051849365, KL: 70.79922485351562, Loss: 0.8132011890411377, Learning Rate: 0.020999999999999998\n",
      "Epoch [979/20000], Bound: 1.3090949058532715, Entropy: 137.7462921142578, Temp: 0.623191237449646, KL: 71.26893615722656, Loss: 0.8764756321907043, Learning Rate: 0.020999999999999998\n",
      "Epoch [980/20000], Bound: 1.2933226823806763, Entropy: 137.2578125, Temp: 0.6234441995620728, KL: 72.02430725097656, Loss: 0.841176450252533, Learning Rate: 0.020999999999999998\n",
      "Epoch [981/20000], Bound: 1.3356040716171265, Entropy: 137.09271240234375, Temp: 0.6234952211380005, KL: 79.31657409667969, Loss: 0.8634199500083923, Learning Rate: 0.020999999999999998\n",
      "Epoch [982/20000], Bound: 1.3607674837112427, Entropy: 138.32308959960938, Temp: 0.6234722137451172, KL: 80.37303161621094, Loss: 0.9049045443534851, Learning Rate: 0.020999999999999998\n",
      "Epoch [983/20000], Bound: 1.3439923524856567, Entropy: 136.59117126464844, Temp: 0.6233242154121399, KL: 82.878173828125, Loss: 0.8510608673095703, Learning Rate: 0.020999999999999998\n",
      "Epoch [984/20000], Bound: 1.3338528871536255, Entropy: 134.7294158935547, Temp: 0.6232193112373352, KL: 76.42854309082031, Loss: 0.8827185034751892, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [985/20000], Bound: 1.2929749488830566, Entropy: 133.88681030273438, Temp: 0.6229612231254578, KL: 74.70451354980469, Loss: 0.8183270692825317, Learning Rate: 0.020999999999999998\n",
      "Epoch [986/20000], Bound: 1.3195841312408447, Entropy: 132.5880889892578, Temp: 0.6226562261581421, KL: 76.16790771484375, Loss: 0.856332540512085, Learning Rate: 0.020999999999999998\n",
      "Epoch [987/20000], Bound: 1.3611953258514404, Entropy: 131.0255584716797, Temp: 0.6222672462463379, KL: 84.46965026855469, Loss: 0.8707728981971741, Learning Rate: 0.020999999999999998\n",
      "Epoch [988/20000], Bound: 1.3106043338775635, Entropy: 132.6904296875, Temp: 0.621946394443512, KL: 77.11587524414062, Loss: 0.8305071592330933, Learning Rate: 0.020999999999999998\n",
      "Epoch [989/20000], Bound: 1.3010609149932861, Entropy: 131.74057006835938, Temp: 0.6216182708740234, KL: 75.17036437988281, Loss: 0.8276790976524353, Learning Rate: 0.020999999999999998\n",
      "Epoch [990/20000], Bound: 1.287116527557373, Entropy: 132.56712341308594, Temp: 0.6212506890296936, KL: 76.22419738769531, Loss: 0.7927590608596802, Learning Rate: 0.020999999999999998\n",
      "Epoch [991/20000], Bound: 1.2504464387893677, Entropy: 131.79249572753906, Temp: 0.6209415793418884, KL: 66.17718505859375, Loss: 0.8070696592330933, Learning Rate: 0.020999999999999998\n",
      "Epoch [992/20000], Bound: 1.2920172214508057, Entropy: 130.96243286132812, Temp: 0.620442271232605, KL: 76.91807556152344, Loss: 0.7950149178504944, Learning Rate: 0.020999999999999998\n",
      "Epoch [993/20000], Bound: 1.2938822507858276, Entropy: 131.95901489257812, Temp: 0.6200302839279175, KL: 78.08285522460938, Loss: 0.7884563207626343, Learning Rate: 0.020999999999999998\n",
      "Epoch [994/20000], Bound: 1.3224834203720093, Entropy: 130.82186889648438, Temp: 0.6197372674942017, KL: 85.64913940429688, Loss: 0.7808407545089722, Learning Rate: 0.020999999999999998\n",
      "Epoch [995/20000], Bound: 1.2750239372253418, Entropy: 129.90614318847656, Temp: 0.6197242140769958, KL: 76.0556640625, Loss: 0.7697849869728088, Learning Rate: 0.020999999999999998\n",
      "Epoch [996/20000], Bound: 1.3285346031188965, Entropy: 131.24684143066406, Temp: 0.6197876930236816, KL: 85.25260925292969, Loss: 0.7957579493522644, Learning Rate: 0.020999999999999998\n",
      "Epoch [997/20000], Bound: 1.262110710144043, Entropy: 129.62318420410156, Temp: 0.6200571656227112, KL: 73.23860168457031, Loss: 0.7697280645370483, Learning Rate: 0.020999999999999998\n",
      "Epoch [998/20000], Bound: 1.3489744663238525, Entropy: 130.49783325195312, Temp: 0.6203129291534424, KL: 94.54534912109375, Loss: 0.761704683303833, Learning Rate: 0.020999999999999998\n",
      "Epoch [999/20000], Bound: 1.2718472480773926, Entropy: 131.7466583251953, Temp: 0.6210053563117981, KL: 75.79150390625, Loss: 0.7680094838142395, Learning Rate: 0.020999999999999998\n",
      "Epoch [1000/20000], Bound: 1.2795493602752686, Entropy: 130.56900024414062, Temp: 0.6216940879821777, KL: 77.3154296875, Loss: 0.7707527279853821, Learning Rate: 0.020999999999999998\n",
      "Epoch [1001/20000], Bound: 1.2775267362594604, Entropy: 129.50619506835938, Temp: 0.6224013566970825, KL: 80.21279907226562, Loss: 0.7448227405548096, Learning Rate: 0.020999999999999998\n",
      "Epoch [1002/20000], Bound: 1.2229194641113281, Entropy: 130.01437377929688, Temp: 0.6232348680496216, KL: 72.02684020996094, Loss: 0.7149991989135742, Learning Rate: 0.020999999999999998\n",
      "Epoch [1003/20000], Bound: 1.3115125894546509, Entropy: 129.91409301757812, Temp: 0.6238158345222473, KL: 94.111083984375, Loss: 0.698893129825592, Learning Rate: 0.020999999999999998\n",
      "Epoch [1004/20000], Bound: 1.3060345649719238, Entropy: 130.0354461669922, Temp: 0.6247327923774719, KL: 84.47726440429688, Loss: 0.7672143578529358, Learning Rate: 0.020999999999999998\n",
      "Epoch [1005/20000], Bound: 1.3535593748092651, Entropy: 131.38926696777344, Temp: 0.6257140636444092, KL: 88.11367797851562, Loss: 0.8323737978935242, Learning Rate: 0.020999999999999998\n",
      "Epoch [1006/20000], Bound: 1.2576847076416016, Entropy: 128.5411834716797, Temp: 0.6267061233520508, KL: 78.09278869628906, Loss: 0.732195258140564, Learning Rate: 0.020999999999999998\n",
      "Epoch [1007/20000], Bound: 1.2708724737167358, Entropy: 126.25872802734375, Temp: 0.6277052164077759, KL: 72.73564147949219, Loss: 0.8001376986503601, Learning Rate: 0.020999999999999998\n",
      "Epoch [1008/20000], Bound: 1.29490065574646, Entropy: 128.14553833007812, Temp: 0.6285276412963867, KL: 76.27665710449219, Loss: 0.8175475597381592, Learning Rate: 0.020999999999999998\n",
      "Epoch [1009/20000], Bound: 1.3189918994903564, Entropy: 127.24345397949219, Temp: 0.6292146444320679, KL: 78.67532348632812, Loss: 0.8453881144523621, Learning Rate: 0.020999999999999998\n",
      "Epoch [1010/20000], Bound: 1.2664273977279663, Entropy: 129.15000915527344, Temp: 0.6297714710235596, KL: 70.56587219238281, Loss: 0.8121044039726257, Learning Rate: 0.020999999999999998\n",
      "Epoch [1011/20000], Bound: 1.2692241668701172, Entropy: 129.57476806640625, Temp: 0.6301350593566895, KL: 76.3695068359375, Loss: 0.7716328501701355, Learning Rate: 0.020999999999999998\n",
      "Epoch [1012/20000], Bound: 1.3464328050613403, Entropy: 129.34201049804688, Temp: 0.6304678916931152, KL: 86.61103820800781, Loss: 0.8384459018707275, Learning Rate: 0.020999999999999998\n",
      "Epoch [1013/20000], Bound: 1.3116182088851929, Entropy: 130.442138671875, Temp: 0.6308216452598572, KL: 80.63371276855469, Loss: 0.818160891532898, Learning Rate: 0.020999999999999998\n",
      "Epoch [1014/20000], Bound: 1.3292450904846191, Entropy: 130.1564178466797, Temp: 0.6311370134353638, KL: 84.98686218261719, Loss: 0.8183853030204773, Learning Rate: 0.020999999999999998\n",
      "Epoch [1015/20000], Bound: 1.2589675188064575, Entropy: 129.52084350585938, Temp: 0.6314778327941895, KL: 71.62730407714844, Loss: 0.7924445867538452, Learning Rate: 0.020999999999999998\n",
      "Epoch [1016/20000], Bound: 1.3107432126998901, Entropy: 130.07716369628906, Temp: 0.6316823363304138, KL: 80.73216247558594, Loss: 0.8170483112335205, Learning Rate: 0.020999999999999998\n",
      "Epoch [1017/20000], Bound: 1.355819582939148, Entropy: 131.46266174316406, Temp: 0.6318619251251221, KL: 85.06291198730469, Loss: 0.8720731139183044, Learning Rate: 0.020999999999999998\n",
      "Epoch [1018/20000], Bound: 1.2686864137649536, Entropy: 133.3525390625, Temp: 0.6320008635520935, KL: 66.47280883789062, Loss: 0.8515778183937073, Learning Rate: 0.020999999999999998\n",
      "Epoch [1019/20000], Bound: 1.317672610282898, Entropy: 134.89498901367188, Temp: 0.6318578720092773, KL: 75.30879211425781, Loss: 0.8736013770103455, Learning Rate: 0.020999999999999998\n",
      "Epoch [1020/20000], Bound: 1.2662910223007202, Entropy: 137.02255249023438, Temp: 0.631564199924469, KL: 72.47579956054688, Loss: 0.7991243004798889, Learning Rate: 0.020999999999999998\n",
      "Epoch [1021/20000], Bound: 1.3565579652786255, Entropy: 135.7288818359375, Temp: 0.6311997771263123, KL: 88.21591186523438, Loss: 0.8474391102790833, Learning Rate: 0.020999999999999998\n",
      "Epoch [1022/20000], Bound: 1.363309621810913, Entropy: 136.04946899414062, Temp: 0.6309314966201782, KL: 84.1058349609375, Loss: 0.893298864364624, Learning Rate: 0.020999999999999998\n",
      "Epoch [1023/20000], Bound: 1.3373289108276367, Entropy: 138.61264038085938, Temp: 0.6306285858154297, KL: 75.65621948242188, Loss: 0.9074468612670898, Learning Rate: 0.020999999999999998\n",
      "Epoch [1024/20000], Bound: 1.3075249195098877, Entropy: 135.76812744140625, Temp: 0.6301551461219788, KL: 73.24578857421875, Loss: 0.8679049611091614, Learning Rate: 0.020999999999999998\n",
      "Epoch [1025/20000], Bound: 1.2765716314315796, Entropy: 137.74012756347656, Temp: 0.6295498609542847, KL: 71.12080383300781, Loss: 0.8259426951408386, Learning Rate: 0.020999999999999998\n",
      "Epoch [1026/20000], Bound: 1.347498893737793, Entropy: 136.36184692382812, Temp: 0.6288558840751648, KL: 81.85679626464844, Loss: 0.8755694627761841, Learning Rate: 0.020999999999999998\n",
      "Epoch [1027/20000], Bound: 1.279145359992981, Entropy: 136.42120361328125, Temp: 0.6281737685203552, KL: 69.19898986816406, Loss: 0.8440897464752197, Learning Rate: 0.020999999999999998\n",
      "Epoch [1028/20000], Bound: 1.2871116399765015, Entropy: 142.017578125, Temp: 0.6273626089096069, KL: 80.94895935058594, Loss: 0.7640658617019653, Learning Rate: 0.020999999999999998\n",
      "Epoch [1029/20000], Bound: 1.303745150566101, Entropy: 139.0164794921875, Temp: 0.62673020362854, KL: 78.12457275390625, Loss: 0.8167868852615356, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1030/20000], Bound: 1.2570266723632812, Entropy: 136.0917205810547, Temp: 0.626143217086792, KL: 65.20222473144531, Loss: 0.8331531286239624, Learning Rate: 0.020999999999999998\n",
      "Epoch [1031/20000], Bound: 1.3108274936676025, Entropy: 137.15585327148438, Temp: 0.62538081407547, KL: 77.95590209960938, Loss: 0.8294997811317444, Learning Rate: 0.020999999999999998\n",
      "Epoch [1032/20000], Bound: 1.3534198999404907, Entropy: 136.6943817138672, Temp: 0.6246631145477295, KL: 87.41252136230469, Loss: 0.8358206748962402, Learning Rate: 0.020999999999999998\n",
      "Epoch [1033/20000], Bound: 1.3233007192611694, Entropy: 138.44313049316406, Temp: 0.624116837978363, KL: 76.70761108398438, Loss: 0.861426591873169, Learning Rate: 0.020999999999999998\n",
      "Epoch [1034/20000], Bound: 1.3482356071472168, Entropy: 134.22032165527344, Temp: 0.6235366463661194, KL: 81.07965087890625, Loss: 0.8742537498474121, Learning Rate: 0.020999999999999998\n",
      "Epoch [1035/20000], Bound: 1.3357460498809814, Entropy: 136.49998474121094, Temp: 0.6229751110076904, KL: 81.08076477050781, Loss: 0.8486876487731934, Learning Rate: 0.020999999999999998\n",
      "Epoch [1036/20000], Bound: 1.3698104619979858, Entropy: 135.38482666015625, Temp: 0.6224691271781921, KL: 83.85870361328125, Loss: 0.893502950668335, Learning Rate: 0.020999999999999998\n",
      "Epoch [1037/20000], Bound: 1.331559658050537, Entropy: 134.96658325195312, Temp: 0.6219932436943054, KL: 75.63011169433594, Loss: 0.8827212452888489, Learning Rate: 0.020999999999999998\n",
      "Epoch [1038/20000], Bound: 1.3023322820663452, Entropy: 138.95652770996094, Temp: 0.6214416027069092, KL: 69.77066040039062, Loss: 0.8732417225837708, Learning Rate: 0.020999999999999998\n",
      "Epoch [1039/20000], Bound: 1.3442678451538086, Entropy: 139.0133056640625, Temp: 0.6207488179206848, KL: 83.43911743164062, Loss: 0.8426867723464966, Learning Rate: 0.020999999999999998\n",
      "Epoch [1040/20000], Bound: 1.3357131481170654, Entropy: 136.87301635742188, Temp: 0.6201790571212769, KL: 78.49592590332031, Loss: 0.8648150563240051, Learning Rate: 0.020999999999999998\n",
      "Epoch [1041/20000], Bound: 1.3084765672683716, Entropy: 139.3875274658203, Temp: 0.6196194887161255, KL: 76.685791015625, Loss: 0.8263668417930603, Learning Rate: 0.020999999999999998\n",
      "Epoch [1042/20000], Bound: 1.3561841249465942, Entropy: 137.01828002929688, Temp: 0.6190987825393677, KL: 83.669921875, Loss: 0.8615620732307434, Learning Rate: 0.020999999999999998\n",
      "Epoch [1043/20000], Bound: 1.3381034135818481, Entropy: 135.52499389648438, Temp: 0.618669331073761, KL: 77.16154479980469, Loss: 0.8777831792831421, Learning Rate: 0.020999999999999998\n",
      "Epoch [1044/20000], Bound: 1.2768367528915405, Entropy: 137.24505615234375, Temp: 0.6182059645652771, KL: 65.7166748046875, Loss: 0.8544836044311523, Learning Rate: 0.020999999999999998\n",
      "Epoch [1045/20000], Bound: 1.3735402822494507, Entropy: 136.33432006835938, Temp: 0.6175704002380371, KL: 87.99057006835938, Loss: 0.8587754368782043, Learning Rate: 0.020999999999999998\n",
      "Epoch [1046/20000], Bound: 1.346826434135437, Entropy: 137.52613830566406, Temp: 0.6171118021011353, KL: 74.30717468261719, Loss: 0.9154238700866699, Learning Rate: 0.020999999999999998\n",
      "Epoch [1047/20000], Bound: 1.3483816385269165, Entropy: 139.83360290527344, Temp: 0.6165366172790527, KL: 74.78022766113281, Loss: 0.9137131571769714, Learning Rate: 0.020999999999999998\n",
      "Epoch [1048/20000], Bound: 1.3396443128585815, Entropy: 138.94688415527344, Temp: 0.6158689260482788, KL: 77.2525634765625, Loss: 0.8754744529724121, Learning Rate: 0.020999999999999998\n",
      "Epoch [1049/20000], Bound: 1.3928377628326416, Entropy: 143.24758911132812, Temp: 0.6152103543281555, KL: 88.6201171875, Loss: 0.8887693881988525, Learning Rate: 0.020999999999999998\n",
      "Epoch [1050/20000], Bound: 1.387951135635376, Entropy: 141.18588256835938, Temp: 0.6147111654281616, KL: 79.29957580566406, Loss: 0.9535086750984192, Learning Rate: 0.020999999999999998\n",
      "Epoch [1051/20000], Bound: 1.3458495140075684, Entropy: 145.6865997314453, Temp: 0.6141358613967896, KL: 74.18026733398438, Loss: 0.9097247123718262, Learning Rate: 0.020999999999999998\n",
      "Epoch [1052/20000], Bound: 1.3641915321350098, Entropy: 143.82887268066406, Temp: 0.6134768128395081, KL: 77.53517150878906, Loss: 0.9175186157226562, Learning Rate: 0.020999999999999998\n",
      "Epoch [1053/20000], Bound: 1.4147394895553589, Entropy: 144.45664978027344, Temp: 0.6127859354019165, KL: 93.22889709472656, Loss: 0.892310619354248, Learning Rate: 0.020999999999999998\n",
      "Epoch [1054/20000], Bound: 1.4158376455307007, Entropy: 143.13177490234375, Temp: 0.6123313903808594, KL: 92.40849304199219, Loss: 0.9003890752792358, Learning Rate: 0.020999999999999998\n",
      "Epoch [1055/20000], Bound: 1.2934386730194092, Entropy: 143.4734649658203, Temp: 0.612069308757782, KL: 64.44941711425781, Loss: 0.8868228197097778, Learning Rate: 0.020999999999999998\n",
      "Epoch [1056/20000], Bound: 1.358130693435669, Entropy: 142.08409118652344, Temp: 0.6115807890892029, KL: 78.53097534179688, Loss: 0.8940940499305725, Learning Rate: 0.020999999999999998\n",
      "Epoch [1057/20000], Bound: 1.337920069694519, Entropy: 141.11158752441406, Temp: 0.6110997796058655, KL: 74.67449951171875, Loss: 0.8853979110717773, Learning Rate: 0.020999999999999998\n",
      "Epoch [1058/20000], Bound: 1.318358063697815, Entropy: 138.8649444580078, Temp: 0.6105813980102539, KL: 72.04571533203125, Loss: 0.8688431978225708, Learning Rate: 0.020999999999999998\n",
      "Epoch [1059/20000], Bound: 1.3675642013549805, Entropy: 137.55897521972656, Temp: 0.6100137233734131, KL: 81.02949523925781, Loss: 0.8896162509918213, Learning Rate: 0.020999999999999998\n",
      "Epoch [1060/20000], Bound: 1.3404486179351807, Entropy: 138.04017639160156, Temp: 0.6095132231712341, KL: 75.70411682128906, Loss: 0.8792608380317688, Learning Rate: 0.020999999999999998\n",
      "Epoch [1061/20000], Bound: 1.3134140968322754, Entropy: 139.20115661621094, Temp: 0.6090095639228821, KL: 74.83346557617188, Loss: 0.8342916965484619, Learning Rate: 0.020999999999999998\n",
      "Epoch [1062/20000], Bound: 1.3107656240463257, Entropy: 139.859130859375, Temp: 0.6085536479949951, KL: 68.00277709960938, Loss: 0.8847677707672119, Learning Rate: 0.020999999999999998\n",
      "Epoch [1063/20000], Bound: 1.3017728328704834, Entropy: 140.11366271972656, Temp: 0.6079673767089844, KL: 69.52598571777344, Loss: 0.8547448515892029, Learning Rate: 0.020999999999999998\n",
      "Epoch [1064/20000], Bound: 1.3083568811416626, Entropy: 141.90931701660156, Temp: 0.6073312163352966, KL: 71.751220703125, Loss: 0.8476530313491821, Learning Rate: 0.020999999999999998\n",
      "Epoch [1065/20000], Bound: 1.3101475238800049, Entropy: 143.21900939941406, Temp: 0.6066982746124268, KL: 72.28953552246094, Loss: 0.8455816507339478, Learning Rate: 0.020999999999999998\n",
      "Epoch [1066/20000], Bound: 1.368424654006958, Entropy: 139.699462890625, Temp: 0.606082558631897, KL: 79.26307678222656, Loss: 0.898854672908783, Learning Rate: 0.020999999999999998\n",
      "Epoch [1067/20000], Bound: 1.3084169626235962, Entropy: 141.05384826660156, Temp: 0.6055213212966919, KL: 74.83364868164062, Loss: 0.8195809721946716, Learning Rate: 0.020999999999999998\n",
      "Epoch [1068/20000], Bound: 1.3453518152236938, Entropy: 141.8505401611328, Temp: 0.6050516963005066, KL: 77.16641235351562, Loss: 0.8692704439163208, Learning Rate: 0.020999999999999998\n",
      "Epoch [1069/20000], Bound: 1.3738716840744019, Entropy: 135.48130798339844, Temp: 0.6046351790428162, KL: 78.02699279785156, Loss: 0.9173216223716736, Learning Rate: 0.020999999999999998\n",
      "Epoch [1070/20000], Bound: 1.309937834739685, Entropy: 138.803466796875, Temp: 0.604217529296875, KL: 65.78976440429688, Loss: 0.8952024579048157, Learning Rate: 0.020999999999999998\n",
      "Epoch [1071/20000], Bound: 1.2890323400497437, Entropy: 137.2667694091797, Temp: 0.6036379337310791, KL: 71.36241149902344, Loss: 0.8100308775901794, Learning Rate: 0.020999999999999998\n",
      "Epoch [1072/20000], Bound: 1.3622634410858154, Entropy: 141.8636932373047, Temp: 0.6031198501586914, KL: 76.44931030273438, Loss: 0.9047883152961731, Learning Rate: 0.020999999999999998\n",
      "Epoch [1073/20000], Bound: 1.371245265007019, Entropy: 135.93421936035156, Temp: 0.6026114225387573, KL: 80.10574340820312, Loss: 0.891261100769043, Learning Rate: 0.020999999999999998\n",
      "Epoch [1074/20000], Bound: 1.3299143314361572, Entropy: 138.5553436279297, Temp: 0.6021883487701416, KL: 72.88188171386719, Loss: 0.870646059513092, Learning Rate: 0.020999999999999998\n",
      "Epoch [1075/20000], Bound: 1.3771203756332397, Entropy: 137.58485412597656, Temp: 0.60176020860672, KL: 82.84417724609375, Loss: 0.8786286115646362, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1076/20000], Bound: 1.3300607204437256, Entropy: 140.1887969970703, Temp: 0.6014725565910339, KL: 70.61888122558594, Loss: 0.888595461845398, Learning Rate: 0.020999999999999998\n",
      "Epoch [1077/20000], Bound: 1.398665189743042, Entropy: 138.4271697998047, Temp: 0.60111004114151, KL: 85.50590515136719, Loss: 0.8986982703208923, Learning Rate: 0.020999999999999998\n",
      "Epoch [1078/20000], Bound: 1.3528646230697632, Entropy: 139.41758728027344, Temp: 0.6008979678153992, KL: 76.83006286621094, Loss: 0.8795133829116821, Learning Rate: 0.020999999999999998\n",
      "Epoch [1079/20000], Bound: 1.3660647869110107, Entropy: 139.9181365966797, Temp: 0.6007163524627686, KL: 75.68730163574219, Loss: 0.914416491985321, Learning Rate: 0.020999999999999998\n",
      "Epoch [1080/20000], Bound: 1.2657259702682495, Entropy: 139.4740753173828, Temp: 0.6004990339279175, KL: 64.55592346191406, Loss: 0.8206931948661804, Learning Rate: 0.020999999999999998\n",
      "Epoch [1081/20000], Bound: 1.34992516040802, Entropy: 139.3885955810547, Temp: 0.6001967191696167, KL: 78.51934814453125, Loss: 0.8585752844810486, Learning Rate: 0.020999999999999998\n",
      "Epoch [1082/20000], Bound: 1.4082458019256592, Entropy: 136.0858612060547, Temp: 0.5999922156333923, KL: 85.79074096679688, Loss: 0.9137629270553589, Learning Rate: 0.020999999999999998\n",
      "Epoch [1083/20000], Bound: 1.351722002029419, Entropy: 137.63430786132812, Temp: 0.5999128222465515, KL: 75.21769714355469, Loss: 0.8890620470046997, Learning Rate: 0.020999999999999998\n",
      "Epoch [1084/20000], Bound: 1.356533169746399, Entropy: 139.7881622314453, Temp: 0.599818229675293, KL: 80.71435546875, Loss: 0.852380096912384, Learning Rate: 0.020999999999999998\n",
      "Epoch [1085/20000], Bound: 1.326338768005371, Entropy: 134.69171142578125, Temp: 0.5998451709747314, KL: 77.01902770996094, Loss: 0.8257198929786682, Learning Rate: 0.020999999999999998\n",
      "Epoch [1086/20000], Bound: 1.3220082521438599, Entropy: 137.0203857421875, Temp: 0.5999608635902405, KL: 70.72355651855469, Loss: 0.870286762714386, Learning Rate: 0.020999999999999998\n",
      "Epoch [1087/20000], Bound: 1.3462046384811401, Entropy: 134.4042205810547, Temp: 0.5999948978424072, KL: 77.97386169433594, Loss: 0.8556311130523682, Learning Rate: 0.020999999999999998\n",
      "Epoch [1088/20000], Bound: 1.3720847368240356, Entropy: 135.30319213867188, Temp: 0.6000902056694031, KL: 84.69816589355469, Loss: 0.850094735622406, Learning Rate: 0.020999999999999998\n",
      "Epoch [1089/20000], Bound: 1.3376259803771973, Entropy: 132.34765625, Temp: 0.6003509759902954, KL: 71.07588195800781, Loss: 0.8973305225372314, Learning Rate: 0.020999999999999998\n",
      "Epoch [1090/20000], Bound: 1.4005578756332397, Entropy: 135.492919921875, Temp: 0.6004831790924072, KL: 87.40760803222656, Loss: 0.8854932188987732, Learning Rate: 0.020999999999999998\n",
      "Epoch [1091/20000], Bound: 1.3568333387374878, Entropy: 134.5560760498047, Temp: 0.6007671356201172, KL: 77.7000732421875, Loss: 0.87973552942276, Learning Rate: 0.020999999999999998\n",
      "Epoch [1092/20000], Bound: 1.3693979978561401, Entropy: 131.8765106201172, Temp: 0.6010465621948242, KL: 80.085693359375, Loss: 0.8849555850028992, Learning Rate: 0.020999999999999998\n",
      "Epoch [1093/20000], Bound: 1.294334888458252, Entropy: 134.77122497558594, Temp: 0.6013501882553101, KL: 68.0181884765625, Loss: 0.8440484404563904, Learning Rate: 0.020999999999999998\n",
      "Epoch [1094/20000], Bound: 1.3869980573654175, Entropy: 134.11578369140625, Temp: 0.6015373468399048, KL: 81.76577758789062, Loss: 0.9069570899009705, Learning Rate: 0.020999999999999998\n",
      "Epoch [1095/20000], Bound: 1.3569574356079102, Entropy: 134.69564819335938, Temp: 0.6017512679100037, KL: 80.21263122558594, Loss: 0.8608049750328064, Learning Rate: 0.020999999999999998\n",
      "Epoch [1096/20000], Bound: 1.3812530040740967, Entropy: 130.4601593017578, Temp: 0.6020272374153137, KL: 81.45143127441406, Loss: 0.898946225643158, Learning Rate: 0.020999999999999998\n",
      "Epoch [1097/20000], Bound: 1.3351389169692993, Entropy: 135.3929901123047, Temp: 0.6023244857788086, KL: 77.33207702636719, Loss: 0.8438045382499695, Learning Rate: 0.020999999999999998\n",
      "Epoch [1098/20000], Bound: 1.3190429210662842, Entropy: 133.3986053466797, Temp: 0.602651059627533, KL: 71.55697631835938, Loss: 0.8619851469993591, Learning Rate: 0.020999999999999998\n",
      "Epoch [1099/20000], Bound: 1.2802448272705078, Entropy: 134.90087890625, Temp: 0.6028851866722107, KL: 62.933685302734375, Loss: 0.8630545735359192, Learning Rate: 0.020999999999999998\n",
      "Epoch [1100/20000], Bound: 1.2682324647903442, Entropy: 133.3562774658203, Temp: 0.60289067029953, KL: 65.41578674316406, Loss: 0.82115238904953, Learning Rate: 0.020999999999999998\n",
      "Epoch [1101/20000], Bound: 1.35562264919281, Entropy: 141.78256225585938, Temp: 0.6027892827987671, KL: 82.968505859375, Loss: 0.8371803164482117, Learning Rate: 0.020999999999999998\n",
      "Epoch [1102/20000], Bound: 1.3188663721084595, Entropy: 138.62619018554688, Temp: 0.6028513312339783, KL: 78.1859130859375, Loss: 0.8069848418235779, Learning Rate: 0.020999999999999998\n",
      "Epoch [1103/20000], Bound: 1.2908248901367188, Entropy: 140.49728393554688, Temp: 0.6030285358428955, KL: 71.623291015625, Loss: 0.8102103471755981, Learning Rate: 0.020999999999999998\n",
      "Epoch [1104/20000], Bound: 1.2936652898788452, Entropy: 141.41456604003906, Temp: 0.6031989455223083, KL: 72.64425659179688, Loss: 0.8071426153182983, Learning Rate: 0.020999999999999998\n",
      "Epoch [1105/20000], Bound: 1.2400997877120972, Entropy: 143.64414978027344, Temp: 0.6033833026885986, KL: 61.051971435546875, Loss: 0.8090535402297974, Learning Rate: 0.020999999999999998\n",
      "Epoch [1106/20000], Bound: 1.3084068298339844, Entropy: 142.5809326171875, Temp: 0.6033830046653748, KL: 71.35691833496094, Loss: 0.8450442552566528, Learning Rate: 0.020999999999999998\n",
      "Epoch [1107/20000], Bound: 1.3089492321014404, Entropy: 144.45936584472656, Temp: 0.6033384799957275, KL: 71.05914306640625, Loss: 0.848444938659668, Learning Rate: 0.020999999999999998\n",
      "Epoch [1108/20000], Bound: 1.3285518884658813, Entropy: 142.8727264404297, Temp: 0.603244960308075, KL: 75.95376586914062, Loss: 0.8442922830581665, Learning Rate: 0.020999999999999998\n",
      "Epoch [1109/20000], Bound: 1.3075312376022339, Entropy: 142.07920837402344, Temp: 0.6031923890113831, KL: 73.80300903320312, Loss: 0.822866678237915, Learning Rate: 0.020999999999999998\n",
      "Epoch [1110/20000], Bound: 1.3799437284469604, Entropy: 143.97349548339844, Temp: 0.6031725406646729, KL: 94.11590576171875, Loss: 0.793463408946991, Learning Rate: 0.020999999999999998\n",
      "Epoch [1111/20000], Bound: 1.2978076934814453, Entropy: 142.1696014404297, Temp: 0.6035366654396057, KL: 69.64073181152344, Loss: 0.8400625586509705, Learning Rate: 0.020999999999999998\n",
      "Epoch [1112/20000], Bound: 1.2839993238449097, Entropy: 141.0526885986328, Temp: 0.6037985682487488, KL: 71.72174072265625, Loss: 0.798227071762085, Learning Rate: 0.020999999999999998\n",
      "Epoch [1113/20000], Bound: 1.3086535930633545, Entropy: 137.3971710205078, Temp: 0.6040599346160889, KL: 73.26138305664062, Loss: 0.8307585120201111, Learning Rate: 0.020999999999999998\n",
      "Epoch [1114/20000], Bound: 1.3057447671890259, Entropy: 136.65585327148438, Temp: 0.6042985916137695, KL: 74.41740417480469, Loss: 0.8161978721618652, Learning Rate: 0.020999999999999998\n",
      "Epoch [1115/20000], Bound: 1.2967302799224854, Entropy: 134.64822387695312, Temp: 0.6045548319816589, KL: 79.15786743164062, Loss: 0.7608738541603088, Learning Rate: 0.020999999999999998\n",
      "Epoch [1116/20000], Bound: 1.2904139757156372, Entropy: 133.78375244140625, Temp: 0.6049811840057373, KL: 69.52857971191406, Loss: 0.8296540379524231, Learning Rate: 0.020999999999999998\n",
      "Epoch [1117/20000], Bound: 1.2493070363998413, Entropy: 133.77821350097656, Temp: 0.6053041815757751, KL: 62.30458068847656, Loss: 0.816940188407898, Learning Rate: 0.020999999999999998\n",
      "Epoch [1118/20000], Bound: 1.2695761919021606, Entropy: 134.35459899902344, Temp: 0.6054285168647766, KL: 71.20166015625, Loss: 0.7791446447372437, Learning Rate: 0.020999999999999998\n",
      "Epoch [1119/20000], Bound: 1.3230509757995605, Entropy: 134.24087524414062, Temp: 0.6055768132209778, KL: 79.64543151855469, Loss: 0.807221531867981, Learning Rate: 0.020999999999999998\n",
      "Epoch [1120/20000], Bound: 1.2934179306030273, Entropy: 131.95516967773438, Temp: 0.6058411598205566, KL: 70.84068298339844, Loss: 0.8255183100700378, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1121/20000], Bound: 1.275728702545166, Entropy: 134.9616241455078, Temp: 0.6060410737991333, KL: 59.73146057128906, Loss: 0.8855876922607422, Learning Rate: 0.020999999999999998\n",
      "Epoch [1122/20000], Bound: 1.319713830947876, Entropy: 134.1104278564453, Temp: 0.6059115529060364, KL: 73.15763854980469, Loss: 0.855048656463623, Learning Rate: 0.020999999999999998\n",
      "Epoch [1123/20000], Bound: 1.3353315591812134, Entropy: 137.0745391845703, Temp: 0.6057523488998413, KL: 76.83522033691406, Loss: 0.8539626598358154, Learning Rate: 0.020999999999999998\n",
      "Epoch [1124/20000], Bound: 1.3784898519515991, Entropy: 136.35903930664062, Temp: 0.6056278944015503, KL: 92.17782592773438, Loss: 0.811499297618866, Learning Rate: 0.020999999999999998\n",
      "Epoch [1125/20000], Bound: 1.3577133417129517, Entropy: 139.4341583251953, Temp: 0.6058316826820374, KL: 83.01502990722656, Loss: 0.84633868932724, Learning Rate: 0.020999999999999998\n",
      "Epoch [1126/20000], Bound: 1.3360531330108643, Entropy: 139.2225799560547, Temp: 0.6061411499977112, KL: 78.1220703125, Loss: 0.8453652858734131, Learning Rate: 0.020999999999999998\n",
      "Epoch [1127/20000], Bound: 1.3216320276260376, Entropy: 137.22007751464844, Temp: 0.6064693927764893, KL: 73.07150268554688, Loss: 0.8602275848388672, Learning Rate: 0.020999999999999998\n",
      "Epoch [1128/20000], Bound: 1.380502700805664, Entropy: 133.69078063964844, Temp: 0.6067107915878296, KL: 86.01095581054688, Loss: 0.8685181140899658, Learning Rate: 0.020999999999999998\n",
      "Epoch [1129/20000], Bound: 1.3690162897109985, Entropy: 135.1366729736328, Temp: 0.607063889503479, KL: 84.81179809570312, Loss: 0.856069803237915, Learning Rate: 0.020999999999999998\n",
      "Epoch [1130/20000], Bound: 1.3122080564498901, Entropy: 133.2217254638672, Temp: 0.6075150966644287, KL: 78.27677917480469, Loss: 0.8013774752616882, Learning Rate: 0.020999999999999998\n",
      "Epoch [1131/20000], Bound: 1.3014973402023315, Entropy: 133.922119140625, Temp: 0.6080297827720642, KL: 74.02693176269531, Loss: 0.8173167109489441, Learning Rate: 0.020999999999999998\n",
      "Epoch [1132/20000], Bound: 1.3157870769500732, Entropy: 135.54718017578125, Temp: 0.6085074543952942, KL: 79.30117797851562, Loss: 0.8012353181838989, Learning Rate: 0.020999999999999998\n",
      "Epoch [1133/20000], Bound: 1.3190170526504517, Entropy: 132.3782958984375, Temp: 0.6090571284294128, KL: 78.89334106445312, Loss: 0.8115368485450745, Learning Rate: 0.020999999999999998\n",
      "Epoch [1134/20000], Bound: 1.2530879974365234, Entropy: 135.2444610595703, Temp: 0.6096475124359131, KL: 71.02232360839844, Loss: 0.7574656009674072, Learning Rate: 0.020999999999999998\n",
      "Epoch [1135/20000], Bound: 1.3037506341934204, Entropy: 137.3228302001953, Temp: 0.6102238893508911, KL: 80.11993408203125, Loss: 0.7748816013336182, Learning Rate: 0.020999999999999998\n",
      "Epoch [1136/20000], Bound: 1.3052847385406494, Entropy: 134.56423950195312, Temp: 0.6109057664871216, KL: 75.72776794433594, Loss: 0.8147633075714111, Learning Rate: 0.020999999999999998\n",
      "Epoch [1137/20000], Bound: 1.3466519117355347, Entropy: 132.96995544433594, Temp: 0.611550509929657, KL: 89.21429443359375, Loss: 0.7841740846633911, Learning Rate: 0.020999999999999998\n",
      "Epoch [1138/20000], Bound: 1.3140063285827637, Entropy: 137.0171356201172, Temp: 0.6124131083488464, KL: 79.94242858886719, Loss: 0.7989701628684998, Learning Rate: 0.020999999999999998\n",
      "Epoch [1139/20000], Bound: 1.3349025249481201, Entropy: 136.36268615722656, Temp: 0.6133031845092773, KL: 84.85429382324219, Loss: 0.8001285195350647, Learning Rate: 0.020999999999999998\n",
      "Epoch [1140/20000], Bound: 1.262264370918274, Entropy: 134.5690460205078, Temp: 0.6142879724502563, KL: 71.54904174804688, Loss: 0.7757118940353394, Learning Rate: 0.020999999999999998\n",
      "Epoch [1141/20000], Bound: 1.3303767442703247, Entropy: 132.45179748535156, Temp: 0.615178108215332, KL: 83.00001525878906, Loss: 0.8097253441810608, Learning Rate: 0.020999999999999998\n",
      "Epoch [1142/20000], Bound: 1.2614295482635498, Entropy: 130.1158905029297, Temp: 0.6161102056503296, KL: 70.63433837890625, Loss: 0.784160315990448, Learning Rate: 0.020999999999999998\n",
      "Epoch [1143/20000], Bound: 1.3083363771438599, Entropy: 132.2792510986328, Temp: 0.616915762424469, KL: 76.79658508300781, Loss: 0.8210359811782837, Learning Rate: 0.020999999999999998\n",
      "Epoch [1144/20000], Bound: 1.253798246383667, Entropy: 129.9413299560547, Temp: 0.6176480650901794, KL: 68.06532287597656, Loss: 0.7935076951980591, Learning Rate: 0.020999999999999998\n",
      "Epoch [1145/20000], Bound: 1.3404436111450195, Entropy: 129.49700927734375, Temp: 0.618208646774292, KL: 87.4959716796875, Loss: 0.79801344871521, Learning Rate: 0.020999999999999998\n",
      "Epoch [1146/20000], Bound: 1.2991384267807007, Entropy: 128.9275360107422, Temp: 0.6189139485359192, KL: 82.591796875, Loss: 0.7600882649421692, Learning Rate: 0.020999999999999998\n",
      "Epoch [1147/20000], Bound: 1.2164254188537598, Entropy: 131.78500366210938, Temp: 0.6197290420532227, KL: 66.61741638183594, Loss: 0.7430382370948792, Learning Rate: 0.020999999999999998\n",
      "Epoch [1148/20000], Bound: 1.2744783163070679, Entropy: 132.27407836914062, Temp: 0.6204062700271606, KL: 74.26116943359375, Loss: 0.7842463254928589, Learning Rate: 0.020999999999999998\n",
      "Epoch [1149/20000], Bound: 1.301871418952942, Entropy: 129.35635375976562, Temp: 0.6210190057754517, KL: 94.1212158203125, Loss: 0.6757216453552246, Learning Rate: 0.020999999999999998\n",
      "Epoch [1150/20000], Bound: 1.3239597082138062, Entropy: 130.4828338623047, Temp: 0.6220480799674988, KL: 90.31204223632812, Loss: 0.7500988841056824, Learning Rate: 0.020999999999999998\n",
      "Epoch [1151/20000], Bound: 1.2681858539581299, Entropy: 127.8923110961914, Temp: 0.6232714653015137, KL: 70.8396224975586, Loss: 0.8043534755706787, Learning Rate: 0.020999999999999998\n",
      "Epoch [1152/20000], Bound: 1.3091471195220947, Entropy: 130.4245147705078, Temp: 0.6242759823799133, KL: 80.47853088378906, Loss: 0.8043954372406006, Learning Rate: 0.020999999999999998\n",
      "Epoch [1153/20000], Bound: 1.3097487688064575, Entropy: 131.09153747558594, Temp: 0.6252316832542419, KL: 82.69705200195312, Loss: 0.7893022894859314, Learning Rate: 0.020999999999999998\n",
      "Epoch [1154/20000], Bound: 1.2610994577407837, Entropy: 133.228759765625, Temp: 0.626196026802063, KL: 78.86404418945312, Loss: 0.7314437031745911, Learning Rate: 0.020999999999999998\n",
      "Epoch [1155/20000], Bound: 1.2236944437026978, Entropy: 133.09063720703125, Temp: 0.6271944642066956, KL: 69.995361328125, Loss: 0.7375927567481995, Learning Rate: 0.020999999999999998\n",
      "Epoch [1156/20000], Bound: 1.2602735757827759, Entropy: 129.86981201171875, Temp: 0.6280654072761536, KL: 80.12881469726562, Loss: 0.7225701808929443, Learning Rate: 0.020999999999999998\n",
      "Epoch [1157/20000], Bound: 1.2757707834243774, Entropy: 129.5718994140625, Temp: 0.6290043592453003, KL: 87.6094970703125, Loss: 0.6926572322845459, Learning Rate: 0.020999999999999998\n",
      "Epoch [1158/20000], Bound: 1.2231277227401733, Entropy: 127.9940185546875, Temp: 0.6301637291908264, KL: 67.59457397460938, Loss: 0.7593536376953125, Learning Rate: 0.020999999999999998\n",
      "Epoch [1159/20000], Bound: 1.2823095321655273, Entropy: 135.38780212402344, Temp: 0.6310907006263733, KL: 74.87104797363281, Loss: 0.8089355230331421, Learning Rate: 0.020999999999999998\n",
      "Epoch [1160/20000], Bound: 1.2495496273040771, Entropy: 131.8485565185547, Temp: 0.6318448781967163, KL: 75.54182434082031, Loss: 0.7450302243232727, Learning Rate: 0.020999999999999998\n",
      "Epoch [1161/20000], Bound: 1.2553918361663818, Entropy: 131.01531982421875, Temp: 0.6325506567955017, KL: 73.3162841796875, Loss: 0.774064302444458, Learning Rate: 0.020999999999999998\n",
      "Epoch [1162/20000], Bound: 1.2591629028320312, Entropy: 133.52359008789062, Temp: 0.6331270933151245, KL: 71.27496337890625, Loss: 0.7977665662765503, Learning Rate: 0.020999999999999998\n",
      "Epoch [1163/20000], Bound: 1.327443242073059, Entropy: 134.9961700439453, Temp: 0.6335133910179138, KL: 89.78228759765625, Loss: 0.7809498906135559, Learning Rate: 0.020999999999999998\n",
      "Epoch [1164/20000], Bound: 1.3160344362258911, Entropy: 136.09036254882812, Temp: 0.6340411305427551, KL: 76.68177795410156, Loss: 0.862893283367157, Learning Rate: 0.020999999999999998\n",
      "Epoch [1165/20000], Bound: 1.3146063089370728, Entropy: 136.49595642089844, Temp: 0.6343646049499512, KL: 86.54359436035156, Loss: 0.7828884124755859, Learning Rate: 0.020999999999999998\n",
      "Epoch [1166/20000], Bound: 1.309501051902771, Entropy: 135.41043090820312, Temp: 0.634779691696167, KL: 79.49076843261719, Loss: 0.8292635083198547, Learning Rate: 0.020999999999999998\n",
      "Epoch [1167/20000], Bound: 1.2882206439971924, Entropy: 135.1255645751953, Temp: 0.6350934505462646, KL: 76.19422912597656, Loss: 0.81524258852005, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1168/20000], Bound: 1.3112648725509644, Entropy: 132.29861450195312, Temp: 0.6352843046188354, KL: 82.86233520507812, Loss: 0.806904137134552, Learning Rate: 0.020999999999999998\n",
      "Epoch [1169/20000], Bound: 1.2508732080459595, Entropy: 134.14730834960938, Temp: 0.6354805827140808, KL: 79.15684509277344, Loss: 0.7238765358924866, Learning Rate: 0.020999999999999998\n",
      "Epoch [1170/20000], Bound: 1.346970558166504, Entropy: 131.02403259277344, Temp: 0.6357573866844177, KL: 90.61444091796875, Loss: 0.8172137141227722, Learning Rate: 0.020999999999999998\n",
      "Epoch [1171/20000], Bound: 1.2865335941314697, Entropy: 130.19810485839844, Temp: 0.6361290812492371, KL: 81.73088073730469, Loss: 0.7700485587120056, Learning Rate: 0.020999999999999998\n",
      "Epoch [1172/20000], Bound: 1.203764796257019, Entropy: 131.43870544433594, Temp: 0.636525571346283, KL: 68.69119262695312, Loss: 0.7249587774276733, Learning Rate: 0.020999999999999998\n",
      "Epoch [1173/20000], Bound: 1.2569069862365723, Entropy: 129.35540771484375, Temp: 0.6368059515953064, KL: 76.89781188964844, Loss: 0.7543591260910034, Learning Rate: 0.020999999999999998\n",
      "Epoch [1174/20000], Bound: 1.2823808193206787, Entropy: 129.69293212890625, Temp: 0.6370660662651062, KL: 80.40817260742188, Loss: 0.7740580439567566, Learning Rate: 0.020999999999999998\n",
      "Epoch [1175/20000], Bound: 1.2445828914642334, Entropy: 128.40481567382812, Temp: 0.6373299956321716, KL: 69.18377685546875, Loss: 0.7933977842330933, Learning Rate: 0.020999999999999998\n",
      "Epoch [1176/20000], Bound: 1.2903529405593872, Entropy: 128.14627075195312, Temp: 0.6373844146728516, KL: 83.94630432128906, Loss: 0.7617460489273071, Learning Rate: 0.020999999999999998\n",
      "Epoch [1177/20000], Bound: 1.290661334991455, Entropy: 126.64582061767578, Temp: 0.6375364065170288, KL: 83.06999969482422, Loss: 0.7694340348243713, Learning Rate: 0.020999999999999998\n",
      "Epoch [1178/20000], Bound: 1.2200969457626343, Entropy: 129.47103881835938, Temp: 0.6377494931221008, KL: 68.69493103027344, Loss: 0.7545385360717773, Learning Rate: 0.020999999999999998\n",
      "Epoch [1179/20000], Bound: 1.2775312662124634, Entropy: 130.84005737304688, Temp: 0.6378099918365479, KL: 73.03770446777344, Loss: 0.823889434337616, Learning Rate: 0.020999999999999998\n",
      "Epoch [1180/20000], Bound: 1.2398697137832642, Entropy: 131.80287170410156, Temp: 0.6376930475234985, KL: 70.08419799804688, Loss: 0.7783809304237366, Learning Rate: 0.020999999999999998\n",
      "Epoch [1181/20000], Bound: 1.2501379251480103, Entropy: 138.31332397460938, Temp: 0.6374411582946777, KL: 72.00895690917969, Loss: 0.7813451886177063, Learning Rate: 0.020999999999999998\n",
      "Epoch [1182/20000], Bound: 1.264237880706787, Entropy: 139.7008514404297, Temp: 0.6370957493782043, KL: 76.29820251464844, Loss: 0.7728154063224792, Learning Rate: 0.020999999999999998\n",
      "Epoch [1183/20000], Bound: 1.3417766094207764, Entropy: 137.69171142578125, Temp: 0.6367510557174683, KL: 85.66621398925781, Loss: 0.8473601937294006, Learning Rate: 0.020999999999999998\n",
      "Epoch [1184/20000], Bound: 1.3072948455810547, Entropy: 140.13377380371094, Temp: 0.6364361047744751, KL: 82.77931213378906, Loss: 0.8017197847366333, Learning Rate: 0.020999999999999998\n",
      "Epoch [1185/20000], Bound: 1.2949888706207275, Entropy: 139.85629272460938, Temp: 0.6361777782440186, KL: 80.38409423828125, Loss: 0.7966468930244446, Learning Rate: 0.020999999999999998\n",
      "Epoch [1186/20000], Bound: 1.2906606197357178, Entropy: 139.9049835205078, Temp: 0.6359429359436035, KL: 83.94441223144531, Loss: 0.7601228952407837, Learning Rate: 0.020999999999999998\n",
      "Epoch [1187/20000], Bound: 1.2389565706253052, Entropy: 136.6748809814453, Temp: 0.6358452439308167, KL: 62.45863342285156, Loss: 0.8344042897224426, Learning Rate: 0.020999999999999998\n",
      "Epoch [1188/20000], Bound: 1.2768338918685913, Entropy: 133.60610961914062, Temp: 0.6354032754898071, KL: 75.69085693359375, Loss: 0.7984136939048767, Learning Rate: 0.020999999999999998\n",
      "Epoch [1189/20000], Bound: 1.259147047996521, Entropy: 132.65371704101562, Temp: 0.6349296569824219, KL: 72.69093322753906, Loss: 0.7889612317085266, Learning Rate: 0.020999999999999998\n",
      "Epoch [1190/20000], Bound: 1.3193577527999878, Entropy: 131.74807739257812, Temp: 0.6343964338302612, KL: 84.87879943847656, Loss: 0.8052875399589539, Learning Rate: 0.020999999999999998\n",
      "Epoch [1191/20000], Bound: 1.2753615379333496, Entropy: 132.32180786132812, Temp: 0.633979856967926, KL: 75.60337829589844, Loss: 0.7943769097328186, Learning Rate: 0.020999999999999998\n",
      "Epoch [1192/20000], Bound: 1.2968310117721558, Entropy: 131.75814819335938, Temp: 0.633541464805603, KL: 78.46150207519531, Loss: 0.811319887638092, Learning Rate: 0.020999999999999998\n",
      "Epoch [1193/20000], Bound: 1.2508745193481445, Entropy: 134.04954528808594, Temp: 0.6331047415733337, KL: 69.50650024414062, Loss: 0.7967742681503296, Learning Rate: 0.020999999999999998\n",
      "Epoch [1194/20000], Bound: 1.2681427001953125, Entropy: 134.31895446777344, Temp: 0.6325488686561584, KL: 67.17436218261719, Loss: 0.8457416892051697, Learning Rate: 0.020999999999999998\n",
      "Epoch [1195/20000], Bound: 1.3084473609924316, Entropy: 137.70999145507812, Temp: 0.6317738890647888, KL: 78.31304931640625, Loss: 0.8319353461265564, Learning Rate: 0.020999999999999998\n",
      "Epoch [1196/20000], Bound: 1.2352244853973389, Entropy: 139.489990234375, Temp: 0.6310092806816101, KL: 65.55189514160156, Loss: 0.7976780533790588, Learning Rate: 0.020999999999999998\n",
      "Epoch [1197/20000], Bound: 1.2888072729110718, Entropy: 139.0830078125, Temp: 0.6301015019416809, KL: 79.56489562988281, Loss: 0.7823612689971924, Learning Rate: 0.020999999999999998\n",
      "Epoch [1198/20000], Bound: 1.310997724533081, Entropy: 140.46539306640625, Temp: 0.6293249130249023, KL: 85.117431640625, Loss: 0.7790014743804932, Learning Rate: 0.020999999999999998\n",
      "Epoch [1199/20000], Bound: 1.2179021835327148, Entropy: 140.25, Temp: 0.6287633776664734, KL: 64.64422607421875, Loss: 0.772105872631073, Learning Rate: 0.020999999999999998\n",
      "Epoch [1200/20000], Bound: 1.2900495529174805, Entropy: 139.7591552734375, Temp: 0.6280739903450012, KL: 79.21861267089844, Loss: 0.7843888401985168, Learning Rate: 0.020999999999999998\n",
      "Epoch [1201/20000], Bound: 1.245766043663025, Entropy: 140.2217559814453, Temp: 0.6274958252906799, KL: 75.23130798339844, Loss: 0.7348666191101074, Learning Rate: 0.020999999999999998\n",
      "Epoch [1202/20000], Bound: 1.3282690048217773, Entropy: 132.96006774902344, Temp: 0.6270357370376587, KL: 85.36344909667969, Loss: 0.8066147565841675, Learning Rate: 0.020999999999999998\n",
      "Epoch [1203/20000], Bound: 1.209352970123291, Entropy: 132.9273223876953, Temp: 0.626731276512146, KL: 70.34454345703125, Loss: 0.7096482515335083, Learning Rate: 0.020999999999999998\n",
      "Epoch [1204/20000], Bound: 1.276436448097229, Entropy: 135.09327697753906, Temp: 0.6264811158180237, KL: 79.26408386230469, Loss: 0.7565023899078369, Learning Rate: 0.020999999999999998\n",
      "Epoch [1205/20000], Bound: 1.2422653436660767, Entropy: 131.484130859375, Temp: 0.6263522505760193, KL: 76.64958190917969, Loss: 0.7158119082450867, Learning Rate: 0.020999999999999998\n",
      "Epoch [1206/20000], Bound: 1.2639822959899902, Entropy: 130.69363403320312, Temp: 0.6263572573661804, KL: 75.35031127929688, Loss: 0.7649203538894653, Learning Rate: 0.020999999999999998\n",
      "Epoch [1207/20000], Bound: 1.2991381883621216, Entropy: 125.96231079101562, Temp: 0.6263814568519592, KL: 78.25520324707031, Loss: 0.8065274357795715, Learning Rate: 0.020999999999999998\n",
      "Epoch [1208/20000], Bound: 1.3153964281082153, Entropy: 125.73664093017578, Temp: 0.6264041662216187, KL: 78.97039031982422, Loss: 0.8317152261734009, Learning Rate: 0.020999999999999998\n",
      "Epoch [1209/20000], Bound: 1.2458938360214233, Entropy: 130.19309997558594, Temp: 0.6263972520828247, KL: 71.794921875, Loss: 0.7610301971435547, Learning Rate: 0.020999999999999998\n",
      "Epoch [1210/20000], Bound: 1.3190877437591553, Entropy: 125.42001342773438, Temp: 0.6263575553894043, KL: 81.4552001953125, Loss: 0.8188979029655457, Learning Rate: 0.020999999999999998\n",
      "Epoch [1211/20000], Bound: 1.2856266498565674, Entropy: 126.26815795898438, Temp: 0.6263546943664551, KL: 78.54812622070312, Loss: 0.7789479494094849, Learning Rate: 0.020999999999999998\n",
      "Epoch [1212/20000], Bound: 1.2874475717544556, Entropy: 130.63845825195312, Temp: 0.6264017820358276, KL: 74.05723571777344, Loss: 0.8182377219200134, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1213/20000], Bound: 1.3189347982406616, Entropy: 128.77919006347656, Temp: 0.6263576149940491, KL: 80.85226440429688, Loss: 0.8234164118766785, Learning Rate: 0.020999999999999998\n",
      "Epoch [1214/20000], Bound: 1.319219708442688, Entropy: 131.4679718017578, Temp: 0.6263340711593628, KL: 82.08351135253906, Loss: 0.8140981793403625, Learning Rate: 0.020999999999999998\n",
      "Epoch [1215/20000], Bound: 1.2959798574447632, Entropy: 133.41336059570312, Temp: 0.6263633370399475, KL: 71.43746948242188, Loss: 0.8549965620040894, Learning Rate: 0.020999999999999998\n",
      "Epoch [1216/20000], Bound: 1.323414921760559, Entropy: 133.74913024902344, Temp: 0.6262027621269226, KL: 76.17036437988281, Loss: 0.8691967725753784, Learning Rate: 0.020999999999999998\n",
      "Epoch [1217/20000], Bound: 1.3390401601791382, Entropy: 132.51287841796875, Temp: 0.6259282827377319, KL: 83.55836486816406, Loss: 0.8402640223503113, Learning Rate: 0.020999999999999998\n",
      "Epoch [1218/20000], Bound: 1.269240140914917, Entropy: 137.9564971923828, Temp: 0.6257163882255554, KL: 74.36900329589844, Loss: 0.7813769578933716, Learning Rate: 0.020999999999999998\n",
      "Epoch [1219/20000], Bound: 1.260940432548523, Entropy: 136.38417053222656, Temp: 0.6255060434341431, KL: 78.62809753417969, Loss: 0.7320511341094971, Learning Rate: 0.020999999999999998\n",
      "Epoch [1220/20000], Bound: 1.2986955642700195, Entropy: 138.333740234375, Temp: 0.6254488229751587, KL: 84.359130859375, Loss: 0.755483865737915, Learning Rate: 0.020999999999999998\n",
      "Epoch [1221/20000], Bound: 1.2470488548278809, Entropy: 137.4279022216797, Temp: 0.6255838871002197, KL: 69.91168212890625, Loss: 0.7770538926124573, Learning Rate: 0.020999999999999998\n",
      "Epoch [1222/20000], Bound: 1.3537062406539917, Entropy: 133.43934631347656, Temp: 0.6256184577941895, KL: 89.10432434082031, Loss: 0.8245797753334045, Learning Rate: 0.020999999999999998\n",
      "Epoch [1223/20000], Bound: 1.1791901588439941, Entropy: 136.38406372070312, Temp: 0.6257979869842529, KL: 69.31488037109375, Loss: 0.6662799715995789, Learning Rate: 0.020999999999999998\n",
      "Epoch [1224/20000], Bound: 1.314569115638733, Entropy: 132.8085174560547, Temp: 0.6260439157485962, KL: 69.45268249511719, Loss: 0.9055812358856201, Learning Rate: 0.020999999999999998\n",
      "Epoch [1225/20000], Bound: 1.3367611169815063, Entropy: 134.63128662109375, Temp: 0.6259691119194031, KL: 81.50550842285156, Loss: 0.8522434234619141, Learning Rate: 0.020999999999999998\n",
      "Epoch [1226/20000], Bound: 1.263383388519287, Entropy: 133.51556396484375, Temp: 0.6258852481842041, KL: 76.33572387695312, Loss: 0.755307137966156, Learning Rate: 0.020999999999999998\n",
      "Epoch [1227/20000], Bound: 1.2732760906219482, Entropy: 134.7743377685547, Temp: 0.6258642673492432, KL: 73.77288818359375, Loss: 0.7936877012252808, Learning Rate: 0.020999999999999998\n",
      "Epoch [1228/20000], Bound: 1.2649003267288208, Entropy: 134.1553497314453, Temp: 0.6257951855659485, KL: 72.03578186035156, Loss: 0.7922728061676025, Learning Rate: 0.020999999999999998\n",
      "Epoch [1229/20000], Bound: 1.2830733060836792, Entropy: 134.88671875, Temp: 0.6256563067436218, KL: 77.53819274902344, Loss: 0.7812645435333252, Learning Rate: 0.020999999999999998\n",
      "Epoch [1230/20000], Bound: 1.3054490089416504, Entropy: 134.61160278320312, Temp: 0.6255648732185364, KL: 86.09355163574219, Loss: 0.7545332908630371, Learning Rate: 0.020999999999999998\n",
      "Epoch [1231/20000], Bound: 1.2337071895599365, Entropy: 137.6893310546875, Temp: 0.625698447227478, KL: 71.15512084960938, Loss: 0.7438242435455322, Learning Rate: 0.020999999999999998\n",
      "Epoch [1232/20000], Bound: 1.3324394226074219, Entropy: 135.0116424560547, Temp: 0.6258057355880737, KL: 84.62232971191406, Loss: 0.8185983300209045, Learning Rate: 0.020999999999999998\n",
      "Epoch [1233/20000], Bound: 1.2939951419830322, Entropy: 138.3383026123047, Temp: 0.6259898543357849, KL: 84.64520263671875, Loss: 0.7452589273452759, Learning Rate: 0.020999999999999998\n",
      "Epoch [1234/20000], Bound: 1.2730425596237183, Entropy: 139.2128143310547, Temp: 0.6263619661331177, KL: 70.33633422851562, Loss: 0.8213913440704346, Learning Rate: 0.020999999999999998\n",
      "Epoch [1235/20000], Bound: 1.2806092500686646, Entropy: 138.60560607910156, Temp: 0.6265420317649841, KL: 79.01469421386719, Loss: 0.7662432789802551, Learning Rate: 0.020999999999999998\n",
      "Epoch [1236/20000], Bound: 1.2605350017547607, Entropy: 136.45391845703125, Temp: 0.6267819404602051, KL: 86.85658264160156, Loss: 0.6675117611885071, Learning Rate: 0.020999999999999998\n",
      "Epoch [1237/20000], Bound: 1.3102892637252808, Entropy: 134.0868682861328, Temp: 0.6273670792579651, KL: 83.1983642578125, Loss: 0.7897552251815796, Learning Rate: 0.020999999999999998\n",
      "Epoch [1238/20000], Bound: 1.233267068862915, Entropy: 135.51547241210938, Temp: 0.6279969811439514, KL: 70.90948486328125, Loss: 0.7479591369628906, Learning Rate: 0.020999999999999998\n",
      "Epoch [1239/20000], Bound: 1.284753441810608, Entropy: 134.42861938476562, Temp: 0.6285286545753479, KL: 78.18751525878906, Loss: 0.7834262847900391, Learning Rate: 0.020999999999999998\n",
      "Epoch [1240/20000], Bound: 1.2955822944641113, Entropy: 133.8625946044922, Temp: 0.6290331482887268, KL: 79.39088439941406, Loss: 0.794819176197052, Learning Rate: 0.020999999999999998\n",
      "Epoch [1241/20000], Bound: 1.3464142084121704, Entropy: 138.10093688964844, Temp: 0.6295113563537598, KL: 88.03385925292969, Loss: 0.8254418969154358, Learning Rate: 0.020999999999999998\n",
      "Epoch [1242/20000], Bound: 1.3106313943862915, Entropy: 133.25103759765625, Temp: 0.6300516128540039, KL: 78.57876586914062, Loss: 0.8313701152801514, Learning Rate: 0.020999999999999998\n",
      "Epoch [1243/20000], Bound: 1.3246393203735352, Entropy: 135.89784240722656, Temp: 0.6304842233657837, KL: 86.69284057617188, Loss: 0.7947628498077393, Learning Rate: 0.020999999999999998\n",
      "Epoch [1244/20000], Bound: 1.3081861734390259, Entropy: 139.6729278564453, Temp: 0.6310070157051086, KL: 81.94790649414062, Loss: 0.801461935043335, Learning Rate: 0.020999999999999998\n",
      "Epoch [1245/20000], Bound: 1.3257472515106201, Entropy: 135.53274536132812, Temp: 0.6315216422080994, KL: 77.89276123046875, Loss: 0.8683404922485352, Learning Rate: 0.020999999999999998\n",
      "Epoch [1246/20000], Bound: 1.330829381942749, Entropy: 137.3520050048828, Temp: 0.6318526268005371, KL: 81.99247741699219, Loss: 0.846388578414917, Learning Rate: 0.020999999999999998\n",
      "Epoch [1247/20000], Bound: 1.2954033613204956, Entropy: 134.67684936523438, Temp: 0.632117509841919, KL: 70.85002136230469, Loss: 0.8667004704475403, Learning Rate: 0.020999999999999998\n",
      "Epoch [1248/20000], Bound: 1.3239881992340088, Entropy: 134.6420135498047, Temp: 0.6321053504943848, KL: 86.80494689941406, Loss: 0.7953216433525085, Learning Rate: 0.020999999999999998\n",
      "Epoch [1249/20000], Bound: 1.3302116394042969, Entropy: 136.24374389648438, Temp: 0.6322194933891296, KL: 89.36773681640625, Loss: 0.7874432802200317, Learning Rate: 0.020999999999999998\n",
      "Epoch [1250/20000], Bound: 1.3037875890731812, Entropy: 131.39048767089844, Temp: 0.6324998140335083, KL: 80.73927307128906, Loss: 0.8049575686454773, Learning Rate: 0.020999999999999998\n",
      "Epoch [1251/20000], Bound: 1.2580660581588745, Entropy: 133.3129425048828, Temp: 0.6327624320983887, KL: 72.38851928710938, Loss: 0.7865031361579895, Learning Rate: 0.020999999999999998\n",
      "Epoch [1252/20000], Bound: 1.26708984375, Entropy: 133.3917999267578, Temp: 0.6328991055488586, KL: 74.96597290039062, Loss: 0.7827141880989075, Learning Rate: 0.020999999999999998\n",
      "Epoch [1253/20000], Bound: 1.2528561353683472, Entropy: 130.389404296875, Temp: 0.632971465587616, KL: 71.33970642089844, Loss: 0.7856787443161011, Learning Rate: 0.020999999999999998\n",
      "Epoch [1254/20000], Bound: 1.262732744216919, Entropy: 128.2997589111328, Temp: 0.6329191327095032, KL: 76.51239013671875, Loss: 0.7625894546508789, Learning Rate: 0.020999999999999998\n",
      "Epoch [1255/20000], Bound: 1.250478744506836, Entropy: 132.08804321289062, Temp: 0.6328802704811096, KL: 74.64022827148438, Loss: 0.755219042301178, Learning Rate: 0.020999999999999998\n",
      "Epoch [1256/20000], Bound: 1.249674916267395, Entropy: 132.47024536132812, Temp: 0.632834792137146, KL: 77.05068969726562, Loss: 0.7346736788749695, Learning Rate: 0.020999999999999998\n",
      "Epoch [1257/20000], Bound: 1.2391318082809448, Entropy: 134.82675170898438, Temp: 0.6328588128089905, KL: 75.59834289550781, Loss: 0.7274231314659119, Learning Rate: 0.020999999999999998\n",
      "Epoch [1258/20000], Bound: 1.2576225996017456, Entropy: 133.01321411132812, Temp: 0.6329331994056702, KL: 81.13612365722656, Loss: 0.716825544834137, Learning Rate: 0.020999999999999998\n",
      "Epoch [1259/20000], Bound: 1.249356985092163, Entropy: 130.82427978515625, Temp: 0.6331631541252136, KL: 70.81678771972656, Loss: 0.783783495426178, Learning Rate: 0.020999999999999998\n",
      "Epoch [1260/20000], Bound: 1.2077115774154663, Entropy: 132.81553649902344, Temp: 0.6332453489303589, KL: 67.10664367675781, Loss: 0.7403466105461121, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1261/20000], Bound: 1.201833963394165, Entropy: 134.37025451660156, Temp: 0.633202075958252, KL: 62.92384338378906, Loss: 0.7633105516433716, Learning Rate: 0.020999999999999998\n",
      "Epoch [1262/20000], Bound: 1.2670884132385254, Entropy: 133.31118774414062, Temp: 0.632933497428894, KL: 76.25967407226562, Loss: 0.7725393772125244, Learning Rate: 0.020999999999999998\n",
      "Epoch [1263/20000], Bound: 1.305603265762329, Entropy: 131.513671875, Temp: 0.6326790452003479, KL: 85.522216796875, Loss: 0.7709001898765564, Learning Rate: 0.020999999999999998\n",
      "Epoch [1264/20000], Bound: 1.2871088981628418, Entropy: 131.89036560058594, Temp: 0.6325932145118713, KL: 79.15779113769531, Loss: 0.7861274480819702, Learning Rate: 0.020999999999999998\n",
      "Epoch [1265/20000], Bound: 1.2801212072372437, Entropy: 130.9857177734375, Temp: 0.6325308084487915, KL: 80.70037841796875, Loss: 0.760839581489563, Learning Rate: 0.020999999999999998\n",
      "Epoch [1266/20000], Bound: 1.2199503183364868, Entropy: 128.30052185058594, Temp: 0.6325575709342957, KL: 70.08013916015625, Loss: 0.7370835542678833, Learning Rate: 0.020999999999999998\n",
      "Epoch [1267/20000], Bound: 1.2684853076934814, Entropy: 126.03216552734375, Temp: 0.6325251460075378, KL: 82.77848815917969, Loss: 0.7229887843132019, Learning Rate: 0.020999999999999998\n",
      "Epoch [1268/20000], Bound: 1.236803650856018, Entropy: 127.7303466796875, Temp: 0.6326780319213867, KL: 74.49038696289062, Loss: 0.7318264842033386, Learning Rate: 0.020999999999999998\n",
      "Epoch [1269/20000], Bound: 1.2616021633148193, Entropy: 126.12496948242188, Temp: 0.6328433752059937, KL: 89.25303649902344, Loss: 0.6597694158554077, Learning Rate: 0.020999999999999998\n",
      "Epoch [1270/20000], Bound: 1.2472423315048218, Entropy: 127.51151275634766, Temp: 0.633388340473175, KL: 78.24933624267578, Loss: 0.7216203808784485, Learning Rate: 0.020999999999999998\n",
      "Epoch [1271/20000], Bound: 1.2389869689941406, Entropy: 123.8908920288086, Temp: 0.6339842081069946, KL: 83.7862777709961, Loss: 0.6640892028808594, Learning Rate: 0.020999999999999998\n",
      "Epoch [1272/20000], Bound: 1.2062638998031616, Entropy: 123.59111022949219, Temp: 0.6348153352737427, KL: 74.66545104980469, Loss: 0.6801632046699524, Learning Rate: 0.020999999999999998\n",
      "Epoch [1273/20000], Bound: 1.2022181749343872, Entropy: 119.52376556396484, Temp: 0.6356738805770874, KL: 75.9855728149414, Loss: 0.6639482975006104, Learning Rate: 0.020999999999999998\n",
      "Epoch [1274/20000], Bound: 1.2016634941101074, Entropy: 122.8809814453125, Temp: 0.6366042494773865, KL: 81.50999450683594, Loss: 0.6207852959632874, Learning Rate: 0.020999999999999998\n",
      "Epoch [1275/20000], Bound: 1.2059308290481567, Entropy: 119.45249938964844, Temp: 0.6377649903297424, KL: 76.41339111328125, Loss: 0.6695750951766968, Learning Rate: 0.020999999999999998\n",
      "Epoch [1276/20000], Bound: 1.226706624031067, Entropy: 120.23149871826172, Temp: 0.6389546990394592, KL: 79.11629486083984, Loss: 0.6859937906265259, Learning Rate: 0.020999999999999998\n",
      "Epoch [1277/20000], Bound: 1.2770488262176514, Entropy: 124.83206939697266, Temp: 0.6401804089546204, KL: 86.61383819580078, Loss: 0.7201985120773315, Learning Rate: 0.020999999999999998\n",
      "Epoch [1278/20000], Bound: 1.2687140703201294, Entropy: 125.90044403076172, Temp: 0.6414933204650879, KL: 79.45938873291016, Loss: 0.7624832987785339, Learning Rate: 0.020999999999999998\n",
      "Epoch [1279/20000], Bound: 1.228212833404541, Entropy: 125.40159606933594, Temp: 0.6426884531974792, KL: 75.44566345214844, Loss: 0.7221717834472656, Learning Rate: 0.020999999999999998\n",
      "Epoch [1280/20000], Bound: 1.2548351287841797, Entropy: 127.50772857666016, Temp: 0.6437743306159973, KL: 77.35704803466797, Loss: 0.7565236687660217, Learning Rate: 0.020999999999999998\n",
      "Epoch [1281/20000], Bound: 1.2661452293395996, Entropy: 125.70292663574219, Temp: 0.6447276473045349, KL: 94.09066772460938, Loss: 0.6488308310508728, Learning Rate: 0.020999999999999998\n",
      "Epoch [1282/20000], Bound: 1.294437050819397, Entropy: 132.46397399902344, Temp: 0.6460163593292236, KL: 88.73442077636719, Loss: 0.7456615567207336, Learning Rate: 0.020999999999999998\n",
      "Epoch [1283/20000], Bound: 1.2679624557495117, Entropy: 130.37828063964844, Temp: 0.6473430395126343, KL: 81.11335754394531, Loss: 0.7565670609474182, Learning Rate: 0.020999999999999998\n",
      "Epoch [1284/20000], Bound: 1.2365580797195435, Entropy: 131.52334594726562, Temp: 0.6485562324523926, KL: 77.95460510253906, Loss: 0.7252163290977478, Learning Rate: 0.020999999999999998\n",
      "Epoch [1285/20000], Bound: 1.2476937770843506, Entropy: 131.20030212402344, Temp: 0.6496639251708984, KL: 79.03453063964844, Loss: 0.7385027408599854, Learning Rate: 0.020999999999999998\n",
      "Epoch [1286/20000], Bound: 1.1768956184387207, Entropy: 133.4982147216797, Temp: 0.6506648659706116, KL: 73.1207275390625, Loss: 0.6613600254058838, Learning Rate: 0.020999999999999998\n",
      "Epoch [1287/20000], Bound: 1.214781403541565, Entropy: 131.10267639160156, Temp: 0.651605486869812, KL: 70.08216857910156, Loss: 0.7508861422538757, Learning Rate: 0.020999999999999998\n",
      "Epoch [1288/20000], Bound: 1.2594867944717407, Entropy: 133.21282958984375, Temp: 0.6522724032402039, KL: 91.47323608398438, Loss: 0.6683495044708252, Learning Rate: 0.020999999999999998\n",
      "Epoch [1289/20000], Bound: 1.2070057392120361, Entropy: 134.13760375976562, Temp: 0.6531883478164673, KL: 81.40327453613281, Loss: 0.6524561643600464, Learning Rate: 0.020999999999999998\n",
      "Epoch [1290/20000], Bound: 1.2620606422424316, Entropy: 131.5769500732422, Temp: 0.6541945338249207, KL: 95.25526428222656, Loss: 0.6471304893493652, Learning Rate: 0.020999999999999998\n",
      "Epoch [1291/20000], Bound: 1.2424339056015015, Entropy: 131.1803436279297, Temp: 0.6555036902427673, KL: 82.76155090332031, Loss: 0.7082801461219788, Learning Rate: 0.020999999999999998\n",
      "Epoch [1292/20000], Bound: 1.2425395250320435, Entropy: 124.41514587402344, Temp: 0.6567707061767578, KL: 78.90902709960938, Loss: 0.739527702331543, Learning Rate: 0.020999999999999998\n",
      "Epoch [1293/20000], Bound: 1.2007406949996948, Entropy: 125.17665100097656, Temp: 0.6578733921051025, KL: 74.85740661621094, Loss: 0.6972560882568359, Learning Rate: 0.020999999999999998\n",
      "Epoch [1294/20000], Bound: 1.2845617532730103, Entropy: 122.38169860839844, Temp: 0.6588323712348938, KL: 86.17787170410156, Loss: 0.7659496068954468, Learning Rate: 0.020999999999999998\n",
      "Epoch [1295/20000], Bound: 1.3647351264953613, Entropy: 121.07350158691406, Temp: 0.6597145199775696, KL: 108.89907836914062, Loss: 0.7581789493560791, Learning Rate: 0.020999999999999998\n",
      "Epoch [1296/20000], Bound: 1.2405178546905518, Entropy: 117.30282592773438, Temp: 0.6608728766441345, KL: 73.96742248535156, Loss: 0.7785888314247131, Learning Rate: 0.020999999999999998\n",
      "Epoch [1297/20000], Bound: 1.2690116167068481, Entropy: 120.8749771118164, Temp: 0.6617029905319214, KL: 75.66390228271484, Loss: 0.8198893666267395, Learning Rate: 0.020999999999999998\n",
      "Epoch [1298/20000], Bound: 1.2793736457824707, Entropy: 122.28934478759766, Temp: 0.6621874570846558, KL: 87.83353424072266, Loss: 0.7484313249588013, Learning Rate: 0.020999999999999998\n",
      "Epoch [1299/20000], Bound: 1.256386160850525, Entropy: 124.47676849365234, Temp: 0.662680983543396, KL: 84.9725570678711, Loss: 0.7271683812141418, Learning Rate: 0.020999999999999998\n",
      "Epoch [1300/20000], Bound: 1.3063106536865234, Entropy: 121.62992095947266, Temp: 0.6631744503974915, KL: 94.88167572021484, Loss: 0.7495222091674805, Learning Rate: 0.020999999999999998\n",
      "Epoch [1301/20000], Bound: 1.224513292312622, Entropy: 125.46847534179688, Temp: 0.6637763381004333, KL: 69.53858947753906, Loss: 0.7864659428596497, Learning Rate: 0.020999999999999998\n",
      "Epoch [1302/20000], Bound: 1.2214882373809814, Entropy: 129.5102081298828, Temp: 0.6640000939369202, KL: 67.937255859375, Loss: 0.7933456897735596, Learning Rate: 0.020999999999999998\n",
      "Epoch [1303/20000], Bound: 1.2076976299285889, Entropy: 131.43333435058594, Temp: 0.663843035697937, KL: 71.87409973144531, Loss: 0.7389804124832153, Learning Rate: 0.020999999999999998\n",
      "Epoch [1304/20000], Bound: 1.238702416419983, Entropy: 132.779541015625, Temp: 0.6635087132453918, KL: 77.41836547851562, Loss: 0.7525297999382019, Learning Rate: 0.020999999999999998\n",
      "Epoch [1305/20000], Bound: 1.307090163230896, Entropy: 129.4960174560547, Temp: 0.6630838513374329, KL: 92.39065551757812, Loss: 0.7697117924690247, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1306/20000], Bound: 1.2714000940322876, Entropy: 130.16879272460938, Temp: 0.6627843379974365, KL: 79.20294189453125, Loss: 0.7991658449172974, Learning Rate: 0.020999999999999998\n",
      "Epoch [1307/20000], Bound: 1.2820130586624146, Entropy: 129.7013397216797, Temp: 0.6623395085334778, KL: 77.56753540039062, Loss: 0.8312392830848694, Learning Rate: 0.020999999999999998\n",
      "Epoch [1308/20000], Bound: 1.3009226322174072, Entropy: 133.44866943359375, Temp: 0.6616837978363037, KL: 86.191162109375, Loss: 0.8020681738853455, Learning Rate: 0.020999999999999998\n",
      "Epoch [1309/20000], Bound: 1.262088418006897, Entropy: 133.1409454345703, Temp: 0.6610304117202759, KL: 86.91670227050781, Loss: 0.7208209037780762, Learning Rate: 0.020999999999999998\n",
      "Epoch [1310/20000], Bound: 1.1908587217330933, Entropy: 136.25070190429688, Temp: 0.6605426669120789, KL: 74.33566284179688, Loss: 0.6872169375419617, Learning Rate: 0.020999999999999998\n",
      "Epoch [1311/20000], Bound: 1.2509926557540894, Entropy: 135.36492919921875, Temp: 0.6600645184516907, KL: 87.25743103027344, Loss: 0.6961727738380432, Learning Rate: 0.020999999999999998\n",
      "Epoch [1312/20000], Bound: 1.1954293251037598, Entropy: 137.79067993164062, Temp: 0.6597914099693298, KL: 73.83497619628906, Loss: 0.6980413794517517, Learning Rate: 0.020999999999999998\n",
      "Epoch [1313/20000], Bound: 1.1762644052505493, Entropy: 134.760498046875, Temp: 0.6594815254211426, KL: 77.84172058105469, Loss: 0.6344683170318604, Learning Rate: 0.020999999999999998\n",
      "Epoch [1314/20000], Bound: 1.1684597730636597, Entropy: 134.78091430664062, Temp: 0.6593282222747803, KL: 78.81591796875, Loss: 0.6137319207191467, Learning Rate: 0.020999999999999998\n",
      "Epoch [1315/20000], Bound: 1.2070343494415283, Entropy: 133.70458984375, Temp: 0.6593732833862305, KL: 80.85450744628906, Loss: 0.6645600199699402, Learning Rate: 0.020999999999999998\n",
      "Epoch [1316/20000], Bound: 1.2775293588638306, Entropy: 131.42576599121094, Temp: 0.6595320701599121, KL: 88.8353271484375, Loss: 0.7333450317382812, Learning Rate: 0.020999999999999998\n",
      "Epoch [1317/20000], Bound: 1.2851918935775757, Entropy: 134.08676147460938, Temp: 0.6597906351089478, KL: 92.60670471191406, Loss: 0.7198753356933594, Learning Rate: 0.020999999999999998\n",
      "Epoch [1318/20000], Bound: 1.2826530933380127, Entropy: 130.89065551757812, Temp: 0.6602213978767395, KL: 88.51329040527344, Loss: 0.7466527223587036, Learning Rate: 0.020999999999999998\n",
      "Epoch [1319/20000], Bound: 1.2738158702850342, Entropy: 131.77439880371094, Temp: 0.6606916189193726, KL: 79.80986022949219, Loss: 0.7962970733642578, Learning Rate: 0.020999999999999998\n",
      "Epoch [1320/20000], Bound: 1.305854082107544, Entropy: 128.24594116210938, Temp: 0.660965621471405, KL: 91.11616516113281, Loss: 0.7734774351119995, Learning Rate: 0.020999999999999998\n",
      "Epoch [1321/20000], Bound: 1.2944453954696655, Entropy: 128.9105224609375, Temp: 0.6612817049026489, KL: 90.16383361816406, Loss: 0.758682370185852, Learning Rate: 0.020999999999999998\n",
      "Epoch [1322/20000], Bound: 1.2357604503631592, Entropy: 126.59263610839844, Temp: 0.6616462469100952, KL: 84.11407470703125, Loss: 0.6941896080970764, Learning Rate: 0.020999999999999998\n",
      "Epoch [1323/20000], Bound: 1.2353694438934326, Entropy: 121.80755615234375, Temp: 0.6620779037475586, KL: 95.847900390625, Loss: 0.6054461598396301, Learning Rate: 0.020999999999999998\n",
      "Epoch [1324/20000], Bound: 1.2325392961502075, Entropy: 121.72737884521484, Temp: 0.6629236936569214, KL: 87.18871307373047, Loss: 0.6668604612350464, Learning Rate: 0.020999999999999998\n",
      "Epoch [1325/20000], Bound: 1.204593539237976, Entropy: 116.9652099609375, Temp: 0.6638836860656738, KL: 87.35498046875, Loss: 0.616965651512146, Learning Rate: 0.020999999999999998\n",
      "Epoch [1326/20000], Bound: 1.2327849864959717, Entropy: 119.92121124267578, Temp: 0.6650418639183044, KL: 85.4155044555664, Loss: 0.6835428476333618, Learning Rate: 0.020999999999999998\n",
      "Epoch [1327/20000], Bound: 1.2723442316055298, Entropy: 119.45751953125, Temp: 0.6662124991416931, KL: 91.46060180664062, Loss: 0.7136595249176025, Learning Rate: 0.020999999999999998\n",
      "Epoch [1328/20000], Bound: 1.2205060720443726, Entropy: 120.21063995361328, Temp: 0.6674249172210693, KL: 80.43999481201172, Loss: 0.7017719745635986, Learning Rate: 0.020999999999999998\n",
      "Epoch [1329/20000], Bound: 1.1873382329940796, Entropy: 124.72283172607422, Temp: 0.6685170531272888, KL: 72.44646453857422, Loss: 0.7043654322624207, Learning Rate: 0.020999999999999998\n",
      "Epoch [1330/20000], Bound: 1.3060758113861084, Entropy: 125.69071197509766, Temp: 0.6693567037582397, KL: 93.71520233154297, Loss: 0.7678351998329163, Learning Rate: 0.020999999999999998\n",
      "Epoch [1331/20000], Bound: 1.1989721059799194, Entropy: 126.19969177246094, Temp: 0.6701851487159729, KL: 76.22799682617188, Loss: 0.6983149647712708, Learning Rate: 0.020999999999999998\n",
      "Epoch [1332/20000], Bound: 1.2352856397628784, Entropy: 128.77862548828125, Temp: 0.6708533763885498, KL: 74.58131408691406, Loss: 0.7766732573509216, Learning Rate: 0.020999999999999998\n",
      "Epoch [1333/20000], Bound: 1.1757358312606812, Entropy: 130.108642578125, Temp: 0.6711983680725098, KL: 70.73565673828125, Loss: 0.7000608444213867, Learning Rate: 0.020999999999999998\n",
      "Epoch [1334/20000], Bound: 1.2620432376861572, Entropy: 130.9765167236328, Temp: 0.6713296175003052, KL: 91.84346008300781, Loss: 0.6987354159355164, Learning Rate: 0.020999999999999998\n",
      "Epoch [1335/20000], Bound: 1.2025271654129028, Entropy: 130.70098876953125, Temp: 0.6716123819351196, KL: 77.0341796875, Loss: 0.7002488374710083, Learning Rate: 0.020999999999999998\n",
      "Epoch [1336/20000], Bound: 1.1941841840744019, Entropy: 130.62083435058594, Temp: 0.6717907786369324, KL: 70.94775390625, Loss: 0.7310752868652344, Learning Rate: 0.020999999999999998\n",
      "Epoch [1337/20000], Bound: 1.2714993953704834, Entropy: 133.0055694580078, Temp: 0.6717134714126587, KL: 89.40599060058594, Loss: 0.7355486750602722, Learning Rate: 0.020999999999999998\n",
      "Epoch [1338/20000], Bound: 1.2034499645233154, Entropy: 135.6562042236328, Temp: 0.6716974973678589, KL: 74.49063110351562, Loss: 0.720915675163269, Learning Rate: 0.020999999999999998\n",
      "Epoch [1339/20000], Bound: 1.2450977563858032, Entropy: 130.19473266601562, Temp: 0.6715244650840759, KL: 77.29426574707031, Loss: 0.7754525542259216, Learning Rate: 0.020999999999999998\n",
      "Epoch [1340/20000], Bound: 1.277427077293396, Entropy: 134.49911499023438, Temp: 0.6711549758911133, KL: 83.79368591308594, Loss: 0.7879971265792847, Learning Rate: 0.020999999999999998\n",
      "Epoch [1341/20000], Bound: 1.1780316829681396, Entropy: 131.4191131591797, Temp: 0.6706923246383667, KL: 75.07247924804688, Loss: 0.67112797498703, Learning Rate: 0.020999999999999998\n",
      "Epoch [1342/20000], Bound: 1.2480967044830322, Entropy: 132.5126495361328, Temp: 0.6702281832695007, KL: 85.01457214355469, Loss: 0.721799910068512, Learning Rate: 0.020999999999999998\n",
      "Epoch [1343/20000], Bound: 1.1843427419662476, Entropy: 128.6376190185547, Temp: 0.6698295474052429, KL: 80.75286865234375, Loss: 0.6386280655860901, Learning Rate: 0.020999999999999998\n",
      "Epoch [1344/20000], Bound: 1.2746707201004028, Entropy: 126.42198181152344, Temp: 0.6695861220359802, KL: 86.02793884277344, Loss: 0.7637608647346497, Learning Rate: 0.020999999999999998\n",
      "Epoch [1345/20000], Bound: 1.2594375610351562, Entropy: 125.02903747558594, Temp: 0.6693260669708252, KL: 75.36259460449219, Loss: 0.8139674067497253, Learning Rate: 0.020999999999999998\n",
      "Epoch [1346/20000], Bound: 1.2546563148498535, Entropy: 120.24302673339844, Temp: 0.6687875986099243, KL: 78.10856628417969, Loss: 0.7837372422218323, Learning Rate: 0.020999999999999998\n",
      "Epoch [1347/20000], Bound: 1.24444580078125, Entropy: 121.52452087402344, Temp: 0.6681022047996521, KL: 80.67326354980469, Loss: 0.7446043491363525, Learning Rate: 0.020999999999999998\n",
      "Epoch [1348/20000], Bound: 1.2680507898330688, Entropy: 126.36314392089844, Temp: 0.6674028635025024, KL: 81.28692626953125, Loss: 0.7834516167640686, Learning Rate: 0.020999999999999998\n",
      "Epoch [1349/20000], Bound: 1.2880029678344727, Entropy: 122.17687225341797, Temp: 0.6666324138641357, KL: 87.25386810302734, Loss: 0.7761499881744385, Learning Rate: 0.020999999999999998\n",
      "Epoch [1350/20000], Bound: 1.2579878568649292, Entropy: 124.19755554199219, Temp: 0.6659111380577087, KL: 78.91799926757812, Loss: 0.780148446559906, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1351/20000], Bound: 1.185569167137146, Entropy: 127.63270568847656, Temp: 0.6650967597961426, KL: 67.60588073730469, Loss: 0.7339034676551819, Learning Rate: 0.020999999999999998\n",
      "Epoch [1352/20000], Bound: 1.2236369848251343, Entropy: 131.7738800048828, Temp: 0.6640973687171936, KL: 79.34332275390625, Loss: 0.7114392518997192, Learning Rate: 0.020999999999999998\n",
      "Epoch [1353/20000], Bound: 1.2417364120483398, Entropy: 130.2173614501953, Temp: 0.6631779670715332, KL: 82.12345886230469, Loss: 0.7221964597702026, Learning Rate: 0.020999999999999998\n",
      "Epoch [1354/20000], Bound: 1.1871871948242188, Entropy: 134.68753051757812, Temp: 0.6623609066009521, KL: 70.42695617675781, Loss: 0.7124862670898438, Learning Rate: 0.020999999999999998\n",
      "Epoch [1355/20000], Bound: 1.205916404724121, Entropy: 138.1793212890625, Temp: 0.6614617109298706, KL: 74.06146240234375, Loss: 0.7165722250938416, Learning Rate: 0.020999999999999998\n",
      "Epoch [1356/20000], Bound: 1.1783976554870605, Entropy: 140.39871215820312, Temp: 0.6605480909347534, KL: 74.11196899414062, Loss: 0.6675734519958496, Learning Rate: 0.020999999999999998\n",
      "Epoch [1357/20000], Bound: 1.227354884147644, Entropy: 140.32919311523438, Temp: 0.6597200632095337, KL: 75.96734619140625, Loss: 0.7381410598754883, Learning Rate: 0.020999999999999998\n",
      "Epoch [1358/20000], Bound: 1.110682725906372, Entropy: 141.97561645507812, Temp: 0.6588717103004456, KL: 63.19660949707031, Loss: 0.6376774311065674, Learning Rate: 0.020999999999999998\n",
      "Epoch [1359/20000], Bound: 1.3070669174194336, Entropy: 144.1714630126953, Temp: 0.6579757928848267, KL: 95.90609741210938, Loss: 0.7346878051757812, Learning Rate: 0.020999999999999998\n",
      "Epoch [1360/20000], Bound: 1.2967913150787354, Entropy: 140.69419860839844, Temp: 0.6574039459228516, KL: 93.77630615234375, Loss: 0.7297286987304688, Learning Rate: 0.020999999999999998\n",
      "Epoch [1361/20000], Bound: 1.2204740047454834, Entropy: 138.13490295410156, Temp: 0.6571031808853149, KL: 79.47239685058594, Loss: 0.6959220170974731, Learning Rate: 0.020999999999999998\n",
      "Epoch [1362/20000], Bound: 1.284844160079956, Entropy: 135.32960510253906, Temp: 0.6568809747695923, KL: 81.61709594726562, Loss: 0.7982929944992065, Learning Rate: 0.020999999999999998\n",
      "Epoch [1363/20000], Bound: 1.2432968616485596, Entropy: 135.19190979003906, Temp: 0.6565771698951721, KL: 77.26396179199219, Loss: 0.7531828284263611, Learning Rate: 0.020999999999999998\n",
      "Epoch [1364/20000], Bound: 1.23434579372406, Entropy: 132.74595642089844, Temp: 0.6562110185623169, KL: 77.83186340332031, Loss: 0.7321290373802185, Learning Rate: 0.020999999999999998\n",
      "Epoch [1365/20000], Bound: 1.2353923320770264, Entropy: 129.1774139404297, Temp: 0.6558393836021423, KL: 75.47067260742188, Loss: 0.7515419125556946, Learning Rate: 0.020999999999999998\n",
      "Epoch [1366/20000], Bound: 1.2155652046203613, Entropy: 123.8606185913086, Temp: 0.6553888916969299, KL: 78.98250579833984, Loss: 0.6887780427932739, Learning Rate: 0.020999999999999998\n",
      "Epoch [1367/20000], Bound: 1.289989948272705, Entropy: 126.52923583984375, Temp: 0.6550465822219849, KL: 88.56857299804688, Loss: 0.7524797916412354, Learning Rate: 0.020999999999999998\n",
      "Epoch [1368/20000], Bound: 1.3237297534942627, Entropy: 122.05435180664062, Temp: 0.6548414826393127, KL: 88.39434814453125, Loss: 0.8200365900993347, Learning Rate: 0.020999999999999998\n",
      "Epoch [1369/20000], Bound: 1.2386436462402344, Entropy: 127.06352996826172, Temp: 0.6546347141265869, KL: 68.75756072998047, Loss: 0.8071714639663696, Learning Rate: 0.020999999999999998\n",
      "Epoch [1370/20000], Bound: 1.299211859703064, Entropy: 122.43009185791016, Temp: 0.6541231870651245, KL: 81.88770294189453, Loss: 0.8200063109397888, Learning Rate: 0.020999999999999998\n",
      "Epoch [1371/20000], Bound: 1.2481286525726318, Entropy: 126.26200866699219, Temp: 0.6535400748252869, KL: 71.84764099121094, Loss: 0.7994824051856995, Learning Rate: 0.020999999999999998\n",
      "Epoch [1372/20000], Bound: 1.324524164199829, Entropy: 130.0348358154297, Temp: 0.6527634263038635, KL: 92.44528198242188, Loss: 0.7872059345245361, Learning Rate: 0.020999999999999998\n",
      "Epoch [1373/20000], Bound: 1.275551438331604, Entropy: 127.66650390625, Temp: 0.6521782875061035, KL: 80.42776489257812, Loss: 0.782937228679657, Learning Rate: 0.020999999999999998\n",
      "Epoch [1374/20000], Bound: 1.310450792312622, Entropy: 128.7362060546875, Temp: 0.6515824794769287, KL: 83.33731079101562, Loss: 0.8270930051803589, Learning Rate: 0.020999999999999998\n",
      "Epoch [1375/20000], Bound: 1.2592277526855469, Entropy: 131.91363525390625, Temp: 0.6509494781494141, KL: 75.85928344726562, Loss: 0.7857999801635742, Learning Rate: 0.020999999999999998\n",
      "Epoch [1376/20000], Bound: 1.2498818635940552, Entropy: 131.29971313476562, Temp: 0.6502351760864258, KL: 76.41363525390625, Loss: 0.7634172439575195, Learning Rate: 0.020999999999999998\n",
      "Epoch [1377/20000], Bound: 1.2754602432250977, Entropy: 136.59536743164062, Temp: 0.6495014429092407, KL: 75.32720947265625, Loss: 0.8182224631309509, Learning Rate: 0.020999999999999998\n",
      "Epoch [1378/20000], Bound: 1.2712721824645996, Entropy: 134.4090118408203, Temp: 0.6486385464668274, KL: 78.52894592285156, Loss: 0.7845083475112915, Learning Rate: 0.020999999999999998\n",
      "Epoch [1379/20000], Bound: 1.2377800941467285, Entropy: 133.94688415527344, Temp: 0.6477781534194946, KL: 69.78526306152344, Loss: 0.7894430160522461, Learning Rate: 0.020999999999999998\n",
      "Epoch [1380/20000], Bound: 1.3025461435317993, Entropy: 136.49851989746094, Temp: 0.6467653512954712, KL: 83.88983154296875, Loss: 0.7999464273452759, Learning Rate: 0.020999999999999998\n",
      "Epoch [1381/20000], Bound: 1.2765660285949707, Entropy: 136.995361328125, Temp: 0.6458428502082825, KL: 77.72096252441406, Loss: 0.7967526912689209, Learning Rate: 0.020999999999999998\n",
      "Epoch [1382/20000], Bound: 1.2646125555038452, Entropy: 136.57080078125, Temp: 0.6449093222618103, KL: 76.11778259277344, Loss: 0.7856282591819763, Learning Rate: 0.020999999999999998\n",
      "Epoch [1383/20000], Bound: 1.2549575567245483, Entropy: 135.06822204589844, Temp: 0.6439633369445801, KL: 73.38978576660156, Loss: 0.787807047367096, Learning Rate: 0.020999999999999998\n",
      "Epoch [1384/20000], Bound: 1.2560325860977173, Entropy: 134.3318328857422, Temp: 0.6429606080055237, KL: 74.67132568359375, Loss: 0.7784874439239502, Learning Rate: 0.020999999999999998\n",
      "Epoch [1385/20000], Bound: 1.2812256813049316, Entropy: 132.7868194580078, Temp: 0.6419510841369629, KL: 89.07485961914062, Loss: 0.7115563750267029, Learning Rate: 0.020999999999999998\n",
      "Epoch [1386/20000], Bound: 1.3082250356674194, Entropy: 128.33412170410156, Temp: 0.641305148601532, KL: 90.80047607421875, Loss: 0.7485844492912292, Learning Rate: 0.020999999999999998\n",
      "Epoch [1387/20000], Bound: 1.2498548030853271, Entropy: 128.64801025390625, Temp: 0.6409497857093811, KL: 81.95608520507812, Loss: 0.7077768445014954, Learning Rate: 0.020999999999999998\n",
      "Epoch [1388/20000], Bound: 1.2575607299804688, Entropy: 129.94235229492188, Temp: 0.6407861113548279, KL: 90.73611450195312, Loss: 0.6530212163925171, Learning Rate: 0.020999999999999998\n",
      "Epoch [1389/20000], Bound: 1.2256113290786743, Entropy: 130.31967163085938, Temp: 0.6410441398620605, KL: 76.71710205078125, Loss: 0.7055622935295105, Learning Rate: 0.020999999999999998\n",
      "Epoch [1390/20000], Bound: 1.2518281936645508, Entropy: 129.06808471679688, Temp: 0.6413463354110718, KL: 79.27848815917969, Loss: 0.7327821850776672, Learning Rate: 0.020999999999999998\n",
      "Epoch [1391/20000], Bound: 1.2959537506103516, Entropy: 130.1708984375, Temp: 0.6416813135147095, KL: 82.66424560546875, Loss: 0.7889775633811951, Learning Rate: 0.020999999999999998\n",
      "Epoch [1392/20000], Bound: 1.1604909896850586, Entropy: 134.93772888183594, Temp: 0.6420004963874817, KL: 65.55319213867188, Loss: 0.683173656463623, Learning Rate: 0.020999999999999998\n",
      "Epoch [1393/20000], Bound: 1.1689127683639526, Entropy: 133.7643280029297, Temp: 0.6421942710876465, KL: 74.257568359375, Loss: 0.6294746398925781, Learning Rate: 0.020999999999999998\n",
      "Epoch [1394/20000], Bound: 1.241278886795044, Entropy: 132.82772827148438, Temp: 0.6425328254699707, KL: 79.03831481933594, Loss: 0.717261791229248, Learning Rate: 0.020999999999999998\n",
      "Epoch [1395/20000], Bound: 1.177465558052063, Entropy: 137.14707946777344, Temp: 0.6429182887077332, KL: 67.06741333007812, Loss: 0.7004581093788147, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1396/20000], Bound: 1.1955726146697998, Entropy: 138.82313537597656, Temp: 0.6431629657745361, KL: 72.36563110351562, Loss: 0.6900880932807922, Learning Rate: 0.020999999999999998\n",
      "Epoch [1397/20000], Bound: 1.1691392660140991, Entropy: 137.4174041748047, Temp: 0.6433941125869751, KL: 63.56648254394531, Loss: 0.7143312692642212, Learning Rate: 0.020999999999999998\n",
      "Epoch [1398/20000], Bound: 1.262436866760254, Entropy: 137.01199340820312, Temp: 0.6434071063995361, KL: 79.07884216308594, Loss: 0.7565635442733765, Learning Rate: 0.020999999999999998\n",
      "Epoch [1399/20000], Bound: 1.2467187643051147, Entropy: 138.0293426513672, Temp: 0.6434236168861389, KL: 79.15251159667969, Loss: 0.7273698449134827, Learning Rate: 0.020999999999999998\n",
      "Epoch [1400/20000], Bound: 1.272626280784607, Entropy: 131.21194458007812, Temp: 0.6434978246688843, KL: 84.81402587890625, Loss: 0.7309843301773071, Learning Rate: 0.020999999999999998\n",
      "Epoch [1401/20000], Bound: 1.2419630289077759, Entropy: 132.11488342285156, Temp: 0.6437127590179443, KL: 77.07011413574219, Loss: 0.7353723049163818, Learning Rate: 0.020999999999999998\n",
      "Epoch [1402/20000], Bound: 1.2263069152832031, Entropy: 130.73052978515625, Temp: 0.6439133882522583, KL: 76.48526000976562, Loss: 0.7123008370399475, Learning Rate: 0.020999999999999998\n",
      "Epoch [1403/20000], Bound: 1.2620598077774048, Entropy: 130.15142822265625, Temp: 0.6441323757171631, KL: 90.03617858886719, Loss: 0.671826183795929, Learning Rate: 0.020999999999999998\n",
      "Epoch [1404/20000], Bound: 1.242081642150879, Entropy: 128.12962341308594, Temp: 0.6446718573570251, KL: 76.66676330566406, Loss: 0.7399850487709045, Learning Rate: 0.020999999999999998\n",
      "Epoch [1405/20000], Bound: 1.1850210428237915, Entropy: 129.23402404785156, Temp: 0.6451441049575806, KL: 72.14768981933594, Loss: 0.6762070059776306, Learning Rate: 0.020999999999999998\n",
      "Epoch [1406/20000], Bound: 1.250677227973938, Entropy: 129.37603759765625, Temp: 0.6455923914909363, KL: 81.12571716308594, Loss: 0.7222186923027039, Learning Rate: 0.020999999999999998\n",
      "Epoch [1407/20000], Bound: 1.261038064956665, Entropy: 128.16790771484375, Temp: 0.6460870504379272, KL: 93.82469177246094, Loss: 0.6435987949371338, Learning Rate: 0.020999999999999998\n",
      "Epoch [1408/20000], Bound: 1.262136459350586, Entropy: 129.45152282714844, Temp: 0.6469796895980835, KL: 90.3955078125, Loss: 0.6735203266143799, Learning Rate: 0.020999999999999998\n",
      "Epoch [1409/20000], Bound: 1.2390493154525757, Entropy: 125.07089233398438, Temp: 0.6481128931045532, KL: 89.08956909179688, Loss: 0.6432064175605774, Learning Rate: 0.020999999999999998\n",
      "Epoch [1410/20000], Bound: 1.1906306743621826, Entropy: 127.35841369628906, Temp: 0.6494935750961304, KL: 69.18804931640625, Loss: 0.7135432958602905, Learning Rate: 0.020999999999999998\n",
      "Epoch [1411/20000], Bound: 1.249523401260376, Entropy: 128.0101776123047, Temp: 0.6506152749061584, KL: 90.51727294921875, Loss: 0.6548761129379272, Learning Rate: 0.020999999999999998\n",
      "Epoch [1412/20000], Bound: 1.2419615983963013, Entropy: 122.75558471679688, Temp: 0.6519728899002075, KL: 85.68339538574219, Loss: 0.6801839470863342, Learning Rate: 0.020999999999999998\n",
      "Epoch [1413/20000], Bound: 1.2009936571121216, Entropy: 121.59749603271484, Temp: 0.6534082889556885, KL: 82.3197250366211, Loss: 0.6352760791778564, Learning Rate: 0.020999999999999998\n",
      "Epoch [1414/20000], Bound: 1.2361397743225098, Entropy: 123.19818115234375, Temp: 0.6549379229545593, KL: 90.18757629394531, Loss: 0.6393983960151672, Learning Rate: 0.020999999999999998\n",
      "Epoch [1415/20000], Bound: 1.1649657487869263, Entropy: 120.8030014038086, Temp: 0.6566653251647949, KL: 75.16828155517578, Loss: 0.6325305700302124, Learning Rate: 0.020999999999999998\n",
      "Epoch [1416/20000], Bound: 1.101256251335144, Entropy: 117.15715789794922, Temp: 0.6583251357078552, KL: 80.56653594970703, Loss: 0.4904533326625824, Learning Rate: 0.020999999999999998\n",
      "Epoch [1417/20000], Bound: 1.1180741786956787, Entropy: 115.7255859375, Temp: 0.6603008508682251, KL: 75.05561828613281, Loss: 0.560903012752533, Learning Rate: 0.020999999999999998\n",
      "Epoch [1418/20000], Bound: 1.1419988870620728, Entropy: 117.42140197753906, Temp: 0.6623104214668274, KL: 87.84773254394531, Loss: 0.5050963163375854, Learning Rate: 0.020999999999999998\n",
      "Epoch [1419/20000], Bound: 1.1924946308135986, Entropy: 114.33525085449219, Temp: 0.6646700501441956, KL: 94.79493713378906, Loss: 0.5408911108970642, Learning Rate: 0.020999999999999998\n",
      "Epoch [1420/20000], Bound: 1.0718046426773071, Entropy: 116.41109466552734, Temp: 0.6673660278320312, KL: 73.92565155029297, Loss: 0.5044165849685669, Learning Rate: 0.020999999999999998\n",
      "Epoch [1421/20000], Bound: 1.0921630859375, Entropy: 115.18140411376953, Temp: 0.6700931787490845, KL: 77.88031768798828, Loss: 0.5089964866638184, Learning Rate: 0.020999999999999998\n",
      "Epoch [1422/20000], Bound: 1.1297335624694824, Entropy: 117.36528778076172, Temp: 0.672893226146698, KL: 92.97547149658203, Loss: 0.45968854427337646, Learning Rate: 0.020999999999999998\n",
      "Epoch [1423/20000], Bound: 1.1546400785446167, Entropy: 119.76837921142578, Temp: 0.676091194152832, KL: 91.46825408935547, Loss: 0.516031801700592, Learning Rate: 0.020999999999999998\n",
      "Epoch [1424/20000], Bound: 1.0798953771591187, Entropy: 125.64290618896484, Temp: 0.6794886589050293, KL: 84.14505767822266, Loss: 0.4532642364501953, Learning Rate: 0.020999999999999998\n",
      "Epoch [1425/20000], Bound: 1.077434778213501, Entropy: 118.56843566894531, Temp: 0.6830740571022034, KL: 84.02851867675781, Loss: 0.4540612995624542, Learning Rate: 0.020999999999999998\n",
      "Epoch [1426/20000], Bound: 1.120945692062378, Entropy: 116.62269592285156, Temp: 0.6868100166320801, KL: 92.22100830078125, Loss: 0.46764975786209106, Learning Rate: 0.020999999999999998\n",
      "Epoch [1427/20000], Bound: 1.1263600587844849, Entropy: 115.47193908691406, Temp: 0.6907590627670288, KL: 97.45887756347656, Loss: 0.44323962926864624, Learning Rate: 0.020999999999999998\n",
      "Epoch [1428/20000], Bound: 1.0453988313674927, Entropy: 112.14203643798828, Temp: 0.6950108408927917, KL: 78.28458404541016, Loss: 0.45831918716430664, Learning Rate: 0.020999999999999998\n",
      "Epoch [1429/20000], Bound: 1.040781855583191, Entropy: 110.94962310791016, Temp: 0.6991982460021973, KL: 91.83765411376953, Loss: 0.35822853446006775, Learning Rate: 0.020999999999999998\n",
      "Epoch [1430/20000], Bound: 1.111899495124817, Entropy: 107.69979858398438, Temp: 0.7037426829338074, KL: 89.15916442871094, Loss: 0.4939517378807068, Learning Rate: 0.020999999999999998\n",
      "Epoch [1431/20000], Bound: 1.053194284439087, Entropy: 103.29335021972656, Temp: 0.7082427144050598, KL: 80.07337951660156, Loss: 0.4695530831813812, Learning Rate: 0.020999999999999998\n",
      "Epoch [1432/20000], Bound: 1.0700331926345825, Entropy: 99.03853607177734, Temp: 0.7126017808914185, KL: 92.75212860107422, Loss: 0.4107772707939148, Learning Rate: 0.020999999999999998\n",
      "Epoch [1433/20000], Bound: 0.9954807162284851, Entropy: 101.39729309082031, Temp: 0.7171383500099182, KL: 81.95491027832031, Loss: 0.37781569361686707, Learning Rate: 0.020999999999999998\n",
      "Epoch [1434/20000], Bound: 1.0690455436706543, Entropy: 100.33403778076172, Temp: 0.7217400074005127, KL: 83.47525787353516, Loss: 0.48290297389030457, Learning Rate: 0.020999999999999998\n",
      "Epoch [1435/20000], Bound: 0.9902269840240479, Entropy: 103.80529022216797, Temp: 0.7261553406715393, KL: 80.58539581298828, Loss: 0.3871942162513733, Learning Rate: 0.020999999999999998\n",
      "Epoch [1436/20000], Bound: 1.0661163330078125, Entropy: 107.33720397949219, Temp: 0.7305705547332764, KL: 84.6893310546875, Loss: 0.4780523180961609, Learning Rate: 0.020999999999999998\n",
      "Epoch [1437/20000], Bound: 1.0862977504730225, Entropy: 109.72992706298828, Temp: 0.7348092198371887, KL: 89.74567413330078, Loss: 0.4800017476081848, Learning Rate: 0.020999999999999998\n",
      "Epoch [1438/20000], Bound: 1.0310001373291016, Entropy: 118.0093002319336, Temp: 0.7389391660690308, KL: 82.58885955810547, Loss: 0.44495949149131775, Learning Rate: 0.020999999999999998\n",
      "Epoch [1439/20000], Bound: 1.0521771907806396, Entropy: 115.29605865478516, Temp: 0.7429342865943909, KL: 90.85254669189453, Loss: 0.42561668157577515, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1440/20000], Bound: 1.0741596221923828, Entropy: 119.56474304199219, Temp: 0.7469589114189148, KL: 84.90470886230469, Loss: 0.5043046474456787, Learning Rate: 0.020999999999999998\n",
      "Epoch [1441/20000], Bound: 1.046557903289795, Entropy: 118.70539093017578, Temp: 0.750714123249054, KL: 89.56417083740234, Loss: 0.4324996769428253, Learning Rate: 0.020999999999999998\n",
      "Epoch [1442/20000], Bound: 1.0340664386749268, Entropy: 118.62789154052734, Temp: 0.7544555068016052, KL: 84.66165924072266, Loss: 0.44878339767456055, Learning Rate: 0.020999999999999998\n",
      "Epoch [1443/20000], Bound: 0.9842142462730408, Entropy: 117.4906997680664, Temp: 0.758057713508606, KL: 85.0210189819336, Loss: 0.37400558590888977, Learning Rate: 0.020999999999999998\n",
      "Epoch [1444/20000], Bound: 1.0381786823272705, Entropy: 111.9469223022461, Temp: 0.761719822883606, KL: 86.1162338256836, Loss: 0.45164352655410767, Learning Rate: 0.020999999999999998\n",
      "Epoch [1445/20000], Bound: 0.9677419066429138, Entropy: 113.11656188964844, Temp: 0.7652347683906555, KL: 72.86062622070312, Loss: 0.43494096398353577, Learning Rate: 0.020999999999999998\n",
      "Epoch [1446/20000], Bound: 1.0290967226028442, Entropy: 113.82588958740234, Temp: 0.768447995185852, KL: 84.78726959228516, Loss: 0.45162269473075867, Learning Rate: 0.020999999999999998\n",
      "Epoch [1447/20000], Bound: 1.0595531463623047, Entropy: 113.26837921142578, Temp: 0.7715109586715698, KL: 87.12963104248047, Loss: 0.4872909188270569, Learning Rate: 0.020999999999999998\n",
      "Epoch [1448/20000], Bound: 0.9567511677742004, Entropy: 109.68705749511719, Temp: 0.774366557598114, KL: 77.16754150390625, Loss: 0.3971095681190491, Learning Rate: 0.020999999999999998\n",
      "Epoch [1449/20000], Bound: 1.0709259510040283, Entropy: 112.42860412597656, Temp: 0.7771168947219849, KL: 88.74397277832031, Loss: 0.500231146812439, Learning Rate: 0.020999999999999998\n",
      "Epoch [1450/20000], Bound: 1.020470380783081, Entropy: 115.25823974609375, Temp: 0.7796556949615479, KL: 81.93479919433594, Loss: 0.46528029441833496, Learning Rate: 0.020999999999999998\n",
      "Epoch [1451/20000], Bound: 1.0815441608428955, Entropy: 110.5404281616211, Temp: 0.7819860577583313, KL: 93.8352279663086, Loss: 0.48954281210899353, Learning Rate: 0.020999999999999998\n",
      "Epoch [1452/20000], Bound: 1.0132811069488525, Entropy: 115.13619995117188, Temp: 0.7842238545417786, KL: 88.20512390136719, Loss: 0.41754424571990967, Learning Rate: 0.020999999999999998\n",
      "Epoch [1453/20000], Bound: 1.0754990577697754, Entropy: 114.30985260009766, Temp: 0.7864838242530823, KL: 87.56787872314453, Loss: 0.5232952833175659, Learning Rate: 0.020999999999999998\n",
      "Epoch [1454/20000], Bound: 1.0327084064483643, Entropy: 113.7901611328125, Temp: 0.7884624004364014, KL: 94.03129577636719, Loss: 0.4142967760562897, Learning Rate: 0.020999999999999998\n",
      "Epoch [1455/20000], Bound: 1.00019109249115, Entropy: 112.92276763916016, Temp: 0.7905617952346802, KL: 85.63956451416016, Loss: 0.4185681939125061, Learning Rate: 0.020999999999999998\n",
      "Epoch [1456/20000], Bound: 1.0106806755065918, Entropy: 115.00484466552734, Temp: 0.7926334142684937, KL: 81.96630096435547, Loss: 0.4593689739704132, Learning Rate: 0.020999999999999998\n",
      "Epoch [1457/20000], Bound: 0.9398897290229797, Entropy: 114.54200744628906, Temp: 0.7945062518119812, KL: 79.88629150390625, Loss: 0.36876624822616577, Learning Rate: 0.020999999999999998\n",
      "Epoch [1458/20000], Bound: 1.050360918045044, Entropy: 110.76193237304688, Temp: 0.7964175343513489, KL: 90.13992309570312, Loss: 0.4737548828125, Learning Rate: 0.020999999999999998\n",
      "Epoch [1459/20000], Bound: 0.9797552227973938, Entropy: 109.99107360839844, Temp: 0.7982075214385986, KL: 84.99385070800781, Loss: 0.3972887396812439, Learning Rate: 0.020999999999999998\n",
      "Epoch [1460/20000], Bound: 1.0201873779296875, Entropy: 110.47291564941406, Temp: 0.8000221252441406, KL: 83.76634216308594, Loss: 0.46817827224731445, Learning Rate: 0.020999999999999998\n",
      "Epoch [1461/20000], Bound: 1.0005477666854858, Entropy: 110.88241577148438, Temp: 0.8016352653503418, KL: 84.32273864746094, Loss: 0.43536609411239624, Learning Rate: 0.020999999999999998\n",
      "Epoch [1462/20000], Bound: 0.9870307445526123, Entropy: 109.19442749023438, Temp: 0.8031596541404724, KL: 82.02908325195312, Loss: 0.4301319420337677, Learning Rate: 0.020999999999999998\n",
      "Epoch [1463/20000], Bound: 0.9702003598213196, Entropy: 111.3616943359375, Temp: 0.8045797944068909, KL: 80.48805236816406, Loss: 0.41547924280166626, Learning Rate: 0.020999999999999998\n",
      "Epoch [1464/20000], Bound: 1.0246115922927856, Entropy: 111.50052642822266, Temp: 0.8059200048446655, KL: 80.78313446044922, Loss: 0.49791091680526733, Learning Rate: 0.020999999999999998\n",
      "Epoch [1465/20000], Bound: 0.9985954165458679, Entropy: 114.34919738769531, Temp: 0.8069552779197693, KL: 94.69732666015625, Loss: 0.37181341648101807, Learning Rate: 0.020999999999999998\n",
      "Epoch [1466/20000], Bound: 0.8939483165740967, Entropy: 116.81450653076172, Temp: 0.8082663416862488, KL: 65.21520233154297, Loss: 0.4040452539920807, Learning Rate: 0.020999999999999998\n",
      "Epoch [1467/20000], Bound: 0.9832800626754761, Entropy: 122.59734344482422, Temp: 0.8092960119247437, KL: 81.36597442626953, Loss: 0.43266192078590393, Learning Rate: 0.020999999999999998\n",
      "Epoch [1468/20000], Bound: 0.9979314804077148, Entropy: 120.88221740722656, Temp: 0.8102293610572815, KL: 85.530029296875, Loss: 0.4298906624317169, Learning Rate: 0.020999999999999998\n",
      "Epoch [1469/20000], Bound: 1.0749543905258179, Entropy: 122.80187225341797, Temp: 0.811140239238739, KL: 100.32160186767578, Loss: 0.4638633131980896, Learning Rate: 0.020999999999999998\n",
      "Epoch [1470/20000], Bound: 0.9833256006240845, Entropy: 126.9515609741211, Temp: 0.8121325969696045, KL: 73.5658187866211, Loss: 0.4826065003871918, Learning Rate: 0.020999999999999998\n",
      "Epoch [1471/20000], Bound: 1.0339350700378418, Entropy: 125.01873779296875, Temp: 0.812764048576355, KL: 80.76118469238281, Loss: 0.5176891684532166, Learning Rate: 0.020999999999999998\n",
      "Epoch [1472/20000], Bound: 1.0084823369979858, Entropy: 126.96383666992188, Temp: 0.8130770921707153, KL: 74.46417236328125, Loss: 0.5162697434425354, Learning Rate: 0.020999999999999998\n",
      "Epoch [1473/20000], Bound: 1.1096059083938599, Entropy: 123.0302505493164, Temp: 0.8130130767822266, KL: 93.96514129638672, Loss: 0.5651105642318726, Learning Rate: 0.020999999999999998\n",
      "Epoch [1474/20000], Bound: 1.046597957611084, Entropy: 119.22981262207031, Temp: 0.8127557039260864, KL: 86.98924255371094, Loss: 0.49994781613349915, Learning Rate: 0.020999999999999998\n",
      "Epoch [1475/20000], Bound: 1.0979177951812744, Entropy: 122.44513702392578, Temp: 0.8124061822891235, KL: 92.83829498291016, Loss: 0.5507793426513672, Learning Rate: 0.020999999999999998\n",
      "Epoch [1476/20000], Bound: 1.0161523818969727, Entropy: 120.07913970947266, Temp: 0.8119175434112549, KL: 78.8513412475586, Loss: 0.5005488395690918, Learning Rate: 0.020999999999999998\n",
      "Epoch [1477/20000], Bound: 1.0897223949432373, Entropy: 117.26863098144531, Temp: 0.8112449049949646, KL: 89.41470336914062, Loss: 0.5565626621246338, Learning Rate: 0.020999999999999998\n",
      "Epoch [1478/20000], Bound: 1.0732855796813965, Entropy: 108.92147827148438, Temp: 0.8104081153869629, KL: 88.78726196289062, Loss: 0.5315409898757935, Learning Rate: 0.020999999999999998\n",
      "Epoch [1479/20000], Bound: 1.0985093116760254, Entropy: 108.77523040771484, Temp: 0.8094859719276428, KL: 82.7095718383789, Loss: 0.6118539571762085, Learning Rate: 0.020999999999999998\n",
      "Epoch [1480/20000], Bound: 1.0161129236221313, Entropy: 107.23092651367188, Temp: 0.8081947565078735, KL: 87.12516784667969, Loss: 0.4468284547328949, Learning Rate: 0.020999999999999998\n",
      "Epoch [1481/20000], Bound: 1.0588538646697998, Entropy: 107.63785552978516, Temp: 0.8070847988128662, KL: 87.11788177490234, Loss: 0.5150271654129028, Learning Rate: 0.020999999999999998\n",
      "Epoch [1482/20000], Bound: 0.9841848015785217, Entropy: 108.07980346679688, Temp: 0.8059526085853577, KL: 86.979248046875, Loss: 0.39700034260749817, Learning Rate: 0.020999999999999998\n",
      "Epoch [1483/20000], Bound: 1.0272830724716187, Entropy: 105.92316436767578, Temp: 0.8051364421844482, KL: 96.17452239990234, Loss: 0.4060249626636505, Learning Rate: 0.020999999999999998\n",
      "Epoch [1484/20000], Bound: 1.0390408039093018, Entropy: 105.61865997314453, Temp: 0.8047088384628296, KL: 85.31610870361328, Loss: 0.4919759929180145, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1485/20000], Bound: 1.068705677986145, Entropy: 104.46241760253906, Temp: 0.8042376637458801, KL: 98.38604736328125, Loss: 0.4591747224330902, Learning Rate: 0.020999999999999998\n",
      "Epoch [1486/20000], Bound: 0.992952287197113, Entropy: 108.54572296142578, Temp: 0.8040018677711487, KL: 88.98876190185547, Loss: 0.3963985741138458, Learning Rate: 0.020999999999999998\n",
      "Epoch [1487/20000], Bound: 1.0058294534683228, Entropy: 105.86280822753906, Temp: 0.804029643535614, KL: 85.10179138183594, Loss: 0.44034865498542786, Learning Rate: 0.020999999999999998\n",
      "Epoch [1488/20000], Bound: 1.0295225381851196, Entropy: 107.11917877197266, Temp: 0.8041125535964966, KL: 87.1751480102539, Loss: 0.4647103250026703, Learning Rate: 0.020999999999999998\n",
      "Epoch [1489/20000], Bound: 1.0233089923858643, Entropy: 105.3097152709961, Temp: 0.8042056560516357, KL: 92.32633209228516, Loss: 0.4228906035423279, Learning Rate: 0.020999999999999998\n",
      "Epoch [1490/20000], Bound: 1.0128659009933472, Entropy: 104.00639343261719, Temp: 0.8044987916946411, KL: 89.0030517578125, Loss: 0.42736876010894775, Learning Rate: 0.020999999999999998\n",
      "Epoch [1491/20000], Bound: 1.0200903415679932, Entropy: 105.74996948242188, Temp: 0.8049116134643555, KL: 84.06060791015625, Loss: 0.46970853209495544, Learning Rate: 0.020999999999999998\n",
      "Epoch [1492/20000], Bound: 1.013431191444397, Entropy: 104.86241149902344, Temp: 0.8052401542663574, KL: 89.59747314453125, Loss: 0.4251149594783783, Learning Rate: 0.020999999999999998\n",
      "Epoch [1493/20000], Bound: 1.053252935409546, Entropy: 108.60116577148438, Temp: 0.805696964263916, KL: 88.1546630859375, Loss: 0.49826309084892273, Learning Rate: 0.020999999999999998\n",
      "Epoch [1494/20000], Bound: 0.9682778120040894, Entropy: 106.22525787353516, Temp: 0.80604088306427, KL: 91.70125579833984, Loss: 0.3440178632736206, Learning Rate: 0.020999999999999998\n",
      "Epoch [1495/20000], Bound: 1.0506973266601562, Entropy: 111.115478515625, Temp: 0.8067780137062073, KL: 87.04928588867188, Loss: 0.5017557144165039, Learning Rate: 0.020999999999999998\n",
      "Epoch [1496/20000], Bound: 1.0791592597961426, Entropy: 109.14964294433594, Temp: 0.8073447942733765, KL: 89.56878662109375, Loss: 0.5342166423797607, Learning Rate: 0.020999999999999998\n",
      "Epoch [1497/20000], Bound: 1.0225602388381958, Entropy: 110.27330017089844, Temp: 0.8077020049095154, KL: 83.9490966796875, Loss: 0.4762941002845764, Learning Rate: 0.020999999999999998\n",
      "Epoch [1498/20000], Bound: 1.0622773170471191, Entropy: 112.01422882080078, Temp: 0.8079488277435303, KL: 83.57881927490234, Loss: 0.5432969331741333, Learning Rate: 0.020999999999999998\n",
      "Epoch [1499/20000], Bound: 1.0973550081253052, Entropy: 112.87847137451172, Temp: 0.8079054355621338, KL: 95.25211334228516, Loss: 0.5309551954269409, Learning Rate: 0.020999999999999998\n",
      "Epoch [1500/20000], Bound: 1.0788894891738892, Entropy: 115.65921783447266, Temp: 0.8077982068061829, KL: 75.9886245727539, Loss: 0.6181851625442505, Learning Rate: 0.020999999999999998\n",
      "Epoch [1501/20000], Bound: 1.0731165409088135, Entropy: 114.15657806396484, Temp: 0.8071295022964478, KL: 80.5660171508789, Loss: 0.5795453786849976, Learning Rate: 0.020999999999999998\n",
      "Epoch [1502/20000], Bound: 1.0868210792541504, Entropy: 116.86299896240234, Temp: 0.8061274290084839, KL: 81.04508209228516, Loss: 0.599212646484375, Learning Rate: 0.020999999999999998\n",
      "Epoch [1503/20000], Bound: 1.1118179559707642, Entropy: 118.38350677490234, Temp: 0.8047869801521301, KL: 92.17899322509766, Loss: 0.5727858543395996, Learning Rate: 0.020999999999999998\n",
      "Epoch [1504/20000], Bound: 0.988613486289978, Entropy: 125.32202911376953, Temp: 0.8033736348152161, KL: 83.33135223388672, Loss: 0.42456457018852234, Learning Rate: 0.020999999999999998\n",
      "Epoch [1505/20000], Bound: 1.0400036573410034, Entropy: 119.35282897949219, Temp: 0.8021822571754456, KL: 83.27890014648438, Loss: 0.5043376684188843, Learning Rate: 0.020999999999999998\n",
      "Epoch [1506/20000], Bound: 1.0590589046478271, Entropy: 124.30318450927734, Temp: 0.800971508026123, KL: 81.56827545166016, Loss: 0.5452250838279724, Learning Rate: 0.020999999999999998\n",
      "Epoch [1507/20000], Bound: 1.1354001760482788, Entropy: 127.20221710205078, Temp: 0.7996139526367188, KL: 90.88745880126953, Loss: 0.6187924146652222, Learning Rate: 0.020999999999999998\n",
      "Epoch [1508/20000], Bound: 1.0233653783798218, Entropy: 123.00122833251953, Temp: 0.7980726957321167, KL: 84.40662384033203, Loss: 0.46775129437446594, Learning Rate: 0.020999999999999998\n",
      "Epoch [1509/20000], Bound: 1.0421589612960815, Entropy: 126.27069091796875, Temp: 0.7966812252998352, KL: 78.46430969238281, Loss: 0.5339551568031311, Learning Rate: 0.020999999999999998\n",
      "Epoch [1510/20000], Bound: 1.049533486366272, Entropy: 118.59333038330078, Temp: 0.7951638698577881, KL: 90.36614227294922, Loss: 0.469969779253006, Learning Rate: 0.020999999999999998\n",
      "Epoch [1511/20000], Bound: 1.1082346439361572, Entropy: 119.5749740600586, Temp: 0.7938847541809082, KL: 97.97478485107422, Loss: 0.5201472640037537, Learning Rate: 0.020999999999999998\n",
      "Epoch [1512/20000], Bound: 1.0423295497894287, Entropy: 117.49495697021484, Temp: 0.7927942276000977, KL: 88.96308135986328, Loss: 0.465248167514801, Learning Rate: 0.020999999999999998\n",
      "Epoch [1513/20000], Bound: 1.039780855178833, Entropy: 117.47222137451172, Temp: 0.7919018864631653, KL: 85.87300872802734, Loss: 0.4799557328224182, Learning Rate: 0.020999999999999998\n",
      "Epoch [1514/20000], Bound: 1.0189523696899414, Entropy: 112.28047943115234, Temp: 0.7911064028739929, KL: 85.01287078857422, Loss: 0.45186153054237366, Learning Rate: 0.020999999999999998\n",
      "Epoch [1515/20000], Bound: 1.0195916891098022, Entropy: 113.52484130859375, Temp: 0.79046630859375, KL: 88.83468627929688, Loss: 0.42820602655410767, Learning Rate: 0.020999999999999998\n",
      "Epoch [1516/20000], Bound: 1.0445387363433838, Entropy: 113.94933319091797, Temp: 0.7900902032852173, KL: 89.48163604736328, Loss: 0.4633506238460541, Learning Rate: 0.020999999999999998\n",
      "Epoch [1517/20000], Bound: 1.0920335054397583, Entropy: 113.36073303222656, Temp: 0.7898643612861633, KL: 101.63685607910156, Loss: 0.46506544947624207, Learning Rate: 0.020999999999999998\n",
      "Epoch [1518/20000], Bound: 1.0354118347167969, Entropy: 109.69719696044922, Temp: 0.7899386882781982, KL: 90.0534439086914, Loss: 0.4450034201145172, Learning Rate: 0.020999999999999998\n",
      "Epoch [1519/20000], Bound: 1.0790334939956665, Entropy: 104.84281921386719, Temp: 0.7901781797409058, KL: 96.62384033203125, Loss: 0.4750111997127533, Learning Rate: 0.020999999999999998\n",
      "Epoch [1520/20000], Bound: 1.0830421447753906, Entropy: 104.88044738769531, Temp: 0.7905745506286621, KL: 90.54734802246094, Loss: 0.5205705761909485, Learning Rate: 0.020999999999999998\n",
      "Epoch [1521/20000], Bound: 1.0010998249053955, Entropy: 108.56523895263672, Temp: 0.7909021973609924, KL: 77.70616912841797, Loss: 0.47035878896713257, Learning Rate: 0.020999999999999998\n",
      "Epoch [1522/20000], Bound: 1.0176900625228882, Entropy: 110.09343719482422, Temp: 0.7911141514778137, KL: 83.70789337158203, Loss: 0.45814740657806396, Learning Rate: 0.020999999999999998\n",
      "Epoch [1523/20000], Bound: 0.9561570286750793, Entropy: 114.07251739501953, Temp: 0.7913444638252258, KL: 84.2324447631836, Loss: 0.3626145124435425, Learning Rate: 0.020999999999999998\n",
      "Epoch [1524/20000], Bound: 1.0433624982833862, Entropy: 115.00888061523438, Temp: 0.791871964931488, KL: 94.67634582519531, Loss: 0.4300973415374756, Learning Rate: 0.020999999999999998\n",
      "Epoch [1525/20000], Bound: 0.964644730091095, Entropy: 114.63731384277344, Temp: 0.7926191687583923, KL: 85.32901000976562, Loss: 0.3689311146736145, Learning Rate: 0.020999999999999998\n",
      "Epoch [1526/20000], Bound: 1.0891355276107788, Entropy: 115.20323944091797, Temp: 0.7936050295829773, KL: 105.22449493408203, Loss: 0.4410923719406128, Learning Rate: 0.020999999999999998\n",
      "Epoch [1527/20000], Bound: 0.9891177415847778, Entropy: 113.24556732177734, Temp: 0.7948693037033081, KL: 83.18196868896484, Loss: 0.4203748106956482, Learning Rate: 0.020999999999999998\n",
      "Epoch [1528/20000], Bound: 1.0045647621154785, Entropy: 115.19197082519531, Temp: 0.7961323261260986, KL: 86.5318603515625, Loss: 0.42374032735824585, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1529/20000], Bound: 1.094745397567749, Entropy: 109.59791564941406, Temp: 0.7974284887313843, KL: 92.77301025390625, Loss: 0.5325722098350525, Learning Rate: 0.020999999999999998\n",
      "Epoch [1530/20000], Bound: 0.9820765256881714, Entropy: 111.30184173583984, Temp: 0.7985364198684692, KL: 76.17801666259766, Loss: 0.45619115233421326, Learning Rate: 0.020999999999999998\n",
      "Epoch [1531/20000], Bound: 1.0735725164413452, Entropy: 106.59364318847656, Temp: 0.7994372844696045, KL: 90.29798889160156, Loss: 0.5136727094650269, Learning Rate: 0.020999999999999998\n",
      "Epoch [1532/20000], Bound: 1.0759751796722412, Entropy: 111.13387298583984, Temp: 0.8001958727836609, KL: 89.78166961669922, Loss: 0.5215832591056824, Learning Rate: 0.020999999999999998\n",
      "Epoch [1533/20000], Bound: 1.0459107160568237, Entropy: 112.38501739501953, Temp: 0.800794243812561, KL: 90.5677719116211, Loss: 0.46736299991607666, Learning Rate: 0.020999999999999998\n",
      "Epoch [1534/20000], Bound: 1.0924111604690552, Entropy: 112.68728637695312, Temp: 0.8014064431190491, KL: 89.06654357910156, Loss: 0.555156946182251, Learning Rate: 0.020999999999999998\n",
      "Epoch [1535/20000], Bound: 1.0885708332061768, Entropy: 114.97607421875, Temp: 0.8017662763595581, KL: 108.12771606445312, Loss: 0.42996466159820557, Learning Rate: 0.020999999999999998\n",
      "Epoch [1536/20000], Bound: 1.0888206958770752, Entropy: 110.56362915039062, Temp: 0.8025036454200745, KL: 92.02742004394531, Loss: 0.5314354300498962, Learning Rate: 0.020999999999999998\n",
      "Epoch [1537/20000], Bound: 1.0140300989151, Entropy: 112.8194808959961, Temp: 0.803077220916748, KL: 93.39058685302734, Loss: 0.40080320835113525, Learning Rate: 0.020999999999999998\n",
      "Epoch [1538/20000], Bound: 1.0340018272399902, Entropy: 110.85405731201172, Temp: 0.8038892149925232, KL: 92.11331939697266, Loss: 0.44098588824272156, Learning Rate: 0.020999999999999998\n",
      "Epoch [1539/20000], Bound: 1.0028420686721802, Entropy: 110.50113677978516, Temp: 0.8047781586647034, KL: 86.56253814697266, Loss: 0.42719191312789917, Learning Rate: 0.020999999999999998\n",
      "Epoch [1540/20000], Bound: 0.9723216891288757, Entropy: 112.04499053955078, Temp: 0.8056939840316772, KL: 80.21648406982422, Loss: 0.4210321009159088, Learning Rate: 0.020999999999999998\n",
      "Epoch [1541/20000], Bound: 0.9788432121276855, Entropy: 112.49028778076172, Temp: 0.8065559267997742, KL: 73.5563735961914, Loss: 0.47260698676109314, Learning Rate: 0.020999999999999998\n",
      "Epoch [1542/20000], Bound: 1.0692896842956543, Entropy: 115.41630554199219, Temp: 0.807115912437439, KL: 83.10000610351562, Loss: 0.5573804378509521, Learning Rate: 0.020999999999999998\n",
      "Epoch [1543/20000], Bound: 1.0114097595214844, Entropy: 114.95701599121094, Temp: 0.8073107004165649, KL: 84.42567443847656, Loss: 0.4555407166481018, Learning Rate: 0.020999999999999998\n",
      "Epoch [1544/20000], Bound: 1.0268079042434692, Entropy: 119.0422592163086, Temp: 0.807478129863739, KL: 77.24568939208984, Loss: 0.5243840217590332, Learning Rate: 0.020999999999999998\n",
      "Epoch [1545/20000], Bound: 1.0416289567947388, Entropy: 122.51782989501953, Temp: 0.8073207139968872, KL: 78.44408416748047, Loss: 0.5406685471534729, Learning Rate: 0.020999999999999998\n",
      "Epoch [1546/20000], Bound: 1.0189296007156372, Entropy: 125.1553955078125, Temp: 0.8068454265594482, KL: 76.18229675292969, Loss: 0.5180825591087341, Learning Rate: 0.020999999999999998\n",
      "Epoch [1547/20000], Bound: 1.093855619430542, Entropy: 128.2388458251953, Temp: 0.8061140179634094, KL: 89.96208190917969, Loss: 0.5560709238052368, Learning Rate: 0.020999999999999998\n",
      "Epoch [1548/20000], Bound: 0.9912377595901489, Entropy: 131.30569458007812, Temp: 0.8052531480789185, KL: 69.91265869140625, Loss: 0.5131514072418213, Learning Rate: 0.020999999999999998\n",
      "Epoch [1549/20000], Bound: 0.9643028378486633, Entropy: 130.2123260498047, Temp: 0.8040993809700012, KL: 72.41903686523438, Loss: 0.45663875341415405, Learning Rate: 0.020999999999999998\n",
      "Epoch [1550/20000], Bound: 1.0056507587432861, Entropy: 133.3827362060547, Temp: 0.8028830289840698, KL: 80.97479248046875, Loss: 0.46495547890663147, Learning Rate: 0.020999999999999998\n",
      "Epoch [1551/20000], Bound: 1.017591118812561, Entropy: 137.5100555419922, Temp: 0.8017210364341736, KL: 69.57536315917969, Loss: 0.5538303852081299, Learning Rate: 0.020999999999999998\n",
      "Epoch [1552/20000], Bound: 1.0308003425598145, Entropy: 133.0029754638672, Temp: 0.8001976013183594, KL: 85.60395812988281, Loss: 0.4736076891422272, Learning Rate: 0.020999999999999998\n",
      "Epoch [1553/20000], Bound: 0.9557990431785583, Entropy: 128.8253936767578, Temp: 0.7988142967224121, KL: 75.84214782714844, Loss: 0.41970962285995483, Learning Rate: 0.020999999999999998\n",
      "Epoch [1554/20000], Bound: 1.0082587003707886, Entropy: 126.2495346069336, Temp: 0.7975702285766602, KL: 73.08977508544922, Loss: 0.5147621631622314, Learning Rate: 0.020999999999999998\n",
      "Epoch [1555/20000], Bound: 1.0595016479492188, Entropy: 130.11578369140625, Temp: 0.7961484789848328, KL: 82.46128845214844, Loss: 0.5367340445518494, Learning Rate: 0.020999999999999998\n",
      "Epoch [1556/20000], Bound: 1.085736632347107, Entropy: 126.43910217285156, Temp: 0.7946553230285645, KL: 81.90586853027344, Loss: 0.5830375552177429, Learning Rate: 0.020999999999999998\n",
      "Epoch [1557/20000], Bound: 1.0513392686843872, Entropy: 123.79595184326172, Temp: 0.7929753065109253, KL: 77.53311920166016, Loss: 0.5520365238189697, Learning Rate: 0.020999999999999998\n",
      "Epoch [1558/20000], Bound: 1.0099741220474243, Entropy: 127.3840103149414, Temp: 0.7911508083343506, KL: 82.70478057861328, Loss: 0.452553391456604, Learning Rate: 0.020999999999999998\n",
      "Epoch [1559/20000], Bound: 1.0648972988128662, Entropy: 122.98912048339844, Temp: 0.7895501852035522, KL: 96.2191162109375, Loss: 0.4534355700016022, Learning Rate: 0.020999999999999998\n",
      "Epoch [1560/20000], Bound: 0.9976779222488403, Entropy: 125.61821746826172, Temp: 0.788349986076355, KL: 85.0907974243164, Loss: 0.41659635305404663, Learning Rate: 0.020999999999999998\n",
      "Epoch [1561/20000], Bound: 0.9726637005805969, Entropy: 118.94429779052734, Temp: 0.7874585390090942, KL: 76.02654266357422, Loss: 0.4361076354980469, Learning Rate: 0.020999999999999998\n",
      "Epoch [1562/20000], Bound: 1.0419886112213135, Entropy: 117.20982360839844, Temp: 0.7866562604904175, KL: 74.64317321777344, Loss: 0.5507808923721313, Learning Rate: 0.020999999999999998\n",
      "Epoch [1563/20000], Bound: 1.0756285190582275, Entropy: 117.76380920410156, Temp: 0.7856058478355408, KL: 88.66961669921875, Loss: 0.5157630443572998, Learning Rate: 0.020999999999999998\n",
      "Epoch [1564/20000], Bound: 0.9673594832420349, Entropy: 113.18346405029297, Temp: 0.7846390008926392, KL: 70.3772201538086, Loss: 0.4625120759010315, Learning Rate: 0.020999999999999998\n",
      "Epoch [1565/20000], Bound: 0.9428136944770813, Entropy: 109.00817108154297, Temp: 0.7836176156997681, KL: 77.0637435913086, Loss: 0.3838682472705841, Learning Rate: 0.020999999999999998\n",
      "Epoch [1566/20000], Bound: 0.8832594156265259, Entropy: 115.56794738769531, Temp: 0.7828786969184875, KL: 72.10818481445312, Loss: 0.3331238925457001, Learning Rate: 0.020999999999999998\n",
      "Epoch [1567/20000], Bound: 0.9750205278396606, Entropy: 115.07551574707031, Temp: 0.7824691534042358, KL: 88.29608154296875, Loss: 0.35794681310653687, Learning Rate: 0.020999999999999998\n",
      "Epoch [1568/20000], Bound: 0.9348090291023254, Entropy: 116.53868103027344, Temp: 0.7825288772583008, KL: 72.90582275390625, Loss: 0.3984212875366211, Learning Rate: 0.020999999999999998\n",
      "Epoch [1569/20000], Bound: 0.9601212739944458, Entropy: 121.08836364746094, Temp: 0.7826595306396484, KL: 79.50741577148438, Loss: 0.3924717903137207, Learning Rate: 0.020999999999999998\n",
      "Epoch [1570/20000], Bound: 0.9984545707702637, Entropy: 122.73274993896484, Temp: 0.7829731702804565, KL: 91.76712799072266, Loss: 0.37116608023643494, Learning Rate: 0.020999999999999998\n",
      "Epoch [1571/20000], Bound: 1.0270272493362427, Entropy: 125.76726531982422, Temp: 0.7836927175521851, KL: 85.14177703857422, Loss: 0.458051860332489, Learning Rate: 0.020999999999999998\n",
      "Epoch [1572/20000], Bound: 0.8750870227813721, Entropy: 126.7770767211914, Temp: 0.784432590007782, KL: 65.20499420166016, Loss: 0.3671529293060303, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1573/20000], Bound: 0.9110754132270813, Entropy: 119.36605834960938, Temp: 0.785136878490448, KL: 76.24598693847656, Loss: 0.34568724036216736, Learning Rate: 0.020999999999999998\n",
      "Epoch [1574/20000], Bound: 0.9738131761550903, Entropy: 119.18026733398438, Temp: 0.7860456705093384, KL: 90.19697570800781, Loss: 0.34675413370132446, Learning Rate: 0.020999999999999998\n",
      "Epoch [1575/20000], Bound: 1.0025410652160645, Entropy: 120.71917724609375, Temp: 0.78733891248703, KL: 87.371826171875, Loss: 0.4087657034397125, Learning Rate: 0.020999999999999998\n",
      "Epoch [1576/20000], Bound: 0.9861720204353333, Entropy: 117.58204650878906, Temp: 0.7887524366378784, KL: 77.59764099121094, Loss: 0.4470449388027191, Learning Rate: 0.020999999999999998\n",
      "Epoch [1577/20000], Bound: 1.0290114879608154, Entropy: 112.99446868896484, Temp: 0.7900151014328003, KL: 91.6430892944336, Loss: 0.42486149072647095, Learning Rate: 0.020999999999999998\n",
      "Epoch [1578/20000], Bound: 0.887835681438446, Entropy: 114.29890441894531, Temp: 0.791406512260437, KL: 76.23245239257812, Loss: 0.31795841455459595, Learning Rate: 0.020999999999999998\n",
      "Epoch [1579/20000], Bound: 0.8965216279029846, Entropy: 113.57980346679688, Temp: 0.7929968237876892, KL: 82.97886657714844, Loss: 0.2880752980709076, Learning Rate: 0.020999999999999998\n",
      "Epoch [1580/20000], Bound: 0.9304562211036682, Entropy: 109.55311584472656, Temp: 0.7949536442756653, KL: 86.51828002929688, Loss: 0.313979834318161, Learning Rate: 0.020999999999999998\n",
      "Epoch [1581/20000], Bound: 0.9070337414741516, Entropy: 108.75440979003906, Temp: 0.7972044944763184, KL: 90.75856018066406, Loss: 0.2563236653804779, Learning Rate: 0.020999999999999998\n",
      "Epoch [1582/20000], Bound: 0.9306899309158325, Entropy: 106.90308380126953, Temp: 0.7999486923217773, KL: 89.69351959228516, Loss: 0.297858327627182, Learning Rate: 0.020999999999999998\n",
      "Epoch [1583/20000], Bound: 0.9242095947265625, Entropy: 110.20858764648438, Temp: 0.8029840588569641, KL: 86.2093505859375, Loss: 0.3125414252281189, Learning Rate: 0.020999999999999998\n",
      "Epoch [1584/20000], Bound: 0.8794969916343689, Entropy: 105.42599487304688, Temp: 0.8061787486076355, KL: 87.35716247558594, Loss: 0.2462378591299057, Learning Rate: 0.020999999999999998\n",
      "Epoch [1585/20000], Bound: 0.9470998644828796, Entropy: 107.52633666992188, Temp: 0.8097273111343384, KL: 89.84611511230469, Loss: 0.3271596133708954, Learning Rate: 0.020999999999999998\n",
      "Epoch [1586/20000], Bound: 0.9457439184188843, Entropy: 109.87033081054688, Temp: 0.8133673667907715, KL: 80.63490295410156, Loss: 0.3843337595462799, Learning Rate: 0.020999999999999998\n",
      "Epoch [1587/20000], Bound: 0.9288730025291443, Entropy: 109.86166381835938, Temp: 0.8167759776115417, KL: 83.75247192382812, Loss: 0.34316161274909973, Learning Rate: 0.020999999999999998\n",
      "Epoch [1588/20000], Bound: 0.8937296271324158, Entropy: 115.36934661865234, Temp: 0.8201324343681335, KL: 83.75617218017578, Loss: 0.2962765693664551, Learning Rate: 0.020999999999999998\n",
      "Epoch [1589/20000], Bound: 0.8191773891448975, Entropy: 115.21807098388672, Temp: 0.8235765099525452, KL: 76.67752838134766, Loss: 0.2434559166431427, Learning Rate: 0.020999999999999998\n",
      "Epoch [1590/20000], Bound: 0.9053984880447388, Entropy: 113.89900970458984, Temp: 0.8271598815917969, KL: 75.64488983154297, Loss: 0.36555084586143494, Learning Rate: 0.020999999999999998\n",
      "Epoch [1591/20000], Bound: 0.8007796406745911, Entropy: 114.90538787841797, Temp: 0.8304529786109924, KL: 69.76648712158203, Loss: 0.26553767919540405, Learning Rate: 0.020999999999999998\n",
      "Epoch [1592/20000], Bound: 0.8838142156600952, Entropy: 117.34197998046875, Temp: 0.8337072730064392, KL: 83.73204040527344, Loss: 0.2908746898174286, Learning Rate: 0.020999999999999998\n",
      "Epoch [1593/20000], Bound: 0.858041524887085, Entropy: 118.93656921386719, Temp: 0.8370330333709717, KL: 79.26397705078125, Loss: 0.28496477007865906, Learning Rate: 0.020999999999999998\n",
      "Epoch [1594/20000], Bound: 0.9224178791046143, Entropy: 119.7225570678711, Temp: 0.8403699398040771, KL: 86.02266693115234, Loss: 0.33469146490097046, Learning Rate: 0.020999999999999998\n",
      "Epoch [1595/20000], Bound: 0.7852310538291931, Entropy: 125.06785583496094, Temp: 0.8436393737792969, KL: 72.93800354003906, Loss: 0.23335501551628113, Learning Rate: 0.020999999999999998\n",
      "Epoch [1596/20000], Bound: 0.9280834197998047, Entropy: 119.64115142822266, Temp: 0.8469909429550171, KL: 91.79582977294922, Loss: 0.31263983249664307, Learning Rate: 0.020999999999999998\n",
      "Epoch [1597/20000], Bound: 0.9199933409690857, Entropy: 123.04989624023438, Temp: 0.8503994941711426, KL: 92.75119018554688, Loss: 0.2975884675979614, Learning Rate: 0.020999999999999998\n",
      "Epoch [1598/20000], Bound: 0.8356028199195862, Entropy: 119.12734985351562, Temp: 0.8539094924926758, KL: 78.00010681152344, Loss: 0.27162376046180725, Learning Rate: 0.020999999999999998\n",
      "Epoch [1599/20000], Bound: 0.9179033637046814, Entropy: 115.24571990966797, Temp: 0.8573895692825317, KL: 84.77129364013672, Loss: 0.345488041639328, Learning Rate: 0.020999999999999998\n",
      "Epoch [1600/20000], Bound: 0.8573963046073914, Entropy: 116.72594451904297, Temp: 0.8606791496276855, KL: 79.26404571533203, Loss: 0.296091765165329, Learning Rate: 0.020999999999999998\n",
      "Epoch [1601/20000], Bound: 0.9442126154899597, Entropy: 111.39716339111328, Temp: 0.8638743162155151, KL: 96.52474212646484, Loss: 0.31924858689308167, Learning Rate: 0.020999999999999998\n",
      "Epoch [1602/20000], Bound: 0.864909827709198, Entropy: 108.05326843261719, Temp: 0.8671249747276306, KL: 87.96656799316406, Loss: 0.2590758800506592, Learning Rate: 0.020999999999999998\n",
      "Epoch [1603/20000], Bound: 0.8176207542419434, Entropy: 106.6689224243164, Temp: 0.8705087304115295, KL: 76.94939422607422, Loss: 0.2623741030693054, Learning Rate: 0.020999999999999998\n",
      "Epoch [1604/20000], Bound: 0.907694935798645, Entropy: 112.04144287109375, Temp: 0.8738441467285156, KL: 80.72831726074219, Loss: 0.36310553550720215, Learning Rate: 0.020999999999999998\n",
      "Epoch [1605/20000], Bound: 0.915572464466095, Entropy: 113.74470520019531, Temp: 0.876833975315094, KL: 84.24517822265625, Loss: 0.35577017068862915, Learning Rate: 0.020999999999999998\n",
      "Epoch [1606/20000], Bound: 0.9159913659095764, Entropy: 113.7890396118164, Temp: 0.8795738220214844, KL: 88.78856658935547, Loss: 0.33198440074920654, Learning Rate: 0.020999999999999998\n",
      "Epoch [1607/20000], Bound: 0.9243075251579285, Entropy: 113.69114685058594, Temp: 0.8822188377380371, KL: 88.79776000976562, Loss: 0.3453855514526367, Learning Rate: 0.020999999999999998\n",
      "Epoch [1608/20000], Bound: 0.8835753798484802, Entropy: 118.2743148803711, Temp: 0.8847245573997498, KL: 79.32685089111328, Loss: 0.34267210960388184, Learning Rate: 0.020999999999999998\n",
      "Epoch [1609/20000], Bound: 0.8682034611701965, Entropy: 115.50167846679688, Temp: 0.8869774341583252, KL: 81.05058288574219, Loss: 0.31298938393592834, Learning Rate: 0.020999999999999998\n",
      "Epoch [1610/20000], Bound: 0.9467325806617737, Entropy: 113.38743591308594, Temp: 0.8891211748123169, KL: 94.77912902832031, Loss: 0.34863272309303284, Learning Rate: 0.020999999999999998\n",
      "Epoch [1611/20000], Bound: 1.0134568214416504, Entropy: 112.82733917236328, Temp: 0.891217052936554, KL: 94.2368392944336, Loss: 0.45767152309417725, Learning Rate: 0.020999999999999998\n",
      "Epoch [1612/20000], Bound: 0.8931864500045776, Entropy: 117.68075561523438, Temp: 0.8928946852684021, KL: 88.92729187011719, Loss: 0.30607712268829346, Learning Rate: 0.020999999999999998\n",
      "Epoch [1613/20000], Bound: 0.8595647811889648, Entropy: 116.03773498535156, Temp: 0.8946295380592346, KL: 79.09098815917969, Loss: 0.31583818793296814, Learning Rate: 0.020999999999999998\n",
      "Epoch [1614/20000], Bound: 0.9405557513237, Entropy: 114.10409545898438, Temp: 0.8962451219558716, KL: 90.44035339355469, Loss: 0.3678146302700043, Learning Rate: 0.020999999999999998\n",
      "Epoch [1615/20000], Bound: 0.8937165141105652, Entropy: 113.2453384399414, Temp: 0.8977186679840088, KL: 87.5115737915039, Loss: 0.3172175884246826, Learning Rate: 0.020999999999999998\n",
      "Epoch [1616/20000], Bound: 1.016713261604309, Entropy: 116.56707000732422, Temp: 0.8991967439651489, KL: 95.1587905883789, Loss: 0.46305349469184875, Learning Rate: 0.020999999999999998\n",
      "Epoch [1617/20000], Bound: 0.7727475166320801, Entropy: 115.76154327392578, Temp: 0.9002795219421387, KL: 72.22917938232422, Loss: 0.24533595144748688, Learning Rate: 0.020999999999999998\n",
      "Epoch [1618/20000], Bound: 0.8490496873855591, Entropy: 115.69558715820312, Temp: 0.9014534950256348, KL: 76.03271484375, Loss: 0.3217448592185974, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1619/20000], Bound: 0.940540611743927, Entropy: 120.01939392089844, Temp: 0.9024786949157715, KL: 96.83097839355469, Loss: 0.3358239531517029, Learning Rate: 0.020999999999999998\n",
      "Epoch [1620/20000], Bound: 0.9254421591758728, Entropy: 116.67056274414062, Temp: 0.9035906791687012, KL: 81.70025634765625, Loss: 0.3978612422943115, Learning Rate: 0.020999999999999998\n",
      "Epoch [1621/20000], Bound: 0.9293626546859741, Entropy: 121.17538452148438, Temp: 0.9043657779693604, KL: 85.02668762207031, Loss: 0.385603666305542, Learning Rate: 0.020999999999999998\n",
      "Epoch [1622/20000], Bound: 0.8142625689506531, Entropy: 119.53141021728516, Temp: 0.9049208164215088, KL: 78.64238739013672, Loss: 0.263406902551651, Learning Rate: 0.020999999999999998\n",
      "Epoch [1623/20000], Bound: 0.9383772015571594, Entropy: 118.89628601074219, Temp: 0.9056269526481628, KL: 89.3758544921875, Loss: 0.37559184432029724, Learning Rate: 0.020999999999999998\n",
      "Epoch [1624/20000], Bound: 0.9463563561439514, Entropy: 114.28389739990234, Temp: 0.9062072038650513, KL: 90.49144744873047, Loss: 0.38171619176864624, Learning Rate: 0.020999999999999998\n",
      "Epoch [1625/20000], Bound: 0.8367327451705933, Entropy: 116.84078216552734, Temp: 0.9066652655601501, KL: 86.84246063232422, Loss: 0.24799945950508118, Learning Rate: 0.020999999999999998\n",
      "Epoch [1626/20000], Bound: 0.8982192873954773, Entropy: 109.55292510986328, Temp: 0.9074460864067078, KL: 89.64937591552734, Loss: 0.3166830837726593, Learning Rate: 0.020999999999999998\n",
      "Epoch [1627/20000], Bound: 0.8601862788200378, Entropy: 113.0619125366211, Temp: 0.9082985520362854, KL: 88.40984344482422, Loss: 0.27135610580444336, Learning Rate: 0.020999999999999998\n",
      "Epoch [1628/20000], Bound: 0.9131537675857544, Entropy: 110.47661590576172, Temp: 0.9093628525733948, KL: 88.9659194946289, Loss: 0.342783123254776, Learning Rate: 0.020999999999999998\n",
      "Epoch [1629/20000], Bound: 0.9888941645622253, Entropy: 104.82423400878906, Temp: 0.9103626012802124, KL: 102.8587646484375, Loss: 0.38228538632392883, Learning Rate: 0.020999999999999998\n",
      "Epoch [1630/20000], Bound: 0.9194566011428833, Entropy: 104.43771362304688, Temp: 0.91133713722229, KL: 91.98110961914062, Loss: 0.3363996148109436, Learning Rate: 0.020999999999999998\n",
      "Epoch [1631/20000], Bound: 0.9720678925514221, Entropy: 104.38624572753906, Temp: 0.9123109579086304, KL: 89.12730407714844, Loss: 0.4320964813232422, Learning Rate: 0.020999999999999998\n",
      "Epoch [1632/20000], Bound: 0.9716089367866516, Entropy: 99.14847564697266, Temp: 0.9129121899604797, KL: 84.67436981201172, Loss: 0.456095427274704, Learning Rate: 0.020999999999999998\n",
      "Epoch [1633/20000], Bound: 0.9055259227752686, Entropy: 106.5877914428711, Temp: 0.9130367636680603, KL: 83.30559539794922, Loss: 0.3646761178970337, Learning Rate: 0.020999999999999998\n",
      "Epoch [1634/20000], Bound: 0.8869276642799377, Entropy: 106.25563049316406, Temp: 0.9130255579948425, KL: 92.20053100585938, Loss: 0.28964343667030334, Learning Rate: 0.020999999999999998\n",
      "Epoch [1635/20000], Bound: 0.9372097849845886, Entropy: 107.3802490234375, Temp: 0.9132785201072693, KL: 88.19978332519531, Loss: 0.3843415677547455, Learning Rate: 0.020999999999999998\n",
      "Epoch [1636/20000], Bound: 0.8612112402915955, Entropy: 107.5045166015625, Temp: 0.9133777618408203, KL: 77.63816833496094, Loss: 0.33416053652763367, Learning Rate: 0.020999999999999998\n",
      "Epoch [1637/20000], Bound: 0.864208459854126, Entropy: 108.60509490966797, Temp: 0.9133745431900024, KL: 87.21025848388672, Loss: 0.28582486510276794, Learning Rate: 0.020999999999999998\n",
      "Epoch [1638/20000], Bound: 0.8810111284255981, Entropy: 112.82231903076172, Temp: 0.9135839939117432, KL: 80.62836456298828, Loss: 0.34502163529396057, Learning Rate: 0.020999999999999998\n",
      "Epoch [1639/20000], Bound: 0.913963258266449, Entropy: 117.15259552001953, Temp: 0.9136807322502136, KL: 80.92430877685547, Loss: 0.3901631534099579, Learning Rate: 0.020999999999999998\n",
      "Epoch [1640/20000], Bound: 0.897628128528595, Entropy: 120.36518096923828, Temp: 0.9135207533836365, KL: 80.7593002319336, Loss: 0.3675854504108429, Learning Rate: 0.020999999999999998\n",
      "Epoch [1641/20000], Bound: 0.8661065101623535, Entropy: 119.38738250732422, Temp: 0.9132067561149597, KL: 80.46768951416016, Loss: 0.325244277715683, Learning Rate: 0.020999999999999998\n",
      "Epoch [1642/20000], Bound: 0.8318367004394531, Entropy: 113.74372863769531, Temp: 0.9129024744033813, KL: 83.4071044921875, Loss: 0.2633086144924164, Learning Rate: 0.020999999999999998\n",
      "Epoch [1643/20000], Bound: 0.8946431875228882, Entropy: 116.80968475341797, Temp: 0.9128777384757996, KL: 86.00971221923828, Loss: 0.33431583642959595, Learning Rate: 0.020999999999999998\n",
      "Epoch [1644/20000], Bound: 0.9295985698699951, Entropy: 114.0111083984375, Temp: 0.912876307964325, KL: 87.87913513183594, Loss: 0.37457770109176636, Learning Rate: 0.020999999999999998\n",
      "Epoch [1645/20000], Bound: 0.8782491683959961, Entropy: 113.62232208251953, Temp: 0.912777304649353, KL: 84.2810287475586, Loss: 0.32083117961883545, Learning Rate: 0.020999999999999998\n",
      "Epoch [1646/20000], Bound: 0.9424943923950195, Entropy: 111.13249969482422, Temp: 0.9127352237701416, KL: 87.04706573486328, Loss: 0.39829757809638977, Learning Rate: 0.020999999999999998\n",
      "Epoch [1647/20000], Bound: 0.921217679977417, Entropy: 111.47090911865234, Temp: 0.912506639957428, KL: 94.28902435302734, Loss: 0.3269457519054413, Learning Rate: 0.020999999999999998\n",
      "Epoch [1648/20000], Bound: 0.9108023047447205, Entropy: 113.5076904296875, Temp: 0.9124568700790405, KL: 87.53787231445312, Loss: 0.3487928807735443, Learning Rate: 0.020999999999999998\n",
      "Epoch [1649/20000], Bound: 0.9462957978248596, Entropy: 118.2751235961914, Temp: 0.9124025106430054, KL: 81.1095962524414, Loss: 0.43639814853668213, Learning Rate: 0.020999999999999998\n",
      "Epoch [1650/20000], Bound: 0.9628082513809204, Entropy: 115.31205749511719, Temp: 0.9119543433189392, KL: 90.32929992675781, Loss: 0.410888671875, Learning Rate: 0.020999999999999998\n",
      "Epoch [1651/20000], Bound: 0.8798628449440002, Entropy: 110.53617095947266, Temp: 0.9113626480102539, KL: 79.95728302001953, Loss: 0.3461291193962097, Learning Rate: 0.020999999999999998\n",
      "Epoch [1652/20000], Bound: 0.8656517267227173, Entropy: 108.92738342285156, Temp: 0.9107319116592407, KL: 80.04153442382812, Loss: 0.3258867561817169, Learning Rate: 0.020999999999999998\n",
      "Epoch [1653/20000], Bound: 0.8831889629364014, Entropy: 108.98689270019531, Temp: 0.9101422429084778, KL: 80.62278747558594, Loss: 0.34655019640922546, Learning Rate: 0.020999999999999998\n",
      "Epoch [1654/20000], Bound: 0.8502055406570435, Entropy: 105.30146026611328, Temp: 0.90952467918396, KL: 77.04302215576172, Loss: 0.3210405707359314, Learning Rate: 0.020999999999999998\n",
      "Epoch [1655/20000], Bound: 0.8683373332023621, Entropy: 108.85609436035156, Temp: 0.9089268445968628, KL: 81.83534240722656, Loss: 0.31888994574546814, Learning Rate: 0.020999999999999998\n",
      "Epoch [1656/20000], Bound: 0.8415003418922424, Entropy: 111.20773315429688, Temp: 0.9084226489067078, KL: 78.59457397460938, Loss: 0.3004956543445587, Learning Rate: 0.020999999999999998\n",
      "Epoch [1657/20000], Bound: 0.9139403104782104, Entropy: 112.74134826660156, Temp: 0.9080272316932678, KL: 79.49375915527344, Loss: 0.39539140462875366, Learning Rate: 0.020999999999999998\n",
      "Epoch [1658/20000], Bound: 0.9599370360374451, Entropy: 116.26280212402344, Temp: 0.9074044227600098, KL: 88.83555603027344, Loss: 0.4121808111667633, Learning Rate: 0.020999999999999998\n",
      "Epoch [1659/20000], Bound: 0.8771317005157471, Entropy: 118.0956039428711, Temp: 0.9066474437713623, KL: 79.7149429321289, Loss: 0.34161123633384705, Learning Rate: 0.020999999999999998\n",
      "Epoch [1660/20000], Bound: 0.8889849185943604, Entropy: 124.12033081054688, Temp: 0.905896008014679, KL: 81.53767395019531, Loss: 0.3476707339286804, Learning Rate: 0.020999999999999998\n",
      "Epoch [1661/20000], Bound: 1.0054702758789062, Entropy: 130.4250030517578, Temp: 0.9051550626754761, KL: 94.55752563476562, Loss: 0.4515955150127411, Learning Rate: 0.020999999999999998\n",
      "Epoch [1662/20000], Bound: 0.8775491118431091, Entropy: 129.30349731445312, Temp: 0.904242217540741, KL: 74.78872680664062, Loss: 0.36835864186286926, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1663/20000], Bound: 0.9620190262794495, Entropy: 132.9237823486328, Temp: 0.9031945466995239, KL: 72.57594299316406, Loss: 0.5030880570411682, Learning Rate: 0.020999999999999998\n",
      "Epoch [1664/20000], Bound: 0.9209004044532776, Entropy: 132.02305603027344, Temp: 0.9015519618988037, KL: 76.09060668945312, Loss: 0.4213583171367645, Learning Rate: 0.020999999999999998\n",
      "Epoch [1665/20000], Bound: 0.944222629070282, Entropy: 136.11773681640625, Temp: 0.8996964693069458, KL: 83.92948913574219, Loss: 0.41139331459999084, Learning Rate: 0.020999999999999998\n",
      "Epoch [1666/20000], Bound: 0.9036749601364136, Entropy: 133.81341552734375, Temp: 0.8977973461151123, KL: 76.98457336425781, Loss: 0.3899434506893158, Learning Rate: 0.020999999999999998\n",
      "Epoch [1667/20000], Bound: 1.0010740756988525, Entropy: 126.5822982788086, Temp: 0.8958426713943481, KL: 94.30594635009766, Loss: 0.4400111734867096, Learning Rate: 0.020999999999999998\n",
      "Epoch [1668/20000], Bound: 1.0062007904052734, Entropy: 121.3445053100586, Temp: 0.893910825252533, KL: 95.20394134521484, Loss: 0.44210192561149597, Learning Rate: 0.020999999999999998\n",
      "Epoch [1669/20000], Bound: 0.9194021224975586, Entropy: 115.41700744628906, Temp: 0.8920115232467651, KL: 85.18745422363281, Loss: 0.36386993527412415, Learning Rate: 0.020999999999999998\n",
      "Epoch [1670/20000], Bound: 0.912312924861908, Entropy: 108.13068389892578, Temp: 0.8902785778045654, KL: 83.8934097290039, Loss: 0.36004140973091125, Learning Rate: 0.020999999999999998\n",
      "Epoch [1671/20000], Bound: 0.8930801749229431, Entropy: 105.7431640625, Temp: 0.8886967897415161, KL: 91.43569946289062, Loss: 0.28960397839546204, Learning Rate: 0.020999999999999998\n",
      "Epoch [1672/20000], Bound: 0.8761103749275208, Entropy: 102.28807067871094, Temp: 0.8876075744628906, KL: 91.75642395019531, Loss: 0.26374420523643494, Learning Rate: 0.020999999999999998\n",
      "Epoch [1673/20000], Bound: 0.8630080223083496, Entropy: 97.88906860351562, Temp: 0.8870629668235779, KL: 98.00175476074219, Loss: 0.21046607196331024, Learning Rate: 0.020999999999999998\n",
      "Epoch [1674/20000], Bound: 0.8637440800666809, Entropy: 100.98179626464844, Temp: 0.8872883915901184, KL: 97.78048706054688, Loss: 0.21283435821533203, Learning Rate: 0.020999999999999998\n",
      "Epoch [1675/20000], Bound: 0.8767582178115845, Entropy: 98.11849212646484, Temp: 0.8881902098655701, KL: 89.4911117553711, Loss: 0.2776985168457031, Learning Rate: 0.020999999999999998\n",
      "Epoch [1676/20000], Bound: 0.8757754564285278, Entropy: 96.81390380859375, Temp: 0.8893517255783081, KL: 89.77284240722656, Loss: 0.27537986636161804, Learning Rate: 0.020999999999999998\n",
      "Epoch [1677/20000], Bound: 0.8883567452430725, Entropy: 101.39997863769531, Temp: 0.8907559514045715, KL: 89.171630859375, Loss: 0.29684752225875854, Learning Rate: 0.020999999999999998\n",
      "Epoch [1678/20000], Bound: 0.8886560201644897, Entropy: 107.55039978027344, Temp: 0.8922895789146423, KL: 83.66452026367188, Loss: 0.32892853021621704, Learning Rate: 0.020999999999999998\n",
      "Epoch [1679/20000], Bound: 0.9608762860298157, Entropy: 104.76416015625, Temp: 0.8937475681304932, KL: 94.16949462890625, Loss: 0.3762173354625702, Learning Rate: 0.020999999999999998\n",
      "Epoch [1680/20000], Bound: 0.8512392044067383, Entropy: 111.427490234375, Temp: 0.8951077461242676, KL: 73.20469665527344, Loss: 0.3377975523471832, Learning Rate: 0.020999999999999998\n",
      "Epoch [1681/20000], Bound: 0.841438889503479, Entropy: 115.15705108642578, Temp: 0.8962230086326599, KL: 82.45287322998047, Loss: 0.27368396520614624, Learning Rate: 0.020999999999999998\n",
      "Epoch [1682/20000], Bound: 0.9805759191513062, Entropy: 119.56314086914062, Temp: 0.897474467754364, KL: 90.27113342285156, Loss: 0.4307519197463989, Learning Rate: 0.020999999999999998\n",
      "Epoch [1683/20000], Bound: 0.8370301127433777, Entropy: 122.8807373046875, Temp: 0.8983999490737915, KL: 77.42861938476562, Loss: 0.2968514561653137, Learning Rate: 0.020999999999999998\n",
      "Epoch [1684/20000], Bound: 0.825640082359314, Entropy: 123.5779037475586, Temp: 0.8993199467658997, KL: 68.89844512939453, Loss: 0.32984450459480286, Learning Rate: 0.020999999999999998\n",
      "Epoch [1685/20000], Bound: 0.9061789512634277, Entropy: 127.8949203491211, Temp: 0.8999911546707153, KL: 76.42491912841797, Loss: 0.39759984612464905, Learning Rate: 0.020999999999999998\n",
      "Epoch [1686/20000], Bound: 0.8860659599304199, Entropy: 127.30217742919922, Temp: 0.9003067016601562, KL: 75.41339874267578, Loss: 0.3750285506248474, Learning Rate: 0.020999999999999998\n",
      "Epoch [1687/20000], Bound: 0.9831844568252563, Entropy: 127.82803344726562, Temp: 0.9003645181655884, KL: 84.29106140136719, Loss: 0.4697616398334503, Learning Rate: 0.020999999999999998\n",
      "Epoch [1688/20000], Bound: 0.9030728936195374, Entropy: 131.07818603515625, Temp: 0.8999955058097839, KL: 75.04563903808594, Loss: 0.4008425772190094, Learning Rate: 0.020999999999999998\n",
      "Epoch [1689/20000], Bound: 0.9434129595756531, Entropy: 126.49850463867188, Temp: 0.899344801902771, KL: 89.49551391601562, Loss: 0.3790544867515564, Learning Rate: 0.020999999999999998\n",
      "Epoch [1690/20000], Bound: 0.9633122086524963, Entropy: 118.20164489746094, Temp: 0.8987143039703369, KL: 88.00482177734375, Loss: 0.41720160841941833, Learning Rate: 0.020999999999999998\n",
      "Epoch [1691/20000], Bound: 0.9081423878669739, Entropy: 118.0824203491211, Temp: 0.8979551792144775, KL: 78.9984359741211, Loss: 0.38516637682914734, Learning Rate: 0.020999999999999998\n",
      "Epoch [1692/20000], Bound: 0.9086908102035522, Entropy: 113.9110336303711, Temp: 0.8970691561698914, KL: 77.1596908569336, Loss: 0.39578762650489807, Learning Rate: 0.020999999999999998\n",
      "Epoch [1693/20000], Bound: 0.9267324209213257, Entropy: 117.0411605834961, Temp: 0.8960099816322327, KL: 98.47449493408203, Loss: 0.3024463653564453, Learning Rate: 0.020999999999999998\n",
      "Epoch [1694/20000], Bound: 0.8960658311843872, Entropy: 116.3577880859375, Temp: 0.8954096436500549, KL: 90.2183837890625, Loss: 0.3042220175266266, Learning Rate: 0.020999999999999998\n",
      "Epoch [1695/20000], Bound: 0.7970730066299438, Entropy: 118.38121032714844, Temp: 0.8951108455657959, KL: 77.77861022949219, Loss: 0.2424013912677765, Learning Rate: 0.020999999999999998\n",
      "Epoch [1696/20000], Bound: 0.9399067759513855, Entropy: 117.36576843261719, Temp: 0.8951457142829895, KL: 88.75880432128906, Loss: 0.37562960386276245, Learning Rate: 0.020999999999999998\n",
      "Epoch [1697/20000], Bound: 0.9534804224967957, Entropy: 116.19978332519531, Temp: 0.8951492309570312, KL: 91.01373291015625, Loss: 0.38341638445854187, Learning Rate: 0.020999999999999998\n",
      "Epoch [1698/20000], Bound: 0.8914045691490173, Entropy: 118.9050521850586, Temp: 0.8951274156570435, KL: 80.2745590209961, Loss: 0.35308319330215454, Learning Rate: 0.020999999999999998\n",
      "Epoch [1699/20000], Bound: 0.8689447641372681, Entropy: 117.78614044189453, Temp: 0.8950428366661072, KL: 80.69766998291016, Loss: 0.31972503662109375, Learning Rate: 0.020999999999999998\n",
      "Epoch [1700/20000], Bound: 0.8494257926940918, Entropy: 117.33599090576172, Temp: 0.895025908946991, KL: 82.43912506103516, Loss: 0.2837665379047394, Learning Rate: 0.020999999999999998\n",
      "Epoch [1701/20000], Bound: 0.7912631630897522, Entropy: 117.6461410522461, Temp: 0.8952245712280273, KL: 75.16690826416016, Loss: 0.24979670345783234, Learning Rate: 0.020999999999999998\n",
      "Epoch [1702/20000], Bound: 0.910578191280365, Entropy: 121.21896362304688, Temp: 0.8956431150436401, KL: 87.32533264160156, Loss: 0.3410894572734833, Learning Rate: 0.020999999999999998\n",
      "Epoch [1703/20000], Bound: 0.8422726988792419, Entropy: 123.76490020751953, Temp: 0.896091639995575, KL: 83.35350799560547, Loss: 0.26969611644744873, Learning Rate: 0.020999999999999998\n",
      "Epoch [1704/20000], Bound: 0.8765701651573181, Entropy: 116.94247436523438, Temp: 0.8967709541320801, KL: 80.62164306640625, Loss: 0.3313593566417694, Learning Rate: 0.020999999999999998\n",
      "Epoch [1705/20000], Bound: 0.8593809008598328, Entropy: 117.37859344482422, Temp: 0.8973940014839172, KL: 80.5374526977539, Loss: 0.3087586760520935, Learning Rate: 0.020999999999999998\n",
      "Epoch [1706/20000], Bound: 0.8478708863258362, Entropy: 115.51765441894531, Temp: 0.898044764995575, KL: 88.24769592285156, Loss: 0.25075167417526245, Learning Rate: 0.020999999999999998\n",
      "Epoch [1707/20000], Bound: 0.8013288378715515, Entropy: 116.47982025146484, Temp: 0.8990370631217957, KL: 80.98331451416016, Loss: 0.23154492676258087, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1708/20000], Bound: 0.8347967863082886, Entropy: 114.58580017089844, Temp: 0.900310218334198, KL: 96.23966979980469, Loss: 0.19026438891887665, Learning Rate: 0.020999999999999998\n",
      "Epoch [1709/20000], Bound: 0.8401740789413452, Entropy: 113.04042053222656, Temp: 0.9021881818771362, KL: 89.09950256347656, Loss: 0.23789188265800476, Learning Rate: 0.020999999999999998\n",
      "Epoch [1710/20000], Bound: 0.8534663319587708, Entropy: 112.7167739868164, Temp: 0.904332160949707, KL: 79.75824737548828, Loss: 0.3082272708415985, Learning Rate: 0.020999999999999998\n",
      "Epoch [1711/20000], Bound: 0.8479157090187073, Entropy: 112.1352310180664, Temp: 0.9063242673873901, KL: 90.0726089477539, Loss: 0.24478667974472046, Learning Rate: 0.020999999999999998\n",
      "Epoch [1712/20000], Bound: 0.8510878682136536, Entropy: 111.6745376586914, Temp: 0.9085456132888794, KL: 94.0411148071289, Loss: 0.2282687872648239, Learning Rate: 0.020999999999999998\n",
      "Epoch [1713/20000], Bound: 0.7865198254585266, Entropy: 108.47870635986328, Temp: 0.9110796451568604, KL: 90.3386001586914, Loss: 0.16683636605739594, Learning Rate: 0.020999999999999998\n",
      "Epoch [1714/20000], Bound: 0.9416410326957703, Entropy: 108.53282928466797, Temp: 0.9140807390213013, KL: 100.38001251220703, Loss: 0.3247743248939514, Learning Rate: 0.020999999999999998\n",
      "Epoch [1715/20000], Bound: 0.9573420882225037, Entropy: 109.87406158447266, Temp: 0.9170217514038086, KL: 89.78685760498047, Loss: 0.408170223236084, Learning Rate: 0.020999999999999998\n",
      "Epoch [1716/20000], Bound: 0.9085748195648193, Entropy: 106.78451538085938, Temp: 0.9194703698158264, KL: 96.58140563964844, Loss: 0.2998671531677246, Learning Rate: 0.020999999999999998\n",
      "Epoch [1717/20000], Bound: 0.9644253253936768, Entropy: 106.92642211914062, Temp: 0.9219388365745544, KL: 88.07121276855469, Loss: 0.4310779869556427, Learning Rate: 0.020999999999999998\n",
      "Epoch [1718/20000], Bound: 0.8362600803375244, Entropy: 103.47682189941406, Temp: 0.9238408207893372, KL: 86.97868347167969, Loss: 0.2545291483402252, Learning Rate: 0.020999999999999998\n",
      "Epoch [1719/20000], Bound: 0.8559449315071106, Entropy: 98.45931243896484, Temp: 0.9258527755737305, KL: 87.1885757446289, Loss: 0.2805453836917877, Learning Rate: 0.020999999999999998\n",
      "Epoch [1720/20000], Bound: 0.858035147190094, Entropy: 105.766845703125, Temp: 0.9278615713119507, KL: 88.82733154296875, Loss: 0.2754528224468231, Learning Rate: 0.020999999999999998\n",
      "Epoch [1721/20000], Bound: 0.8835645318031311, Entropy: 106.4808578491211, Temp: 0.9299017786979675, KL: 94.72058868408203, Loss: 0.2798633277416229, Learning Rate: 0.020999999999999998\n",
      "Epoch [1722/20000], Bound: 0.844619870185852, Entropy: 108.57071685791016, Temp: 0.9320213794708252, KL: 84.40196990966797, Loss: 0.28307491540908813, Learning Rate: 0.020999999999999998\n",
      "Epoch [1723/20000], Bound: 0.8214122653007507, Entropy: 109.2751235961914, Temp: 0.9340634346008301, KL: 86.45589447021484, Loss: 0.2424371987581253, Learning Rate: 0.020999999999999998\n",
      "Epoch [1724/20000], Bound: 0.7744877338409424, Entropy: 111.02557373046875, Temp: 0.9362138509750366, KL: 78.68948364257812, Loss: 0.22565147280693054, Learning Rate: 0.020999999999999998\n",
      "Epoch [1725/20000], Bound: 0.9113396406173706, Entropy: 115.82886505126953, Temp: 0.9384230971336365, KL: 93.34191131591797, Loss: 0.33118781447410583, Learning Rate: 0.020999999999999998\n",
      "Epoch [1726/20000], Bound: 0.8744099140167236, Entropy: 115.2035903930664, Temp: 0.9404582977294922, KL: 92.00792694091797, Loss: 0.2867654860019684, Learning Rate: 0.020999999999999998\n",
      "Epoch [1727/20000], Bound: 0.8211912512779236, Entropy: 119.58683013916016, Temp: 0.9424816966056824, KL: 81.09317779541016, Loss: 0.2741764485836029, Learning Rate: 0.020999999999999998\n",
      "Epoch [1728/20000], Bound: 0.8983883261680603, Entropy: 115.8241958618164, Temp: 0.9443979263305664, KL: 87.99494171142578, Loss: 0.34371858835220337, Learning Rate: 0.020999999999999998\n",
      "Epoch [1729/20000], Bound: 0.9433783292770386, Entropy: 117.16800689697266, Temp: 0.9460347890853882, KL: 79.04686737060547, Loss: 0.45839887857437134, Learning Rate: 0.020999999999999998\n",
      "Epoch [1730/20000], Bound: 0.9603768587112427, Entropy: 116.55253601074219, Temp: 0.9468816518783569, KL: 97.15467834472656, Loss: 0.3894703984260559, Learning Rate: 0.020999999999999998\n",
      "Epoch [1731/20000], Bound: 0.9205156564712524, Entropy: 120.04427337646484, Temp: 0.9474920034408569, KL: 73.77384185791016, Loss: 0.45244038105010986, Learning Rate: 0.020999999999999998\n",
      "Epoch [1732/20000], Bound: 0.8780573010444641, Entropy: 116.21634674072266, Temp: 0.9473613500595093, KL: 83.7137680053711, Loss: 0.3388582170009613, Learning Rate: 0.020999999999999998\n",
      "Epoch [1733/20000], Bound: 0.8980981111526489, Entropy: 114.79867553710938, Temp: 0.9471067786216736, KL: 89.35012817382812, Loss: 0.3373796045780182, Learning Rate: 0.020999999999999998\n",
      "Epoch [1734/20000], Bound: 0.9083173871040344, Entropy: 112.81935119628906, Temp: 0.9468193054199219, KL: 88.99087524414062, Loss: 0.3539155423641205, Learning Rate: 0.020999999999999998\n",
      "Epoch [1735/20000], Bound: 0.911695659160614, Entropy: 105.99380493164062, Temp: 0.9464370608329773, KL: 92.21881103515625, Loss: 0.34161296486854553, Learning Rate: 0.020999999999999998\n",
      "Epoch [1736/20000], Bound: 0.8436993360519409, Entropy: 106.91847229003906, Temp: 0.9460569620132446, KL: 91.84925842285156, Loss: 0.24837394058704376, Learning Rate: 0.020999999999999998\n",
      "Epoch [1737/20000], Bound: 0.851118803024292, Entropy: 99.92243957519531, Temp: 0.9460353851318359, KL: 95.33692932128906, Loss: 0.23989231884479523, Learning Rate: 0.020999999999999998\n",
      "Epoch [1738/20000], Bound: 0.827443540096283, Entropy: 102.26577758789062, Temp: 0.9464133977890015, KL: 89.32577514648438, Loss: 0.24036288261413574, Learning Rate: 0.020999999999999998\n",
      "Epoch [1739/20000], Bound: 0.833359956741333, Entropy: 98.47813415527344, Temp: 0.9470738172531128, KL: 89.46714782714844, Loss: 0.24767963588237762, Learning Rate: 0.020999999999999998\n",
      "Epoch [1740/20000], Bound: 0.8073800802230835, Entropy: 102.51429748535156, Temp: 0.9479595422744751, KL: 86.27668762207031, Loss: 0.23115362226963043, Learning Rate: 0.020999999999999998\n",
      "Epoch [1741/20000], Bound: 0.8193883299827576, Entropy: 105.73685455322266, Temp: 0.9490722417831421, KL: 84.86766815185547, Loss: 0.2544953525066376, Learning Rate: 0.020999999999999998\n",
      "Epoch [1742/20000], Bound: 0.7899063229560852, Entropy: 109.34135437011719, Temp: 0.9502748250961304, KL: 86.506103515625, Loss: 0.20878112316131592, Learning Rate: 0.020999999999999998\n",
      "Epoch [1743/20000], Bound: 0.7662398219108582, Entropy: 113.39509582519531, Temp: 0.9517613649368286, KL: 84.78446960449219, Loss: 0.18919436633586884, Learning Rate: 0.020999999999999998\n",
      "Epoch [1744/20000], Bound: 0.7780174612998962, Entropy: 122.41310119628906, Temp: 0.9535603523254395, KL: 80.06599426269531, Loss: 0.22908064723014832, Learning Rate: 0.020999999999999998\n",
      "Epoch [1745/20000], Bound: 0.8109228610992432, Entropy: 127.55731964111328, Temp: 0.955410897731781, KL: 86.6366958618164, Loss: 0.236831933259964, Learning Rate: 0.020999999999999998\n",
      "Epoch [1746/20000], Bound: 0.9586183428764343, Entropy: 127.62376403808594, Temp: 0.9573547840118408, KL: 109.96260070800781, Loss: 0.32543256878852844, Learning Rate: 0.020999999999999998\n",
      "Epoch [1747/20000], Bound: 0.9077471494674683, Entropy: 127.96505737304688, Temp: 0.9593065977096558, KL: 94.52812194824219, Loss: 0.32993072271347046, Learning Rate: 0.020999999999999998\n",
      "Epoch [1748/20000], Bound: 0.8952287435531616, Entropy: 127.64506530761719, Temp: 0.9610611200332642, KL: 97.91056823730469, Loss: 0.2950291633605957, Learning Rate: 0.020999999999999998\n",
      "Epoch [1749/20000], Bound: 0.842435896396637, Entropy: 124.86683654785156, Temp: 0.962807834148407, KL: 82.68875122070312, Loss: 0.3016766607761383, Learning Rate: 0.020999999999999998\n",
      "Epoch [1750/20000], Bound: 0.8226273059844971, Entropy: 118.9725570678711, Temp: 0.9643273949623108, KL: 81.34790802001953, Loss: 0.28299540281295776, Learning Rate: 0.020999999999999998\n",
      "Epoch [1751/20000], Bound: 0.8123593330383301, Entropy: 115.39552307128906, Temp: 0.9656944274902344, KL: 85.45864868164062, Loss: 0.24887852370738983, Learning Rate: 0.020999999999999998\n",
      "Epoch [1752/20000], Bound: 0.8001217246055603, Entropy: 110.27265167236328, Temp: 0.9671109914779663, KL: 77.4689712524414, Loss: 0.2750546634197235, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1753/20000], Bound: 0.8312994241714478, Entropy: 102.57713317871094, Temp: 0.9683586359024048, KL: 83.29472351074219, Loss: 0.285828173160553, Learning Rate: 0.020999999999999998\n",
      "Epoch [1754/20000], Bound: 0.7880897521972656, Entropy: 100.36929321289062, Temp: 0.969482421875, KL: 83.84764099121094, Loss: 0.2277677059173584, Learning Rate: 0.020999999999999998\n",
      "Epoch [1755/20000], Bound: 0.844022810459137, Entropy: 102.87908172607422, Temp: 0.9707373380661011, KL: 89.86161041259766, Loss: 0.26987504959106445, Learning Rate: 0.020999999999999998\n",
      "Epoch [1756/20000], Bound: 0.7625512480735779, Entropy: 109.92156219482422, Temp: 0.9720078706741333, KL: 80.11609649658203, Loss: 0.2163553237915039, Learning Rate: 0.020999999999999998\n",
      "Epoch [1757/20000], Bound: 0.7310165762901306, Entropy: 113.15058135986328, Temp: 0.973389744758606, KL: 78.02095794677734, Loss: 0.18993207812309265, Learning Rate: 0.020999999999999998\n",
      "Epoch [1758/20000], Bound: 0.7492465972900391, Entropy: 115.20044708251953, Temp: 0.9749560356140137, KL: 76.9861068725586, Loss: 0.217351496219635, Learning Rate: 0.020999999999999998\n",
      "Epoch [1759/20000], Bound: 0.7619903087615967, Entropy: 120.20072937011719, Temp: 0.9765528440475464, KL: 76.37425231933594, Loss: 0.23637479543685913, Learning Rate: 0.020999999999999998\n",
      "Epoch [1760/20000], Bound: 0.714182436466217, Entropy: 126.58794403076172, Temp: 0.9780840277671814, KL: 77.1726303100586, Loss: 0.17622783780097961, Learning Rate: 0.020999999999999998\n",
      "Epoch [1761/20000], Bound: 0.7909260988235474, Entropy: 129.33087158203125, Temp: 0.9798240661621094, KL: 90.28330993652344, Loss: 0.20225273072719574, Learning Rate: 0.020999999999999998\n",
      "Epoch [1762/20000], Bound: 0.8562024831771851, Entropy: 128.4152374267578, Temp: 0.9817957878112793, KL: 96.67330932617188, Loss: 0.25626587867736816, Learning Rate: 0.020999999999999998\n",
      "Epoch [1763/20000], Bound: 0.8445667624473572, Entropy: 126.7112045288086, Temp: 0.9838190078735352, KL: 89.27193450927734, Loss: 0.27893561124801636, Learning Rate: 0.020999999999999998\n",
      "Epoch [1764/20000], Bound: 0.7844501733779907, Entropy: 127.26763153076172, Temp: 0.9857007265090942, KL: 87.5643539428711, Loss: 0.21019862592220306, Learning Rate: 0.020999999999999998\n",
      "Epoch [1765/20000], Bound: 0.8487956523895264, Entropy: 120.04113006591797, Temp: 0.9877187013626099, KL: 94.79141998291016, Loss: 0.25827139616012573, Learning Rate: 0.020999999999999998\n",
      "Epoch [1766/20000], Bound: 0.8196650147438049, Entropy: 112.1810302734375, Temp: 0.9897364377975464, KL: 89.72564697265625, Loss: 0.245844766497612, Learning Rate: 0.020999999999999998\n",
      "Epoch [1767/20000], Bound: 0.8267158269882202, Entropy: 107.58338928222656, Temp: 0.9917402267456055, KL: 95.77395629882812, Loss: 0.22539936006069183, Learning Rate: 0.020999999999999998\n",
      "Epoch [1768/20000], Bound: 0.7662327289581299, Entropy: 106.41888427734375, Temp: 0.9938846230506897, KL: 87.63824462890625, Loss: 0.19026431441307068, Learning Rate: 0.020999999999999998\n",
      "Epoch [1769/20000], Bound: 0.7971146702766418, Entropy: 101.34019470214844, Temp: 0.996207594871521, KL: 89.80262756347656, Loss: 0.21880576014518738, Learning Rate: 0.020999999999999998\n",
      "Epoch [1770/20000], Bound: 0.7490488290786743, Entropy: 99.36618041992188, Temp: 0.9985862970352173, KL: 79.16021728515625, Loss: 0.21355397999286652, Learning Rate: 0.020999999999999998\n",
      "Epoch [1771/20000], Bound: 0.8312913179397583, Entropy: 99.56478118896484, Temp: 1.0009026527404785, KL: 88.10997772216797, Loss: 0.27353838086128235, Learning Rate: 0.020999999999999998\n",
      "Epoch [1772/20000], Bound: 0.863713264465332, Entropy: 96.58039093017578, Temp: 1.0030081272125244, KL: 98.76567840576172, Loss: 0.265381395816803, Learning Rate: 0.020999999999999998\n",
      "Epoch [1773/20000], Bound: 0.8096453547477722, Entropy: 100.7488784790039, Temp: 1.005077838897705, KL: 89.27696990966797, Loss: 0.2408137023448944, Learning Rate: 0.020999999999999998\n",
      "Epoch [1774/20000], Bound: 0.7817714810371399, Entropy: 99.71552276611328, Temp: 1.0071035623550415, KL: 84.66236114501953, Loss: 0.22896651923656464, Learning Rate: 0.020999999999999998\n",
      "Epoch [1775/20000], Bound: 0.8074436187744141, Entropy: 107.54640197753906, Temp: 1.0090800523757935, KL: 93.06509399414062, Loss: 0.22065001726150513, Learning Rate: 0.020999999999999998\n",
      "Epoch [1776/20000], Bound: 0.7534996867179871, Entropy: 106.37688446044922, Temp: 1.0111439228057861, KL: 79.02471160888672, Loss: 0.22342097759246826, Learning Rate: 0.020999999999999998\n",
      "Epoch [1777/20000], Bound: 0.7998093962669373, Entropy: 109.09236145019531, Temp: 1.0130996704101562, KL: 87.32908630371094, Loss: 0.24064888060092926, Learning Rate: 0.020999999999999998\n",
      "Epoch [1778/20000], Bound: 0.7209451794624329, Entropy: 113.19656372070312, Temp: 1.014978289604187, KL: 79.60038757324219, Loss: 0.18313491344451904, Learning Rate: 0.020999999999999998\n",
      "Epoch [1779/20000], Bound: 0.7507692575454712, Entropy: 119.60142517089844, Temp: 1.0169484615325928, KL: 81.13583374023438, Loss: 0.21147087216377258, Learning Rate: 0.020999999999999998\n",
      "Epoch [1780/20000], Bound: 0.8475028872489929, Entropy: 122.33460998535156, Temp: 1.0188853740692139, KL: 82.4970703125, Loss: 0.3295202851295471, Learning Rate: 0.020999999999999998\n",
      "Epoch [1781/20000], Bound: 0.8603528141975403, Entropy: 123.8826904296875, Temp: 1.0202906131744385, KL: 85.666015625, Loss: 0.3322005271911621, Learning Rate: 0.020999999999999998\n",
      "Epoch [1782/20000], Bound: 0.8660094738006592, Entropy: 126.61772918701172, Temp: 1.021240472793579, KL: 89.49808502197266, Loss: 0.32168740034103394, Learning Rate: 0.020999999999999998\n",
      "Epoch [1783/20000], Bound: 0.8280855417251587, Entropy: 122.96583557128906, Temp: 1.0218678712844849, KL: 87.92626953125, Loss: 0.27771177887916565, Learning Rate: 0.020999999999999998\n",
      "Epoch [1784/20000], Bound: 0.8634921908378601, Entropy: 118.29600524902344, Temp: 1.0223705768585205, KL: 83.38552856445312, Loss: 0.34846702218055725, Learning Rate: 0.020999999999999998\n",
      "Epoch [1785/20000], Bound: 0.8731817603111267, Entropy: 117.76034545898438, Temp: 1.022405743598938, KL: 85.73794555664062, Loss: 0.35064059495925903, Learning Rate: 0.020999999999999998\n",
      "Epoch [1786/20000], Bound: 0.7707034945487976, Entropy: 113.71470642089844, Temp: 1.0220403671264648, KL: 82.05329895019531, Loss: 0.2328619658946991, Learning Rate: 0.020999999999999998\n",
      "Epoch [1787/20000], Bound: 0.7916046977043152, Entropy: 113.8333969116211, Temp: 1.0217745304107666, KL: 86.35790252685547, Loss: 0.23790115118026733, Learning Rate: 0.020999999999999998\n",
      "Epoch [1788/20000], Bound: 0.7230919003486633, Entropy: 107.47420501708984, Temp: 1.0216283798217773, KL: 79.75965118408203, Loss: 0.18680553138256073, Learning Rate: 0.020999999999999998\n",
      "Epoch [1789/20000], Bound: 0.7915581464767456, Entropy: 110.7242660522461, Temp: 1.0217440128326416, KL: 81.05876922607422, Loss: 0.2637637257575989, Learning Rate: 0.020999999999999998\n",
      "Epoch [1790/20000], Bound: 0.7795915007591248, Entropy: 112.01044464111328, Temp: 1.0217623710632324, KL: 77.8964614868164, Loss: 0.2641673684120178, Learning Rate: 0.020999999999999998\n",
      "Epoch [1791/20000], Bound: 0.8797565698623657, Entropy: 113.18960571289062, Temp: 1.0216516256332397, KL: 93.14083862304688, Loss: 0.3235243260860443, Learning Rate: 0.020999999999999998\n",
      "Epoch [1792/20000], Bound: 0.8138437271118164, Entropy: 115.18463897705078, Temp: 1.021357774734497, KL: 80.21102142333984, Loss: 0.2965050935745239, Learning Rate: 0.020999999999999998\n",
      "Epoch [1793/20000], Bound: 0.7784398794174194, Entropy: 119.32827758789062, Temp: 1.0208561420440674, KL: 87.19003295898438, Loss: 0.2169475555419922, Learning Rate: 0.020999999999999998\n",
      "Epoch [1794/20000], Bound: 0.7665178179740906, Entropy: 123.23472595214844, Temp: 1.0206050872802734, KL: 75.665771484375, Loss: 0.2585521936416626, Learning Rate: 0.020999999999999998\n",
      "Epoch [1795/20000], Bound: 0.8310804963111877, Entropy: 124.57485961914062, Temp: 1.0202510356903076, KL: 88.72402954101562, Loss: 0.2772354185581207, Learning Rate: 0.020999999999999998\n",
      "Epoch [1796/20000], Bound: 0.7373448610305786, Entropy: 125.4531478881836, Temp: 1.019885778427124, KL: 86.40872955322266, Loss: 0.17045967280864716, Learning Rate: 0.020999999999999998\n",
      "Epoch [1797/20000], Bound: 0.819180428981781, Entropy: 126.70913696289062, Temp: 1.01996648311615, KL: 92.51828002929688, Loss: 0.2427433282136917, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1798/20000], Bound: 0.7788999080657959, Entropy: 121.24595642089844, Temp: 1.020189642906189, KL: 94.00579833984375, Loss: 0.18389418721199036, Learning Rate: 0.020999999999999998\n",
      "Epoch [1799/20000], Bound: 0.7628800868988037, Entropy: 116.60556030273438, Temp: 1.0208250284194946, KL: 87.91825866699219, Loss: 0.1941380500793457, Learning Rate: 0.020999999999999998\n",
      "Epoch [1800/20000], Bound: 0.816504716873169, Entropy: 110.87655639648438, Temp: 1.0217111110687256, KL: 100.59944152832031, Loss: 0.20032325387001038, Learning Rate: 0.020999999999999998\n",
      "Epoch [1801/20000], Bound: 0.786653995513916, Entropy: 104.40092468261719, Temp: 1.0229380130767822, KL: 94.87687683105469, Loss: 0.1903805434703827, Learning Rate: 0.020999999999999998\n",
      "Epoch [1802/20000], Bound: 0.8146687746047974, Entropy: 104.53925323486328, Temp: 1.0244498252868652, KL: 93.30480194091797, Loss: 0.23462817072868347, Learning Rate: 0.020999999999999998\n",
      "Epoch [1803/20000], Bound: 0.8145050406455994, Entropy: 95.83998107910156, Temp: 1.0259960889816284, KL: 97.76408386230469, Loss: 0.21325093507766724, Learning Rate: 0.020999999999999998\n",
      "Epoch [1804/20000], Bound: 0.8174430131912231, Entropy: 97.37162017822266, Temp: 1.0277163982391357, KL: 92.69905853271484, Loss: 0.24241600930690765, Learning Rate: 0.020999999999999998\n",
      "Epoch [1805/20000], Bound: 0.7516111731529236, Entropy: 93.30669403076172, Temp: 1.02940034866333, KL: 83.77953338623047, Loss: 0.20337684452533722, Learning Rate: 0.020999999999999998\n",
      "Epoch [1806/20000], Bound: 0.7780371904373169, Entropy: 94.1277847290039, Temp: 1.0311187505722046, KL: 89.33458709716797, Loss: 0.2094336301088333, Learning Rate: 0.020999999999999998\n",
      "Epoch [1807/20000], Bound: 0.7708021402359009, Entropy: 97.6318359375, Temp: 1.0329023599624634, KL: 90.01669311523438, Loss: 0.19772924482822418, Learning Rate: 0.020999999999999998\n",
      "Epoch [1808/20000], Bound: 0.8653440475463867, Entropy: 103.81745910644531, Temp: 1.0348024368286133, KL: 95.13667297363281, Loss: 0.2984374463558197, Learning Rate: 0.020999999999999998\n",
      "Epoch [1809/20000], Bound: 0.7031840085983276, Entropy: 109.86949920654297, Temp: 1.0364105701446533, KL: 77.8536148071289, Loss: 0.17725995182991028, Learning Rate: 0.020999999999999998\n",
      "Epoch [1810/20000], Bound: 0.6916515827178955, Entropy: 120.3689193725586, Temp: 1.038096308708191, KL: 78.9142837524414, Loss: 0.15954813361167908, Learning Rate: 0.020999999999999998\n",
      "Epoch [1811/20000], Bound: 0.762444019317627, Entropy: 124.99876403808594, Temp: 1.0399470329284668, KL: 87.99974060058594, Loss: 0.19946949183940887, Learning Rate: 0.020999999999999998\n",
      "Epoch [1812/20000], Bound: 0.7316334247589111, Entropy: 126.64453125, Temp: 1.0418591499328613, KL: 80.95985412597656, Loss: 0.1968039721250534, Learning Rate: 0.020999999999999998\n",
      "Epoch [1813/20000], Bound: 0.7382228374481201, Entropy: 128.89669799804688, Temp: 1.0437510013580322, KL: 78.94493103027344, Loss: 0.21479551494121552, Learning Rate: 0.020999999999999998\n",
      "Epoch [1814/20000], Bound: 0.7789883613586426, Entropy: 128.5866241455078, Temp: 1.0455108880996704, KL: 81.98356628417969, Loss: 0.2505204379558563, Learning Rate: 0.020999999999999998\n",
      "Epoch [1815/20000], Bound: 0.6998533606529236, Entropy: 123.50080108642578, Temp: 1.0470184087753296, KL: 71.47782135009766, Loss: 0.20673415064811707, Learning Rate: 0.020999999999999998\n",
      "Epoch [1816/20000], Bound: 0.7484007477760315, Entropy: 119.61009216308594, Temp: 1.0483686923980713, KL: 93.00614929199219, Loss: 0.16117019951343536, Learning Rate: 0.020999999999999998\n",
      "Epoch [1817/20000], Bound: 0.7289360761642456, Entropy: 115.48941040039062, Temp: 1.0500508546829224, KL: 86.03102111816406, Loss: 0.1717589795589447, Learning Rate: 0.020999999999999998\n",
      "Epoch [1818/20000], Bound: 0.7232187390327454, Entropy: 113.18970489501953, Temp: 1.0518975257873535, KL: 86.43607330322266, Loss: 0.16367793083190918, Learning Rate: 0.020999999999999998\n",
      "Epoch [1819/20000], Bound: 0.7446935772895813, Entropy: 107.57130432128906, Temp: 1.0539326667785645, KL: 90.67994689941406, Loss: 0.16958533227443695, Learning Rate: 0.020999999999999998\n",
      "Epoch [1820/20000], Bound: 0.7030128240585327, Entropy: 110.69456481933594, Temp: 1.0561522245407104, KL: 77.79737854003906, Loss: 0.18250726163387299, Learning Rate: 0.020999999999999998\n",
      "Epoch [1821/20000], Bound: 0.732480525970459, Entropy: 112.59547424316406, Temp: 1.058320164680481, KL: 79.73016357421875, Loss: 0.20814740657806396, Learning Rate: 0.020999999999999998\n",
      "Epoch [1822/20000], Bound: 0.7461426258087158, Entropy: 111.5504150390625, Temp: 1.060335397720337, KL: 82.789306640625, Loss: 0.21055102348327637, Learning Rate: 0.020999999999999998\n",
      "Epoch [1823/20000], Bound: 0.7315413355827332, Entropy: 109.20530700683594, Temp: 1.0622334480285645, KL: 83.86985778808594, Loss: 0.18857771158218384, Learning Rate: 0.020999999999999998\n",
      "Epoch [1824/20000], Bound: 0.8115993738174438, Entropy: 110.24747467041016, Temp: 1.0641402006149292, KL: 96.3298568725586, Loss: 0.23027583956718445, Learning Rate: 0.020999999999999998\n",
      "Epoch [1825/20000], Bound: 0.7201332449913025, Entropy: 108.61497497558594, Temp: 1.065993309020996, KL: 93.97169494628906, Loss: 0.12885263562202454, Learning Rate: 0.020999999999999998\n",
      "Epoch [1826/20000], Bound: 0.762024998664856, Entropy: 106.46340942382812, Temp: 1.0682625770568848, KL: 82.20150756835938, Loss: 0.2348003387451172, Learning Rate: 0.020999999999999998\n",
      "Epoch [1827/20000], Bound: 0.7716714143753052, Entropy: 106.96436309814453, Temp: 1.0702474117279053, KL: 98.40836334228516, Loss: 0.17155811190605164, Learning Rate: 0.020999999999999998\n",
      "Epoch [1828/20000], Bound: 0.7025560140609741, Entropy: 110.27376556396484, Temp: 1.072460412979126, KL: 76.9353256225586, Loss: 0.1900654137134552, Learning Rate: 0.020999999999999998\n",
      "Epoch [1829/20000], Bound: 0.6848393082618713, Entropy: 109.67020416259766, Temp: 1.0745384693145752, KL: 83.9200210571289, Loss: 0.1380699723958969, Learning Rate: 0.020999999999999998\n",
      "Epoch [1830/20000], Bound: 0.7561191320419312, Entropy: 113.58374786376953, Temp: 1.076835036277771, KL: 88.1205062866211, Loss: 0.20237061381340027, Learning Rate: 0.020999999999999998\n",
      "Epoch [1831/20000], Bound: 0.7731189131736755, Entropy: 118.32575225830078, Temp: 1.0790492296218872, KL: 89.91413116455078, Loss: 0.2156948447227478, Learning Rate: 0.020999999999999998\n",
      "Epoch [1832/20000], Bound: 0.7761777639389038, Entropy: 117.51266479492188, Temp: 1.081139326095581, KL: 85.44212341308594, Loss: 0.24082687497138977, Learning Rate: 0.020999999999999998\n",
      "Epoch [1833/20000], Bound: 0.8321579098701477, Entropy: 113.98615264892578, Temp: 1.082939624786377, KL: 99.9744644165039, Loss: 0.24730178713798523, Learning Rate: 0.020999999999999998\n",
      "Epoch [1834/20000], Bound: 0.7715762257575989, Entropy: 108.25670623779297, Temp: 1.0846070051193237, KL: 94.04883575439453, Loss: 0.19634860754013062, Learning Rate: 0.020999999999999998\n",
      "Epoch [1835/20000], Bound: 0.7975809574127197, Entropy: 102.27826690673828, Temp: 1.0863313674926758, KL: 97.8076400756836, Loss: 0.2126156985759735, Learning Rate: 0.020999999999999998\n",
      "Epoch [1836/20000], Bound: 0.744808554649353, Entropy: 102.8302993774414, Temp: 1.088064193725586, KL: 91.16602325439453, Loss: 0.17785106599330902, Learning Rate: 0.020999999999999998\n",
      "Epoch [1837/20000], Bound: 0.7773357033729553, Entropy: 104.97557067871094, Temp: 1.0899001359939575, KL: 89.07015991210938, Loss: 0.22803878784179688, Learning Rate: 0.020999999999999998\n",
      "Epoch [1838/20000], Bound: 0.7654884457588196, Entropy: 103.1246337890625, Temp: 1.0915528535842896, KL: 101.67730712890625, Loss: 0.1559733897447586, Learning Rate: 0.020999999999999998\n",
      "Epoch [1839/20000], Bound: 0.7158534526824951, Entropy: 105.376220703125, Temp: 1.093531608581543, KL: 84.25314331054688, Loss: 0.17680911719799042, Learning Rate: 0.020999999999999998\n",
      "Epoch [1840/20000], Bound: 0.7361487150192261, Entropy: 105.88163757324219, Temp: 1.0955051183700562, KL: 91.40504455566406, Loss: 0.1685372292995453, Learning Rate: 0.020999999999999998\n",
      "Epoch [1841/20000], Bound: 0.7350699305534363, Entropy: 109.54067993164062, Temp: 1.0975914001464844, KL: 79.97962951660156, Loss: 0.21989700198173523, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1842/20000], Bound: 0.7703167200088501, Entropy: 111.22140502929688, Temp: 1.0993856191635132, KL: 87.36393737792969, Loss: 0.2296810895204544, Learning Rate: 0.020999999999999998\n",
      "Epoch [1843/20000], Bound: 0.7681022882461548, Entropy: 111.0511474609375, Temp: 1.1009491682052612, KL: 84.5323486328125, Loss: 0.24020476639270782, Learning Rate: 0.020999999999999998\n",
      "Epoch [1844/20000], Bound: 0.7320520877838135, Entropy: 116.8495101928711, Temp: 1.1022166013717651, KL: 79.28673553466797, Loss: 0.22054751217365265, Learning Rate: 0.020999999999999998\n",
      "Epoch [1845/20000], Bound: 0.7970075011253357, Entropy: 114.12892150878906, Temp: 1.1032495498657227, KL: 93.8780517578125, Loss: 0.23514148592948914, Learning Rate: 0.020999999999999998\n",
      "Epoch [1846/20000], Bound: 0.7833606600761414, Entropy: 113.69671630859375, Temp: 1.104163408279419, KL: 88.48014831542969, Loss: 0.24234351515769958, Learning Rate: 0.020999999999999998\n",
      "Epoch [1847/20000], Bound: 0.765734851360321, Entropy: 118.38568115234375, Temp: 1.1048717498779297, KL: 88.37568664550781, Loss: 0.22087687253952026, Learning Rate: 0.020999999999999998\n",
      "Epoch [1848/20000], Bound: 0.7660714387893677, Entropy: 114.72879028320312, Temp: 1.1054980754852295, KL: 83.41902160644531, Loss: 0.24388206005096436, Learning Rate: 0.020999999999999998\n",
      "Epoch [1849/20000], Bound: 0.7193953394889832, Entropy: 111.92464447021484, Temp: 1.1058776378631592, KL: 70.72241973876953, Loss: 0.24522654712200165, Learning Rate: 0.020999999999999998\n",
      "Epoch [1850/20000], Bound: 0.7694616317749023, Entropy: 113.72894287109375, Temp: 1.10587477684021, KL: 82.39117431640625, Loss: 0.25284120440483093, Learning Rate: 0.020999999999999998\n",
      "Epoch [1851/20000], Bound: 0.7308729290962219, Entropy: 113.90235900878906, Temp: 1.1056309938430786, KL: 92.92646789550781, Loss: 0.15825387835502625, Learning Rate: 0.020999999999999998\n",
      "Epoch [1852/20000], Bound: 0.7762643098831177, Entropy: 116.03739166259766, Temp: 1.1057690382003784, KL: 87.45319366455078, Loss: 0.23844917118549347, Learning Rate: 0.020999999999999998\n",
      "Epoch [1853/20000], Bound: 0.7519031763076782, Entropy: 119.20376586914062, Temp: 1.1057816743850708, KL: 78.93666076660156, Loss: 0.24680373072624207, Learning Rate: 0.020999999999999998\n",
      "Epoch [1854/20000], Bound: 0.7451997995376587, Entropy: 114.81304168701172, Temp: 1.1055407524108887, KL: 85.1975326538086, Loss: 0.21030762791633606, Learning Rate: 0.020999999999999998\n",
      "Epoch [1855/20000], Bound: 0.7516385316848755, Entropy: 119.80723571777344, Temp: 1.1053271293640137, KL: 90.07449340820312, Loss: 0.19599461555480957, Learning Rate: 0.020999999999999998\n",
      "Epoch [1856/20000], Bound: 0.752035915851593, Entropy: 113.44884490966797, Temp: 1.1052665710449219, KL: 88.3799819946289, Loss: 0.2041272073984146, Learning Rate: 0.020999999999999998\n",
      "Epoch [1857/20000], Bound: 0.7295496463775635, Entropy: 111.00216674804688, Temp: 1.1052837371826172, KL: 73.20974731445312, Loss: 0.24578239023685455, Learning Rate: 0.020999999999999998\n",
      "Epoch [1858/20000], Bound: 0.692348837852478, Entropy: 116.78871154785156, Temp: 1.1049830913543701, KL: 76.15484619140625, Loss: 0.18946123123168945, Learning Rate: 0.020999999999999998\n",
      "Epoch [1859/20000], Bound: 0.7139893174171448, Entropy: 118.89237976074219, Temp: 1.1047180891036987, KL: 76.30206298828125, Loss: 0.2134644091129303, Learning Rate: 0.020999999999999998\n",
      "Epoch [1860/20000], Bound: 0.7527887225151062, Entropy: 116.46513366699219, Temp: 1.104364275932312, KL: 71.09327697753906, Loss: 0.28306707739830017, Learning Rate: 0.020999999999999998\n",
      "Epoch [1861/20000], Bound: 0.7882789969444275, Entropy: 118.38347625732422, Temp: 1.1035244464874268, KL: 85.25066375732422, Loss: 0.2630736231803894, Learning Rate: 0.020999999999999998\n",
      "Epoch [1862/20000], Bound: 0.8175042867660522, Entropy: 118.74034881591797, Temp: 1.102516770362854, KL: 95.51520538330078, Loss: 0.2544492185115814, Learning Rate: 0.020999999999999998\n",
      "Epoch [1863/20000], Bound: 0.8555201292037964, Entropy: 124.08898162841797, Temp: 1.1015188694000244, KL: 97.26836395263672, Loss: 0.2983816862106323, Learning Rate: 0.020999999999999998\n",
      "Epoch [1864/20000], Bound: 0.7665807008743286, Entropy: 113.33761596679688, Temp: 1.1003432273864746, KL: 80.10353088378906, Loss: 0.25828227400779724, Learning Rate: 0.020999999999999998\n",
      "Epoch [1865/20000], Bound: 0.6791044473648071, Entropy: 107.6288070678711, Temp: 1.0990040302276611, KL: 78.21770477294922, Loss: 0.16397380828857422, Learning Rate: 0.020999999999999998\n",
      "Epoch [1866/20000], Bound: 0.7295883297920227, Entropy: 103.71519470214844, Temp: 1.097975492477417, KL: 80.84152221679688, Loss: 0.20956389605998993, Learning Rate: 0.020999999999999998\n",
      "Epoch [1867/20000], Bound: 0.7270744442939758, Entropy: 104.6239013671875, Temp: 1.0970239639282227, KL: 88.46151733398438, Loss: 0.17163945734500885, Learning Rate: 0.020999999999999998\n",
      "Epoch [1868/20000], Bound: 0.7326443195343018, Entropy: 99.43168640136719, Temp: 1.0964264869689941, KL: 88.4212646484375, Loss: 0.17824220657348633, Learning Rate: 0.020999999999999998\n",
      "Epoch [1869/20000], Bound: 0.7679125070571899, Entropy: 101.5072250366211, Temp: 1.0961140394210815, KL: 88.21175384521484, Loss: 0.22192981839179993, Learning Rate: 0.020999999999999998\n",
      "Epoch [1870/20000], Bound: 0.7170676589012146, Entropy: 106.72217559814453, Temp: 1.0958352088928223, KL: 81.9067611694336, Loss: 0.1895151138305664, Learning Rate: 0.020999999999999998\n",
      "Epoch [1871/20000], Bound: 0.7503299713134766, Entropy: 108.7862777709961, Temp: 1.0956780910491943, KL: 88.37821197509766, Loss: 0.19945335388183594, Learning Rate: 0.020999999999999998\n",
      "Epoch [1872/20000], Bound: 0.6705108880996704, Entropy: 113.83909606933594, Temp: 1.095655083656311, KL: 80.59333801269531, Loss: 0.14288918673992157, Learning Rate: 0.020999999999999998\n",
      "Epoch [1873/20000], Bound: 0.7546950578689575, Entropy: 117.30864715576172, Temp: 1.0959585905075073, KL: 82.66474151611328, Loss: 0.23091071844100952, Learning Rate: 0.020999999999999998\n",
      "Epoch [1874/20000], Bound: 0.7267076969146729, Entropy: 116.48014068603516, Temp: 1.0961254835128784, KL: 89.79729461669922, Loss: 0.16486923396587372, Learning Rate: 0.020999999999999998\n",
      "Epoch [1875/20000], Bound: 0.7226783037185669, Entropy: 121.3469467163086, Temp: 1.0965873003005981, KL: 78.10773468017578, Loss: 0.2135675698518753, Learning Rate: 0.020999999999999998\n",
      "Epoch [1876/20000], Bound: 0.7705091238021851, Entropy: 120.0607681274414, Temp: 1.0969277620315552, KL: 96.6120834350586, Loss: 0.18709799647331238, Learning Rate: 0.020999999999999998\n",
      "Epoch [1877/20000], Bound: 0.7639991641044617, Entropy: 119.49649810791016, Temp: 1.0975046157836914, KL: 88.52277374267578, Loss: 0.21604453027248383, Learning Rate: 0.020999999999999998\n",
      "Epoch [1878/20000], Bound: 0.7021586894989014, Entropy: 117.84882354736328, Temp: 1.098056435585022, KL: 76.80704498291016, Loss: 0.1961108297109604, Learning Rate: 0.020999999999999998\n",
      "Epoch [1879/20000], Bound: 0.6835067868232727, Entropy: 119.18029022216797, Temp: 1.0985479354858398, KL: 75.69486236572266, Loss: 0.18024739623069763, Learning Rate: 0.020999999999999998\n",
      "Epoch [1880/20000], Bound: 0.7485316395759583, Entropy: 115.54092407226562, Temp: 1.099053144454956, KL: 86.40231323242188, Loss: 0.20718662440776825, Learning Rate: 0.020999999999999998\n",
      "Epoch [1881/20000], Bound: 0.7775750756263733, Entropy: 111.94970703125, Temp: 1.0995569229125977, KL: 102.25337219238281, Loss: 0.171121746301651, Learning Rate: 0.020999999999999998\n",
      "Epoch [1882/20000], Bound: 0.7302053570747375, Entropy: 111.85276794433594, Temp: 1.1004178524017334, KL: 90.14915466308594, Loss: 0.16858737170696259, Learning Rate: 0.020999999999999998\n",
      "Epoch [1883/20000], Bound: 0.7153432369232178, Entropy: 108.1785888671875, Temp: 1.1014808416366577, KL: 81.74171447753906, Loss: 0.18963728845119476, Learning Rate: 0.020999999999999998\n",
      "Epoch [1884/20000], Bound: 0.7510069012641907, Entropy: 106.99683380126953, Temp: 1.1025176048278809, KL: 86.3982162475586, Loss: 0.21112161874771118, Learning Rate: 0.020999999999999998\n",
      "Epoch [1885/20000], Bound: 0.7383861541748047, Entropy: 110.36175537109375, Temp: 1.103472113609314, KL: 95.93167114257812, Loss: 0.15295806527137756, Learning Rate: 0.020999999999999998\n",
      "Epoch [1886/20000], Bound: 0.6793190836906433, Entropy: 110.19699096679688, Temp: 1.104758381843567, KL: 76.50950622558594, Loss: 0.17323918640613556, Learning Rate: 0.020999999999999998\n",
      "Epoch [1887/20000], Bound: 0.7564173936843872, Entropy: 115.50269317626953, Temp: 1.106013536453247, KL: 89.75431060791016, Loss: 0.20346617698669434, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1888/20000], Bound: 0.7419886589050293, Entropy: 115.78680419921875, Temp: 1.107234001159668, KL: 76.160400390625, Loss: 0.24768054485321045, Learning Rate: 0.020999999999999998\n",
      "Epoch [1889/20000], Bound: 0.7022860646247864, Entropy: 117.99620819091797, Temp: 1.108035922050476, KL: 73.88155364990234, Loss: 0.21164435148239136, Learning Rate: 0.020999999999999998\n",
      "Epoch [1890/20000], Bound: 0.7567723393440247, Entropy: 118.3859634399414, Temp: 1.1086139678955078, KL: 82.76883697509766, Loss: 0.23611754179000854, Learning Rate: 0.020999999999999998\n",
      "Epoch [1891/20000], Bound: 0.7220924496650696, Entropy: 118.45059204101562, Temp: 1.1089712381362915, KL: 73.29351806640625, Loss: 0.23738166689872742, Learning Rate: 0.020999999999999998\n",
      "Epoch [1892/20000], Bound: 0.7081738710403442, Entropy: 123.03530883789062, Temp: 1.1090075969696045, KL: 76.8533935546875, Loss: 0.20518949627876282, Learning Rate: 0.020999999999999998\n",
      "Epoch [1893/20000], Bound: 0.806087851524353, Entropy: 116.36410522460938, Temp: 1.1089632511138916, KL: 87.9371337890625, Loss: 0.2754809558391571, Learning Rate: 0.020999999999999998\n",
      "Epoch [1894/20000], Bound: 0.7270632386207581, Entropy: 115.22783660888672, Temp: 1.1086249351501465, KL: 88.2009048461914, Loss: 0.17591622471809387, Learning Rate: 0.020999999999999998\n",
      "Epoch [1895/20000], Bound: 0.7621902227401733, Entropy: 108.25691986083984, Temp: 1.108530044555664, KL: 89.71283721923828, Loss: 0.21144253015518188, Learning Rate: 0.020999999999999998\n",
      "Epoch [1896/20000], Bound: 0.6951877474784851, Entropy: 112.06130981445312, Temp: 1.1084868907928467, KL: 81.79878234863281, Loss: 0.1679587960243225, Learning Rate: 0.020999999999999998\n",
      "Epoch [1897/20000], Bound: 0.6558902263641357, Entropy: 109.1700439453125, Temp: 1.1086273193359375, KL: 74.11827087402344, Loss: 0.15923775732517242, Learning Rate: 0.020999999999999998\n",
      "Epoch [1898/20000], Bound: 0.670311450958252, Entropy: 112.63042449951172, Temp: 1.1088889837265015, KL: 80.38053131103516, Loss: 0.14673765003681183, Learning Rate: 0.020999999999999998\n",
      "Epoch [1899/20000], Bound: 0.7174137234687805, Entropy: 113.89628601074219, Temp: 1.1094014644622803, KL: 81.91213989257812, Loss: 0.19316387176513672, Learning Rate: 0.020999999999999998\n",
      "Epoch [1900/20000], Bound: 0.6783908605575562, Entropy: 112.59026336669922, Temp: 1.1099082231521606, KL: 88.91883087158203, Loss: 0.11741441488265991, Learning Rate: 0.020999999999999998\n",
      "Epoch [1901/20000], Bound: 0.6854354739189148, Entropy: 111.66941833496094, Temp: 1.110897421836853, KL: 85.3326416015625, Loss: 0.14165477454662323, Learning Rate: 0.020999999999999998\n",
      "Epoch [1902/20000], Bound: 0.6914106607437134, Entropy: 112.72013092041016, Temp: 1.1121454238891602, KL: 81.20915985107422, Loss: 0.16720549762248993, Learning Rate: 0.020999999999999998\n",
      "Epoch [1903/20000], Bound: 0.7028930187225342, Entropy: 113.27631378173828, Temp: 1.113438606262207, KL: 83.9261245727539, Loss: 0.1683259904384613, Learning Rate: 0.020999999999999998\n",
      "Epoch [1904/20000], Bound: 0.709247887134552, Entropy: 111.48182678222656, Temp: 1.1147950887680054, KL: 82.19329833984375, Loss: 0.18371474742889404, Learning Rate: 0.020999999999999998\n",
      "Epoch [1905/20000], Bound: 0.7377330660820007, Entropy: 111.49405670166016, Temp: 1.1161034107208252, KL: 85.96466827392578, Loss: 0.20055130124092102, Learning Rate: 0.020999999999999998\n",
      "Epoch [1906/20000], Bound: 0.7364166975021362, Entropy: 114.4648208618164, Temp: 1.1173204183578491, KL: 83.7641830444336, Loss: 0.20912963151931763, Learning Rate: 0.020999999999999998\n",
      "Epoch [1907/20000], Bound: 0.6675159931182861, Entropy: 109.7120361328125, Temp: 1.1183817386627197, KL: 81.07998657226562, Loss: 0.14270207285881042, Learning Rate: 0.020999999999999998\n",
      "Epoch [1908/20000], Bound: 0.6680099964141846, Entropy: 111.59286499023438, Temp: 1.1196266412734985, KL: 74.01611328125, Loss: 0.17506961524486542, Learning Rate: 0.020999999999999998\n",
      "Epoch [1909/20000], Bound: 0.7422780394554138, Entropy: 110.66716766357422, Temp: 1.1207736730575562, KL: 91.95291900634766, Loss: 0.18044911324977875, Learning Rate: 0.020999999999999998\n",
      "Epoch [1910/20000], Bound: 0.6444766521453857, Entropy: 113.21226501464844, Temp: 1.122008204460144, KL: 78.23152160644531, Loss: 0.1313781887292862, Learning Rate: 0.020999999999999998\n",
      "Epoch [1911/20000], Bound: 0.7005252838134766, Entropy: 113.5748291015625, Temp: 1.1234328746795654, KL: 72.288818359375, Loss: 0.21979933977127075, Learning Rate: 0.020999999999999998\n",
      "Epoch [1912/20000], Bound: 0.6470078825950623, Entropy: 116.25415802001953, Temp: 1.1244733333587646, KL: 71.3514175415039, Loss: 0.1651904284954071, Learning Rate: 0.020999999999999998\n",
      "Epoch [1913/20000], Bound: 0.6874750256538391, Entropy: 118.8108139038086, Temp: 1.1254481077194214, KL: 85.53997039794922, Loss: 0.14656281471252441, Learning Rate: 0.020999999999999998\n",
      "Epoch [1914/20000], Bound: 0.6976348757743835, Entropy: 116.87747955322266, Temp: 1.1266303062438965, KL: 88.8111801147461, Loss: 0.14378690719604492, Learning Rate: 0.020999999999999998\n",
      "Epoch [1915/20000], Bound: 0.6608133316040039, Entropy: 121.90318298339844, Temp: 1.1280486583709717, KL: 77.43295288085938, Loss: 0.1537335216999054, Learning Rate: 0.020999999999999998\n",
      "Epoch [1916/20000], Bound: 0.7413266897201538, Entropy: 123.59071350097656, Temp: 1.129492998123169, KL: 94.64027404785156, Loss: 0.16974496841430664, Learning Rate: 0.020999999999999998\n",
      "Epoch [1917/20000], Bound: 0.709418535232544, Entropy: 124.95049285888672, Temp: 1.1310627460479736, KL: 88.08875274658203, Loss: 0.1615711897611618, Learning Rate: 0.020999999999999998\n",
      "Epoch [1918/20000], Bound: 0.7458357214927673, Entropy: 124.15953063964844, Temp: 1.1327154636383057, KL: 91.30413818359375, Loss: 0.19079570472240448, Learning Rate: 0.020999999999999998\n",
      "Epoch [1919/20000], Bound: 0.6930134892463684, Entropy: 120.47863006591797, Temp: 1.1343162059783936, KL: 81.82093048095703, Loss: 0.17128677666187286, Learning Rate: 0.020999999999999998\n",
      "Epoch [1920/20000], Bound: 0.7095116376876831, Entropy: 121.91875457763672, Temp: 1.1358656883239746, KL: 96.14705657958984, Loss: 0.12738411128520966, Learning Rate: 0.020999999999999998\n",
      "Epoch [1921/20000], Bound: 0.6769001483917236, Entropy: 115.64999389648438, Temp: 1.1377668380737305, KL: 84.03074645996094, Loss: 0.1443299800157547, Learning Rate: 0.020999999999999998\n",
      "Epoch [1922/20000], Bound: 0.6765028834342957, Entropy: 109.97232055664062, Temp: 1.139755129814148, KL: 94.31248474121094, Loss: 0.09923437982797623, Learning Rate: 0.020999999999999998\n",
      "Epoch [1923/20000], Bound: 0.6582959294319153, Entropy: 108.08950805664062, Temp: 1.1421846151351929, KL: 87.17684936523438, Loss: 0.11121753603219986, Learning Rate: 0.020999999999999998\n",
      "Epoch [1924/20000], Bound: 0.6410113573074341, Entropy: 106.74244689941406, Temp: 1.1448633670806885, KL: 81.51872253417969, Loss: 0.11808308959007263, Learning Rate: 0.020999999999999998\n",
      "Epoch [1925/20000], Bound: 0.6729411482810974, Entropy: 108.85169982910156, Temp: 1.147660732269287, KL: 88.48167419433594, Loss: 0.12278104573488235, Learning Rate: 0.020999999999999998\n",
      "Epoch [1926/20000], Bound: 0.6233148574829102, Entropy: 112.75342559814453, Temp: 1.1506080627441406, KL: 78.1129379272461, Loss: 0.11558566987514496, Learning Rate: 0.020999999999999998\n",
      "Epoch [1927/20000], Bound: 0.7141088247299194, Entropy: 112.46466827392578, Temp: 1.1536145210266113, KL: 91.70638275146484, Loss: 0.15672044456005096, Learning Rate: 0.020999999999999998\n",
      "Epoch [1928/20000], Bound: 0.6941682696342468, Entropy: 111.55882263183594, Temp: 1.1565810441970825, KL: 83.30517578125, Loss: 0.17092326283454895, Learning Rate: 0.020999999999999998\n",
      "Epoch [1929/20000], Bound: 0.648854672908783, Entropy: 113.69200897216797, Temp: 1.159334421157837, KL: 69.7866439819336, Loss: 0.18003997206687927, Learning Rate: 0.020999999999999998\n",
      "Epoch [1930/20000], Bound: 0.6850073337554932, Entropy: 116.37187194824219, Temp: 1.1616816520690918, KL: 84.92390441894531, Loss: 0.15473587810993195, Learning Rate: 0.020999999999999998\n",
      "Epoch [1931/20000], Bound: 0.6739332675933838, Entropy: 121.4373779296875, Temp: 1.1639750003814697, KL: 75.13407897949219, Loss: 0.18499800562858582, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1932/20000], Bound: 0.6882305145263672, Entropy: 123.44599151611328, Temp: 1.1659328937530518, KL: 72.75189971923828, Loss: 0.21145354211330414, Learning Rate: 0.020999999999999998\n",
      "Epoch [1933/20000], Bound: 0.7155106663703918, Entropy: 127.94368743896484, Temp: 1.1674081087112427, KL: 77.04647064208984, Loss: 0.22447383403778076, Learning Rate: 0.020999999999999998\n",
      "Epoch [1934/20000], Bound: 0.6099677681922913, Entropy: 131.6536865234375, Temp: 1.168424367904663, KL: 73.89248657226562, Loss: 0.12340348213911057, Learning Rate: 0.020999999999999998\n",
      "Epoch [1935/20000], Bound: 0.7167918086051941, Entropy: 132.65940856933594, Temp: 1.169565200805664, KL: 82.05474853515625, Loss: 0.20495006442070007, Learning Rate: 0.020999999999999998\n",
      "Epoch [1936/20000], Bound: 0.69426429271698, Entropy: 131.90318298339844, Temp: 1.1704399585723877, KL: 82.19367980957031, Loss: 0.1786685734987259, Learning Rate: 0.020999999999999998\n",
      "Epoch [1937/20000], Bound: 0.6553583145141602, Entropy: 128.6098175048828, Temp: 1.1712212562561035, KL: 80.25349426269531, Loss: 0.14420545101165771, Learning Rate: 0.020999999999999998\n",
      "Epoch [1938/20000], Bound: 0.6553291082382202, Entropy: 126.64722442626953, Temp: 1.1720936298370361, KL: 80.41394805908203, Loss: 0.14365822076797485, Learning Rate: 0.020999999999999998\n",
      "Epoch [1939/20000], Bound: 0.615902304649353, Entropy: 122.68904113769531, Temp: 1.1730515956878662, KL: 74.87103271484375, Loss: 0.12609189748764038, Learning Rate: 0.020999999999999998\n",
      "Epoch [1940/20000], Bound: 0.685255765914917, Entropy: 118.7478256225586, Temp: 1.1741268634796143, KL: 95.14127349853516, Loss: 0.11413956433534622, Learning Rate: 0.020999999999999998\n",
      "Epoch [1941/20000], Bound: 0.6665064096450806, Entropy: 120.44500732421875, Temp: 1.175593614578247, KL: 86.20512390136719, Loss: 0.13180629909038544, Learning Rate: 0.020999999999999998\n",
      "Epoch [1942/20000], Bound: 0.7148204445838928, Entropy: 116.22972106933594, Temp: 1.1772130727767944, KL: 91.05824279785156, Loss: 0.16593533754348755, Learning Rate: 0.020999999999999998\n",
      "Epoch [1943/20000], Bound: 0.6538860201835632, Entropy: 117.5831298828125, Temp: 1.1788203716278076, KL: 83.69757080078125, Loss: 0.12947125732898712, Learning Rate: 0.020999999999999998\n",
      "Epoch [1944/20000], Bound: 0.6858375072479248, Entropy: 117.61737823486328, Temp: 1.1805475950241089, KL: 83.89144134521484, Loss: 0.16399922966957092, Learning Rate: 0.020999999999999998\n",
      "Epoch [1945/20000], Bound: 0.6152353286743164, Entropy: 112.9163818359375, Temp: 1.1821794509887695, KL: 73.63615417480469, Loss: 0.13221144676208496, Learning Rate: 0.020999999999999998\n",
      "Epoch [1946/20000], Bound: 0.6794257164001465, Entropy: 110.11419677734375, Temp: 1.18379545211792, KL: 85.89556884765625, Loss: 0.14904391765594482, Learning Rate: 0.020999999999999998\n",
      "Epoch [1947/20000], Bound: 0.6931158900260925, Entropy: 115.35403442382812, Temp: 1.185429334640503, KL: 90.83717346191406, Loss: 0.14385758340358734, Learning Rate: 0.020999999999999998\n",
      "Epoch [1948/20000], Bound: 0.7374262809753418, Entropy: 112.740234375, Temp: 1.1871589422225952, KL: 99.47505187988281, Loss: 0.15944893658161163, Learning Rate: 0.020999999999999998\n",
      "Epoch [1949/20000], Bound: 0.657433807849884, Entropy: 110.24015045166016, Temp: 1.1889687776565552, KL: 84.0053939819336, Loss: 0.13401350378990173, Learning Rate: 0.020999999999999998\n",
      "Epoch [1950/20000], Bound: 0.6594135165214539, Entropy: 118.9084243774414, Temp: 1.1908364295959473, KL: 78.83219146728516, Loss: 0.1582385003566742, Learning Rate: 0.020999999999999998\n",
      "Epoch [1951/20000], Bound: 0.7231196761131287, Entropy: 112.48967742919922, Temp: 1.192552089691162, KL: 87.49129486083984, Loss: 0.1940477192401886, Learning Rate: 0.020999999999999998\n",
      "Epoch [1952/20000], Bound: 0.6517776846885681, Entropy: 115.13136291503906, Temp: 1.1940146684646606, KL: 73.28004455566406, Loss: 0.17384429275989532, Learning Rate: 0.020999999999999998\n",
      "Epoch [1953/20000], Bound: 0.6463893055915833, Entropy: 112.97767639160156, Temp: 1.1952035427093506, KL: 74.81306457519531, Loss: 0.16187401115894318, Learning Rate: 0.020999999999999998\n",
      "Epoch [1954/20000], Bound: 0.7045338153839111, Entropy: 115.31429290771484, Temp: 1.1962319612503052, KL: 95.36658477783203, Loss: 0.1402975469827652, Learning Rate: 0.020999999999999998\n",
      "Epoch [1955/20000], Bound: 0.6516847014427185, Entropy: 118.48613739013672, Temp: 1.1974624395370483, KL: 73.26932525634766, Loss: 0.17432978749275208, Learning Rate: 0.020999999999999998\n",
      "Epoch [1956/20000], Bound: 0.7503250241279602, Entropy: 116.41759490966797, Temp: 1.198432207107544, KL: 91.11455535888672, Loss: 0.2127995640039444, Learning Rate: 0.020999999999999998\n",
      "Epoch [1957/20000], Bound: 0.7425261735916138, Entropy: 115.88853454589844, Temp: 1.1991416215896606, KL: 95.00965881347656, Loss: 0.18722078204154968, Learning Rate: 0.020999999999999998\n",
      "Epoch [1958/20000], Bound: 0.6318128108978271, Entropy: 116.6297836303711, Temp: 1.1998012065887451, KL: 77.80501556396484, Loss: 0.13480862975120544, Learning Rate: 0.020999999999999998\n",
      "Epoch [1959/20000], Bound: 0.6496953964233398, Entropy: 113.08657836914062, Temp: 1.2005407810211182, KL: 78.48338317871094, Loss: 0.15096774697303772, Learning Rate: 0.020999999999999998\n",
      "Epoch [1960/20000], Bound: 0.5945559144020081, Entropy: 112.51376342773438, Temp: 1.2012603282928467, KL: 74.82218933105469, Loss: 0.10950791090726852, Learning Rate: 0.020999999999999998\n",
      "Epoch [1961/20000], Bound: 0.6375752091407776, Entropy: 113.54423522949219, Temp: 1.2021758556365967, KL: 78.76669311523438, Loss: 0.1372445970773697, Learning Rate: 0.020999999999999998\n",
      "Epoch [1962/20000], Bound: 0.6872814893722534, Entropy: 116.71659851074219, Temp: 1.2031376361846924, KL: 90.73255920410156, Loss: 0.14159096777439117, Learning Rate: 0.020999999999999998\n",
      "Epoch [1963/20000], Bound: 0.6796094179153442, Entropy: 120.69354248046875, Temp: 1.204240322113037, KL: 93.41534423828125, Loss: 0.12213438004255295, Learning Rate: 0.020999999999999998\n",
      "Epoch [1964/20000], Bound: 0.6566240787506104, Entropy: 118.99925231933594, Temp: 1.2056132555007935, KL: 84.36129760742188, Loss: 0.13488149642944336, Learning Rate: 0.020999999999999998\n",
      "Epoch [1965/20000], Bound: 0.6901978254318237, Entropy: 123.38929748535156, Temp: 1.207055687904358, KL: 87.60569763183594, Loss: 0.1586468666791916, Learning Rate: 0.020999999999999998\n",
      "Epoch [1966/20000], Bound: 0.700652003288269, Entropy: 125.13121032714844, Temp: 1.2084490060806274, KL: 88.327880859375, Loss: 0.16778068244457245, Learning Rate: 0.020999999999999998\n",
      "Epoch [1967/20000], Bound: 0.6018884778022766, Entropy: 125.11856842041016, Temp: 1.2097482681274414, KL: 72.84420013427734, Loss: 0.1263868808746338, Learning Rate: 0.020999999999999998\n",
      "Epoch [1968/20000], Bound: 0.7213064432144165, Entropy: 122.99449157714844, Temp: 1.2110443115234375, KL: 99.27772521972656, Loss: 0.1470058560371399, Learning Rate: 0.020999999999999998\n",
      "Epoch [1969/20000], Bound: 0.6115856766700745, Entropy: 120.83114624023438, Temp: 1.2124866247177124, KL: 73.28451538085938, Loss: 0.13478033244609833, Learning Rate: 0.020999999999999998\n",
      "Epoch [1970/20000], Bound: 0.6573278307914734, Entropy: 120.70182037353516, Temp: 1.2138594388961792, KL: 80.07085418701172, Loss: 0.1548626720905304, Learning Rate: 0.020999999999999998\n",
      "Epoch [1971/20000], Bound: 0.6892326474189758, Entropy: 119.68905639648438, Temp: 1.2151180505752563, KL: 82.87918090820312, Loss: 0.17860469222068787, Learning Rate: 0.020999999999999998\n",
      "Epoch [1972/20000], Bound: 0.7017480731010437, Entropy: 115.46558380126953, Temp: 1.2161589860916138, KL: 92.1644515991211, Loss: 0.15479779243469238, Learning Rate: 0.020999999999999998\n",
      "Epoch [1973/20000], Bound: 0.6761665940284729, Entropy: 116.13033294677734, Temp: 1.2172430753707886, KL: 92.25733184814453, Loss: 0.12591490149497986, Learning Rate: 0.020999999999999998\n",
      "Epoch [1974/20000], Bound: 0.6826214790344238, Entropy: 117.02839660644531, Temp: 1.2185413837432861, KL: 81.978271484375, Loss: 0.17552010715007782, Learning Rate: 0.020999999999999998\n",
      "Epoch [1975/20000], Bound: 0.706274151802063, Entropy: 112.35697174072266, Temp: 1.2196195125579834, KL: 83.85082244873047, Loss: 0.19479544460773468, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1976/20000], Bound: 0.6471372246742249, Entropy: 117.56015014648438, Temp: 1.220402717590332, KL: 83.2733154296875, Loss: 0.13195882737636566, Learning Rate: 0.020999999999999998\n",
      "Epoch [1977/20000], Bound: 0.7050648927688599, Entropy: 116.10404968261719, Temp: 1.2212939262390137, KL: 102.285400390625, Loss: 0.11823611706495285, Learning Rate: 0.020999999999999998\n",
      "Epoch [1978/20000], Bound: 0.6251307129859924, Entropy: 117.91547393798828, Temp: 1.2225573062896729, KL: 79.04120635986328, Loss: 0.1266111582517624, Learning Rate: 0.020999999999999998\n",
      "Epoch [1979/20000], Bound: 0.6549599170684814, Entropy: 118.55532836914062, Temp: 1.2238656282424927, KL: 82.19607543945312, Loss: 0.1453281044960022, Learning Rate: 0.020999999999999998\n",
      "Epoch [1980/20000], Bound: 0.745121419429779, Entropy: 118.74128723144531, Temp: 1.2251290082931519, KL: 101.21257019042969, Loss: 0.17088252305984497, Learning Rate: 0.020999999999999998\n",
      "Epoch [1981/20000], Bound: 0.679611325263977, Entropy: 116.77574157714844, Temp: 1.2263894081115723, KL: 85.70960998535156, Loss: 0.1583217829465866, Learning Rate: 0.020999999999999998\n",
      "Epoch [1982/20000], Bound: 0.6721286773681641, Entropy: 116.39755249023438, Temp: 1.2275621891021729, KL: 91.46818542480469, Loss: 0.1268283575773239, Learning Rate: 0.020999999999999998\n",
      "Epoch [1983/20000], Bound: 0.645749032497406, Entropy: 115.91903686523438, Temp: 1.2289084196090698, KL: 83.99180603027344, Loss: 0.12907367944717407, Learning Rate: 0.020999999999999998\n",
      "Epoch [1984/20000], Bound: 0.68073570728302, Entropy: 117.88834381103516, Temp: 1.2303175926208496, KL: 86.80139923095703, Loss: 0.1558499038219452, Learning Rate: 0.020999999999999998\n",
      "Epoch [1985/20000], Bound: 0.6799814701080322, Entropy: 117.09458923339844, Temp: 1.2316431999206543, KL: 81.34320068359375, Loss: 0.17741645872592926, Learning Rate: 0.020999999999999998\n",
      "Epoch [1986/20000], Bound: 0.6765166521072388, Entropy: 118.37664031982422, Temp: 1.232700228691101, KL: 78.76734161376953, Loss: 0.1842067688703537, Learning Rate: 0.020999999999999998\n",
      "Epoch [1987/20000], Bound: 0.7009333968162537, Entropy: 118.0378646850586, Temp: 1.2334434986114502, KL: 95.54788970947266, Loss: 0.14371035993099213, Learning Rate: 0.020999999999999998\n",
      "Epoch [1988/20000], Bound: 0.6565473675727844, Entropy: 112.03507232666016, Temp: 1.2343274354934692, KL: 81.44715118408203, Loss: 0.15186037123203278, Learning Rate: 0.020999999999999998\n",
      "Epoch [1989/20000], Bound: 0.6949815154075623, Entropy: 111.82188415527344, Temp: 1.2351398468017578, KL: 90.82174682617188, Loss: 0.15643447637557983, Learning Rate: 0.020999999999999998\n",
      "Epoch [1990/20000], Bound: 0.6562033891677856, Entropy: 111.01581573486328, Temp: 1.2359563112258911, KL: 84.01435089111328, Loss: 0.1413760483264923, Learning Rate: 0.020999999999999998\n",
      "Epoch [1991/20000], Bound: 0.6419287919998169, Entropy: 112.10545349121094, Temp: 1.2367981672286987, KL: 79.2606201171875, Loss: 0.1455537974834442, Learning Rate: 0.020999999999999998\n",
      "Epoch [1992/20000], Bound: 0.7468852996826172, Entropy: 106.42813873291016, Temp: 1.2375837564468384, KL: 92.41411590576172, Loss: 0.21149446070194244, Learning Rate: 0.020999999999999998\n",
      "Epoch [1993/20000], Bound: 0.6410256624221802, Entropy: 108.75028991699219, Temp: 1.2380552291870117, KL: 81.97126770019531, Loss: 0.13385508954524994, Learning Rate: 0.020999999999999998\n",
      "Epoch [1994/20000], Bound: 0.6705806851387024, Entropy: 106.66181182861328, Temp: 1.2386085987091064, KL: 89.69718170166016, Loss: 0.13448496162891388, Learning Rate: 0.020999999999999998\n",
      "Epoch [1995/20000], Bound: 0.6565202474594116, Entropy: 111.05487060546875, Temp: 1.2393112182617188, KL: 86.59007263183594, Loss: 0.1319066435098648, Learning Rate: 0.020999999999999998\n",
      "Epoch [1996/20000], Bound: 0.6609624028205872, Entropy: 109.01935577392578, Temp: 1.2401313781738281, KL: 93.5858383178711, Loss: 0.10863536596298218, Learning Rate: 0.020999999999999998\n",
      "Epoch [1997/20000], Bound: 0.6239205598831177, Entropy: 111.66974639892578, Temp: 1.2412760257720947, KL: 81.54032135009766, Loss: 0.11832377314567566, Learning Rate: 0.020999999999999998\n",
      "Epoch [1998/20000], Bound: 0.6345770359039307, Entropy: 113.85911560058594, Temp: 1.2425261735916138, KL: 86.15029907226562, Loss: 0.11102462559938431, Learning Rate: 0.020999999999999998\n",
      "Epoch [1999/20000], Bound: 0.6028215289115906, Entropy: 118.87030792236328, Temp: 1.2439641952514648, KL: 80.01795196533203, Loss: 0.10345908999443054, Learning Rate: 0.020999999999999998\n",
      "Epoch [2000/20000], Bound: 0.6704532504081726, Entropy: 120.61066436767578, Temp: 1.2455556392669678, KL: 92.9419937133789, Loss: 0.12263476848602295, Learning Rate: 0.020999999999999998\n",
      "Epoch [2001/20000], Bound: 0.603027880191803, Entropy: 122.57845306396484, Temp: 1.2472894191741943, KL: 77.63774871826172, Loss: 0.1137419268488884, Learning Rate: 0.020999999999999998\n",
      "Epoch [2002/20000], Bound: 0.5716736912727356, Entropy: 128.22767639160156, Temp: 1.2490488290786743, KL: 72.45747375488281, Loss: 0.10393194854259491, Learning Rate: 0.020999999999999998\n",
      "Epoch [2003/20000], Bound: 0.6221652626991272, Entropy: 131.1029815673828, Temp: 1.2508388757705688, KL: 78.76100158691406, Loss: 0.12919513881206512, Learning Rate: 0.020999999999999998\n",
      "Epoch [2004/20000], Bound: 0.6743291020393372, Entropy: 133.78562927246094, Temp: 1.2525538206100464, KL: 83.16876220703125, Loss: 0.1672736257314682, Learning Rate: 0.020999999999999998\n",
      "Epoch [2005/20000], Bound: 0.710019588470459, Entropy: 131.80995178222656, Temp: 1.2540013790130615, KL: 96.65068054199219, Loss: 0.15401126444339752, Learning Rate: 0.020999999999999998\n",
      "Epoch [2006/20000], Bound: 0.6451985239982605, Entropy: 131.47923278808594, Temp: 1.2554267644882202, KL: 83.62287902832031, Loss: 0.13451851904392242, Learning Rate: 0.020999999999999998\n",
      "Epoch [2007/20000], Bound: 0.6190094947814941, Entropy: 123.71343231201172, Temp: 1.2568215131759644, KL: 72.4874038696289, Loss: 0.15182921290397644, Learning Rate: 0.020999999999999998\n",
      "Epoch [2008/20000], Bound: 0.6581834554672241, Entropy: 121.928466796875, Temp: 1.2579537630081177, KL: 87.28158569335938, Loss: 0.13423912227153778, Learning Rate: 0.020999999999999998\n",
      "Epoch [2009/20000], Bound: 0.659112274646759, Entropy: 115.2926254272461, Temp: 1.2591195106506348, KL: 86.91072845458984, Loss: 0.13691473007202148, Learning Rate: 0.020999999999999998\n",
      "Epoch [2010/20000], Bound: 0.6217924356460571, Entropy: 109.99465942382812, Temp: 1.2602921724319458, KL: 80.94566345214844, Loss: 0.12157176434993744, Learning Rate: 0.020999999999999998\n",
      "Epoch [2011/20000], Bound: 0.7089417576789856, Entropy: 105.83257293701172, Temp: 1.2615078687667847, KL: 92.66837310791016, Loss: 0.1700746864080429, Learning Rate: 0.020999999999999998\n",
      "Epoch [2012/20000], Bound: 0.6365962624549866, Entropy: 103.85042572021484, Temp: 1.2625682353973389, KL: 85.53343963623047, Loss: 0.119077168405056, Learning Rate: 0.020999999999999998\n",
      "Epoch [2013/20000], Bound: 0.6321741342544556, Entropy: 102.70658874511719, Temp: 1.2637428045272827, KL: 82.26826477050781, Loss: 0.12758588790893555, Learning Rate: 0.020999999999999998\n",
      "Epoch [2014/20000], Bound: 0.6523849368095398, Entropy: 110.27996063232422, Temp: 1.2649283409118652, KL: 78.75249481201172, Loss: 0.1629468947649002, Learning Rate: 0.020999999999999998\n",
      "Epoch [2015/20000], Bound: 0.6280466318130493, Entropy: 111.20294952392578, Temp: 1.2658536434173584, KL: 83.01380157470703, Loss: 0.12069763243198395, Learning Rate: 0.020999999999999998\n",
      "Epoch [2016/20000], Bound: 0.629540205001831, Entropy: 111.73857116699219, Temp: 1.2668644189834595, KL: 83.1961669921875, Loss: 0.12168312072753906, Learning Rate: 0.020999999999999998\n",
      "Epoch [2017/20000], Bound: 0.6548680067062378, Entropy: 118.52130889892578, Temp: 1.2679458856582642, KL: 93.33626556396484, Loss: 0.10852665454149246, Learning Rate: 0.020999999999999998\n",
      "Epoch [2018/20000], Bound: 0.624927282333374, Entropy: 120.54345703125, Temp: 1.2692784070968628, KL: 89.03385925292969, Loss: 0.09431172907352448, Learning Rate: 0.020999999999999998\n",
      "Epoch [2019/20000], Bound: 0.6661785244941711, Entropy: 127.5935287475586, Temp: 1.2708885669708252, KL: 92.69506072998047, Loss: 0.12380336225032806, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2020/20000], Bound: 0.661439061164856, Entropy: 127.21739959716797, Temp: 1.2725846767425537, KL: 99.35259246826172, Loss: 0.09282415360212326, Learning Rate: 0.020999999999999998\n",
      "Epoch [2021/20000], Bound: 0.629840075969696, Entropy: 122.03598022460938, Temp: 1.2746247053146362, KL: 85.04795837402344, Loss: 0.1159544363617897, Learning Rate: 0.020999999999999998\n",
      "Epoch [2022/20000], Bound: 0.6107730865478516, Entropy: 118.7421646118164, Temp: 1.2766778469085693, KL: 82.39672088623047, Loss: 0.10718591511249542, Learning Rate: 0.020999999999999998\n",
      "Epoch [2023/20000], Bound: 0.6299017071723938, Entropy: 116.28165435791016, Temp: 1.2787718772888184, KL: 86.15799713134766, Loss: 0.11234578490257263, Learning Rate: 0.020999999999999998\n",
      "Epoch [2024/20000], Bound: 0.697827935218811, Entropy: 114.24363708496094, Temp: 1.2809021472930908, KL: 92.31172180175781, Loss: 0.1623009592294693, Learning Rate: 0.020999999999999998\n",
      "Epoch [2025/20000], Bound: 0.615806519985199, Entropy: 111.62599182128906, Temp: 1.2827932834625244, KL: 87.93998718261719, Loss: 0.09161072224378586, Learning Rate: 0.020999999999999998\n",
      "Epoch [2026/20000], Bound: 0.5998066067695618, Entropy: 107.52589416503906, Temp: 1.2848933935165405, KL: 79.79129028320312, Loss: 0.10759899020195007, Learning Rate: 0.020999999999999998\n",
      "Epoch [2027/20000], Bound: 0.6143065690994263, Entropy: 113.3953857421875, Temp: 1.286987066268921, KL: 85.5443115234375, Loss: 0.10009990632534027, Learning Rate: 0.020999999999999998\n",
      "Epoch [2028/20000], Bound: 0.6029852628707886, Entropy: 111.04615020751953, Temp: 1.2891813516616821, KL: 80.40287017822266, Loss: 0.10900834947824478, Learning Rate: 0.020999999999999998\n",
      "Epoch [2029/20000], Bound: 0.5709201097488403, Entropy: 114.15293884277344, Temp: 1.2913496494293213, KL: 77.02662658691406, Loss: 0.09104318171739578, Learning Rate: 0.020999999999999998\n",
      "Epoch [2030/20000], Bound: 0.6028225421905518, Entropy: 118.7197494506836, Temp: 1.2935823202133179, KL: 85.01207733154297, Loss: 0.09166163951158524, Learning Rate: 0.020999999999999998\n",
      "Epoch [2031/20000], Bound: 0.6316006183624268, Entropy: 123.10649108886719, Temp: 1.2959446907043457, KL: 90.3519287109375, Loss: 0.10066861659288406, Learning Rate: 0.020999999999999998\n",
      "Epoch [2032/20000], Bound: 0.6223623156547546, Entropy: 118.69625854492188, Temp: 1.2984095811843872, KL: 89.802001953125, Loss: 0.09369459003210068, Learning Rate: 0.020999999999999998\n",
      "Epoch [2033/20000], Bound: 0.5678203701972961, Entropy: 121.83260345458984, Temp: 1.3010057210922241, KL: 78.34729766845703, Loss: 0.08431192487478256, Learning Rate: 0.020999999999999998\n",
      "Epoch [2034/20000], Bound: 0.6128414273262024, Entropy: 118.96981048583984, Temp: 1.3036710023880005, KL: 87.08072662353516, Loss: 0.0953320637345314, Learning Rate: 0.020999999999999998\n",
      "Epoch [2035/20000], Bound: 0.6501477956771851, Entropy: 118.71366882324219, Temp: 1.3064022064208984, KL: 87.81976318359375, Loss: 0.13153769075870514, Learning Rate: 0.020999999999999998\n",
      "Epoch [2036/20000], Bound: 0.6555366516113281, Entropy: 120.98046875, Temp: 1.3089460134506226, KL: 93.36070251464844, Loss: 0.11650659143924713, Learning Rate: 0.020999999999999998\n",
      "Epoch [2037/20000], Bound: 0.6909537315368652, Entropy: 112.56829833984375, Temp: 1.3114722967147827, KL: 100.08474731445312, Loss: 0.13013556599617004, Learning Rate: 0.020999999999999998\n",
      "Epoch [2038/20000], Bound: 0.6202911734580994, Entropy: 110.13825225830078, Temp: 1.313948631286621, KL: 88.40523529052734, Loss: 0.09943901002407074, Learning Rate: 0.020999999999999998\n",
      "Epoch [2039/20000], Bound: 0.6735396385192871, Entropy: 108.71636199951172, Temp: 1.3164772987365723, KL: 95.3020248413086, Loss: 0.1298932433128357, Learning Rate: 0.020999999999999998\n",
      "Epoch [2040/20000], Bound: 0.6060863733291626, Entropy: 107.83206939697266, Temp: 1.3189036846160889, KL: 81.66315460205078, Loss: 0.11144418269395828, Learning Rate: 0.020999999999999998\n",
      "Epoch [2041/20000], Bound: 0.6016717553138733, Entropy: 103.38311004638672, Temp: 1.3212294578552246, KL: 80.32161712646484, Loss: 0.11244124919176102, Learning Rate: 0.020999999999999998\n",
      "Epoch [2042/20000], Bound: 0.6688154339790344, Entropy: 105.03697967529297, Temp: 1.323440432548523, KL: 91.8365249633789, Loss: 0.13901092112064362, Learning Rate: 0.020999999999999998\n",
      "Epoch [2043/20000], Bound: 0.6370647549629211, Entropy: 109.3490219116211, Temp: 1.3254725933074951, KL: 92.3420639038086, Loss: 0.10362812876701355, Learning Rate: 0.020999999999999998\n",
      "Epoch [2044/20000], Bound: 0.6507327556610107, Entropy: 110.50809478759766, Temp: 1.3275902271270752, KL: 88.14701080322266, Loss: 0.13413138687610626, Learning Rate: 0.020999999999999998\n",
      "Epoch [2045/20000], Bound: 0.6218608021736145, Entropy: 112.48899841308594, Temp: 1.329528570175171, KL: 83.49185180664062, Loss: 0.12190569937229156, Learning Rate: 0.020999999999999998\n",
      "Epoch [2046/20000], Bound: 0.6713215708732605, Entropy: 111.0058364868164, Temp: 1.3313413858413696, KL: 88.5466537475586, Loss: 0.1553436517715454, Learning Rate: 0.020999999999999998\n",
      "Epoch [2047/20000], Bound: 0.6325923800468445, Entropy: 113.41683197021484, Temp: 1.3328564167022705, KL: 84.47728729248047, Loss: 0.12967608869075775, Learning Rate: 0.020999999999999998\n",
      "Epoch [2048/20000], Bound: 0.7001964449882507, Entropy: 115.53468322753906, Temp: 1.3342372179031372, KL: 79.2933349609375, Loss: 0.22266417741775513, Learning Rate: 0.020999999999999998\n",
      "Epoch [2049/20000], Bound: 0.6475093364715576, Entropy: 114.88880920410156, Temp: 1.3348159790039062, KL: 91.76762390136719, Loss: 0.1182183027267456, Learning Rate: 0.020999999999999998\n",
      "Epoch [2050/20000], Bound: 0.6445251107215881, Entropy: 113.86817169189453, Temp: 1.3355004787445068, KL: 85.16252899169922, Loss: 0.13991029560565948, Learning Rate: 0.020999999999999998\n",
      "Epoch [2051/20000], Bound: 0.5979322791099548, Entropy: 113.49544525146484, Temp: 1.3360633850097656, KL: 66.50615692138672, Loss: 0.16236911714076996, Learning Rate: 0.020999999999999998\n",
      "Epoch [2052/20000], Bound: 0.6451699733734131, Entropy: 110.68538665771484, Temp: 1.3361655473709106, KL: 87.86170196533203, Loss: 0.13057900965213776, Learning Rate: 0.020999999999999998\n",
      "Epoch [2053/20000], Bound: 0.6386127471923828, Entropy: 115.9037094116211, Temp: 1.336294412612915, KL: 77.4302749633789, Loss: 0.1627621054649353, Learning Rate: 0.020999999999999998\n",
      "Epoch [2054/20000], Bound: 0.6158829927444458, Entropy: 117.83155822753906, Temp: 1.336119532585144, KL: 78.68389892578125, Loss: 0.13472433388233185, Learning Rate: 0.020999999999999998\n",
      "Epoch [2055/20000], Bound: 0.6188408136367798, Entropy: 115.78931427001953, Temp: 1.335878610610962, KL: 81.14965057373047, Loss: 0.12846079468727112, Learning Rate: 0.020999999999999998\n",
      "Epoch [2056/20000], Bound: 0.6471078395843506, Entropy: 121.56063842773438, Temp: 1.335647702217102, KL: 85.4432373046875, Loss: 0.14159926772117615, Learning Rate: 0.020999999999999998\n",
      "Epoch [2057/20000], Bound: 0.6067575216293335, Entropy: 117.89384460449219, Temp: 1.3353772163391113, KL: 77.68911743164062, Loss: 0.12919415533542633, Learning Rate: 0.020999999999999998\n",
      "Epoch [2058/20000], Bound: 0.6629365086555481, Entropy: 118.34666442871094, Temp: 1.3350800275802612, KL: 91.49864196777344, Loss: 0.13573065400123596, Learning Rate: 0.020999999999999998\n",
      "Epoch [2059/20000], Bound: 0.5686989426612854, Entropy: 118.61458587646484, Temp: 1.3348506689071655, KL: 68.4426498413086, Loss: 0.12670592963695526, Learning Rate: 0.020999999999999998\n",
      "Epoch [2060/20000], Bound: 0.619951069355011, Entropy: 117.40493774414062, Temp: 1.3345123529434204, KL: 79.32070922851562, Loss: 0.13626576960086823, Learning Rate: 0.020999999999999998\n",
      "Epoch [2061/20000], Bound: 0.6822611689567566, Entropy: 118.02999877929688, Temp: 1.3341223001480103, KL: 94.30792236328125, Loss: 0.14620278775691986, Learning Rate: 0.020999999999999998\n",
      "Epoch [2062/20000], Bound: 0.6011444926261902, Entropy: 116.48944854736328, Temp: 1.3337657451629639, KL: 82.6076889038086, Loss: 0.10497947782278061, Learning Rate: 0.020999999999999998\n",
      "Epoch [2063/20000], Bound: 0.5609155893325806, Entropy: 121.90482330322266, Temp: 1.3336169719696045, KL: 78.42247772216797, Loss: 0.08180856704711914, Learning Rate: 0.020999999999999998\n",
      "Epoch [2064/20000], Bound: 0.6912537813186646, Entropy: 122.09326934814453, Temp: 1.3337846994400024, KL: 109.39093780517578, Loss: 0.09965463727712631, Learning Rate: 0.020999999999999998\n",
      "Epoch [2065/20000], Bound: 0.6061741709709167, Entropy: 124.21686553955078, Temp: 1.334391713142395, KL: 80.25199127197266, Loss: 0.11889094114303589, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2066/20000], Bound: 0.5672013163566589, Entropy: 119.16181182861328, Temp: 1.3349859714508057, KL: 72.31806182861328, Loss: 0.11078294366598129, Learning Rate: 0.020999999999999998\n",
      "Epoch [2067/20000], Bound: 0.5414752960205078, Entropy: 121.9779281616211, Temp: 1.335545301437378, KL: 67.60388946533203, Loss: 0.10448873043060303, Learning Rate: 0.020999999999999998\n",
      "Epoch [2068/20000], Bound: 0.7172783613204956, Entropy: 118.92813873291016, Temp: 1.336069107055664, KL: 101.57996368408203, Loss: 0.15921179950237274, Learning Rate: 0.020999999999999998\n",
      "Epoch [2069/20000], Bound: 0.5483682751655579, Entropy: 123.11064147949219, Temp: 1.3365116119384766, KL: 68.30206298828125, Loss: 0.10832814872264862, Learning Rate: 0.020999999999999998\n",
      "Epoch [2070/20000], Bound: 0.6638569831848145, Entropy: 117.52552795410156, Temp: 1.3369077444076538, KL: 86.73675537109375, Loss: 0.154814213514328, Learning Rate: 0.020999999999999998\n",
      "Epoch [2071/20000], Bound: 0.6591704487800598, Entropy: 117.87236022949219, Temp: 1.3371204137802124, KL: 96.89283752441406, Loss: 0.11182035505771637, Learning Rate: 0.020999999999999998\n",
      "Epoch [2072/20000], Bound: 0.6382608413696289, Entropy: 113.7362289428711, Temp: 1.3375660181045532, KL: 79.74625396728516, Loss: 0.15388473868370056, Learning Rate: 0.020999999999999998\n",
      "Epoch [2073/20000], Bound: 0.6062538623809814, Entropy: 111.7576904296875, Temp: 1.3377571105957031, KL: 75.61343383789062, Loss: 0.1367330551147461, Learning Rate: 0.020999999999999998\n",
      "Epoch [2074/20000], Bound: 0.5531601905822754, Entropy: 112.19738006591797, Temp: 1.3377963304519653, KL: 71.92261505126953, Loss: 0.09937402606010437, Learning Rate: 0.020999999999999998\n",
      "Epoch [2075/20000], Bound: 0.668163537979126, Entropy: 115.17436981201172, Temp: 1.337931513786316, KL: 85.99823760986328, Loss: 0.16238372027873993, Learning Rate: 0.020999999999999998\n",
      "Epoch [2076/20000], Bound: 0.6119957566261292, Entropy: 111.61289978027344, Temp: 1.3378474712371826, KL: 81.48936462402344, Loss: 0.12053117156028748, Learning Rate: 0.020999999999999998\n",
      "Epoch [2077/20000], Bound: 0.6161482334136963, Entropy: 113.25437927246094, Temp: 1.3378145694732666, KL: 76.2174072265625, Loss: 0.14441551268100739, Learning Rate: 0.020999999999999998\n",
      "Epoch [2078/20000], Bound: 0.5860929489135742, Entropy: 118.21578216552734, Temp: 1.3376034498214722, KL: 77.3626937866211, Loss: 0.11033618450164795, Learning Rate: 0.020999999999999998\n",
      "Epoch [2079/20000], Bound: 0.5690785050392151, Entropy: 122.18270874023438, Temp: 1.3374888896942139, KL: 67.96353149414062, Loss: 0.12911537289619446, Learning Rate: 0.020999999999999998\n",
      "Epoch [2080/20000], Bound: 0.5923929810523987, Entropy: 124.5186996459961, Temp: 1.3372259140014648, KL: 73.08641815185547, Loss: 0.13242407143115997, Learning Rate: 0.020999999999999998\n",
      "Epoch [2081/20000], Bound: 0.6499980688095093, Entropy: 125.61775207519531, Temp: 1.3368608951568604, KL: 92.32330322265625, Loss: 0.11908868700265884, Learning Rate: 0.020999999999999998\n",
      "Epoch [2082/20000], Bound: 0.5670302510261536, Entropy: 124.90959930419922, Temp: 1.3366931676864624, KL: 72.05274200439453, Loss: 0.11179903149604797, Learning Rate: 0.020999999999999998\n",
      "Epoch [2083/20000], Bound: 0.6466147303581238, Entropy: 125.30146026611328, Temp: 1.3365535736083984, KL: 85.18817901611328, Loss: 0.14215795695781708, Learning Rate: 0.020999999999999998\n",
      "Epoch [2084/20000], Bound: 0.651485800743103, Entropy: 125.48331451416016, Temp: 1.3363561630249023, KL: 86.8546371459961, Loss: 0.14104823768138885, Learning Rate: 0.020999999999999998\n",
      "Epoch [2085/20000], Bound: 0.5970286726951599, Entropy: 120.4036865234375, Temp: 1.3361316919326782, KL: 74.26646423339844, Loss: 0.1324441283941269, Learning Rate: 0.020999999999999998\n",
      "Epoch [2086/20000], Bound: 0.636563777923584, Entropy: 121.87103271484375, Temp: 1.3358149528503418, KL: 83.68916320800781, Loss: 0.1371467411518097, Learning Rate: 0.020999999999999998\n",
      "Epoch [2087/20000], Bound: 0.5783094763755798, Entropy: 121.27132415771484, Temp: 1.3354796171188354, KL: 68.33589935302734, Loss: 0.13635331392288208, Learning Rate: 0.020999999999999998\n",
      "Epoch [2088/20000], Bound: 0.6406286358833313, Entropy: 122.79102325439453, Temp: 1.3349723815917969, KL: 84.1557846069336, Loss: 0.1395239382982254, Learning Rate: 0.020999999999999998\n",
      "Epoch [2089/20000], Bound: 0.5424171090126038, Entropy: 120.00286865234375, Temp: 1.334454894065857, KL: 72.09431457519531, Loss: 0.08842085301876068, Learning Rate: 0.020999999999999998\n",
      "Epoch [2090/20000], Bound: 0.73321133852005, Entropy: 120.93706512451172, Temp: 1.3341789245605469, KL: 97.77840423583984, Loss: 0.1920049786567688, Learning Rate: 0.020999999999999998\n",
      "Epoch [2091/20000], Bound: 0.6915406584739685, Entropy: 116.07611083984375, Temp: 1.333647608757019, KL: 95.97161865234375, Loss: 0.15026013553142548, Learning Rate: 0.020999999999999998\n",
      "Epoch [2092/20000], Bound: 0.6809663772583008, Entropy: 113.6953125, Temp: 1.3331525325775146, KL: 93.30392456054688, Loss: 0.14837589859962463, Learning Rate: 0.020999999999999998\n",
      "Epoch [2093/20000], Bound: 0.7112073302268982, Entropy: 113.40310668945312, Temp: 1.332678198814392, KL: 97.27902221679688, Loss: 0.16766074299812317, Learning Rate: 0.020999999999999998\n",
      "Epoch [2094/20000], Bound: 0.7302658557891846, Entropy: 108.09098052978516, Temp: 1.332129716873169, KL: 98.0346450805664, Loss: 0.18716461956501007, Learning Rate: 0.020999999999999998\n",
      "Epoch [2095/20000], Bound: 0.6694843173027039, Entropy: 100.26087951660156, Temp: 1.33139169216156, KL: 91.94151306152344, Loss: 0.1405974179506302, Learning Rate: 0.020999999999999998\n",
      "Epoch [2096/20000], Bound: 0.6114413142204285, Entropy: 97.3025894165039, Temp: 1.3307428359985352, KL: 87.51006317138672, Loss: 0.09642892330884933, Learning Rate: 0.020999999999999998\n",
      "Epoch [2097/20000], Bound: 0.6542459726333618, Entropy: 95.48943328857422, Temp: 1.3304498195648193, KL: 93.54878997802734, Loss: 0.1179882138967514, Learning Rate: 0.020999999999999998\n",
      "Epoch [2098/20000], Bound: 0.6575780510902405, Entropy: 99.95890045166016, Temp: 1.3303790092468262, KL: 85.0186538696289, Loss: 0.15359605848789215, Learning Rate: 0.020999999999999998\n",
      "Epoch [2099/20000], Bound: 0.6509861350059509, Entropy: 100.38491821289062, Temp: 1.3301726579666138, KL: 92.53048706054688, Loss: 0.1183040589094162, Learning Rate: 0.020999999999999998\n",
      "Epoch [2100/20000], Bound: 0.6954447627067566, Entropy: 105.26516723632812, Temp: 1.33016836643219, KL: 94.75447082519531, Loss: 0.15866269171237946, Learning Rate: 0.020999999999999998\n",
      "Epoch [2101/20000], Bound: 0.6678978204727173, Entropy: 111.19053649902344, Temp: 1.3300848007202148, KL: 84.19393920898438, Loss: 0.1677907556295395, Learning Rate: 0.020999999999999998\n",
      "Epoch [2102/20000], Bound: 0.6198691725730896, Entropy: 117.37989807128906, Temp: 1.3297603130340576, KL: 78.33061218261719, Loss: 0.1393168419599533, Learning Rate: 0.020999999999999998\n",
      "Epoch [2103/20000], Bound: 0.7000073194503784, Entropy: 116.64632415771484, Temp: 1.3293575048446655, KL: 89.3521499633789, Loss: 0.18403244018554688, Learning Rate: 0.020999999999999998\n",
      "Epoch [2104/20000], Bound: 0.6323571801185608, Entropy: 118.45835876464844, Temp: 1.3286898136138916, KL: 80.9295654296875, Loss: 0.1422092765569687, Learning Rate: 0.020999999999999998\n",
      "Epoch [2105/20000], Bound: 0.5966320037841797, Entropy: 123.29849243164062, Temp: 1.3279863595962524, KL: 72.75927734375, Loss: 0.1368158459663391, Learning Rate: 0.020999999999999998\n",
      "Epoch [2106/20000], Bound: 0.7129049301147461, Entropy: 116.35995483398438, Temp: 1.3272038698196411, KL: 85.64620971679688, Loss: 0.21252115070819855, Learning Rate: 0.020999999999999998\n",
      "Epoch [2107/20000], Bound: 0.6291630864143372, Entropy: 120.63288879394531, Temp: 1.325971245765686, KL: 88.38291931152344, Loss: 0.1104571670293808, Learning Rate: 0.020999999999999998\n",
      "Epoch [2108/20000], Bound: 0.6720576286315918, Entropy: 116.2787857055664, Temp: 1.3250675201416016, KL: 90.28504180908203, Loss: 0.14866170287132263, Learning Rate: 0.020999999999999998\n",
      "Epoch [2109/20000], Bound: 0.5864011645317078, Entropy: 120.93291473388672, Temp: 1.3242086172103882, KL: 72.42948150634766, Loss: 0.12762483954429626, Learning Rate: 0.020999999999999998\n",
      "Epoch [2110/20000], Bound: 0.5909658074378967, Entropy: 123.28372955322266, Temp: 1.323354959487915, KL: 81.37049102783203, Loss: 0.09820428490638733, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2111/20000], Bound: 0.6559949517250061, Entropy: 126.96463775634766, Temp: 1.3228166103363037, KL: 82.64099884033203, Loss: 0.159837007522583, Learning Rate: 0.020999999999999998\n",
      "Epoch [2112/20000], Bound: 0.6150632500648499, Entropy: 121.00052642822266, Temp: 1.3221349716186523, KL: 79.65076446533203, Loss: 0.12851230800151825, Learning Rate: 0.020999999999999998\n",
      "Epoch [2113/20000], Bound: 0.6504572629928589, Entropy: 122.38050842285156, Temp: 1.3215147256851196, KL: 82.25466918945312, Loss: 0.15522654354572296, Learning Rate: 0.020999999999999998\n",
      "Epoch [2114/20000], Bound: 0.6729816794395447, Entropy: 120.13079833984375, Temp: 1.3207894563674927, KL: 80.8148193359375, Loss: 0.18486171960830688, Learning Rate: 0.020999999999999998\n",
      "Epoch [2115/20000], Bound: 0.6533413529396057, Entropy: 117.95835876464844, Temp: 1.319752812385559, KL: 73.34146118164062, Loss: 0.19182533025741577, Learning Rate: 0.020999999999999998\n",
      "Epoch [2116/20000], Bound: 0.6127362251281738, Entropy: 112.56941223144531, Temp: 1.3183095455169678, KL: 79.31968688964844, Loss: 0.12692399322986603, Learning Rate: 0.020999999999999998\n",
      "Epoch [2117/20000], Bound: 0.7149316072463989, Entropy: 109.96849060058594, Temp: 1.3170186281204224, KL: 98.88003540039062, Loss: 0.163202702999115, Learning Rate: 0.020999999999999998\n",
      "Epoch [2118/20000], Bound: 0.6634718775749207, Entropy: 107.9443130493164, Temp: 1.315812587738037, KL: 86.42290496826172, Loss: 0.15255843102931976, Learning Rate: 0.020999999999999998\n",
      "Epoch [2119/20000], Bound: 0.62986820936203, Entropy: 107.23580169677734, Temp: 1.314632773399353, KL: 86.91849517822266, Loss: 0.11501731723546982, Learning Rate: 0.020999999999999998\n",
      "Epoch [2120/20000], Bound: 0.6269723176956177, Entropy: 105.77400970458984, Temp: 1.3137493133544922, KL: 85.96936798095703, Loss: 0.1155141070485115, Learning Rate: 0.020999999999999998\n",
      "Epoch [2121/20000], Bound: 0.6786491274833679, Entropy: 109.68961334228516, Temp: 1.3131206035614014, KL: 96.82695770263672, Loss: 0.12913158535957336, Learning Rate: 0.020999999999999998\n",
      "Epoch [2122/20000], Bound: 0.6103344559669495, Entropy: 110.2338638305664, Temp: 1.312733769416809, KL: 81.1758041381836, Loss: 0.11670936644077301, Learning Rate: 0.020999999999999998\n",
      "Epoch [2123/20000], Bound: 0.641287088394165, Entropy: 118.05406188964844, Temp: 1.3124960660934448, KL: 89.27471923828125, Loss: 0.1175919696688652, Learning Rate: 0.020999999999999998\n",
      "Epoch [2124/20000], Bound: 0.6311739683151245, Entropy: 119.36618041992188, Temp: 1.312469720840454, KL: 81.09619140625, Loss: 0.13821880519390106, Learning Rate: 0.020999999999999998\n",
      "Epoch [2125/20000], Bound: 0.656676173210144, Entropy: 121.04053497314453, Temp: 1.3124024868011475, KL: 85.0542984008789, Loss: 0.1499578207731247, Learning Rate: 0.020999999999999998\n",
      "Epoch [2126/20000], Bound: 0.6445099115371704, Entropy: 118.24161529541016, Temp: 1.312258005142212, KL: 89.96410369873047, Loss: 0.11831279844045639, Learning Rate: 0.020999999999999998\n",
      "Epoch [2127/20000], Bound: 0.5756415724754333, Entropy: 121.5979232788086, Temp: 1.312317967414856, KL: 74.8971939086914, Loss: 0.10647329688072205, Learning Rate: 0.020999999999999998\n",
      "Epoch [2128/20000], Bound: 0.6459736227989197, Entropy: 118.50849151611328, Temp: 1.3124921321868896, KL: 89.66158294677734, Loss: 0.12104516476392746, Learning Rate: 0.020999999999999998\n",
      "Epoch [2129/20000], Bound: 0.660660445690155, Entropy: 122.81719207763672, Temp: 1.3128161430358887, KL: 90.97502899169922, Loss: 0.1317458301782608, Learning Rate: 0.020999999999999998\n",
      "Epoch [2130/20000], Bound: 0.6356817483901978, Entropy: 111.88651275634766, Temp: 1.3132116794586182, KL: 80.1880874633789, Loss: 0.1464499831199646, Learning Rate: 0.020999999999999998\n",
      "Epoch [2131/20000], Bound: 0.6452051401138306, Entropy: 111.67958068847656, Temp: 1.3134552240371704, KL: 84.89689636230469, Loss: 0.13852566480636597, Learning Rate: 0.020999999999999998\n",
      "Epoch [2132/20000], Bound: 0.6912007331848145, Entropy: 112.41549682617188, Temp: 1.3136669397354126, KL: 91.50672912597656, Loss: 0.16347286105155945, Learning Rate: 0.020999999999999998\n",
      "Epoch [2133/20000], Bound: 0.5850808620452881, Entropy: 114.19630432128906, Temp: 1.313744068145752, KL: 79.21597290039062, Loss: 0.09933754056692123, Learning Rate: 0.020999999999999998\n",
      "Epoch [2134/20000], Bound: 0.5983806848526001, Entropy: 116.51935577392578, Temp: 1.3140302896499634, KL: 82.7496566772461, Loss: 0.09897718578577042, Learning Rate: 0.020999999999999998\n",
      "Epoch [2135/20000], Bound: 0.6165912747383118, Entropy: 113.1406021118164, Temp: 1.314543604850769, KL: 84.77840423583984, Loss: 0.10956834256649017, Learning Rate: 0.020999999999999998\n",
      "Epoch [2136/20000], Bound: 0.605659544467926, Entropy: 118.1891860961914, Temp: 1.3152036666870117, KL: 79.5589370727539, Loss: 0.11850970983505249, Learning Rate: 0.020999999999999998\n",
      "Epoch [2137/20000], Bound: 0.6307695508003235, Entropy: 120.06351470947266, Temp: 1.3158752918243408, KL: 88.0958480834961, Loss: 0.11166150122880936, Learning Rate: 0.020999999999999998\n",
      "Epoch [2138/20000], Bound: 0.6510784029960632, Entropy: 127.328857421875, Temp: 1.3166942596435547, KL: 89.21281433105469, Loss: 0.12881502509117126, Learning Rate: 0.020999999999999998\n",
      "Epoch [2139/20000], Bound: 0.631355881690979, Entropy: 127.36914825439453, Temp: 1.3175318241119385, KL: 86.10832977294922, Loss: 0.12006431072950363, Learning Rate: 0.020999999999999998\n",
      "Epoch [2140/20000], Bound: 0.5894576907157898, Entropy: 126.7778091430664, Temp: 1.3184163570404053, KL: 78.96134185791016, Loss: 0.10519050061702728, Learning Rate: 0.020999999999999998\n",
      "Epoch [2141/20000], Bound: 0.6699171662330627, Entropy: 124.96768951416016, Temp: 1.319376826286316, KL: 96.34552764892578, Loss: 0.1224738284945488, Learning Rate: 0.020999999999999998\n",
      "Epoch [2142/20000], Bound: 0.6468753814697266, Entropy: 116.60324096679688, Temp: 1.320453405380249, KL: 85.67277526855469, Loss: 0.13835106790065765, Learning Rate: 0.020999999999999998\n",
      "Epoch [2143/20000], Bound: 0.5848430395126343, Entropy: 117.08675384521484, Temp: 1.3214120864868164, KL: 72.34101104736328, Loss: 0.1261342614889145, Learning Rate: 0.020999999999999998\n",
      "Epoch [2144/20000], Bound: 0.6129002571105957, Entropy: 110.15724182128906, Temp: 1.3222086429595947, KL: 74.24188232421875, Loss: 0.14679284393787384, Learning Rate: 0.020999999999999998\n",
      "Epoch [2145/20000], Bound: 0.615913450717926, Entropy: 111.31392669677734, Temp: 1.3227286338806152, KL: 85.12793731689453, Loss: 0.10874422639608383, Learning Rate: 0.020999999999999998\n",
      "Epoch [2146/20000], Bound: 0.591797947883606, Entropy: 109.79607391357422, Temp: 1.3233911991119385, KL: 79.2911605834961, Loss: 0.10688045620918274, Learning Rate: 0.020999999999999998\n",
      "Epoch [2147/20000], Bound: 0.6101639866828918, Entropy: 113.30500030517578, Temp: 1.3241349458694458, KL: 79.1886215209961, Loss: 0.12557971477508545, Learning Rate: 0.020999999999999998\n",
      "Epoch [2148/20000], Bound: 0.5811507701873779, Entropy: 117.51451873779297, Temp: 1.3248111009597778, KL: 80.07720184326172, Loss: 0.0937361791729927, Learning Rate: 0.020999999999999998\n",
      "Epoch [2149/20000], Bound: 0.6446806192398071, Entropy: 120.43296813964844, Temp: 1.3256720304489136, KL: 87.10673522949219, Loss: 0.1313762217760086, Learning Rate: 0.020999999999999998\n",
      "Epoch [2150/20000], Bound: 0.591476559638977, Entropy: 125.12175750732422, Temp: 1.3264915943145752, KL: 78.24835968017578, Loss: 0.11089673638343811, Learning Rate: 0.020999999999999998\n",
      "Epoch [2151/20000], Bound: 0.6095919013023376, Entropy: 121.0789794921875, Temp: 1.3273308277130127, KL: 77.09349060058594, Loss: 0.13330279290676117, Learning Rate: 0.020999999999999998\n",
      "Epoch [2152/20000], Bound: 0.5524915456771851, Entropy: 121.52442169189453, Temp: 1.3280086517333984, KL: 72.81620025634766, Loss: 0.09430469572544098, Learning Rate: 0.020999999999999998\n",
      "Epoch [2153/20000], Bound: 0.5919232964515686, Entropy: 127.38765716552734, Temp: 1.328786015510559, KL: 79.16588592529297, Loss: 0.10816913098096848, Learning Rate: 0.020999999999999998\n",
      "Epoch [2154/20000], Bound: 0.6117847561836243, Entropy: 126.2512435913086, Temp: 1.3296136856079102, KL: 74.88762664794922, Loss: 0.14407309889793396, Learning Rate: 0.020999999999999998\n",
      "Epoch [2155/20000], Bound: 0.6286657452583313, Entropy: 127.2253646850586, Temp: 1.330174207687378, KL: 80.35095977783203, Loss: 0.14076761901378632, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2156/20000], Bound: 0.6442632079124451, Entropy: 128.94175720214844, Temp: 1.3305761814117432, KL: 86.45516967773438, Loss: 0.13409827649593353, Learning Rate: 0.020999999999999998\n",
      "Epoch [2157/20000], Bound: 0.609170138835907, Entropy: 123.9083251953125, Temp: 1.330946922302246, KL: 81.1785888671875, Loss: 0.11796601861715317, Learning Rate: 0.020999999999999998\n",
      "Epoch [2158/20000], Bound: 0.6326025724411011, Entropy: 121.7052001953125, Temp: 1.3313528299331665, KL: 84.64022827148438, Loss: 0.12886826694011688, Learning Rate: 0.020999999999999998\n",
      "Epoch [2159/20000], Bound: 0.6296803951263428, Entropy: 118.12747955322266, Temp: 1.3317452669143677, KL: 91.91600799560547, Loss: 0.09858999401330948, Learning Rate: 0.020999999999999998\n",
      "Epoch [2160/20000], Bound: 0.5808078050613403, Entropy: 115.10611724853516, Temp: 1.3324226140975952, KL: 81.67728424072266, Loss: 0.08840028196573257, Learning Rate: 0.020999999999999998\n",
      "Epoch [2161/20000], Bound: 0.6080406308174133, Entropy: 117.76835632324219, Temp: 1.3333309888839722, KL: 82.80648803710938, Loss: 0.11104105412960052, Learning Rate: 0.020999999999999998\n",
      "Epoch [2162/20000], Bound: 0.7070196270942688, Entropy: 114.45339965820312, Temp: 1.3342859745025635, KL: 88.00929260253906, Loss: 0.19782908260822296, Learning Rate: 0.020999999999999998\n",
      "Epoch [2163/20000], Bound: 0.5887919664382935, Entropy: 113.81317901611328, Temp: 1.334713339805603, KL: 82.3085708618164, Loss: 0.09408662468194962, Learning Rate: 0.020999999999999998\n",
      "Epoch [2164/20000], Bound: 0.6912906169891357, Entropy: 117.54999542236328, Temp: 1.3353557586669922, KL: 89.8757553100586, Loss: 0.1730862408876419, Learning Rate: 0.020999999999999998\n",
      "Epoch [2165/20000], Bound: 0.6743775606155396, Entropy: 116.70404052734375, Temp: 1.3356906175613403, KL: 85.22076416015625, Loss: 0.17177249491214752, Learning Rate: 0.020999999999999998\n",
      "Epoch [2166/20000], Bound: 0.6418260335922241, Entropy: 114.06932067871094, Temp: 1.3357080221176147, KL: 87.02005004882812, Loss: 0.1301538199186325, Learning Rate: 0.020999999999999998\n",
      "Epoch [2167/20000], Bound: 0.6091160178184509, Entropy: 113.90185546875, Temp: 1.3357577323913574, KL: 74.94509887695312, Loss: 0.14186878502368927, Learning Rate: 0.020999999999999998\n",
      "Epoch [2168/20000], Bound: 0.615044116973877, Entropy: 114.3353500366211, Temp: 1.3356225490570068, KL: 79.25176239013672, Loss: 0.13169100880622864, Learning Rate: 0.020999999999999998\n",
      "Epoch [2169/20000], Bound: 0.6611393094062805, Entropy: 116.86997985839844, Temp: 1.3354425430297852, KL: 80.88291931152344, Loss: 0.17359569668769836, Learning Rate: 0.020999999999999998\n",
      "Epoch [2170/20000], Bound: 0.6659059524536133, Entropy: 115.48056030273438, Temp: 1.334937334060669, KL: 85.03752136230469, Loss: 0.16311976313591003, Learning Rate: 0.020999999999999998\n",
      "Epoch [2171/20000], Bound: 0.6029893159866333, Entropy: 114.72349548339844, Temp: 1.3342589139938354, KL: 79.57321166992188, Loss: 0.11824753135442734, Learning Rate: 0.020999999999999998\n",
      "Epoch [2172/20000], Bound: 0.6181721091270447, Entropy: 119.76829528808594, Temp: 1.333695650100708, KL: 83.52287292480469, Loss: 0.11860568821430206, Learning Rate: 0.020999999999999998\n",
      "Epoch [2173/20000], Bound: 0.6143923997879028, Entropy: 124.66587829589844, Temp: 1.3332759141921997, KL: 75.00794982910156, Loss: 0.1466590315103531, Learning Rate: 0.020999999999999998\n",
      "Epoch [2174/20000], Bound: 0.6804813146591187, Entropy: 125.41500091552734, Temp: 1.3326878547668457, KL: 86.5571060180664, Loss: 0.17307761311531067, Learning Rate: 0.020999999999999998\n",
      "Epoch [2175/20000], Bound: 0.6940948963165283, Entropy: 125.63229370117188, Temp: 1.331884503364563, KL: 98.38919067382812, Loss: 0.14377158880233765, Learning Rate: 0.020999999999999998\n",
      "Epoch [2176/20000], Bound: 0.6529169678688049, Entropy: 128.55250549316406, Temp: 1.331217885017395, KL: 77.78684997558594, Loss: 0.17589835822582245, Learning Rate: 0.020999999999999998\n",
      "Epoch [2177/20000], Bound: 0.6309670209884644, Entropy: 121.96154022216797, Temp: 1.3302321434020996, KL: 84.70714569091797, Loss: 0.12677229940891266, Learning Rate: 0.020999999999999998\n",
      "Epoch [2178/20000], Bound: 0.6139782071113586, Entropy: 119.01343536376953, Temp: 1.3293898105621338, KL: 83.04676055908203, Loss: 0.11556904762983322, Learning Rate: 0.020999999999999998\n",
      "Epoch [2179/20000], Bound: 0.6333354711532593, Entropy: 115.21296691894531, Temp: 1.3287444114685059, KL: 81.11643981933594, Loss: 0.1425253450870514, Learning Rate: 0.020999999999999998\n",
      "Epoch [2180/20000], Bound: 0.6433783173561096, Entropy: 110.37197875976562, Temp: 1.3280575275421143, KL: 84.45762634277344, Loss: 0.14032897353172302, Learning Rate: 0.020999999999999998\n",
      "Epoch [2181/20000], Bound: 0.6405889391899109, Entropy: 108.90298461914062, Temp: 1.3273862600326538, KL: 87.41094970703125, Loss: 0.12618857622146606, Learning Rate: 0.020999999999999998\n",
      "Epoch [2182/20000], Bound: 0.5858007073402405, Entropy: 111.45191192626953, Temp: 1.3268646001815796, KL: 76.87108612060547, Loss: 0.11059591174125671, Learning Rate: 0.020999999999999998\n",
      "Epoch [2183/20000], Bound: 0.6941636800765991, Entropy: 106.91875457763672, Temp: 1.3264843225479126, KL: 103.98819732666016, Loss: 0.12180222570896149, Learning Rate: 0.020999999999999998\n",
      "Epoch [2184/20000], Bound: 0.6434029936790466, Entropy: 116.08580017089844, Temp: 1.3264225721359253, KL: 83.31611633300781, Loss: 0.14443188905715942, Learning Rate: 0.020999999999999998\n",
      "Epoch [2185/20000], Bound: 0.6253357529640198, Entropy: 111.93135833740234, Temp: 1.3262749910354614, KL: 90.98625946044922, Loss: 0.09675774723291397, Learning Rate: 0.020999999999999998\n",
      "Epoch [2186/20000], Bound: 0.5580559372901917, Entropy: 115.79732513427734, Temp: 1.3264833688735962, KL: 75.92205047607422, Loss: 0.08762674778699875, Learning Rate: 0.020999999999999998\n",
      "Epoch [2187/20000], Bound: 0.6451718807220459, Entropy: 118.78447723388672, Temp: 1.3269274234771729, KL: 95.62598419189453, Loss: 0.09997504204511642, Learning Rate: 0.020999999999999998\n",
      "Epoch [2188/20000], Bound: 0.6052641868591309, Entropy: 122.03823852539062, Temp: 1.3276885747909546, KL: 73.20863342285156, Loss: 0.14364877343177795, Learning Rate: 0.020999999999999998\n",
      "Epoch [2189/20000], Bound: 0.5860669612884521, Entropy: 126.69843292236328, Temp: 1.3281747102737427, KL: 78.07816314697266, Loss: 0.10647058486938477, Learning Rate: 0.020999999999999998\n",
      "Epoch [2190/20000], Bound: 0.6135775446891785, Entropy: 129.7745819091797, Temp: 1.3287440538406372, KL: 83.3795166015625, Loss: 0.11382512748241425, Learning Rate: 0.020999999999999998\n",
      "Epoch [2191/20000], Bound: 0.6927158236503601, Entropy: 129.58970642089844, Temp: 1.329387903213501, KL: 101.72653198242188, Loss: 0.12922877073287964, Learning Rate: 0.020999999999999998\n",
      "Epoch [2192/20000], Bound: 0.6192294359207153, Entropy: 127.79073333740234, Temp: 1.330167531967163, KL: 83.9594497680664, Loss: 0.1175580844283104, Learning Rate: 0.020999999999999998\n",
      "Epoch [2193/20000], Bound: 0.654180109500885, Entropy: 123.79727935791016, Temp: 1.3309764862060547, KL: 100.52767181396484, Loss: 0.09178601950407028, Learning Rate: 0.020999999999999998\n",
      "Epoch [2194/20000], Bound: 0.6157159209251404, Entropy: 122.22603607177734, Temp: 1.3321681022644043, KL: 81.45772552490234, Loss: 0.12366334348917007, Learning Rate: 0.020999999999999998\n",
      "Epoch [2195/20000], Bound: 0.6865585446357727, Entropy: 115.66685485839844, Temp: 1.3332726955413818, KL: 100.90914916992188, Loss: 0.12609516084194183, Learning Rate: 0.020999999999999998\n",
      "Epoch [2196/20000], Bound: 0.6228015422821045, Entropy: 114.05868530273438, Temp: 1.3344751596450806, KL: 84.10557556152344, Loss: 0.12123538553714752, Learning Rate: 0.020999999999999998\n",
      "Epoch [2197/20000], Bound: 0.6518514752388, Entropy: 110.01165008544922, Temp: 1.3356313705444336, KL: 92.38164520263672, Loss: 0.12064369022846222, Learning Rate: 0.020999999999999998\n",
      "Epoch [2198/20000], Bound: 0.6550164818763733, Entropy: 99.47550201416016, Temp: 1.3368327617645264, KL: 99.96173858642578, Loss: 0.09584934264421463, Learning Rate: 0.020999999999999998\n",
      "Epoch [2199/20000], Bound: 0.6685233116149902, Entropy: 96.94342803955078, Temp: 1.3383326530456543, KL: 103.79784393310547, Loss: 0.09633062034845352, Learning Rate: 0.020999999999999998\n",
      "Epoch [2200/20000], Bound: 0.6187639236450195, Entropy: 100.57935333251953, Temp: 1.3401302099227905, KL: 87.64997100830078, Loss: 0.10467057675123215, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2201/20000], Bound: 0.6000338196754456, Entropy: 99.82161712646484, Temp: 1.3419749736785889, KL: 88.55725860595703, Loss: 0.08280489593744278, Learning Rate: 0.020999999999999998\n",
      "Epoch [2202/20000], Bound: 0.5903717875480652, Entropy: 106.99028778076172, Temp: 1.34403657913208, KL: 78.7175521850586, Loss: 0.11022552102804184, Learning Rate: 0.020999999999999998\n",
      "Epoch [2203/20000], Bound: 0.598064124584198, Entropy: 112.71635437011719, Temp: 1.345978021621704, KL: 79.24020385742188, Loss: 0.11606193333864212, Learning Rate: 0.020999999999999998\n",
      "Epoch [2204/20000], Bound: 0.6109256744384766, Entropy: 121.8364486694336, Temp: 1.3477689027786255, KL: 82.95032501220703, Loss: 0.11529787629842758, Learning Rate: 0.020999999999999998\n",
      "Epoch [2205/20000], Bound: 0.5463644862174988, Entropy: 125.10843658447266, Temp: 1.3494654893875122, KL: 70.3843002319336, Loss: 0.10003414750099182, Learning Rate: 0.020999999999999998\n",
      "Epoch [2206/20000], Bound: 0.546576738357544, Entropy: 135.83343505859375, Temp: 1.3510595560073853, KL: 72.79011535644531, Loss: 0.09148893505334854, Learning Rate: 0.020999999999999998\n",
      "Epoch [2207/20000], Bound: 0.5847821831703186, Entropy: 137.459228515625, Temp: 1.3526527881622314, KL: 76.193115234375, Loss: 0.11516033858060837, Learning Rate: 0.020999999999999998\n",
      "Epoch [2208/20000], Bound: 0.6547721028327942, Entropy: 139.24005126953125, Temp: 1.354093074798584, KL: 83.9342041015625, Loss: 0.15776649117469788, Learning Rate: 0.020999999999999998\n",
      "Epoch [2209/20000], Bound: 0.6204757690429688, Entropy: 137.96710205078125, Temp: 1.35515296459198, KL: 77.55755615234375, Loss: 0.14577113091945648, Learning Rate: 0.020999999999999998\n",
      "Epoch [2210/20000], Bound: 0.6491984128952026, Entropy: 136.32211303710938, Temp: 1.3558892011642456, KL: 87.99577331542969, Loss: 0.13710220158100128, Learning Rate: 0.020999999999999998\n",
      "Epoch [2211/20000], Bound: 0.574007511138916, Entropy: 132.7838897705078, Temp: 1.3565075397491455, KL: 67.16966247558594, Loss: 0.13850143551826477, Learning Rate: 0.020999999999999998\n",
      "Epoch [2212/20000], Bound: 0.6152430772781372, Entropy: 127.38898468017578, Temp: 1.3567830324172974, KL: 92.8470230102539, Loss: 0.08431659638881683, Learning Rate: 0.020999999999999998\n",
      "Epoch [2213/20000], Bound: 0.6717989444732666, Entropy: 120.95980072021484, Temp: 1.3574391603469849, KL: 79.80445098876953, Loss: 0.19175952672958374, Learning Rate: 0.020999999999999998\n",
      "Epoch [2214/20000], Bound: 0.6784554719924927, Entropy: 111.08821105957031, Temp: 1.3574941158294678, KL: 83.43408203125, Loss: 0.1857011318206787, Learning Rate: 0.020999999999999998\n",
      "Epoch [2215/20000], Bound: 0.5886480808258057, Entropy: 102.14048767089844, Temp: 1.3570911884307861, KL: 75.12397766113281, Loss: 0.12334304302930832, Learning Rate: 0.020999999999999998\n",
      "Epoch [2216/20000], Bound: 0.6317440867424011, Entropy: 97.45064544677734, Temp: 1.356650948524475, KL: 89.6349105834961, Loss: 0.11295057833194733, Learning Rate: 0.020999999999999998\n",
      "Epoch [2217/20000], Bound: 0.6231604814529419, Entropy: 98.70703125, Temp: 1.3564083576202393, KL: 90.42120361328125, Loss: 0.10122217237949371, Learning Rate: 0.020999999999999998\n",
      "Epoch [2218/20000], Bound: 0.621433675289154, Entropy: 102.39750671386719, Temp: 1.356442928314209, KL: 75.68174743652344, Loss: 0.15380167961120605, Learning Rate: 0.020999999999999998\n",
      "Epoch [2219/20000], Bound: 0.6022998690605164, Entropy: 113.19876098632812, Temp: 1.3561722040176392, KL: 81.66677856445312, Loss: 0.11251575499773026, Learning Rate: 0.020999999999999998\n",
      "Epoch [2220/20000], Bound: 0.6731207370758057, Entropy: 115.82213592529297, Temp: 1.3560056686401367, KL: 94.16156768798828, Loss: 0.1401030570268631, Learning Rate: 0.020999999999999998\n",
      "Epoch [2221/20000], Bound: 0.537879467010498, Entropy: 124.70394134521484, Temp: 1.3558504581451416, KL: 62.44554901123047, Loss: 0.12218404561281204, Learning Rate: 0.020999999999999998\n",
      "Epoch [2222/20000], Bound: 0.5634345412254333, Entropy: 120.67684173583984, Temp: 1.3555026054382324, KL: 71.45421600341797, Loss: 0.1125912293791771, Learning Rate: 0.020999999999999998\n",
      "Epoch [2223/20000], Bound: 0.6611937880516052, Entropy: 128.1945343017578, Temp: 1.3551582098007202, KL: 81.47331237792969, Loss: 0.1738526076078415, Learning Rate: 0.020999999999999998\n",
      "Epoch [2224/20000], Bound: 0.6443656086921692, Entropy: 129.16064453125, Temp: 1.354464054107666, KL: 78.69493103027344, Loss: 0.16615115106105804, Learning Rate: 0.020999999999999998\n",
      "Epoch [2225/20000], Bound: 0.6741615533828735, Entropy: 129.41888427734375, Temp: 1.3534823656082153, KL: 93.76478576660156, Loss: 0.14232301712036133, Learning Rate: 0.020999999999999998\n",
      "Epoch [2226/20000], Bound: 0.69834303855896, Entropy: 123.50840759277344, Temp: 1.3525776863098145, KL: 97.87294006347656, Loss: 0.15398761630058289, Learning Rate: 0.020999999999999998\n",
      "Epoch [2227/20000], Bound: 0.6504582762718201, Entropy: 117.31324768066406, Temp: 1.3516992330551147, KL: 84.5587158203125, Loss: 0.15057134628295898, Learning Rate: 0.020999999999999998\n",
      "Epoch [2228/20000], Bound: 0.6254033446311951, Entropy: 114.90953826904297, Temp: 1.350734829902649, KL: 85.79305267333984, Loss: 0.11981131881475449, Learning Rate: 0.020999999999999998\n",
      "Epoch [2229/20000], Bound: 0.6073946356773376, Entropy: 111.23812866210938, Temp: 1.3499398231506348, KL: 73.06462097167969, Loss: 0.14866256713867188, Learning Rate: 0.020999999999999998\n",
      "Epoch [2230/20000], Bound: 0.6345366835594177, Entropy: 106.41121673583984, Temp: 1.3489428758621216, KL: 94.00818634033203, Loss: 0.09851647913455963, Learning Rate: 0.020999999999999998\n",
      "Epoch [2231/20000], Bound: 0.5838579535484314, Entropy: 104.81907653808594, Temp: 1.3483686447143555, KL: 81.20381164550781, Loss: 0.09520187228918076, Learning Rate: 0.020999999999999998\n",
      "Epoch [2232/20000], Bound: 0.6192325949668884, Entropy: 107.88735961914062, Temp: 1.3480730056762695, KL: 85.41831970214844, Loss: 0.11455810070037842, Learning Rate: 0.020999999999999998\n",
      "Epoch [2233/20000], Bound: 0.5989221334457397, Entropy: 113.5640869140625, Temp: 1.347921371459961, KL: 77.47262573242188, Loss: 0.12369946390390396, Learning Rate: 0.020999999999999998\n",
      "Epoch [2234/20000], Bound: 0.5778534412384033, Entropy: 119.53628540039062, Temp: 1.3477452993392944, KL: 76.81159973144531, Loss: 0.1056322455406189, Learning Rate: 0.020999999999999998\n",
      "Epoch [2235/20000], Bound: 0.6149409413337708, Entropy: 125.9266586303711, Temp: 1.3476808071136475, KL: 70.63892364501953, Loss: 0.1649990677833557, Learning Rate: 0.020999999999999998\n",
      "Epoch [2236/20000], Bound: 0.6456955075263977, Entropy: 131.68145751953125, Temp: 1.3471953868865967, KL: 79.57240295410156, Loss: 0.16346529126167297, Learning Rate: 0.020999999999999998\n",
      "Epoch [2237/20000], Bound: 0.7062796354293823, Entropy: 141.6862335205078, Temp: 1.3464435338974, KL: 91.58143615722656, Loss: 0.1854211390018463, Learning Rate: 0.020999999999999998\n",
      "Epoch [2238/20000], Bound: 0.7950096726417542, Entropy: 138.64822387695312, Temp: 1.3454238176345825, KL: 93.78529357910156, Loss: 0.2869224548339844, Learning Rate: 0.020999999999999998\n",
      "Epoch [2239/20000], Bound: 0.6485318541526794, Entropy: 131.0819854736328, Temp: 1.343504548072815, KL: 93.11688232421875, Loss: 0.11561346054077148, Learning Rate: 0.020999999999999998\n",
      "Epoch [2240/20000], Bound: 0.6931529641151428, Entropy: 129.4143829345703, Temp: 1.3419697284698486, KL: 76.43669128417969, Loss: 0.22622625529766083, Learning Rate: 0.020999999999999998\n",
      "Epoch [2241/20000], Bound: 0.658819317817688, Entropy: 123.3768310546875, Temp: 1.3398091793060303, KL: 83.14718627929688, Loss: 0.16319166123867035, Learning Rate: 0.020999999999999998\n",
      "Epoch [2242/20000], Bound: 0.6917034387588501, Entropy: 110.25861358642578, Temp: 1.3376078605651855, KL: 86.2105941772461, Loss: 0.1875835359096527, Learning Rate: 0.020999999999999998\n",
      "Epoch [2243/20000], Bound: 0.6545714139938354, Entropy: 105.39640045166016, Temp: 1.3352336883544922, KL: 77.57933807373047, Loss: 0.17890524864196777, Learning Rate: 0.020999999999999998\n",
      "Epoch [2244/20000], Bound: 0.6676546335220337, Entropy: 100.87088012695312, Temp: 1.3326759338378906, KL: 89.26524353027344, Loss: 0.14884807169437408, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2245/20000], Bound: 0.6541186571121216, Entropy: 96.91641998291016, Temp: 1.3303004503250122, KL: 91.57186126708984, Loss: 0.1252586841583252, Learning Rate: 0.020999999999999998\n",
      "Epoch [2246/20000], Bound: 0.7207965850830078, Entropy: 98.24593353271484, Temp: 1.328291416168213, KL: 100.90682220458984, Loss: 0.1644594520330429, Learning Rate: 0.020999999999999998\n",
      "Epoch [2247/20000], Bound: 0.6381445527076721, Entropy: 94.95274353027344, Temp: 1.3264248371124268, KL: 82.6558837890625, Loss: 0.1414192020893097, Learning Rate: 0.020999999999999998\n",
      "Epoch [2248/20000], Bound: 0.634711742401123, Entropy: 101.55779266357422, Temp: 1.3246670961380005, KL: 94.62191009521484, Loss: 0.09244664758443832, Learning Rate: 0.020999999999999998\n",
      "Epoch [2249/20000], Bound: 0.6488050222396851, Entropy: 109.68085479736328, Temp: 1.3235032558441162, KL: 91.75003814697266, Loss: 0.11786863207817078, Learning Rate: 0.020999999999999998\n",
      "Epoch [2250/20000], Bound: 0.6223292350769043, Entropy: 116.94269561767578, Temp: 1.3226535320281982, KL: 77.84412384033203, Loss: 0.14278747141361237, Learning Rate: 0.020999999999999998\n",
      "Epoch [2251/20000], Bound: 0.600680947303772, Entropy: 128.297119140625, Temp: 1.3217549324035645, KL: 71.74467468261719, Loss: 0.14397287368774414, Learning Rate: 0.020999999999999998\n",
      "Epoch [2252/20000], Bound: 0.6201086640357971, Entropy: 133.7023468017578, Temp: 1.320736050605774, KL: 76.29983520507812, Loss: 0.14613480865955353, Learning Rate: 0.020999999999999998\n",
      "Epoch [2253/20000], Bound: 0.5729454159736633, Entropy: 135.16636657714844, Temp: 1.3196468353271484, KL: 74.81367492675781, Loss: 0.10509499907493591, Learning Rate: 0.020999999999999998\n",
      "Epoch [2254/20000], Bound: 0.6969358921051025, Entropy: 140.91848754882812, Temp: 1.3187886476516724, KL: 89.93002319335938, Loss: 0.17675311863422394, Learning Rate: 0.020999999999999998\n",
      "Epoch [2255/20000], Bound: 0.6571019887924194, Entropy: 142.9042510986328, Temp: 1.3177769184112549, KL: 85.1080322265625, Loss: 0.15098197758197784, Learning Rate: 0.020999999999999998\n",
      "Epoch [2256/20000], Bound: 0.649337649345398, Entropy: 138.0693359375, Temp: 1.3167613744735718, KL: 83.79548645019531, Loss: 0.1475495547056198, Learning Rate: 0.020999999999999998\n",
      "Epoch [2257/20000], Bound: 0.62015700340271, Entropy: 133.55970764160156, Temp: 1.315754771232605, KL: 80.527587890625, Loss: 0.12952031195163727, Learning Rate: 0.020999999999999998\n",
      "Epoch [2258/20000], Bound: 0.6506551504135132, Entropy: 127.50747680664062, Temp: 1.3148555755615234, KL: 76.5941162109375, Loss: 0.1760643720626831, Learning Rate: 0.020999999999999998\n",
      "Epoch [2259/20000], Bound: 0.6169003844261169, Entropy: 118.1694107055664, Temp: 1.313671588897705, KL: 81.16913604736328, Loss: 0.12349148839712143, Learning Rate: 0.020999999999999998\n",
      "Epoch [2260/20000], Bound: 0.6599627137184143, Entropy: 116.33550262451172, Temp: 1.3126686811447144, KL: 90.41129302978516, Loss: 0.13311848044395447, Learning Rate: 0.020999999999999998\n",
      "Epoch [2261/20000], Bound: 0.6738134026527405, Entropy: 110.19524383544922, Temp: 1.3118579387664795, KL: 91.73873138427734, Loss: 0.14297625422477722, Learning Rate: 0.020999999999999998\n",
      "Epoch [2262/20000], Bound: 0.6005014181137085, Entropy: 100.52776336669922, Temp: 1.3111640214920044, KL: 78.0457534790039, Loss: 0.11860772222280502, Learning Rate: 0.020999999999999998\n",
      "Epoch [2263/20000], Bound: 0.6594105958938599, Entropy: 101.83340454101562, Temp: 1.3106091022491455, KL: 86.9002685546875, Loss: 0.1455889642238617, Learning Rate: 0.020999999999999998\n",
      "Epoch [2264/20000], Bound: 0.6820387840270996, Entropy: 101.97715759277344, Temp: 1.31007719039917, KL: 89.96923828125, Loss: 0.15851664543151855, Learning Rate: 0.020999999999999998\n",
      "Epoch [2265/20000], Bound: 0.6257120370864868, Entropy: 108.42111206054688, Temp: 1.3095066547393799, KL: 83.102783203125, Loss: 0.12452920526266098, Learning Rate: 0.020999999999999998\n",
      "Epoch [2266/20000], Bound: 0.6322667598724365, Entropy: 106.5916976928711, Temp: 1.3090769052505493, KL: 83.62728118896484, Loss: 0.12922200560569763, Learning Rate: 0.020999999999999998\n",
      "Epoch [2267/20000], Bound: 0.695115864276886, Entropy: 115.34244537353516, Temp: 1.3087456226348877, KL: 99.78374481201172, Loss: 0.13546936213970184, Learning Rate: 0.020999999999999998\n",
      "Epoch [2268/20000], Bound: 0.6714659333229065, Entropy: 114.05287170410156, Temp: 1.3086272478103638, KL: 102.53837585449219, Loss: 0.09861103445291519, Learning Rate: 0.020999999999999998\n",
      "Epoch [2269/20000], Bound: 0.6415510177612305, Entropy: 118.11900329589844, Temp: 1.309000849723816, KL: 80.96232604980469, Loss: 0.14906521141529083, Learning Rate: 0.020999999999999998\n",
      "Epoch [2270/20000], Bound: 0.6312035918235779, Entropy: 116.43673706054688, Temp: 1.3092172145843506, KL: 89.30064392089844, Loss: 0.10647512227296829, Learning Rate: 0.020999999999999998\n",
      "Epoch [2271/20000], Bound: 0.5959773659706116, Entropy: 119.71109008789062, Temp: 1.3096987009048462, KL: 72.93019104003906, Loss: 0.13346946239471436, Learning Rate: 0.020999999999999998\n",
      "Epoch [2272/20000], Bound: 0.5947533249855042, Entropy: 118.71784973144531, Temp: 1.3100348711013794, KL: 80.62738037109375, Loss: 0.10292354971170425, Learning Rate: 0.020999999999999998\n",
      "Epoch [2273/20000], Bound: 0.6051501631736755, Entropy: 123.58003997802734, Temp: 1.3105578422546387, KL: 85.70709991455078, Loss: 0.09392809867858887, Learning Rate: 0.020999999999999998\n",
      "Epoch [2274/20000], Bound: 0.6745558381080627, Entropy: 120.60942840576172, Temp: 1.3113716840744019, KL: 98.12162017822266, Loss: 0.11937500536441803, Learning Rate: 0.020999999999999998\n",
      "Epoch [2275/20000], Bound: 0.588180422782898, Entropy: 125.28022003173828, Temp: 1.3123811483383179, KL: 70.8164291381836, Loss: 0.13417764008045197, Learning Rate: 0.020999999999999998\n",
      "Epoch [2276/20000], Bound: 0.5685692429542542, Entropy: 119.50140380859375, Temp: 1.3131582736968994, KL: 75.80058288574219, Loss: 0.09636897593736649, Learning Rate: 0.020999999999999998\n",
      "Epoch [2277/20000], Bound: 0.6678365468978882, Entropy: 124.96924591064453, Temp: 1.3140711784362793, KL: 78.02069854736328, Loss: 0.18900462985038757, Learning Rate: 0.020999999999999998\n",
      "Epoch [2278/20000], Bound: 0.5890955328941345, Entropy: 120.7606430053711, Temp: 1.3144410848617554, KL: 76.88800811767578, Loss: 0.11220069974660873, Learning Rate: 0.020999999999999998\n",
      "Epoch [2279/20000], Bound: 0.6949832439422607, Entropy: 125.43824005126953, Temp: 1.3148746490478516, KL: 89.10225677490234, Loss: 0.17707917094230652, Learning Rate: 0.020999999999999998\n",
      "Epoch [2280/20000], Bound: 0.6685341000556946, Entropy: 127.26034545898438, Temp: 1.3150205612182617, KL: 90.56626892089844, Loss: 0.14217950403690338, Learning Rate: 0.020999999999999998\n",
      "Epoch [2281/20000], Bound: 0.6585602164268494, Entropy: 123.76647186279297, Temp: 1.3151750564575195, KL: 69.71085357666016, Loss: 0.21070952713489532, Learning Rate: 0.020999999999999998\n",
      "Epoch [2282/20000], Bound: 0.6696858406066895, Entropy: 119.45875549316406, Temp: 1.3146084547042847, KL: 87.57830810546875, Loss: 0.15473337471485138, Learning Rate: 0.020999999999999998\n",
      "Epoch [2283/20000], Bound: 0.5678479075431824, Entropy: 117.09292602539062, Temp: 1.3139985799789429, KL: 77.11705017089844, Loss: 0.09077846258878708, Learning Rate: 0.020999999999999998\n",
      "Epoch [2284/20000], Bound: 0.6382294297218323, Entropy: 114.2176742553711, Temp: 1.3137197494506836, KL: 82.65084075927734, Loss: 0.139797642827034, Learning Rate: 0.020999999999999998\n",
      "Epoch [2285/20000], Bound: 0.6696686148643494, Entropy: 116.82552337646484, Temp: 1.31342613697052, KL: 86.74967193603516, Loss: 0.1576908975839615, Learning Rate: 0.020999999999999998\n",
      "Epoch [2286/20000], Bound: 0.593803346157074, Entropy: 112.16535186767578, Temp: 1.3130336999893188, KL: 78.81751251220703, Loss: 0.10929246991872787, Learning Rate: 0.020999999999999998\n",
      "Epoch [2287/20000], Bound: 0.6529690623283386, Entropy: 112.70669555664062, Temp: 1.3128273487091064, KL: 87.61181640625, Loss: 0.13631999492645264, Learning Rate: 0.020999999999999998\n",
      "Epoch [2288/20000], Bound: 0.6597279906272888, Entropy: 112.23455810546875, Temp: 1.3126804828643799, KL: 90.30458068847656, Loss: 0.1332743763923645, Learning Rate: 0.020999999999999998\n",
      "Epoch [2289/20000], Bound: 0.6202705502510071, Entropy: 117.11216735839844, Temp: 1.3126389980316162, KL: 81.60072326660156, Loss: 0.12513190507888794, Learning Rate: 0.020999999999999998\n",
      "Epoch [2290/20000], Bound: 0.6432740092277527, Entropy: 117.0948486328125, Temp: 1.3126593828201294, KL: 85.99269104003906, Loss: 0.13220442831516266, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2291/20000], Bound: 0.6074196100234985, Entropy: 123.5167465209961, Temp: 1.3127301931381226, KL: 82.9998550415039, Loss: 0.10683690756559372, Learning Rate: 0.020999999999999998\n",
      "Epoch [2292/20000], Bound: 0.608680009841919, Entropy: 126.84015655517578, Temp: 1.3130064010620117, KL: 79.34038543701172, Loss: 0.12207488715648651, Learning Rate: 0.020999999999999998\n",
      "Epoch [2293/20000], Bound: 0.7004818916320801, Entropy: 125.43067932128906, Temp: 1.3133103847503662, KL: 100.2037353515625, Loss: 0.14082057774066925, Learning Rate: 0.020999999999999998\n",
      "Epoch [2294/20000], Bound: 0.6721082329750061, Entropy: 132.59852600097656, Temp: 1.3137210607528687, KL: 78.32025146484375, Loss: 0.19248218834400177, Learning Rate: 0.020999999999999998\n",
      "Epoch [2295/20000], Bound: 0.6638671159744263, Entropy: 129.3538818359375, Temp: 1.3136160373687744, KL: 78.4940185546875, Loss: 0.18284161388874054, Learning Rate: 0.020999999999999998\n",
      "Epoch [2296/20000], Bound: 0.5955190658569336, Entropy: 125.8193130493164, Temp: 1.3131179809570312, KL: 68.73983001708984, Loss: 0.14936630427837372, Learning Rate: 0.020999999999999998\n",
      "Epoch [2297/20000], Bound: 0.614037036895752, Entropy: 118.72904205322266, Temp: 1.312396764755249, KL: 81.28644561767578, Loss: 0.11997295171022415, Learning Rate: 0.020999999999999998\n",
      "Epoch [2298/20000], Bound: 0.5858744382858276, Entropy: 118.06665802001953, Temp: 1.311841368675232, KL: 78.91828155517578, Loss: 0.10098976641893387, Learning Rate: 0.020999999999999998\n",
      "Epoch [2299/20000], Bound: 0.6796457171440125, Entropy: 117.14510345458984, Temp: 1.311556339263916, KL: 99.93987274169922, Loss: 0.1180880144238472, Learning Rate: 0.020999999999999998\n",
      "Epoch [2300/20000], Bound: 0.6498017311096191, Entropy: 116.31220245361328, Temp: 1.3116059303283691, KL: 86.31189727783203, Loss: 0.1377227008342743, Learning Rate: 0.020999999999999998\n",
      "Epoch [2301/20000], Bound: 0.6420332789421082, Entropy: 112.52509307861328, Temp: 1.311667561531067, KL: 86.23090362548828, Loss: 0.12984628975391388, Learning Rate: 0.020999999999999998\n",
      "Epoch [2302/20000], Bound: 0.5910524725914001, Entropy: 112.36610412597656, Temp: 1.311798095703125, KL: 77.21217346191406, Loss: 0.11254654824733734, Learning Rate: 0.020999999999999998\n",
      "Epoch [2303/20000], Bound: 0.642670214176178, Entropy: 113.35306549072266, Temp: 1.3120224475860596, KL: 82.64368438720703, Loss: 0.1442384570837021, Learning Rate: 0.020999999999999998\n",
      "Epoch [2304/20000], Bound: 0.7190417647361755, Entropy: 113.86949920654297, Temp: 1.3121517896652222, KL: 99.55313873291016, Loss: 0.16455860435962677, Learning Rate: 0.020999999999999998\n",
      "Epoch [2305/20000], Bound: 0.6465519666671753, Entropy: 115.21170806884766, Temp: 1.3122292757034302, KL: 88.6551284790039, Loss: 0.12544868886470795, Learning Rate: 0.020999999999999998\n",
      "Epoch [2306/20000], Bound: 0.6004718542098999, Entropy: 114.19169616699219, Temp: 1.3124322891235352, KL: 82.20454406738281, Loss: 0.10289837419986725, Learning Rate: 0.020999999999999998\n",
      "Epoch [2307/20000], Bound: 0.6207000613212585, Entropy: 111.49958801269531, Temp: 1.3128509521484375, KL: 78.9647216796875, Loss: 0.13563792407512665, Learning Rate: 0.020999999999999998\n",
      "Epoch [2308/20000], Bound: 0.5833845138549805, Entropy: 114.56886291503906, Temp: 1.3131766319274902, KL: 78.89445495605469, Loss: 0.09883759915828705, Learning Rate: 0.020999999999999998\n",
      "Epoch [2309/20000], Bound: 0.6156145334243774, Entropy: 118.72962951660156, Temp: 1.3137000799179077, KL: 84.66847229003906, Loss: 0.1088741347193718, Learning Rate: 0.020999999999999998\n",
      "Epoch [2310/20000], Bound: 0.6175485849380493, Entropy: 120.01060485839844, Temp: 1.3143857717514038, KL: 84.96640014648438, Loss: 0.10980082303285599, Learning Rate: 0.020999999999999998\n",
      "Epoch [2311/20000], Bound: 0.5855580568313599, Entropy: 125.85353088378906, Temp: 1.315212607383728, KL: 82.38328552246094, Loss: 0.0879564955830574, Learning Rate: 0.020999999999999998\n",
      "Epoch [2312/20000], Bound: 0.6279057264328003, Entropy: 124.83839416503906, Temp: 1.3163076639175415, KL: 87.40762329101562, Loss: 0.11139023303985596, Learning Rate: 0.020999999999999998\n",
      "Epoch [2313/20000], Bound: 0.6100087761878967, Entropy: 128.43756103515625, Temp: 1.3175139427185059, KL: 75.75434875488281, Loss: 0.13760733604431152, Learning Rate: 0.020999999999999998\n",
      "Epoch [2314/20000], Bound: 0.7130475640296936, Entropy: 126.83696746826172, Temp: 1.318488597869873, KL: 100.2773208618164, Loss: 0.15597563982009888, Learning Rate: 0.020999999999999998\n",
      "Epoch [2315/20000], Bound: 0.6230171322822571, Entropy: 128.99497985839844, Temp: 1.3193832635879517, KL: 79.49552917480469, Loss: 0.13682858645915985, Learning Rate: 0.020999999999999998\n",
      "Epoch [2316/20000], Bound: 0.6352527141571045, Entropy: 122.69965362548828, Temp: 1.3201221227645874, KL: 86.0437240600586, Loss: 0.12472724914550781, Learning Rate: 0.020999999999999998\n",
      "Epoch [2317/20000], Bound: 0.6374896168708801, Entropy: 118.33088684082031, Temp: 1.3208836317062378, KL: 96.06277465820312, Loss: 0.08923995494842529, Learning Rate: 0.020999999999999998\n",
      "Epoch [2318/20000], Bound: 0.6218045949935913, Entropy: 116.65194702148438, Temp: 1.3220425844192505, KL: 81.72683715820312, Loss: 0.1274927258491516, Learning Rate: 0.020999999999999998\n",
      "Epoch [2319/20000], Bound: 0.581087589263916, Entropy: 113.9294204711914, Temp: 1.3231106996536255, KL: 75.40924835205078, Loss: 0.1110890656709671, Learning Rate: 0.020999999999999998\n",
      "Epoch [2320/20000], Bound: 0.5851128101348877, Entropy: 113.37294006347656, Temp: 1.324151873588562, KL: 84.19039916992188, Loss: 0.08195656538009644, Learning Rate: 0.020999999999999998\n",
      "Epoch [2321/20000], Bound: 0.5752291679382324, Entropy: 112.41488647460938, Temp: 1.3254934549331665, KL: 79.70896911621094, Loss: 0.08951442688703537, Learning Rate: 0.020999999999999998\n",
      "Epoch [2322/20000], Bound: 0.610771119594574, Entropy: 121.00634765625, Temp: 1.326995611190796, KL: 76.1370849609375, Loss: 0.13804958760738373, Learning Rate: 0.020999999999999998\n",
      "Epoch [2323/20000], Bound: 0.5910883545875549, Entropy: 118.9228286743164, Temp: 1.328220009803772, KL: 79.61466217041016, Loss: 0.10559000819921494, Learning Rate: 0.020999999999999998\n",
      "Epoch [2324/20000], Bound: 0.615466296672821, Entropy: 124.11189270019531, Temp: 1.3294832706451416, KL: 73.42364501953125, Loss: 0.15327522158622742, Learning Rate: 0.020999999999999998\n",
      "Epoch [2325/20000], Bound: 0.5693702101707458, Entropy: 121.88579559326172, Temp: 1.3303399085998535, KL: 75.27770233154297, Loss: 0.10120990872383118, Learning Rate: 0.020999999999999998\n",
      "Epoch [2326/20000], Bound: 0.5640036463737488, Entropy: 123.35391235351562, Temp: 1.331255316734314, KL: 74.50413513183594, Loss: 0.09913547337055206, Learning Rate: 0.020999999999999998\n",
      "Epoch [2327/20000], Bound: 0.5757905840873718, Entropy: 127.85264587402344, Temp: 1.3322302103042603, KL: 77.96664428710938, Loss: 0.09747366607189178, Learning Rate: 0.020999999999999998\n",
      "Epoch [2328/20000], Bound: 0.5784242153167725, Entropy: 126.83154296875, Temp: 1.3333088159561157, KL: 70.81184387207031, Loss: 0.12696878612041473, Learning Rate: 0.020999999999999998\n",
      "Epoch [2329/20000], Bound: 0.5665647387504578, Entropy: 130.01226806640625, Temp: 1.3341642618179321, KL: 75.98486328125, Loss: 0.09634774923324585, Learning Rate: 0.020999999999999998\n",
      "Epoch [2330/20000], Bound: 0.5879750847816467, Entropy: 127.5859146118164, Temp: 1.3351194858551025, KL: 66.0536880493164, Loss: 0.15421952307224274, Learning Rate: 0.020999999999999998\n",
      "Epoch [2331/20000], Bound: 0.5815286040306091, Entropy: 132.2045440673828, Temp: 1.3355921506881714, KL: 84.88784790039062, Loss: 0.07750146836042404, Learning Rate: 0.020999999999999998\n",
      "Epoch [2332/20000], Bound: 0.6659331321716309, Entropy: 132.78384399414062, Temp: 1.3364481925964355, KL: 91.35295104980469, Loss: 0.13972590863704681, Learning Rate: 0.020999999999999998\n",
      "Epoch [2333/20000], Bound: 0.5604752898216248, Entropy: 130.34823608398438, Temp: 1.337226390838623, KL: 66.89033508300781, Loss: 0.12497224658727646, Learning Rate: 0.020999999999999998\n",
      "Epoch [2334/20000], Bound: 0.6301512122154236, Entropy: 126.44935607910156, Temp: 1.3377740383148193, KL: 84.132080078125, Loss: 0.12911666929721832, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2335/20000], Bound: 0.5515615344047546, Entropy: 122.43558502197266, Temp: 1.3382759094238281, KL: 76.26094818115234, Loss: 0.08173020929098129, Learning Rate: 0.020999999999999998\n",
      "Epoch [2336/20000], Bound: 0.55189448595047, Entropy: 121.81864929199219, Temp: 1.3390288352966309, KL: 76.61470031738281, Loss: 0.08080993592739105, Learning Rate: 0.020999999999999998\n",
      "Epoch [2337/20000], Bound: 0.5998742580413818, Entropy: 123.13382720947266, Temp: 1.3400176763534546, KL: 80.42757415771484, Loss: 0.112690769135952, Learning Rate: 0.020999999999999998\n",
      "Epoch [2338/20000], Bound: 0.5339319705963135, Entropy: 120.3432388305664, Temp: 1.3410016298294067, KL: 72.6050796508789, Loss: 0.07948960363864899, Learning Rate: 0.020999999999999998\n",
      "Epoch [2339/20000], Bound: 0.63554847240448, Entropy: 123.55878448486328, Temp: 1.342163324356079, KL: 94.7983627319336, Loss: 0.09554658830165863, Learning Rate: 0.020999999999999998\n",
      "Epoch [2340/20000], Bound: 0.6231998801231384, Entropy: 120.5048599243164, Temp: 1.34358549118042, KL: 90.17652130126953, Loss: 0.10027522593736649, Learning Rate: 0.020999999999999998\n",
      "Epoch [2341/20000], Bound: 0.6083659529685974, Entropy: 120.1705551147461, Temp: 1.3451554775238037, KL: 82.7532730102539, Loss: 0.11312829703092575, Learning Rate: 0.020999999999999998\n",
      "Epoch [2342/20000], Bound: 0.5326814651489258, Entropy: 121.46341705322266, Temp: 1.3466758728027344, KL: 76.1279525756836, Loss: 0.06590928137302399, Learning Rate: 0.020999999999999998\n",
      "Epoch [2343/20000], Bound: 0.5480616092681885, Entropy: 118.42473602294922, Temp: 1.3484643697738647, KL: 76.71236419677734, Loss: 0.07803410291671753, Learning Rate: 0.020999999999999998\n",
      "Epoch [2344/20000], Bound: 0.6021818518638611, Entropy: 115.26973724365234, Temp: 1.3503966331481934, KL: 85.91197967529297, Loss: 0.09595870971679688, Learning Rate: 0.020999999999999998\n",
      "Epoch [2345/20000], Bound: 0.5719627737998962, Entropy: 119.44302368164062, Temp: 1.3524054288864136, KL: 80.73793029785156, Loss: 0.08602660894393921, Learning Rate: 0.020999999999999998\n",
      "Epoch [2346/20000], Bound: 0.5630388855934143, Entropy: 122.31443786621094, Temp: 1.3545072078704834, KL: 78.27821350097656, Loss: 0.08692817389965057, Learning Rate: 0.020999999999999998\n",
      "Epoch [2347/20000], Bound: 0.5769256949424744, Entropy: 122.39777374267578, Temp: 1.3566557168960571, KL: 80.69905853271484, Loss: 0.0914398729801178, Learning Rate: 0.020999999999999998\n",
      "Epoch [2348/20000], Bound: 0.5985284447669983, Entropy: 119.79904174804688, Temp: 1.358831524848938, KL: 91.41259765625, Loss: 0.07325973361730576, Learning Rate: 0.020999999999999998\n",
      "Epoch [2349/20000], Bound: 0.6223077178001404, Entropy: 123.04737854003906, Temp: 1.361286997795105, KL: 92.53012084960938, Loss: 0.09331435710191727, Learning Rate: 0.020999999999999998\n",
      "Epoch [2350/20000], Bound: 0.5936769247055054, Entropy: 122.60941314697266, Temp: 1.363837718963623, KL: 83.0903549194336, Loss: 0.09975661337375641, Learning Rate: 0.020999999999999998\n",
      "Epoch [2351/20000], Bound: 0.5338708758354187, Entropy: 120.36875915527344, Temp: 1.3663215637207031, KL: 70.04483032226562, Loss: 0.09156180173158646, Learning Rate: 0.020999999999999998\n",
      "Epoch [2352/20000], Bound: 0.626976728439331, Entropy: 118.95352935791016, Temp: 1.368668794631958, KL: 94.1076431274414, Loss: 0.09340560436248779, Learning Rate: 0.020999999999999998\n",
      "Epoch [2353/20000], Bound: 0.561221718788147, Entropy: 119.01904296875, Temp: 1.371124267578125, KL: 75.31404113769531, Loss: 0.09798606485128403, Learning Rate: 0.020999999999999998\n",
      "Epoch [2354/20000], Bound: 0.55354243516922, Entropy: 118.46923828125, Temp: 1.3734426498413086, KL: 75.91059875488281, Loss: 0.08890973031520844, Learning Rate: 0.020999999999999998\n",
      "Epoch [2355/20000], Bound: 0.5701421499252319, Entropy: 115.84368133544922, Temp: 1.3757156133651733, KL: 78.54479217529297, Loss: 0.0951286107301712, Learning Rate: 0.020999999999999998\n",
      "Epoch [2356/20000], Bound: 0.5694476962089539, Entropy: 123.22594451904297, Temp: 1.3779205083847046, KL: 74.9953384399414, Loss: 0.10760065913200378, Learning Rate: 0.020999999999999998\n",
      "Epoch [2357/20000], Bound: 0.5855215787887573, Entropy: 125.96604919433594, Temp: 1.3799186944961548, KL: 87.45710754394531, Loss: 0.07800641655921936, Learning Rate: 0.020999999999999998\n",
      "Epoch [2358/20000], Bound: 0.5556786060333252, Entropy: 124.58527374267578, Temp: 1.382103681564331, KL: 74.57007598876953, Loss: 0.09667469561100006, Learning Rate: 0.020999999999999998\n",
      "Epoch [2359/20000], Bound: 0.59705650806427, Entropy: 124.29032897949219, Temp: 1.384163737297058, KL: 86.49848937988281, Loss: 0.09324923902750015, Learning Rate: 0.020999999999999998\n",
      "Epoch [2360/20000], Bound: 0.5969769358634949, Entropy: 129.1376495361328, Temp: 1.3862608671188354, KL: 81.49180603027344, Loss: 0.11149770766496658, Learning Rate: 0.020999999999999998\n",
      "Epoch [2361/20000], Bound: 0.49660348892211914, Entropy: 124.0672607421875, Temp: 1.3881855010986328, KL: 64.67759704589844, Loss: 0.08012744039297104, Learning Rate: 0.020999999999999998\n",
      "Epoch [2362/20000], Bound: 0.6156861186027527, Entropy: 124.94295501708984, Temp: 1.3900355100631714, KL: 82.63860321044922, Loss: 0.12635967135429382, Learning Rate: 0.020999999999999998\n",
      "Epoch [2363/20000], Bound: 0.5885326862335205, Entropy: 127.41304779052734, Temp: 1.3916209936141968, KL: 85.2945327758789, Loss: 0.09022441506385803, Learning Rate: 0.020999999999999998\n",
      "Epoch [2364/20000], Bound: 0.5823711156845093, Entropy: 127.70542907714844, Temp: 1.39329195022583, KL: 74.03376770019531, Loss: 0.12490366399288177, Learning Rate: 0.020999999999999998\n",
      "Epoch [2365/20000], Bound: 0.6938503384590149, Entropy: 128.3955535888672, Temp: 1.394628882408142, KL: 89.52569580078125, Loss: 0.18532878160476685, Learning Rate: 0.020999999999999998\n",
      "Epoch [2366/20000], Bound: 0.5919980406761169, Entropy: 127.86891174316406, Temp: 1.3953502178192139, KL: 78.44871520996094, Loss: 0.11857468634843826, Learning Rate: 0.020999999999999998\n",
      "Epoch [2367/20000], Bound: 0.599114179611206, Entropy: 123.03294372558594, Temp: 1.3959282636642456, KL: 83.07392883300781, Loss: 0.10901792347431183, Learning Rate: 0.020999999999999998\n",
      "Epoch [2368/20000], Bound: 0.6487296223640442, Entropy: 120.0027084350586, Temp: 1.3965046405792236, KL: 85.91158294677734, Loss: 0.14934635162353516, Learning Rate: 0.020999999999999998\n",
      "Epoch [2369/20000], Bound: 0.6245166063308716, Entropy: 113.76656341552734, Temp: 1.3967795372009277, KL: 81.02420806884766, Loss: 0.14182423055171967, Learning Rate: 0.020999999999999998\n",
      "Epoch [2370/20000], Bound: 0.6180004477500916, Entropy: 108.5272445678711, Temp: 1.3967905044555664, KL: 91.88736724853516, Loss: 0.09634492546319962, Learning Rate: 0.020999999999999998\n",
      "Epoch [2371/20000], Bound: 0.5849927663803101, Entropy: 112.962158203125, Temp: 1.3970494270324707, KL: 79.37078857421875, Loss: 0.10867644101381302, Learning Rate: 0.020999999999999998\n",
      "Epoch [2372/20000], Bound: 0.6216636300086975, Entropy: 104.4072265625, Temp: 1.3973004817962646, KL: 85.99031066894531, Loss: 0.12121511250734329, Learning Rate: 0.020999999999999998\n",
      "Epoch [2373/20000], Bound: 0.5470483899116516, Entropy: 116.02336883544922, Temp: 1.397508978843689, KL: 76.26557159423828, Loss: 0.08421391248703003, Learning Rate: 0.020999999999999998\n",
      "Epoch [2374/20000], Bound: 0.5804235935211182, Entropy: 118.28296661376953, Temp: 1.3978896141052246, KL: 81.7397232055664, Loss: 0.09590885788202286, Learning Rate: 0.020999999999999998\n",
      "Epoch [2375/20000], Bound: 0.5892345905303955, Entropy: 122.65831756591797, Temp: 1.3983815908432007, KL: 78.09833526611328, Loss: 0.11746298521757126, Learning Rate: 0.020999999999999998\n",
      "Epoch [2376/20000], Bound: 0.6551336050033569, Entropy: 127.13265228271484, Temp: 1.3987529277801514, KL: 98.33788299560547, Loss: 0.11196442693471909, Learning Rate: 0.020999999999999998\n",
      "Epoch [2377/20000], Bound: 0.6657893061637878, Entropy: 130.09878540039062, Temp: 1.3992668390274048, KL: 95.63638305664062, Loss: 0.13310836255550385, Learning Rate: 0.020999999999999998\n",
      "Epoch [2378/20000], Bound: 0.6620962023735046, Entropy: 126.38734436035156, Temp: 1.399709701538086, KL: 90.31974792480469, Loss: 0.14818575978279114, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2379/20000], Bound: 0.5744431018829346, Entropy: 123.62954711914062, Temp: 1.3999125957489014, KL: 79.04924011230469, Loss: 0.10005086660385132, Learning Rate: 0.020999999999999998\n",
      "Epoch [2380/20000], Bound: 0.6192073822021484, Entropy: 118.41166687011719, Temp: 1.4001779556274414, KL: 83.87615966796875, Loss: 0.12662678956985474, Learning Rate: 0.020999999999999998\n",
      "Epoch [2381/20000], Bound: 0.5839887857437134, Entropy: 117.5670166015625, Temp: 1.4003276824951172, KL: 78.02963256835938, Loss: 0.11284984648227692, Learning Rate: 0.020999999999999998\n",
      "Epoch [2382/20000], Bound: 0.5821905136108398, Entropy: 115.74052429199219, Temp: 1.4004254341125488, KL: 89.0487060546875, Loss: 0.07179223001003265, Learning Rate: 0.020999999999999998\n",
      "Epoch [2383/20000], Bound: 0.5942991375923157, Entropy: 112.42679595947266, Temp: 1.400940179824829, KL: 82.11254119873047, Loss: 0.10831660777330399, Learning Rate: 0.020999999999999998\n",
      "Epoch [2384/20000], Bound: 0.5721727013587952, Entropy: 112.68995666503906, Temp: 1.401447057723999, KL: 81.19461059570312, Loss: 0.09041009843349457, Learning Rate: 0.020999999999999998\n",
      "Epoch [2385/20000], Bound: 0.5403555035591125, Entropy: 113.24907684326172, Temp: 1.40208899974823, KL: 75.36933135986328, Loss: 0.08178586512804031, Learning Rate: 0.020999999999999998\n",
      "Epoch [2386/20000], Bound: 0.5658360719680786, Entropy: 116.8984603881836, Temp: 1.402866005897522, KL: 81.37345123291016, Loss: 0.08396530896425247, Learning Rate: 0.020999999999999998\n",
      "Epoch [2387/20000], Bound: 0.5802103281021118, Entropy: 119.35051727294922, Temp: 1.4038066864013672, KL: 78.55455780029297, Loss: 0.10771699994802475, Learning Rate: 0.020999999999999998\n",
      "Epoch [2388/20000], Bound: 0.5803101658821106, Entropy: 128.40467834472656, Temp: 1.4046603441238403, KL: 82.65219116210938, Loss: 0.09331513941287994, Learning Rate: 0.020999999999999998\n",
      "Epoch [2389/20000], Bound: 0.5095028281211853, Entropy: 129.6019744873047, Temp: 1.40559983253479, KL: 66.23725891113281, Loss: 0.08718378096818924, Learning Rate: 0.020999999999999998\n",
      "Epoch [2390/20000], Bound: 0.5409948229789734, Entropy: 128.9761199951172, Temp: 1.406493902206421, KL: 77.88357543945312, Loss: 0.07387308776378632, Learning Rate: 0.020999999999999998\n",
      "Epoch [2391/20000], Bound: 0.6194936633110046, Entropy: 130.2919464111328, Temp: 1.4075878858566284, KL: 87.87387084960938, Loss: 0.11355084925889969, Learning Rate: 0.020999999999999998\n",
      "Epoch [2392/20000], Bound: 0.5829601883888245, Entropy: 134.2495574951172, Temp: 1.4086209535598755, KL: 75.53860473632812, Loss: 0.12154985219240189, Learning Rate: 0.020999999999999998\n",
      "Epoch [2393/20000], Bound: 0.5445690155029297, Entropy: 133.26229858398438, Temp: 1.4093997478485107, KL: 72.697265625, Loss: 0.095830537378788, Learning Rate: 0.020999999999999998\n",
      "Epoch [2394/20000], Bound: 0.5798880457878113, Entropy: 132.1879119873047, Temp: 1.4101380109786987, KL: 80.528564453125, Loss: 0.10106179863214493, Learning Rate: 0.020999999999999998\n",
      "Epoch [2395/20000], Bound: 0.6047636270523071, Entropy: 131.08348083496094, Temp: 1.410876750946045, KL: 84.41242980957031, Loss: 0.11152156442403793, Learning Rate: 0.020999999999999998\n",
      "Epoch [2396/20000], Bound: 0.5750430226325989, Entropy: 126.16558074951172, Temp: 1.4115655422210693, KL: 82.1565170288086, Loss: 0.09083977341651917, Learning Rate: 0.020999999999999998\n",
      "Epoch [2397/20000], Bound: 0.6116617321968079, Entropy: 122.44867706298828, Temp: 1.4123626947402954, KL: 77.59156036376953, Loss: 0.14267964661121368, Learning Rate: 0.020999999999999998\n",
      "Epoch [2398/20000], Bound: 0.6183499097824097, Entropy: 118.56979370117188, Temp: 1.4127675294876099, KL: 79.71971130371094, Loss: 0.1418842077255249, Learning Rate: 0.020999999999999998\n",
      "Epoch [2399/20000], Bound: 0.5932044386863708, Entropy: 114.03093719482422, Temp: 1.4128484725952148, KL: 82.8764877319336, Loss: 0.10586123168468475, Learning Rate: 0.020999999999999998\n",
      "Epoch [2400/20000], Bound: 0.5563410520553589, Entropy: 109.07697296142578, Temp: 1.4129738807678223, KL: 83.78797149658203, Loss: 0.06771545857191086, Learning Rate: 0.020999999999999998\n",
      "Epoch [2401/20000], Bound: 0.6562795639038086, Entropy: 109.467041015625, Temp: 1.4134818315505981, KL: 90.01863098144531, Loss: 0.14475402235984802, Learning Rate: 0.020999999999999998\n",
      "Epoch [2402/20000], Bound: 0.6064243316650391, Entropy: 110.1883773803711, Temp: 1.413739562034607, KL: 87.00458526611328, Loss: 0.10431882739067078, Learning Rate: 0.020999999999999998\n",
      "Epoch [2403/20000], Bound: 0.6344296932220459, Entropy: 111.19622802734375, Temp: 1.4140779972076416, KL: 88.95927429199219, Loss: 0.1257098764181137, Learning Rate: 0.020999999999999998\n",
      "Epoch [2404/20000], Bound: 0.613966166973114, Entropy: 116.58759307861328, Temp: 1.4143280982971191, KL: 79.99536895751953, Loss: 0.13666857779026031, Learning Rate: 0.020999999999999998\n",
      "Epoch [2405/20000], Bound: 0.6313323378562927, Entropy: 119.22014617919922, Temp: 1.4143122434616089, KL: 89.7806167602539, Loss: 0.11965274810791016, Learning Rate: 0.020999999999999998\n",
      "Epoch [2406/20000], Bound: 0.5223028063774109, Entropy: 115.61304473876953, Temp: 1.4143019914627075, KL: 73.50226593017578, Loss: 0.07344722747802734, Learning Rate: 0.020999999999999998\n",
      "Epoch [2407/20000], Bound: 0.6364805102348328, Entropy: 119.47551727294922, Temp: 1.4145301580429077, KL: 89.45166778564453, Loss: 0.12613877654075623, Learning Rate: 0.020999999999999998\n",
      "Epoch [2408/20000], Bound: 0.5649102926254272, Entropy: 126.42073059082031, Temp: 1.4146815538406372, KL: 78.81651306152344, Loss: 0.09345300495624542, Learning Rate: 0.020999999999999998\n",
      "Epoch [2409/20000], Bound: 0.587977409362793, Entropy: 128.63194274902344, Temp: 1.4149328470230103, KL: 78.23590087890625, Loss: 0.11743110418319702, Learning Rate: 0.020999999999999998\n",
      "Epoch [2410/20000], Bound: 0.5961460471153259, Entropy: 128.42738342285156, Temp: 1.415060043334961, KL: 82.39656066894531, Loss: 0.11066337674856186, Learning Rate: 0.020999999999999998\n",
      "Epoch [2411/20000], Bound: 0.5746594667434692, Entropy: 129.2808074951172, Temp: 1.4151774644851685, KL: 79.137451171875, Loss: 0.10154347121715546, Learning Rate: 0.020999999999999998\n",
      "Epoch [2412/20000], Bound: 0.47512954473495483, Entropy: 126.17407989501953, Temp: 1.4153302907943726, KL: 61.280372619628906, Loss: 0.07616528123617172, Learning Rate: 0.020999999999999998\n",
      "Epoch [2413/20000], Bound: 0.6335755586624146, Entropy: 131.3183135986328, Temp: 1.4155471324920654, KL: 83.16957092285156, Loss: 0.14545896649360657, Learning Rate: 0.020999999999999998\n",
      "Epoch [2414/20000], Bound: 0.5736905336380005, Entropy: 127.12196350097656, Temp: 1.4154592752456665, KL: 79.33103942871094, Loss: 0.09997164458036423, Learning Rate: 0.020999999999999998\n",
      "Epoch [2415/20000], Bound: 0.615170955657959, Entropy: 127.0556411743164, Temp: 1.415442705154419, KL: 90.18744659423828, Loss: 0.1019827276468277, Learning Rate: 0.020999999999999998\n",
      "Epoch [2416/20000], Bound: 0.552187442779541, Entropy: 124.44505310058594, Temp: 1.4155842065811157, KL: 76.99615478515625, Loss: 0.08818211406469345, Learning Rate: 0.020999999999999998\n",
      "Epoch [2417/20000], Bound: 0.5659513473510742, Entropy: 119.69412994384766, Temp: 1.4158532619476318, KL: 76.14473724365234, Loss: 0.10398316383361816, Learning Rate: 0.020999999999999998\n",
      "Epoch [2418/20000], Bound: 0.6149318218231201, Entropy: 117.37273406982422, Temp: 1.416088581085205, KL: 90.03485870361328, Loss: 0.10236300528049469, Learning Rate: 0.020999999999999998\n",
      "Epoch [2419/20000], Bound: 0.6519870758056641, Entropy: 113.75056457519531, Temp: 1.416451096534729, KL: 101.62948608398438, Loss: 0.09957962483167648, Learning Rate: 0.020999999999999998\n",
      "Epoch [2420/20000], Bound: 0.6206872463226318, Entropy: 110.22649383544922, Temp: 1.4170646667480469, KL: 90.94588470458984, Loss: 0.10505449771881104, Learning Rate: 0.020999999999999998\n",
      "Epoch [2421/20000], Bound: 0.5651735663414001, Entropy: 106.4267349243164, Temp: 1.4177521467208862, KL: 74.7307357788086, Loss: 0.10842494666576385, Learning Rate: 0.020999999999999998\n",
      "Epoch [2422/20000], Bound: 0.5839289426803589, Entropy: 106.61674499511719, Temp: 1.4183073043823242, KL: 79.36790466308594, Loss: 0.10988137871026993, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2423/20000], Bound: 0.5574042797088623, Entropy: 107.9780044555664, Temp: 1.4187792539596558, KL: 81.8602523803711, Loss: 0.07616977393627167, Learning Rate: 0.020999999999999998\n",
      "Epoch [2424/20000], Bound: 0.5764272212982178, Entropy: 116.9930419921875, Temp: 1.419499397277832, KL: 79.90249633789062, Loss: 0.10096573829650879, Learning Rate: 0.020999999999999998\n",
      "Epoch [2425/20000], Bound: 0.5407951474189758, Entropy: 120.04286193847656, Temp: 1.4202014207839966, KL: 71.47544860839844, Loss: 0.0976911261677742, Learning Rate: 0.020999999999999998\n",
      "Epoch [2426/20000], Bound: 0.5728630423545837, Entropy: 125.78517150878906, Temp: 1.420824646949768, KL: 74.62876892089844, Loss: 0.11628835648298264, Learning Rate: 0.020999999999999998\n",
      "Epoch [2427/20000], Bound: 0.5943371653556824, Entropy: 128.03292846679688, Temp: 1.4212467670440674, KL: 83.56686401367188, Loss: 0.10545046627521515, Learning Rate: 0.020999999999999998\n",
      "Epoch [2428/20000], Bound: 0.6522151827812195, Entropy: 128.4542236328125, Temp: 1.421676754951477, KL: 84.94505310058594, Loss: 0.15927980840206146, Learning Rate: 0.020999999999999998\n",
      "Epoch [2429/20000], Bound: 0.6403924226760864, Entropy: 130.54428100585938, Temp: 1.4216718673706055, KL: 97.17190551757812, Loss: 0.10389768332242966, Learning Rate: 0.020999999999999998\n",
      "Epoch [2430/20000], Bound: 0.6639751195907593, Entropy: 125.29161071777344, Temp: 1.4218662977218628, KL: 106.35015869140625, Loss: 0.09656281024217606, Learning Rate: 0.020999999999999998\n",
      "Epoch [2431/20000], Bound: 0.5749224424362183, Entropy: 120.8729248046875, Temp: 1.4223898649215698, KL: 87.61991882324219, Loss: 0.07270924746990204, Learning Rate: 0.020999999999999998\n",
      "Epoch [2432/20000], Bound: 0.5969106554985046, Entropy: 120.29270935058594, Temp: 1.4232401847839355, KL: 80.84799194335938, Loss: 0.11772527545690536, Learning Rate: 0.020999999999999998\n",
      "Epoch [2433/20000], Bound: 0.5817535519599915, Entropy: 113.28118133544922, Temp: 1.423917293548584, KL: 85.36742401123047, Loss: 0.08729805052280426, Learning Rate: 0.020999999999999998\n",
      "Epoch [2434/20000], Bound: 0.6650415658950806, Entropy: 111.28376770019531, Temp: 1.4247511625289917, KL: 96.36531066894531, Loss: 0.13320857286453247, Learning Rate: 0.020999999999999998\n",
      "Epoch [2435/20000], Bound: 0.6081718802452087, Entropy: 111.90618133544922, Temp: 1.4254385232925415, KL: 87.55272674560547, Loss: 0.10549649596214294, Learning Rate: 0.020999999999999998\n",
      "Epoch [2436/20000], Bound: 0.6018055081367493, Entropy: 108.13349151611328, Temp: 1.4261409044265747, KL: 83.3309555053711, Loss: 0.1141030490398407, Learning Rate: 0.020999999999999998\n",
      "Epoch [2437/20000], Bound: 0.6015194058418274, Entropy: 109.4281234741211, Temp: 1.4267373085021973, KL: 88.79273223876953, Loss: 0.09474512934684753, Learning Rate: 0.020999999999999998\n",
      "Epoch [2438/20000], Bound: 0.5959334373474121, Entropy: 110.73155975341797, Temp: 1.4274625778198242, KL: 83.7153549194336, Loss: 0.10715825110673904, Learning Rate: 0.020999999999999998\n",
      "Epoch [2439/20000], Bound: 0.5552971959114075, Entropy: 112.09347534179688, Temp: 1.4281418323516846, KL: 77.998046875, Loss: 0.08877421170473099, Learning Rate: 0.020999999999999998\n",
      "Epoch [2440/20000], Bound: 0.5840335488319397, Entropy: 115.47427368164062, Temp: 1.4288824796676636, KL: 75.19810485839844, Loss: 0.12562909722328186, Learning Rate: 0.020999999999999998\n",
      "Epoch [2441/20000], Bound: 0.5642581582069397, Entropy: 120.57575988769531, Temp: 1.4293209314346313, KL: 66.41259765625, Loss: 0.13772159814834595, Learning Rate: 0.020999999999999998\n",
      "Epoch [2442/20000], Bound: 0.6397930383682251, Entropy: 130.0199737548828, Temp: 1.4292820692062378, KL: 86.1005859375, Loss: 0.1430494636297226, Learning Rate: 0.020999999999999998\n",
      "Epoch [2443/20000], Bound: 0.5877698063850403, Entropy: 132.78704833984375, Temp: 1.4289863109588623, KL: 69.50125122070312, Loss: 0.14915740489959717, Learning Rate: 0.020999999999999998\n",
      "Epoch [2444/20000], Bound: 0.6128491759300232, Entropy: 134.23899841308594, Temp: 1.4282251596450806, KL: 73.16412353515625, Loss: 0.16083911061286926, Learning Rate: 0.020999999999999998\n",
      "Epoch [2445/20000], Bound: 0.5611346960067749, Entropy: 134.53334045410156, Temp: 1.42699134349823, KL: 74.17791748046875, Loss: 0.10744467377662659, Learning Rate: 0.020999999999999998\n",
      "Epoch [2446/20000], Bound: 0.6095638275146484, Entropy: 135.7890625, Temp: 1.425803542137146, KL: 91.48060607910156, Loss: 0.09314481168985367, Learning Rate: 0.020999999999999998\n",
      "Epoch [2447/20000], Bound: 0.5660544037818909, Entropy: 136.1027069091797, Temp: 1.424965262413025, KL: 73.17837524414062, Loss: 0.1153549998998642, Learning Rate: 0.020999999999999998\n",
      "Epoch [2448/20000], Bound: 0.615515947341919, Entropy: 131.17893981933594, Temp: 1.4240561723709106, KL: 91.14105224609375, Loss: 0.1000489816069603, Learning Rate: 0.020999999999999998\n",
      "Epoch [2449/20000], Bound: 0.5778011679649353, Entropy: 130.80189514160156, Temp: 1.4234076738357544, KL: 79.4949951171875, Loss: 0.10410285741090775, Learning Rate: 0.020999999999999998\n",
      "Epoch [2450/20000], Bound: 0.580681324005127, Entropy: 126.76667022705078, Temp: 1.4228392839431763, KL: 88.01667022705078, Loss: 0.07684098184108734, Learning Rate: 0.020999999999999998\n",
      "Epoch [2451/20000], Bound: 0.5670286417007446, Entropy: 118.46875, Temp: 1.4226748943328857, KL: 81.71734619140625, Loss: 0.08605736494064331, Learning Rate: 0.020999999999999998\n",
      "Epoch [2452/20000], Bound: 0.5953969359397888, Entropy: 115.48251342773438, Temp: 1.4227275848388672, KL: 85.40896606445312, Loss: 0.10016927123069763, Learning Rate: 0.020999999999999998\n",
      "Epoch [2453/20000], Bound: 0.6129928827285767, Entropy: 115.16040802001953, Temp: 1.4228883981704712, KL: 83.05677032470703, Loss: 0.12578943371772766, Learning Rate: 0.020999999999999998\n",
      "Epoch [2454/20000], Bound: 0.5055060982704163, Entropy: 112.56536102294922, Temp: 1.4228984117507935, KL: 62.91889190673828, Loss: 0.09676805883646011, Learning Rate: 0.020999999999999998\n",
      "Epoch [2455/20000], Bound: 0.6281777024269104, Entropy: 113.1508560180664, Temp: 1.4228061437606812, KL: 89.5503921508789, Loss: 0.11827343702316284, Learning Rate: 0.020999999999999998\n",
      "Epoch [2456/20000], Bound: 0.5716803073883057, Entropy: 122.19988250732422, Temp: 1.4227209091186523, KL: 86.91008758544922, Loss: 0.07218249887228012, Learning Rate: 0.020999999999999998\n",
      "Epoch [2457/20000], Bound: 0.5672808289527893, Entropy: 122.3277816772461, Temp: 1.4230237007141113, KL: 76.69783782958984, Loss: 0.1039680540561676, Learning Rate: 0.020999999999999998\n",
      "Epoch [2458/20000], Bound: 0.6212719082832336, Entropy: 125.82513427734375, Temp: 1.423283576965332, KL: 87.69607543945312, Loss: 0.11784013360738754, Learning Rate: 0.020999999999999998\n",
      "Epoch [2459/20000], Bound: 0.5750778317451477, Entropy: 128.4533233642578, Temp: 1.423499345779419, KL: 82.46255493164062, Loss: 0.09110625833272934, Learning Rate: 0.020999999999999998\n",
      "Epoch [2460/20000], Bound: 0.607437789440155, Entropy: 130.96568298339844, Temp: 1.423856258392334, KL: 89.43167114257812, Loss: 0.09798678010702133, Learning Rate: 0.020999999999999998\n",
      "Epoch [2461/20000], Bound: 0.5844137072563171, Entropy: 128.718994140625, Temp: 1.4243495464324951, KL: 76.41693115234375, Loss: 0.12131227552890778, Learning Rate: 0.020999999999999998\n",
      "Epoch [2462/20000], Bound: 0.6583471298217773, Entropy: 125.44313049316406, Temp: 1.4246227741241455, KL: 105.12124633789062, Loss: 0.09528802335262299, Learning Rate: 0.020999999999999998\n",
      "Epoch [2463/20000], Bound: 0.5864686369895935, Entropy: 123.48689270019531, Temp: 1.4252153635025024, KL: 91.04756164550781, Loss: 0.07203629612922668, Learning Rate: 0.020999999999999998\n",
      "Epoch [2464/20000], Bound: 0.5746385455131531, Entropy: 121.5993881225586, Temp: 1.4261672496795654, KL: 80.76419830322266, Loss: 0.09693408012390137, Learning Rate: 0.020999999999999998\n",
      "Epoch [2465/20000], Bound: 0.6007614135742188, Entropy: 115.6551742553711, Temp: 1.4271130561828613, KL: 90.54431915283203, Loss: 0.08791015297174454, Learning Rate: 0.020999999999999998\n",
      "Epoch [2466/20000], Bound: 0.5821609497070312, Entropy: 114.25554656982422, Temp: 1.4282325506210327, KL: 89.40206146240234, Loss: 0.07405539602041245, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2467/20000], Bound: 0.5770575404167175, Entropy: 110.22950744628906, Temp: 1.4296202659606934, KL: 89.29219055175781, Loss: 0.06975487619638443, Learning Rate: 0.020999999999999998\n",
      "Epoch [2468/20000], Bound: 0.5874375104904175, Entropy: 111.88146209716797, Temp: 1.4312855005264282, KL: 91.79955291748047, Loss: 0.07111084461212158, Learning Rate: 0.020999999999999998\n",
      "Epoch [2469/20000], Bound: 0.5082661509513855, Entropy: 115.74121856689453, Temp: 1.4332098960876465, KL: 69.38639068603516, Loss: 0.0773133710026741, Learning Rate: 0.020999999999999998\n",
      "Epoch [2470/20000], Bound: 0.5504822731018066, Entropy: 119.84535217285156, Temp: 1.4350794553756714, KL: 79.03691101074219, Loss: 0.08141937106847763, Learning Rate: 0.020999999999999998\n",
      "Epoch [2471/20000], Bound: 0.5858234167098999, Entropy: 127.47469329833984, Temp: 1.4369611740112305, KL: 81.70690155029297, Loss: 0.1053997203707695, Learning Rate: 0.020999999999999998\n",
      "Epoch [2472/20000], Bound: 0.5466833114624023, Entropy: 129.8546142578125, Temp: 1.4386615753173828, KL: 75.01243591308594, Loss: 0.09230154007673264, Learning Rate: 0.020999999999999998\n",
      "Epoch [2473/20000], Bound: 0.5560646653175354, Entropy: 135.56349182128906, Temp: 1.4402432441711426, KL: 76.18766784667969, Loss: 0.09694625437259674, Learning Rate: 0.020999999999999998\n",
      "Epoch [2474/20000], Bound: 0.5610847473144531, Entropy: 139.04002380371094, Temp: 1.441685676574707, KL: 80.82318115234375, Loss: 0.08563645929098129, Learning Rate: 0.020999999999999998\n",
      "Epoch [2475/20000], Bound: 0.5827434659004211, Entropy: 135.72206115722656, Temp: 1.443152666091919, KL: 84.8428955078125, Loss: 0.09221529960632324, Learning Rate: 0.020999999999999998\n",
      "Epoch [2476/20000], Bound: 0.5791828632354736, Entropy: 136.0867919921875, Temp: 1.444620132446289, KL: 76.66044616699219, Loss: 0.11730652302503586, Learning Rate: 0.020999999999999998\n",
      "Epoch [2477/20000], Bound: 0.5454190373420715, Entropy: 132.407470703125, Temp: 1.445773720741272, KL: 79.67758178710938, Loss: 0.07565759122371674, Learning Rate: 0.020999999999999998\n",
      "Epoch [2478/20000], Bound: 0.5357197523117065, Entropy: 133.15585327148438, Temp: 1.4470549821853638, KL: 71.54039001464844, Loss: 0.09516378492116928, Learning Rate: 0.020999999999999998\n",
      "Epoch [2479/20000], Bound: 0.5441893339157104, Entropy: 125.25704956054688, Temp: 1.448182463645935, KL: 74.10438537597656, Loss: 0.0940261259675026, Learning Rate: 0.020999999999999998\n",
      "Epoch [2480/20000], Bound: 0.5475762486457825, Entropy: 123.1483154296875, Temp: 1.4492079019546509, KL: 80.77607727050781, Loss: 0.07416775822639465, Learning Rate: 0.020999999999999998\n",
      "Epoch [2481/20000], Bound: 0.5306980609893799, Entropy: 124.77589416503906, Temp: 1.4503939151763916, KL: 78.09355163574219, Loss: 0.06836602091789246, Learning Rate: 0.020999999999999998\n",
      "Epoch [2482/20000], Bound: 0.5802539587020874, Entropy: 122.61052703857422, Temp: 1.4517508745193481, KL: 84.6145248413086, Loss: 0.09155217558145523, Learning Rate: 0.020999999999999998\n",
      "Epoch [2486/20000], Bound: 0.566716730594635, Entropy: 121.34770965576172, Temp: 1.4574100971221924, KL: 77.19794464111328, Loss: 0.10486453771591187, Learning Rate: 0.020999999999999998\n",
      "Epoch [2487/20000], Bound: 0.5897135138511658, Entropy: 120.44319152832031, Temp: 1.458509922027588, KL: 87.42475891113281, Loss: 0.09165647625923157, Learning Rate: 0.020999999999999998\n",
      "Epoch [2488/20000], Bound: 0.5380796194076538, Entropy: 119.60234069824219, Temp: 1.4596538543701172, KL: 71.79664611816406, Loss: 0.09740962833166122, Learning Rate: 0.020999999999999998\n",
      "Epoch [2489/20000], Bound: 0.593085765838623, Entropy: 117.93548583984375, Temp: 1.4606201648712158, KL: 75.96083068847656, Loss: 0.134377121925354, Learning Rate: 0.020999999999999998\n",
      "Epoch [2490/20000], Bound: 0.5827099084854126, Entropy: 125.50115203857422, Temp: 1.4611332416534424, KL: 84.3712387084961, Loss: 0.09569425135850906, Learning Rate: 0.020999999999999998\n",
      "Epoch [2491/20000], Bound: 0.5673846006393433, Entropy: 124.2811508178711, Temp: 1.461676836013794, KL: 74.39128875732422, Loss: 0.11546148359775543, Learning Rate: 0.020999999999999998\n",
      "Epoch [2492/20000], Bound: 0.6015171408653259, Entropy: 122.46974182128906, Temp: 1.4619604349136353, KL: 90.7568359375, Loss: 0.09206953644752502, Learning Rate: 0.020999999999999998\n",
      "Epoch [2493/20000], Bound: 0.6022679209709167, Entropy: 122.02851104736328, Temp: 1.462392807006836, KL: 74.87627410888672, Loss: 0.14714916050434113, Learning Rate: 0.020999999999999998\n",
      "Epoch [2494/20000], Bound: 0.6541805863380432, Entropy: 122.23126983642578, Temp: 1.4622966051101685, KL: 88.22266387939453, Loss: 0.15425506234169006, Learning Rate: 0.020999999999999998\n",
      "Epoch [2495/20000], Bound: 0.5692031979560852, Entropy: 117.08113861083984, Temp: 1.4618070125579834, KL: 81.99663543701172, Loss: 0.09115460515022278, Learning Rate: 0.020999999999999998\n",
      "Epoch [2496/20000], Bound: 0.5991438627243042, Entropy: 112.34764099121094, Temp: 1.461464524269104, KL: 90.14669799804688, Loss: 0.09178558737039566, Learning Rate: 0.020999999999999998\n",
      "Epoch [2497/20000], Bound: 0.6169998049736023, Entropy: 108.67778778076172, Temp: 1.4613304138183594, KL: 85.58887481689453, Loss: 0.12497550994157791, Learning Rate: 0.020999999999999998\n",
      "Epoch [2498/20000], Bound: 0.6444451212882996, Entropy: 113.26571655273438, Temp: 1.4610376358032227, KL: 78.94584655761719, Loss: 0.17566443979740143, Learning Rate: 0.020999999999999998\n",
      "Epoch [2499/20000], Bound: 0.5782539248466492, Entropy: 108.0876235961914, Temp: 1.4600908756256104, KL: 82.84651947021484, Loss: 0.09658203274011612, Learning Rate: 0.020999999999999998\n",
      "Epoch [2500/20000], Bound: 0.6200884580612183, Entropy: 105.39608764648438, Temp: 1.4592976570129395, KL: 84.67010498046875, Loss: 0.1310117542743683, Learning Rate: 0.020999999999999998\n",
      "Epoch [2501/20000], Bound: 0.5468038320541382, Entropy: 108.32994842529297, Temp: 1.458351969718933, KL: 77.4453353881836, Loss: 0.08580396324396133, Learning Rate: 0.020999999999999998\n",
      "Epoch [2502/20000], Bound: 0.5496435165405273, Entropy: 109.47286224365234, Temp: 1.457607388496399, KL: 80.14957427978516, Loss: 0.07903922349214554, Learning Rate: 0.020999999999999998\n",
      "Epoch [2503/20000], Bound: 0.6135275959968567, Entropy: 121.2729721069336, Temp: 1.4571363925933838, KL: 88.76045989990234, Loss: 0.11020640283823013, Learning Rate: 0.020999999999999998\n",
      "Epoch [2504/20000], Bound: 0.5248103737831116, Entropy: 127.1307373046875, Temp: 1.4567127227783203, KL: 63.1407470703125, Loss: 0.11509176343679428, Learning Rate: 0.020999999999999998\n",
      "Epoch [2505/20000], Bound: 0.6052592396736145, Entropy: 130.2820281982422, Temp: 1.456010341644287, KL: 92.66773986816406, Loss: 0.08849015086889267, Learning Rate: 0.020999999999999998\n",
      "Epoch [2506/20000], Bound: 0.5473808646202087, Entropy: 130.8042449951172, Temp: 1.4556167125701904, KL: 78.43455505371094, Loss: 0.08267867565155029, Learning Rate: 0.020999999999999998\n",
      "Epoch [2507/20000], Bound: 0.6450455188751221, Entropy: 136.2371063232422, Temp: 1.4554131031036377, KL: 76.00543212890625, Loss: 0.18592412769794464, Learning Rate: 0.020999999999999998\n",
      "Epoch [2508/20000], Bound: 0.6439633965492249, Entropy: 134.4981689453125, Temp: 1.454437017440796, KL: 88.27760314941406, Loss: 0.1425333023071289, Learning Rate: 0.020999999999999998\n",
      "Epoch [2509/20000], Bound: 0.6215815544128418, Entropy: 131.5013885498047, Temp: 1.4532722234725952, KL: 89.16026306152344, Loss: 0.11646518111228943, Learning Rate: 0.020999999999999998\n",
      "Epoch [2510/20000], Bound: 0.5787447690963745, Entropy: 129.25619506835938, Temp: 1.4521785974502563, KL: 79.51992797851562, Loss: 0.10770706832408905, Learning Rate: 0.020999999999999998\n",
      "Epoch [2511/20000], Bound: 0.5479145646095276, Entropy: 128.2538299560547, Temp: 1.451129674911499, KL: 74.11378479003906, Loss: 0.09762532263994217, Learning Rate: 0.020999999999999998\n",
      "Epoch [2512/20000], Bound: 0.5718653202056885, Entropy: 122.2130126953125, Temp: 1.4501571655273438, KL: 78.57518005371094, Loss: 0.1042862981557846, Learning Rate: 0.020999999999999998\n",
      "Epoch [2513/20000], Bound: 0.6351619362831116, Entropy: 116.38101196289062, Temp: 1.4492416381835938, KL: 94.60331726074219, Loss: 0.11105087399482727, Learning Rate: 0.020999999999999998\n",
      "Epoch [2514/20000], Bound: 0.6451848149299622, Entropy: 111.38827514648438, Temp: 1.448482632637024, KL: 89.59013366699219, Loss: 0.13863807916641235, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2515/20000], Bound: 0.5784074664115906, Entropy: 106.85408782958984, Temp: 1.4475713968276978, KL: 89.19271087646484, Loss: 0.07354406267404556, Learning Rate: 0.020999999999999998\n",
      "Epoch [2516/20000], Bound: 0.6036290526390076, Entropy: 109.36970520019531, Temp: 1.4471089839935303, KL: 83.48124694824219, Loss: 0.11754803359508514, Learning Rate: 0.020999999999999998\n",
      "Epoch [2517/20000], Bound: 0.6045088171958923, Entropy: 108.37187957763672, Temp: 1.446589708328247, KL: 80.8313980102539, Loss: 0.12751862406730652, Learning Rate: 0.020999999999999998\n",
      "Epoch [2518/20000], Bound: 0.5782469511032104, Entropy: 114.63644409179688, Temp: 1.4459024667739868, KL: 81.63096618652344, Loss: 0.09934394806623459, Learning Rate: 0.020999999999999998\n",
      "Epoch [2519/20000], Bound: 0.6373917460441589, Entropy: 108.04786682128906, Temp: 1.4453281164169312, KL: 94.35475158691406, Loss: 0.11372043937444687, Learning Rate: 0.020999999999999998\n",
      "Epoch [2520/20000], Bound: 0.6185720562934875, Entropy: 116.57501983642578, Temp: 1.444857120513916, KL: 85.35211944580078, Loss: 0.12567530572414398, Learning Rate: 0.020999999999999998\n",
      "Epoch [2521/20000], Bound: 0.5459418296813965, Entropy: 119.0343246459961, Temp: 1.444280743598938, KL: 76.31208038330078, Loss: 0.08763343840837479, Learning Rate: 0.020999999999999998\n",
      "Epoch [2522/20000], Bound: 0.5940045118331909, Entropy: 123.9450912475586, Temp: 1.443861484527588, KL: 90.30353546142578, Loss: 0.08420606702566147, Learning Rate: 0.020999999999999998\n",
      "Epoch [2523/20000], Bound: 0.6328006386756897, Entropy: 124.4931640625, Temp: 1.4437603950500488, KL: 91.34371948242188, Loss: 0.11923444271087646, Learning Rate: 0.020999999999999998\n",
      "Epoch [2524/20000], Bound: 0.6070448756217957, Entropy: 129.8643341064453, Temp: 1.4436384439468384, KL: 83.55952453613281, Loss: 0.12028705328702927, Learning Rate: 0.020999999999999998\n",
      "Epoch [2525/20000], Bound: 0.5582249164581299, Entropy: 128.55624389648438, Temp: 1.4434075355529785, KL: 69.67471313476562, Loss: 0.12178601324558258, Learning Rate: 0.020999999999999998\n",
      "Epoch [2526/20000], Bound: 0.604193925857544, Entropy: 128.7233428955078, Temp: 1.4429121017456055, KL: 80.47470092773438, Loss: 0.12809482216835022, Learning Rate: 0.020999999999999998\n",
      "Epoch [2527/20000], Bound: 0.5898692011833191, Entropy: 130.43003845214844, Temp: 1.442242980003357, KL: 77.47135925292969, Loss: 0.12450378388166428, Learning Rate: 0.020999999999999998\n",
      "Epoch [2528/20000], Bound: 0.6440969109535217, Entropy: 125.34786987304688, Temp: 1.441418170928955, KL: 94.20291137695312, Loss: 0.120711550116539, Learning Rate: 0.020999999999999998\n",
      "Epoch [2529/20000], Bound: 0.6260281205177307, Entropy: 129.7888641357422, Temp: 1.4406650066375732, KL: 84.65081787109375, Loss: 0.13519220054149628, Learning Rate: 0.020999999999999998\n",
      "Epoch [2530/20000], Bound: 0.5497391223907471, Entropy: 121.02619934082031, Temp: 1.4397504329681396, KL: 68.52313232421875, Loss: 0.1177201047539711, Learning Rate: 0.020999999999999998\n",
      "Epoch [2531/20000], Bound: 0.6279484629631042, Entropy: 117.03740692138672, Temp: 1.438668966293335, KL: 87.09383392333984, Loss: 0.12844650447368622, Learning Rate: 0.020999999999999998\n",
      "Epoch [2532/20000], Bound: 0.6139350533485413, Entropy: 116.2862548828125, Temp: 1.437548041343689, KL: 84.72966003417969, Loss: 0.12242961674928665, Learning Rate: 0.020999999999999998\n",
      "Epoch [2533/20000], Bound: 0.5638382434844971, Entropy: 115.47137451171875, Temp: 1.4364218711853027, KL: 86.45623779296875, Loss: 0.06804759055376053, Learning Rate: 0.020999999999999998\n",
      "Epoch [2534/20000], Bound: 0.5975072383880615, Entropy: 110.36312866210938, Temp: 1.4358106851577759, KL: 88.67185974121094, Loss: 0.09232187271118164, Learning Rate: 0.020999999999999998\n",
      "Epoch [2535/20000], Bound: 0.6007870435714722, Entropy: 112.49849700927734, Temp: 1.435459852218628, KL: 88.27864837646484, Loss: 0.09685391932725906, Learning Rate: 0.020999999999999998\n",
      "Epoch [2536/20000], Bound: 0.6073150038719177, Entropy: 109.68069458007812, Temp: 1.4352985620498657, KL: 84.83750915527344, Loss: 0.11524343490600586, Learning Rate: 0.020999999999999998\n",
      "Epoch [2537/20000], Bound: 0.5619180798530579, Entropy: 110.4430160522461, Temp: 1.435105323791504, KL: 86.91277313232422, Loss: 0.06451865285634995, Learning Rate: 0.020999999999999998\n",
      "Epoch [2538/20000], Bound: 0.5959010720252991, Entropy: 112.22838592529297, Temp: 1.4353742599487305, KL: 88.20516204833984, Loss: 0.09233273565769196, Learning Rate: 0.020999999999999998\n",
      "Epoch [2539/20000], Bound: 0.5758707523345947, Entropy: 122.47169494628906, Temp: 1.4358118772506714, KL: 77.61553955078125, Loss: 0.11005212366580963, Learning Rate: 0.020999999999999998\n",
      "Epoch [2540/20000], Bound: 0.5721120834350586, Entropy: 122.789794921875, Temp: 1.4361259937286377, KL: 77.5341796875, Loss: 0.10681863874197006, Learning Rate: 0.020999999999999998\n",
      "Epoch [2541/20000], Bound: 0.5472998023033142, Entropy: 124.04622650146484, Temp: 1.436357021331787, KL: 78.08277130126953, Loss: 0.08196555823087692, Learning Rate: 0.020999999999999998\n",
      "Epoch [2542/20000], Bound: 0.5995531678199768, Entropy: 129.1862335205078, Temp: 1.436750054359436, KL: 83.68124389648438, Loss: 0.11179617047309875, Learning Rate: 0.020999999999999998\n",
      "Epoch [2543/20000], Bound: 0.5224169492721558, Entropy: 129.33352661132812, Temp: 1.4370722770690918, KL: 68.76332092285156, Loss: 0.09216965734958649, Learning Rate: 0.020999999999999998\n",
      "Epoch [2544/20000], Bound: 0.5575304627418518, Entropy: 134.18251037597656, Temp: 1.4373475313186646, KL: 73.78044128417969, Loss: 0.10640504956245422, Learning Rate: 0.020999999999999998\n",
      "Epoch [2545/20000], Bound: 0.5234386920928955, Entropy: 137.2157745361328, Temp: 1.4375039339065552, KL: 70.15351867675781, Loss: 0.08826988190412521, Learning Rate: 0.020999999999999998\n",
      "Epoch [2546/20000], Bound: 0.5665388107299805, Entropy: 139.04347229003906, Temp: 1.4376816749572754, KL: 74.95750427246094, Loss: 0.11070149391889572, Learning Rate: 0.020999999999999998\n",
      "Epoch [2547/20000], Bound: 0.5505884289741516, Entropy: 142.19418334960938, Temp: 1.43772292137146, KL: 71.48094177246094, Loss: 0.10805889964103699, Learning Rate: 0.020999999999999998\n",
      "Epoch [2548/20000], Bound: 0.5580326318740845, Entropy: 139.26329040527344, Temp: 1.4376263618469238, KL: 69.41744995117188, Loss: 0.12206688523292542, Learning Rate: 0.020999999999999998\n",
      "Epoch [2549/20000], Bound: 0.654799222946167, Entropy: 138.7219696044922, Temp: 1.4372533559799194, KL: 100.397705078125, Loss: 0.10988634079694748, Learning Rate: 0.020999999999999998\n",
      "Epoch [2550/20000], Bound: 0.48945850133895874, Entropy: 135.6875, Temp: 1.437073826789856, KL: 62.16291809082031, Loss: 0.08666709810495377, Learning Rate: 0.020999999999999998\n",
      "Epoch [2551/20000], Bound: 0.5848328471183777, Entropy: 131.3842010498047, Temp: 1.4368739128112793, KL: 80.09619140625, Loss: 0.11004670709371567, Learning Rate: 0.020999999999999998\n",
      "Epoch [2552/20000], Bound: 0.5910612344741821, Entropy: 124.38774108886719, Temp: 1.4366390705108643, KL: 80.02452087402344, Loss: 0.11626199632883072, Learning Rate: 0.020999999999999998\n",
      "Epoch [2553/20000], Bound: 0.6228362917900085, Entropy: 123.94113159179688, Temp: 1.4363155364990234, KL: 95.59397888183594, Loss: 0.0934140756726265, Learning Rate: 0.020999999999999998\n",
      "Epoch [2554/20000], Bound: 0.5867420434951782, Entropy: 120.97796630859375, Temp: 1.4362846612930298, KL: 75.5792236328125, Loss: 0.12754379212856293, Learning Rate: 0.020999999999999998\n",
      "Epoch [2555/20000], Bound: 0.6240283846855164, Entropy: 114.20394134521484, Temp: 1.4359931945800781, KL: 83.47534942626953, Loss: 0.13677318394184113, Learning Rate: 0.020999999999999998\n",
      "Epoch [2556/20000], Bound: 0.6304060816764832, Entropy: 110.99742889404297, Temp: 1.4354737997055054, KL: 95.13472747802734, Loss: 0.10259351134300232, Learning Rate: 0.020999999999999998\n",
      "Epoch [2557/20000], Bound: 0.6138898730278015, Entropy: 110.81306457519531, Temp: 1.435179591178894, KL: 86.69587707519531, Loss: 0.11528367549180984, Learning Rate: 0.020999999999999998\n",
      "Epoch [2558/20000], Bound: 0.5887312293052673, Entropy: 105.71028900146484, Temp: 1.4348859786987305, KL: 81.12195587158203, Loss: 0.11002034693956375, Learning Rate: 0.020999999999999998\n",
      "Epoch [2559/20000], Bound: 0.5785160660743713, Entropy: 108.22928619384766, Temp: 1.4345816373825073, KL: 86.0533218383789, Loss: 0.08303548395633698, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2560/20000], Bound: 0.565382182598114, Entropy: 111.12916564941406, Temp: 1.4345707893371582, KL: 80.9613037109375, Loss: 0.08842221647500992, Learning Rate: 0.020999999999999998\n",
      "Epoch [2561/20000], Bound: 0.6250346899032593, Entropy: 113.97032928466797, Temp: 1.4347201585769653, KL: 90.09546661376953, Loss: 0.11459140479564667, Learning Rate: 0.020999999999999998\n",
      "Epoch [2562/20000], Bound: 0.5643336772918701, Entropy: 118.25299835205078, Temp: 1.4348690509796143, KL: 80.3683090209961, Loss: 0.08954211324453354, Learning Rate: 0.020999999999999998\n",
      "Epoch [2563/20000], Bound: 0.6354295015335083, Entropy: 122.87667083740234, Temp: 1.4351451396942139, KL: 94.22095489501953, Loss: 0.11088525503873825, Learning Rate: 0.020999999999999998\n",
      "Epoch [2564/20000], Bound: 0.5516152381896973, Entropy: 120.335205078125, Temp: 1.435483694076538, KL: 85.74208068847656, Loss: 0.05914168804883957, Learning Rate: 0.020999999999999998\n",
      "Epoch [2565/20000], Bound: 0.5964411497116089, Entropy: 124.23814392089844, Temp: 1.436275601387024, KL: 89.31724548339844, Loss: 0.08909162133932114, Learning Rate: 0.020999999999999998\n",
      "Epoch [2566/20000], Bound: 0.5657481551170349, Entropy: 130.4288787841797, Temp: 1.437226414680481, KL: 66.54579162597656, Loss: 0.139187291264534, Learning Rate: 0.020999999999999998\n",
      "Epoch [2567/20000], Bound: 0.5880537629127502, Entropy: 125.76309967041016, Temp: 1.4376049041748047, KL: 79.08856964111328, Loss: 0.11671299487352371, Learning Rate: 0.020999999999999998\n",
      "Epoch [2568/20000], Bound: 0.540639340877533, Entropy: 129.0416259765625, Temp: 1.437817096710205, KL: 76.0343017578125, Loss: 0.08319604396820068, Learning Rate: 0.020999999999999998\n",
      "Epoch [2569/20000], Bound: 0.5382169485092163, Entropy: 129.3439178466797, Temp: 1.438158631324768, KL: 74.12922668457031, Loss: 0.08766911178827286, Learning Rate: 0.020999999999999998\n",
      "Epoch [2570/20000], Bound: 0.6557602286338806, Entropy: 131.13389587402344, Temp: 1.4385528564453125, KL: 83.78431701660156, Loss: 0.16882964968681335, Learning Rate: 0.020999999999999998\n",
      "Epoch [2571/20000], Bound: 0.6213683485984802, Entropy: 130.08860778808594, Temp: 1.4383666515350342, KL: 77.56536865234375, Loss: 0.15487204492092133, Learning Rate: 0.020999999999999998\n",
      "Epoch [2572/20000], Bound: 0.5976181030273438, Entropy: 123.00569915771484, Temp: 1.4377095699310303, KL: 83.00374603271484, Loss: 0.11236467957496643, Learning Rate: 0.020999999999999998\n",
      "Epoch [2573/20000], Bound: 0.605421781539917, Entropy: 120.32146453857422, Temp: 1.4370718002319336, KL: 83.73273468017578, Loss: 0.11740901321172714, Learning Rate: 0.020999999999999998\n",
      "Epoch [2574/20000], Bound: 0.5244965553283691, Entropy: 112.10460662841797, Temp: 1.436414361000061, KL: 68.47173309326172, Loss: 0.0949685201048851, Learning Rate: 0.020999999999999998\n",
      "Epoch [2575/20000], Bound: 0.5523940324783325, Entropy: 114.9028091430664, Temp: 1.4357783794403076, KL: 70.16809844970703, Loss: 0.11412481218576431, Learning Rate: 0.020999999999999998\n",
      "Epoch [2576/20000], Bound: 0.5903521180152893, Entropy: 113.18827819824219, Temp: 1.4350026845932007, KL: 90.201171875, Loss: 0.07995950430631638, Learning Rate: 0.020999999999999998\n",
      "Epoch [2577/20000], Bound: 0.6258085370063782, Entropy: 117.2121353149414, Temp: 1.434639811515808, KL: 85.74503326416016, Loss: 0.13052888214588165, Learning Rate: 0.020999999999999998\n",
      "Epoch [2578/20000], Bound: 0.5644416213035583, Entropy: 123.12368774414062, Temp: 1.4341375827789307, KL: 84.7489013671875, Loss: 0.07429547607898712, Learning Rate: 0.020999999999999998\n",
      "Epoch [2579/20000], Bound: 0.6164805293083191, Entropy: 124.47625732421875, Temp: 1.4340201616287231, KL: 89.52764892578125, Loss: 0.10787032544612885, Learning Rate: 0.020999999999999998\n",
      "Epoch [2580/20000], Bound: 0.6445402503013611, Entropy: 126.89559173583984, Temp: 1.4339854717254639, KL: 87.3693618774414, Loss: 0.14406877756118774, Learning Rate: 0.020999999999999998\n",
      "Epoch [2581/20000], Bound: 0.6714649796485901, Entropy: 127.98779296875, Temp: 1.433677315711975, KL: 90.20480346679688, Loss: 0.16281898319721222, Learning Rate: 0.020999999999999998\n",
      "Epoch [2582/20000], Bound: 0.5941246747970581, Entropy: 121.8646240234375, Temp: 1.4329910278320312, KL: 82.33567810058594, Loss: 0.11080792546272278, Learning Rate: 0.020999999999999998\n",
      "Epoch [2583/20000], Bound: 0.6291317939758301, Entropy: 118.18736267089844, Temp: 1.432342290878296, KL: 86.25218200683594, Loss: 0.13189195096492767, Learning Rate: 0.020999999999999998\n",
      "Epoch [2584/20000], Bound: 0.6496245861053467, Entropy: 113.95099639892578, Temp: 1.4315801858901978, KL: 90.64002227783203, Loss: 0.1377059519290924, Learning Rate: 0.020999999999999998\n",
      "Epoch [2585/20000], Bound: 0.5593735575675964, Entropy: 112.4255142211914, Temp: 1.430713415145874, KL: 76.2212905883789, Loss: 0.0990016907453537, Learning Rate: 0.020999999999999998\n",
      "Epoch [2586/20000], Bound: 0.49934208393096924, Entropy: 112.34044647216797, Temp: 1.429947853088379, KL: 65.94037628173828, Loss: 0.08139021694660187, Learning Rate: 0.020999999999999998\n",
      "Epoch [2587/20000], Bound: 0.5833471417427063, Entropy: 112.84961700439453, Temp: 1.4293256998062134, KL: 79.14002227783203, Loss: 0.11122193187475204, Learning Rate: 0.020999999999999998\n",
      "Epoch [2588/20000], Bound: 0.56252121925354, Entropy: 118.96859741210938, Temp: 1.428701639175415, KL: 83.80596923828125, Loss: 0.07519011944532394, Learning Rate: 0.020999999999999998\n",
      "Epoch [2589/20000], Bound: 0.5906293392181396, Entropy: 124.24748992919922, Temp: 1.4284659624099731, KL: 88.8416976928711, Loss: 0.08417978137731552, Learning Rate: 0.020999999999999998\n",
      "Epoch [2590/20000], Bound: 0.591109037399292, Entropy: 122.85396575927734, Temp: 1.4285476207733154, KL: 84.8767318725586, Loss: 0.0985308438539505, Learning Rate: 0.020999999999999998\n",
      "Epoch [2591/20000], Bound: 0.5578674077987671, Entropy: 129.3040313720703, Temp: 1.428739070892334, KL: 75.63737487792969, Loss: 0.09946633875370026, Learning Rate: 0.020999999999999998\n",
      "Epoch [2592/20000], Bound: 0.5240324139595032, Entropy: 130.44187927246094, Temp: 1.4289186000823975, KL: 76.98065185546875, Loss: 0.06418871879577637, Learning Rate: 0.020999999999999998\n",
      "Epoch [2593/20000], Bound: 0.5710694193840027, Entropy: 132.96707153320312, Temp: 1.429439902305603, KL: 84.86518859863281, Loss: 0.07956237345933914, Learning Rate: 0.020999999999999998\n",
      "Epoch [2594/20000], Bound: 0.540701687335968, Entropy: 132.7254180908203, Temp: 1.430203914642334, KL: 73.29969787597656, Loss: 0.09208936244249344, Learning Rate: 0.020999999999999998\n",
      "Epoch [2595/20000], Bound: 0.6041584610939026, Entropy: 131.32040405273438, Temp: 1.4309403896331787, KL: 76.50289916992188, Loss: 0.14078272879123688, Learning Rate: 0.020999999999999998\n",
      "Epoch [2596/20000], Bound: 0.6248518228530884, Entropy: 127.34730529785156, Temp: 1.4312372207641602, KL: 93.66007995605469, Loss: 0.10153945535421371, Learning Rate: 0.020999999999999998\n",
      "Epoch [2597/20000], Bound: 0.5581021904945374, Entropy: 129.89865112304688, Temp: 1.4316822290420532, KL: 68.77558898925781, Loss: 0.12391956150531769, Learning Rate: 0.020999999999999998\n",
      "Epoch [2598/20000], Bound: 0.5818212628364563, Entropy: 128.16566467285156, Temp: 1.431778073310852, KL: 81.64497375488281, Loss: 0.10125613957643509, Learning Rate: 0.020999999999999998\n",
      "Epoch [2599/20000], Bound: 0.6336716413497925, Entropy: 120.93270874023438, Temp: 1.431916356086731, KL: 87.70431518554688, Loss: 0.13142085075378418, Learning Rate: 0.020999999999999998\n",
      "Epoch [2600/20000], Bound: 0.6515187621116638, Entropy: 119.52477264404297, Temp: 1.4318827390670776, KL: 91.83118438720703, Loss: 0.13557682931423187, Learning Rate: 0.020999999999999998\n",
      "Epoch [2601/20000], Bound: 0.573249101638794, Entropy: 114.98867797851562, Temp: 1.43170166015625, KL: 85.86854553222656, Loss: 0.07836680859327316, Learning Rate: 0.020999999999999998\n",
      "Epoch [2602/20000], Bound: 0.6077508330345154, Entropy: 112.33052825927734, Temp: 1.4318524599075317, KL: 78.5553207397461, Loss: 0.13724060356616974, Learning Rate: 0.020999999999999998\n",
      "Epoch [2603/20000], Bound: 0.6412646770477295, Entropy: 112.97187042236328, Temp: 1.4316751956939697, KL: 96.38831329345703, Loss: 0.10890629887580872, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2604/20000], Bound: 0.6136173009872437, Entropy: 108.26884460449219, Temp: 1.4316534996032715, KL: 85.66143798828125, Loss: 0.11823124438524246, Learning Rate: 0.020999999999999998\n",
      "Epoch [2605/20000], Bound: 0.614361047744751, Entropy: 107.17259216308594, Temp: 1.431573510169983, KL: 92.27783203125, Loss: 0.09585592150688171, Learning Rate: 0.020999999999999998\n",
      "Epoch [2606/20000], Bound: 0.6299094557762146, Entropy: 109.53456115722656, Temp: 1.4317176342010498, KL: 97.73489379882812, Loss: 0.09251618385314941, Learning Rate: 0.020999999999999998\n",
      "Epoch [2607/20000], Bound: 0.6129553914070129, Entropy: 111.78596496582031, Temp: 1.43215012550354, KL: 90.87387084960938, Loss: 0.09942780435085297, Learning Rate: 0.020999999999999998\n",
      "Epoch [2608/20000], Bound: 0.5969063639640808, Entropy: 111.08322143554688, Temp: 1.4327070713043213, KL: 79.42692565917969, Loss: 0.12363411486148834, Learning Rate: 0.020999999999999998\n",
      "Epoch [2609/20000], Bound: 0.6490823030471802, Entropy: 113.4587173461914, Temp: 1.4330271482467651, KL: 98.07666778564453, Loss: 0.11135947704315186, Learning Rate: 0.020999999999999998\n",
      "Epoch [2610/20000], Bound: 0.607554018497467, Entropy: 118.6547622680664, Temp: 1.4334460496902466, KL: 88.6888656616211, Loss: 0.10184673219919205, Learning Rate: 0.020999999999999998\n",
      "Epoch [2611/20000], Bound: 0.5923404693603516, Entropy: 122.16909790039062, Temp: 1.4339430332183838, KL: 85.21878051757812, Loss: 0.09912434965372086, Learning Rate: 0.020999999999999998\n",
      "Epoch [2612/20000], Bound: 0.5229793787002563, Entropy: 118.10237121582031, Temp: 1.4344983100891113, KL: 68.85775756835938, Loss: 0.09213168919086456, Learning Rate: 0.020999999999999998\n",
      "Epoch [2613/20000], Bound: 0.5273235440254211, Entropy: 124.86933135986328, Temp: 1.4349884986877441, KL: 72.95386505126953, Loss: 0.08174341171979904, Learning Rate: 0.020999999999999998\n",
      "Epoch [2614/20000], Bound: 0.5808979272842407, Entropy: 125.03351593017578, Temp: 1.4355671405792236, KL: 81.8274154663086, Loss: 0.10013096779584885, Learning Rate: 0.020999999999999998\n",
      "Epoch [2615/20000], Bound: 0.5613584518432617, Entropy: 129.7482147216797, Temp: 1.4361467361450195, KL: 79.14454650878906, Loss: 0.09116750210523605, Learning Rate: 0.020999999999999998\n",
      "Epoch [2616/20000], Bound: 0.6039230227470398, Entropy: 130.53793334960938, Temp: 1.4367820024490356, KL: 74.99777221679688, Loss: 0.14630132913589478, Learning Rate: 0.020999999999999998\n",
      "Epoch [2617/20000], Bound: 0.6353229284286499, Entropy: 129.78985595703125, Temp: 1.4369059801101685, KL: 78.86361694335938, Loss: 0.16443778574466705, Learning Rate: 0.020999999999999998\n",
      "Epoch [2618/20000], Bound: 0.5751641392707825, Entropy: 127.3572998046875, Temp: 1.4364534616470337, KL: 80.3961181640625, Loss: 0.0997653529047966, Learning Rate: 0.020999999999999998\n",
      "Epoch [2619/20000], Bound: 0.5951691269874573, Entropy: 124.53810119628906, Temp: 1.4360905885696411, KL: 77.92689514160156, Loss: 0.12749050557613373, Learning Rate: 0.020999999999999998\n",
      "Epoch [2620/20000], Bound: 0.5441777110099792, Entropy: 125.14334869384766, Temp: 1.4355230331420898, KL: 74.73430633544922, Loss: 0.09070903807878494, Learning Rate: 0.020999999999999998\n",
      "Epoch [2621/20000], Bound: 0.5997647047042847, Entropy: 122.3486328125, Temp: 1.4350817203521729, KL: 85.56886291503906, Loss: 0.10525088757276535, Learning Rate: 0.020999999999999998\n",
      "Epoch [2622/20000], Bound: 0.6285616755485535, Entropy: 120.0784912109375, Temp: 1.4347363710403442, KL: 92.84298706054688, Loss: 0.10860268026590347, Learning Rate: 0.020999999999999998\n",
      "Epoch [2623/20000], Bound: 0.5804896950721741, Entropy: 113.30109405517578, Temp: 1.4345245361328125, KL: 83.0970230102539, Loss: 0.09520939737558365, Learning Rate: 0.020999999999999998\n",
      "Epoch [2624/20000], Bound: 0.5307427644729614, Entropy: 110.29987335205078, Temp: 1.4344546794891357, KL: 73.88324737548828, Loss: 0.08149760216474533, Learning Rate: 0.020999999999999998\n",
      "Epoch [2625/20000], Bound: 0.557263970375061, Entropy: 117.2082748413086, Temp: 1.4345430135726929, KL: 77.53063201904297, Loss: 0.09284517914056778, Learning Rate: 0.020999999999999998\n",
      "Epoch [2626/20000], Bound: 0.5929428935050964, Entropy: 120.9251937866211, Temp: 1.4347047805786133, KL: 82.37665557861328, Loss: 0.10969632863998413, Learning Rate: 0.020999999999999998\n",
      "Epoch [2627/20000], Bound: 0.539657711982727, Entropy: 118.51543426513672, Temp: 1.4348267316818237, KL: 74.7968521118164, Loss: 0.08634026348590851, Learning Rate: 0.020999999999999998\n",
      "Epoch [2628/20000], Bound: 0.5536994934082031, Entropy: 124.29136657714844, Temp: 1.435050368309021, KL: 75.65953063964844, Loss: 0.09613211452960968, Learning Rate: 0.020999999999999998\n",
      "Epoch [2629/20000], Bound: 0.5281371474266052, Entropy: 127.21878814697266, Temp: 1.435280680656433, KL: 71.47347259521484, Loss: 0.08764901757240295, Learning Rate: 0.020999999999999998\n",
      "Epoch [2630/20000], Bound: 0.5850640535354614, Entropy: 130.65182495117188, Temp: 1.4355510473251343, KL: 87.617431640625, Loss: 0.08394193649291992, Learning Rate: 0.020999999999999998\n",
      "Epoch [2631/20000], Bound: 0.6306064128875732, Entropy: 131.21054077148438, Temp: 1.4360700845718384, KL: 94.03579711914062, Loss: 0.10670164227485657, Learning Rate: 0.020999999999999998\n",
      "Epoch [2632/20000], Bound: 0.5457538366317749, Entropy: 130.55621337890625, Temp: 1.4366649389266968, KL: 72.81878662109375, Loss: 0.09890986979007721, Learning Rate: 0.020999999999999998\n",
      "Epoch [2633/20000], Bound: 0.5620006918907166, Entropy: 128.69247436523438, Temp: 1.4371678829193115, KL: 77.48414611816406, Loss: 0.09764103591442108, Learning Rate: 0.020999999999999998\n",
      "Epoch [2634/20000], Bound: 0.5887420177459717, Entropy: 129.91258239746094, Temp: 1.4376522302627563, KL: 87.47679138183594, Loss: 0.08820629864931107, Learning Rate: 0.020999999999999998\n",
      "Epoch [2635/20000], Bound: 0.6126278042793274, Entropy: 124.68077087402344, Temp: 1.4383186101913452, KL: 89.22091674804688, Loss: 0.10559560358524323, Learning Rate: 0.020999999999999998\n",
      "Epoch [2636/20000], Bound: 0.5733339786529541, Entropy: 117.591064453125, Temp: 1.4390016794204712, KL: 78.29025268554688, Loss: 0.10561006516218185, Learning Rate: 0.020999999999999998\n",
      "Epoch [2637/20000], Bound: 0.5603876709938049, Entropy: 115.7646255493164, Temp: 1.4395781755447388, KL: 81.99292755126953, Loss: 0.08071323484182358, Learning Rate: 0.020999999999999998\n",
      "Epoch [2638/20000], Bound: 0.5775490999221802, Entropy: 110.26583862304688, Temp: 1.4403384923934937, KL: 82.91505432128906, Loss: 0.093663789331913, Learning Rate: 0.020999999999999998\n",
      "Epoch [2639/20000], Bound: 0.553415060043335, Entropy: 111.89132690429688, Temp: 1.441148042678833, KL: 83.34640502929688, Loss: 0.06975783407688141, Learning Rate: 0.020999999999999998\n",
      "Epoch [2640/20000], Bound: 0.5616294741630554, Entropy: 112.93948364257812, Temp: 1.4422377347946167, KL: 84.62968444824219, Loss: 0.07300081849098206, Learning Rate: 0.020999999999999998\n",
      "Epoch [2641/20000], Bound: 0.5521343350410461, Entropy: 118.3249740600586, Temp: 1.443560004234314, KL: 76.56041717529297, Loss: 0.09235101193189621, Learning Rate: 0.020999999999999998\n",
      "Epoch [2642/20000], Bound: 0.5954806804656982, Entropy: 119.47378540039062, Temp: 1.4448128938674927, KL: 90.52589416503906, Loss: 0.08498144149780273, Learning Rate: 0.020999999999999998\n",
      "Epoch [2643/20000], Bound: 0.5742021203041077, Entropy: 125.30003356933594, Temp: 1.4462229013442993, KL: 82.14656066894531, Loss: 0.09377139806747437, Learning Rate: 0.020999999999999998\n",
      "Epoch [2644/20000], Bound: 0.5907309055328369, Entropy: 126.41658782958984, Temp: 1.4475986957550049, KL: 82.86531829833984, Loss: 0.10717941075563431, Learning Rate: 0.020999999999999998\n",
      "Epoch [2645/20000], Bound: 0.544082522392273, Entropy: 131.86300659179688, Temp: 1.4488201141357422, KL: 72.24308776855469, Loss: 0.10040809214115143, Learning Rate: 0.020999999999999998\n",
      "Epoch [2646/20000], Bound: 0.5300873517990112, Entropy: 130.54177856445312, Temp: 1.4498461484909058, KL: 72.97151184082031, Loss: 0.08543403446674347, Learning Rate: 0.020999999999999998\n",
      "Epoch [2647/20000], Bound: 0.5383279919624329, Entropy: 132.83450317382812, Temp: 1.450850009918213, KL: 74.71119689941406, Loss: 0.08688249439001083, Learning Rate: 0.020999999999999998\n",
      "Epoch [2648/20000], Bound: 0.5951387882232666, Entropy: 129.79736328125, Temp: 1.4518375396728516, KL: 91.06060791015625, Loss: 0.08364074677228928, Learning Rate: 0.020999999999999998\n",
      "Epoch [2649/20000], Bound: 0.6413758397102356, Entropy: 131.35667419433594, Temp: 1.4530158042907715, KL: 79.97404479980469, Loss: 0.16826792061328888, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2650/20000], Bound: 0.5619658827781677, Entropy: 129.5006103515625, Temp: 1.4534494876861572, KL: 70.1658935546875, Loss: 0.12429144233465195, Learning Rate: 0.020999999999999998\n",
      "Epoch [2651/20000], Bound: 0.5091145634651184, Entropy: 126.69013214111328, Temp: 1.4535036087036133, KL: 65.27982330322266, Loss: 0.09383250027894974, Learning Rate: 0.020999999999999998\n",
      "Epoch [2652/20000], Bound: 0.49497178196907043, Entropy: 127.68014526367188, Temp: 1.4534540176391602, KL: 64.98191833496094, Loss: 0.08273643255233765, Learning Rate: 0.020999999999999998\n",
      "Epoch [2653/20000], Bound: 0.5273227095603943, Entropy: 126.93914031982422, Temp: 1.4534187316894531, KL: 68.35086822509766, Loss: 0.09918346256017685, Learning Rate: 0.020999999999999998\n",
      "Epoch [2654/20000], Bound: 0.5833371877670288, Entropy: 126.65048217773438, Temp: 1.453271746635437, KL: 82.21830749511719, Loss: 0.10288937389850616, Learning Rate: 0.020999999999999998\n",
      "Epoch [2655/20000], Bound: 0.6010145545005798, Entropy: 127.72284698486328, Temp: 1.453145980834961, KL: 90.28844451904297, Loss: 0.09217098355293274, Learning Rate: 0.020999999999999998\n",
      "Epoch [2656/20000], Bound: 0.5620986223220825, Entropy: 133.17779541015625, Temp: 1.4532291889190674, KL: 69.84489440917969, Loss: 0.1255025863647461, Learning Rate: 0.020999999999999998\n",
      "Epoch [2657/20000], Bound: 0.566167414188385, Entropy: 129.21441650390625, Temp: 1.4529517889022827, KL: 77.51887512207031, Loss: 0.10285263508558273, Learning Rate: 0.020999999999999998\n",
      "Epoch [2658/20000], Bound: 0.5968189835548401, Entropy: 126.6858139038086, Temp: 1.4526572227478027, KL: 87.59358978271484, Loss: 0.09730175137519836, Learning Rate: 0.020999999999999998\n",
      "Epoch [2659/20000], Bound: 0.5886651873588562, Entropy: 123.08855438232422, Temp: 1.4525113105773926, KL: 84.7835464477539, Loss: 0.09908223152160645, Learning Rate: 0.020999999999999998\n",
      "Epoch [2660/20000], Bound: 0.6147106289863586, Entropy: 122.72722625732422, Temp: 1.4524524211883545, KL: 88.65694427490234, Loss: 0.11122439056634903, Learning Rate: 0.020999999999999998\n",
      "Epoch [2661/20000], Bound: 0.623769223690033, Entropy: 116.26499938964844, Temp: 1.452397346496582, KL: 89.99745178222656, Loss: 0.11569201946258545, Learning Rate: 0.020999999999999998\n",
      "Epoch [2662/20000], Bound: 0.5868618488311768, Entropy: 110.03784942626953, Temp: 1.4523179531097412, KL: 87.99657440185547, Loss: 0.08627033233642578, Learning Rate: 0.020999999999999998\n",
      "Epoch [2663/20000], Bound: 0.5394530892372131, Entropy: 109.9051513671875, Temp: 1.4524784088134766, KL: 79.63041687011719, Loss: 0.07110217213630676, Learning Rate: 0.020999999999999998\n",
      "Epoch [2664/20000], Bound: 0.6247955560684204, Entropy: 107.63213348388672, Temp: 1.452915906906128, KL: 90.70569610595703, Loss: 0.1143503412604332, Learning Rate: 0.020999999999999998\n",
      "Epoch [2665/20000], Bound: 0.5365424156188965, Entropy: 110.40678405761719, Temp: 1.4532991647720337, KL: 76.4307861328125, Loss: 0.07958083599805832, Learning Rate: 0.020999999999999998\n",
      "Epoch [2666/20000], Bound: 0.5841527581214905, Entropy: 112.93466186523438, Temp: 1.4538164138793945, KL: 89.36935424804688, Loss: 0.0791269987821579, Learning Rate: 0.020999999999999998\n",
      "Epoch [2667/20000], Bound: 0.5815982818603516, Entropy: 119.63850402832031, Temp: 1.4545965194702148, KL: 87.33314514160156, Loss: 0.0837831124663353, Learning Rate: 0.020999999999999998\n",
      "Epoch [2668/20000], Bound: 0.5588484406471252, Entropy: 124.78966522216797, Temp: 1.4555448293685913, KL: 79.7986068725586, Loss: 0.08847526460886002, Learning Rate: 0.020999999999999998\n",
      "Epoch [2669/20000], Bound: 0.5915555357933044, Entropy: 130.45587158203125, Temp: 1.456516146659851, KL: 81.89883422851562, Loss: 0.11218156665563583, Learning Rate: 0.020999999999999998\n",
      "Epoch [2670/20000], Bound: 0.660536527633667, Entropy: 126.00746154785156, Temp: 1.4572983980178833, KL: 100.25978088378906, Loss: 0.11918722093105316, Learning Rate: 0.020999999999999998\n",
      "Epoch [2671/20000], Bound: 0.6104387044906616, Entropy: 129.6009979248047, Temp: 1.4580379724502563, KL: 77.85568237304688, Loss: 0.1446358859539032, Learning Rate: 0.020999999999999998\n",
      "Epoch [2672/20000], Bound: 0.6464402079582214, Entropy: 126.22554779052734, Temp: 1.458254337310791, KL: 92.25409698486328, Loss: 0.131883442401886, Learning Rate: 0.020999999999999998\n",
      "Epoch [2673/20000], Bound: 0.5884953141212463, Entropy: 121.89498138427734, Temp: 1.4582798480987549, KL: 83.07707977294922, Loss: 0.10536837577819824, Learning Rate: 0.020999999999999998\n",
      "Epoch [2674/20000], Bound: 0.5849743485450745, Entropy: 118.7259750366211, Temp: 1.4582858085632324, KL: 85.65312957763672, Loss: 0.09316638857126236, Learning Rate: 0.020999999999999998\n",
      "Epoch [2675/20000], Bound: 0.568763256072998, Entropy: 116.1230239868164, Temp: 1.458421230316162, KL: 82.46466827392578, Loss: 0.0888056606054306, Learning Rate: 0.020999999999999998\n",
      "Epoch [2676/20000], Bound: 0.5883921384811401, Entropy: 112.75938415527344, Temp: 1.458681583404541, KL: 88.14439392089844, Loss: 0.08793965727090836, Learning Rate: 0.020999999999999998\n",
      "Epoch [2677/20000], Bound: 0.5365339517593384, Entropy: 111.25990295410156, Temp: 1.4591236114501953, KL: 74.01802062988281, Loss: 0.08837182819843292, Learning Rate: 0.020999999999999998\n",
      "Epoch [2678/20000], Bound: 0.5101454854011536, Entropy: 112.86235046386719, Temp: 1.4595698118209839, KL: 74.26873779296875, Loss: 0.06434910744428635, Learning Rate: 0.020999999999999998\n",
      "Epoch [2679/20000], Bound: 0.5439642667770386, Entropy: 119.54817962646484, Temp: 1.460266351699829, KL: 79.49256134033203, Loss: 0.07640008628368378, Learning Rate: 0.020999999999999998\n",
      "Epoch [2680/20000], Bound: 0.5936871767044067, Entropy: 121.34153747558594, Temp: 1.4611214399337769, KL: 76.27043151855469, Loss: 0.13393908739089966, Learning Rate: 0.020999999999999998\n",
      "Epoch [2681/20000], Bound: 0.4973929226398468, Entropy: 127.79363250732422, Temp: 1.4615168571472168, KL: 66.17485809326172, Loss: 0.08127987384796143, Learning Rate: 0.020999999999999998\n",
      "Epoch [2682/20000], Bound: 0.5982750058174133, Entropy: 129.9718475341797, Temp: 1.4618993997573853, KL: 75.23155212402344, Loss: 0.14200237393379211, Learning Rate: 0.020999999999999998\n",
      "Epoch [2683/20000], Bound: 0.5997499823570251, Entropy: 135.36289978027344, Temp: 1.4617793560028076, KL: 84.99861145019531, Loss: 0.11002055555582047, Learning Rate: 0.020999999999999998\n",
      "Epoch [2684/20000], Bound: 0.6076130270957947, Entropy: 140.62841796875, Temp: 1.4616237878799438, KL: 85.86236572265625, Loss: 0.11475567519664764, Learning Rate: 0.020999999999999998\n",
      "Epoch [2685/20000], Bound: 0.5812820196151733, Entropy: 138.87559509277344, Temp: 1.4614002704620361, KL: 80.41787719726562, Loss: 0.10789027810096741, Learning Rate: 0.020999999999999998\n",
      "Epoch [2686/20000], Bound: 0.6133038997650146, Entropy: 136.0599365234375, Temp: 1.4611222743988037, KL: 86.25767517089844, Loss: 0.1189834251999855, Learning Rate: 0.020999999999999998\n",
      "Epoch [2687/20000], Bound: 0.5801042318344116, Entropy: 127.4386978149414, Temp: 1.460753083229065, KL: 85.58954620361328, Loss: 0.08901054412126541, Learning Rate: 0.020999999999999998\n",
      "Epoch [2688/20000], Bound: 0.5818822979927063, Entropy: 121.8670883178711, Temp: 1.4605876207351685, KL: 88.43228912353516, Loss: 0.08094953000545502, Learning Rate: 0.020999999999999998\n",
      "Epoch [2689/20000], Bound: 0.5662185549736023, Entropy: 118.5270767211914, Temp: 1.4607161283493042, KL: 79.85918426513672, Loss: 0.09558197110891342, Learning Rate: 0.020999999999999998\n",
      "Epoch [2690/20000], Bound: 0.6319591403007507, Entropy: 110.46194458007812, Temp: 1.4608712196350098, KL: 97.52191162109375, Loss: 0.09919856488704681, Learning Rate: 0.020999999999999998\n",
      "Epoch [2691/20000], Bound: 0.6027916669845581, Entropy: 102.77455139160156, Temp: 1.461202621459961, KL: 84.36970520019531, Loss: 0.11508481949567795, Learning Rate: 0.020999999999999998\n",
      "Epoch [2692/20000], Bound: 0.5905361175537109, Entropy: 104.94915771484375, Temp: 1.4613983631134033, KL: 83.80149841308594, Loss: 0.1051572933793068, Learning Rate: 0.020999999999999998\n",
      "Epoch [2693/20000], Bound: 0.6203430891036987, Entropy: 106.02951049804688, Temp: 1.4615622758865356, KL: 96.26533508300781, Loss: 0.09182298183441162, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2694/20000], Bound: 0.5696997046470642, Entropy: 113.48351287841797, Temp: 1.4619593620300293, KL: 85.5715560913086, Loss: 0.07940690964460373, Learning Rate: 0.020999999999999998\n",
      "Epoch [2695/20000], Bound: 0.6169065833091736, Entropy: 112.88488006591797, Temp: 1.462578296661377, KL: 91.9911880493164, Loss: 0.10312067717313766, Learning Rate: 0.020999999999999998\n",
      "Epoch [2696/20000], Bound: 0.5531074404716492, Entropy: 122.2831802368164, Temp: 1.4632290601730347, KL: 76.74175262451172, Loss: 0.09438570588827133, Learning Rate: 0.020999999999999998\n",
      "Epoch [2697/20000], Bound: 0.5354573130607605, Entropy: 124.95539093017578, Temp: 1.46382737159729, KL: 76.47827911376953, Loss: 0.07940300554037094, Learning Rate: 0.020999999999999998\n",
      "Epoch [2698/20000], Bound: 0.5571432113647461, Entropy: 129.82333374023438, Temp: 1.4645262956619263, KL: 76.5013427734375, Loss: 0.09901610761880875, Learning Rate: 0.020999999999999998\n",
      "Epoch [2699/20000], Bound: 0.6021701693534851, Entropy: 129.458251953125, Temp: 1.4651167392730713, KL: 82.04904174804688, Loss: 0.12278396636247635, Learning Rate: 0.020999999999999998\n",
      "Epoch [2700/20000], Bound: 0.5790337920188904, Entropy: 132.09109497070312, Temp: 1.4654375314712524, KL: 78.58169555664062, Loss: 0.1123976781964302, Learning Rate: 0.020999999999999998\n",
      "Epoch [2701/20000], Bound: 0.667385995388031, Entropy: 128.84890747070312, Temp: 1.465577244758606, KL: 95.96408081054688, Loss: 0.14227017760276794, Learning Rate: 0.020999999999999998\n",
      "Epoch [2702/20000], Bound: 0.5917608141899109, Entropy: 125.0469970703125, Temp: 1.4654589891433716, KL: 84.6920166015625, Loss: 0.10369941592216492, Learning Rate: 0.020999999999999998\n",
      "Epoch [2703/20000], Bound: 0.6180933117866516, Entropy: 116.74595642089844, Temp: 1.465356469154358, KL: 88.89071655273438, Loss: 0.11520484834909439, Learning Rate: 0.020999999999999998\n",
      "Epoch [2704/20000], Bound: 0.6883058547973633, Entropy: 115.73243713378906, Temp: 1.4652020931243896, KL: 91.36137390136719, Loss: 0.18086709082126617, Learning Rate: 0.020999999999999998\n",
      "Epoch [2705/20000], Bound: 0.587566614151001, Entropy: 113.00625610351562, Temp: 1.464415192604065, KL: 85.34736633300781, Loss: 0.09732911735773087, Learning Rate: 0.020999999999999998\n",
      "Epoch [2706/20000], Bound: 0.6308647990226746, Entropy: 106.73107147216797, Temp: 1.4637830257415771, KL: 89.51798248291016, Loss: 0.12579071521759033, Learning Rate: 0.020999999999999998\n",
      "Epoch [2707/20000], Bound: 0.6043581366539001, Entropy: 101.19190979003906, Temp: 1.4630595445632935, KL: 88.70161437988281, Loss: 0.10199932754039764, Learning Rate: 0.020999999999999998\n",
      "Epoch [2708/20000], Bound: 0.6306653022766113, Entropy: 102.3248291015625, Temp: 1.4624769687652588, KL: 92.81657409667969, Loss: 0.11416909843683243, Learning Rate: 0.020999999999999998\n",
      "Epoch [2709/20000], Bound: 0.5512312650680542, Entropy: 103.89728546142578, Temp: 1.4619477987289429, KL: 84.9346694946289, Loss: 0.06454048305749893, Learning Rate: 0.020999999999999998\n",
      "Epoch [2710/20000], Bound: 0.6036069393157959, Entropy: 108.86687469482422, Temp: 1.4618792533874512, KL: 91.18692779541016, Loss: 0.09263376891613007, Learning Rate: 0.020999999999999998\n",
      "Epoch [2711/20000], Bound: 0.5370287895202637, Entropy: 114.11283111572266, Temp: 1.4620075225830078, KL: 76.28385162353516, Loss: 0.08130872249603271, Learning Rate: 0.020999999999999998\n",
      "Epoch [2712/20000], Bound: 0.5931225419044495, Entropy: 119.53095245361328, Temp: 1.4622650146484375, KL: 75.01651763916016, Loss: 0.13777455687522888, Learning Rate: 0.020999999999999998\n",
      "Epoch [2713/20000], Bound: 0.5365515351295471, Entropy: 129.5463409423828, Temp: 1.4620641469955444, KL: 62.91139221191406, Loss: 0.12661847472190857, Learning Rate: 0.020999999999999998\n",
      "Epoch [2714/20000], Bound: 0.6337926983833313, Entropy: 135.0719451904297, Temp: 1.4614123106002808, KL: 86.24519348144531, Loss: 0.13972415030002594, Learning Rate: 0.020999999999999998\n",
      "Epoch [2715/20000], Bound: 0.6094930171966553, Entropy: 141.22628784179688, Temp: 1.4605066776275635, KL: 88.33596801757812, Loss: 0.10802842676639557, Learning Rate: 0.020999999999999998\n",
      "Epoch [2716/20000], Bound: 0.6578222513198853, Entropy: 140.45443725585938, Temp: 1.4597017765045166, KL: 87.60523986816406, Loss: 0.15996024012565613, Learning Rate: 0.020999999999999998\n",
      "Epoch [2717/20000], Bound: 0.586601734161377, Entropy: 134.75550842285156, Temp: 1.4584888219833374, KL: 83.22120666503906, Loss: 0.10308045893907547, Learning Rate: 0.020999999999999998\n",
      "Epoch [2718/20000], Bound: 0.5739684104919434, Entropy: 135.21971130371094, Temp: 1.4574036598205566, KL: 76.69926452636719, Loss: 0.11336164176464081, Learning Rate: 0.020999999999999998\n",
      "Epoch [2719/20000], Bound: 0.6550584435462952, Entropy: 127.52127838134766, Temp: 1.456259846687317, KL: 109.0914535522461, Loss: 0.08290484547615051, Learning Rate: 0.020999999999999998\n",
      "Epoch [2720/20000], Bound: 0.625469446182251, Entropy: 120.17579650878906, Temp: 1.45570707321167, KL: 88.12052917480469, Loss: 0.12422921508550644, Learning Rate: 0.020999999999999998\n",
      "Epoch [2721/20000], Bound: 0.5984500050544739, Entropy: 110.16195678710938, Temp: 1.4550697803497314, KL: 81.83079528808594, Loss: 0.1189548671245575, Learning Rate: 0.020999999999999998\n",
      "Epoch [2722/20000], Bound: 0.610781729221344, Entropy: 106.51294708251953, Temp: 1.4543380737304688, KL: 93.90973663330078, Loss: 0.08946927636861801, Learning Rate: 0.020999999999999998\n",
      "Epoch [2723/20000], Bound: 0.5789183378219604, Entropy: 104.20027160644531, Temp: 1.453942894935608, KL: 82.05413818359375, Loss: 0.09932095557451248, Learning Rate: 0.020999999999999998\n",
      "Epoch [2724/20000], Bound: 0.580722987651825, Entropy: 100.51258850097656, Temp: 1.453626036643982, KL: 87.6676025390625, Loss: 0.0816931426525116, Learning Rate: 0.020999999999999998\n",
      "Epoch [2725/20000], Bound: 0.5938464999198914, Entropy: 104.21062469482422, Temp: 1.4536174535751343, KL: 87.10819244384766, Loss: 0.09619548916816711, Learning Rate: 0.020999999999999998\n",
      "Epoch [2726/20000], Bound: 0.5755736231803894, Entropy: 106.12700653076172, Temp: 1.4537358283996582, KL: 82.34456634521484, Loss: 0.09513994306325912, Learning Rate: 0.020999999999999998\n",
      "Epoch [2727/20000], Bound: 0.5802273750305176, Entropy: 114.45027160644531, Temp: 1.4539263248443604, KL: 80.48109436035156, Loss: 0.10597025603055954, Learning Rate: 0.020999999999999998\n",
      "Epoch [2728/20000], Bound: 0.6161323189735413, Entropy: 124.0765609741211, Temp: 1.4540531635284424, KL: 82.20755767822266, Loss: 0.1349964588880539, Learning Rate: 0.020999999999999998\n",
      "Epoch [2729/20000], Bound: 0.5770939588546753, Entropy: 129.7248992919922, Temp: 1.4538605213165283, KL: 79.03559875488281, Loss: 0.10796759277582169, Learning Rate: 0.020999999999999998\n",
      "Epoch [2730/20000], Bound: 0.5409066677093506, Entropy: 137.25823974609375, Temp: 1.4536060094833374, KL: 72.02851867675781, Loss: 0.09866967052221298, Learning Rate: 0.020999999999999998\n",
      "Epoch [2731/20000], Bound: 0.6209412813186646, Entropy: 138.7895050048828, Temp: 1.4533073902130127, KL: 86.52201843261719, Loss: 0.12490158528089523, Learning Rate: 0.020999999999999998\n",
      "Epoch [2732/20000], Bound: 0.5747660398483276, Entropy: 138.42518615722656, Temp: 1.4528785943984985, KL: 82.06918334960938, Loss: 0.09524043649435043, Learning Rate: 0.020999999999999998\n",
      "Epoch [2733/20000], Bound: 0.6078919172286987, Entropy: 143.2826690673828, Temp: 1.4525742530822754, KL: 85.98794555664062, Loss: 0.11366217583417892, Learning Rate: 0.020999999999999998\n",
      "Epoch [2734/20000], Bound: 0.5654929876327515, Entropy: 138.1732940673828, Temp: 1.452244520187378, KL: 68.57632446289062, Loss: 0.13294976949691772, Learning Rate: 0.020999999999999998\n",
      "Epoch [2735/20000], Bound: 0.5246868133544922, Entropy: 135.52450561523438, Temp: 1.4514998197555542, KL: 70.32220458984375, Loss: 0.0899215117096901, Learning Rate: 0.020999999999999998\n",
      "Epoch [2736/20000], Bound: 0.5535537600517273, Entropy: 132.8048858642578, Temp: 1.4508315324783325, KL: 75.75004577636719, Loss: 0.0971064567565918, Learning Rate: 0.020999999999999998\n",
      "Epoch [2737/20000], Bound: 0.5706033110618591, Entropy: 129.0170135498047, Temp: 1.4502243995666504, KL: 85.34072875976562, Loss: 0.07978294044733047, Learning Rate: 0.020999999999999998\n",
      "Epoch [2738/20000], Bound: 0.6231651902198792, Entropy: 124.11106872558594, Temp: 1.4499560594558716, KL: 89.03694152832031, Loss: 0.11811798810958862, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2739/20000], Bound: 0.5549034476280212, Entropy: 123.03582000732422, Temp: 1.4496543407440186, KL: 79.57274627685547, Loss: 0.08505500853061676, Learning Rate: 0.020999999999999998\n",
      "Epoch [2740/20000], Bound: 0.5985645651817322, Entropy: 120.24951934814453, Temp: 1.4495445489883423, KL: 94.60626983642578, Loss: 0.07446707785129547, Learning Rate: 0.020999999999999998\n",
      "Epoch [2741/20000], Bound: 0.5725560188293457, Entropy: 117.82638549804688, Temp: 1.4498772621154785, KL: 82.92866516113281, Loss: 0.08989591151475906, Learning Rate: 0.020999999999999998\n",
      "Epoch [2742/20000], Bound: 0.5752876996994019, Entropy: 112.19203186035156, Temp: 1.4503272771835327, KL: 82.60054016113281, Loss: 0.09364499896764755, Learning Rate: 0.020999999999999998\n",
      "Epoch [2743/20000], Bound: 0.5984739661216736, Entropy: 115.00234985351562, Temp: 1.4508408308029175, KL: 81.58624267578125, Loss: 0.11941438168287277, Learning Rate: 0.020999999999999998\n",
      "Epoch [2744/20000], Bound: 0.620579183101654, Entropy: 114.8211669921875, Temp: 1.4511443376541138, KL: 99.74951171875, Loss: 0.0787343755364418, Learning Rate: 0.020999999999999998\n",
      "Epoch [2745/20000], Bound: 0.5626158714294434, Entropy: 114.19103240966797, Temp: 1.4518564939498901, KL: 78.03736114501953, Loss: 0.09766841679811478, Learning Rate: 0.020999999999999998\n",
      "Epoch [2746/20000], Bound: 0.5671711564064026, Entropy: 119.04702758789062, Temp: 1.4525113105773926, KL: 88.41595458984375, Loss: 0.06623704731464386, Learning Rate: 0.020999999999999998\n",
      "Epoch [2747/20000], Bound: 0.5553180575370789, Entropy: 120.0561294555664, Temp: 1.4535473585128784, KL: 76.12366485595703, Loss: 0.09767719358205795, Learning Rate: 0.020999999999999998\n",
      "Epoch [2748/20000], Bound: 0.5700328946113586, Entropy: 123.5810317993164, Temp: 1.4544686079025269, KL: 77.10118865966797, Loss: 0.10803315043449402, Learning Rate: 0.020999999999999998\n",
      "Epoch [2749/20000], Bound: 0.5181426405906677, Entropy: 127.48918914794922, Temp: 1.4551928043365479, KL: 72.84131622314453, Loss: 0.0758146196603775, Learning Rate: 0.020999999999999998\n",
      "Epoch [2750/20000], Bound: 0.5500811338424683, Entropy: 131.44366455078125, Temp: 1.456017017364502, KL: 74.83355712890625, Loss: 0.09753759205341339, Learning Rate: 0.020999999999999998\n",
      "Epoch [2751/20000], Bound: 0.5943308472633362, Entropy: 131.47715759277344, Temp: 1.4567296504974365, KL: 84.0552978515625, Loss: 0.10748060792684555, Learning Rate: 0.020999999999999998\n",
      "Epoch [2752/20000], Bound: 0.5339064002037048, Entropy: 135.82406616210938, Temp: 1.45734703540802, KL: 68.58023071289062, Loss: 0.10453077405691147, Learning Rate: 0.020999999999999998\n",
      "Epoch [2753/20000], Bound: 0.6695178151130676, Entropy: 135.7290496826172, Temp: 1.4577257633209229, KL: 101.8858642578125, Loss: 0.12331786006689072, Learning Rate: 0.020999999999999998\n",
      "Epoch [2754/20000], Bound: 0.5708107352256775, Entropy: 134.10531616210938, Temp: 1.4580795764923096, KL: 81.92335510253906, Loss: 0.0925421342253685, Learning Rate: 0.020999999999999998\n",
      "Epoch [2755/20000], Bound: 0.5764169692993164, Entropy: 131.16371154785156, Temp: 1.4584968090057373, KL: 81.25241088867188, Loss: 0.100152887403965, Learning Rate: 0.020999999999999998\n",
      "Epoch [2756/20000], Bound: 0.5536138415336609, Entropy: 126.23373413085938, Temp: 1.4588865041732788, KL: 78.87059020996094, Loss: 0.0871695876121521, Learning Rate: 0.020999999999999998\n",
      "Epoch [2757/20000], Bound: 0.5212543606758118, Entropy: 120.73599243164062, Temp: 1.459355354309082, KL: 74.01296997070312, Loss: 0.07487837970256805, Learning Rate: 0.020999999999999998\n",
      "Epoch [2758/20000], Bound: 0.562396228313446, Entropy: 118.23057556152344, Temp: 1.459966778755188, KL: 83.664306640625, Loss: 0.07893352955579758, Learning Rate: 0.020999999999999998\n",
      "Epoch [2759/20000], Bound: 0.5967139601707458, Entropy: 115.5482177734375, Temp: 1.4607717990875244, KL: 91.60646057128906, Loss: 0.08434850722551346, Learning Rate: 0.020999999999999998\n",
      "Epoch [2760/20000], Bound: 0.5468634366989136, Entropy: 114.64930725097656, Temp: 1.461780071258545, KL: 76.48703002929688, Loss: 0.0894479751586914, Learning Rate: 0.020999999999999998\n",
      "Epoch [2761/20000], Bound: 0.5649800300598145, Entropy: 116.40681457519531, Temp: 1.462750792503357, KL: 83.77198791503906, Loss: 0.08124662190675735, Learning Rate: 0.020999999999999998\n",
      "Epoch [2762/20000], Bound: 0.6181247234344482, Entropy: 115.11603546142578, Temp: 1.4638525247573853, KL: 92.7527084350586, Loss: 0.10188373923301697, Learning Rate: 0.020999999999999998\n",
      "Epoch [2763/20000], Bound: 0.5440518260002136, Entropy: 120.27989959716797, Temp: 1.4649584293365479, KL: 80.89832305908203, Loss: 0.0721297487616539, Learning Rate: 0.020999999999999998\n",
      "Epoch [2764/20000], Bound: 0.5483715534210205, Entropy: 125.06851196289062, Temp: 1.4662415981292725, KL: 73.99063110351562, Loss: 0.09971967339515686, Learning Rate: 0.020999999999999998\n",
      "Epoch [2765/20000], Bound: 0.57168048620224, Entropy: 127.23988342285156, Temp: 1.4673181772232056, KL: 86.80874633789062, Loss: 0.07760876417160034, Learning Rate: 0.020999999999999998\n",
      "Epoch [2766/20000], Bound: 0.5099055767059326, Entropy: 128.45947265625, Temp: 1.4685784578323364, KL: 77.06732177734375, Loss: 0.05540992692112923, Learning Rate: 0.020999999999999998\n",
      "Epoch [2767/20000], Bound: 0.5264765620231628, Entropy: 129.90235900878906, Temp: 1.470130443572998, KL: 69.01597595214844, Loss: 0.09738387912511826, Learning Rate: 0.020999999999999998\n",
      "Epoch [2768/20000], Bound: 0.5906890630722046, Entropy: 130.67869567871094, Temp: 1.4714078903198242, KL: 90.88426208496094, Loss: 0.08221956342458725, Learning Rate: 0.020999999999999998\n",
      "Epoch [2769/20000], Bound: 0.5631353855133057, Entropy: 135.63821411132812, Temp: 1.472838044166565, KL: 75.44635009765625, Loss: 0.10882405191659927, Learning Rate: 0.020999999999999998\n",
      "Epoch [2770/20000], Bound: 0.5466485619544983, Entropy: 131.4288787841797, Temp: 1.4739598035812378, KL: 82.53471374511719, Loss: 0.06979728490114212, Learning Rate: 0.020999999999999998\n",
      "Epoch [2771/20000], Bound: 0.5760078430175781, Entropy: 127.4806900024414, Temp: 1.4752857685089111, KL: 88.0152816772461, Loss: 0.07842613011598587, Learning Rate: 0.020999999999999998\n",
      "Epoch [2772/20000], Bound: 0.5154377222061157, Entropy: 128.02793884277344, Temp: 1.4767619371414185, KL: 74.21940612792969, Loss: 0.07059529423713684, Learning Rate: 0.020999999999999998\n",
      "Epoch [2773/20000], Bound: 0.5531677007675171, Entropy: 127.98512268066406, Temp: 1.4783031940460205, KL: 78.0684814453125, Loss: 0.09124847501516342, Learning Rate: 0.020999999999999998\n",
      "Epoch [2774/20000], Bound: 0.5707064270973206, Entropy: 126.09427642822266, Temp: 1.4797265529632568, KL: 86.55577850341797, Loss: 0.07886692136526108, Learning Rate: 0.020999999999999998\n",
      "Epoch [2775/20000], Bound: 0.5042465329170227, Entropy: 125.6403579711914, Temp: 1.4812633991241455, KL: 72.9080581665039, Loss: 0.06576820462942123, Learning Rate: 0.020999999999999998\n",
      "Epoch [2776/20000], Bound: 0.5239403247833252, Entropy: 126.50296783447266, Temp: 1.482890009880066, KL: 75.1623764038086, Loss: 0.0753384605050087, Learning Rate: 0.020999999999999998\n",
      "Epoch [2777/20000], Bound: 0.5342013835906982, Entropy: 128.18521118164062, Temp: 1.4845176935195923, KL: 71.5465087890625, Loss: 0.09670142829418182, Learning Rate: 0.020999999999999998\n",
      "Epoch [2778/20000], Bound: 0.5547617673873901, Entropy: 128.06423950195312, Temp: 1.4858765602111816, KL: 78.5482177734375, Loss: 0.09174042195081711, Learning Rate: 0.020999999999999998\n",
      "Epoch [2779/20000], Bound: 0.6040225028991699, Entropy: 124.15174865722656, Temp: 1.4871233701705933, KL: 84.28598022460938, Loss: 0.11905717849731445, Learning Rate: 0.020999999999999998\n",
      "Epoch [2780/20000], Bound: 0.5902163982391357, Entropy: 129.1896209716797, Temp: 1.4880506992340088, KL: 75.71574401855469, Loss: 0.13458095490932465, Learning Rate: 0.020999999999999998\n",
      "Epoch [2781/20000], Bound: 0.5945444703102112, Entropy: 123.4328842163086, Temp: 1.4884337186813354, KL: 93.47246551513672, Loss: 0.07911951094865799, Learning Rate: 0.020999999999999998\n",
      "Epoch [2782/20000], Bound: 0.6009842753410339, Entropy: 122.13636779785156, Temp: 1.489089012145996, KL: 78.99577331542969, Loss: 0.13403916358947754, Learning Rate: 0.020999999999999998\n",
      "Epoch [2783/20000], Bound: 0.5456712245941162, Entropy: 121.12340545654297, Temp: 1.4892692565917969, KL: 78.68445587158203, Loss: 0.08333680778741837, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2784/20000], Bound: 0.5532433390617371, Entropy: 115.50384521484375, Temp: 1.4895384311676025, KL: 82.53285217285156, Loss: 0.07729647308588028, Learning Rate: 0.020999999999999998\n",
      "Epoch [2785/20000], Bound: 0.5793032646179199, Entropy: 116.39761352539062, Temp: 1.4899938106536865, KL: 88.32527160644531, Loss: 0.0820394903421402, Learning Rate: 0.020999999999999998\n",
      "Epoch [2786/20000], Bound: 0.5865077376365662, Entropy: 115.98904418945312, Temp: 1.4906277656555176, KL: 80.03959655761719, Loss: 0.1167263612151146, Learning Rate: 0.020999999999999998\n",
      "Epoch [2787/20000], Bound: 0.5721946954727173, Entropy: 116.567138671875, Temp: 1.4909720420837402, KL: 85.72785949707031, Loss: 0.08418155461549759, Learning Rate: 0.020999999999999998\n",
      "Epoch [2788/20000], Bound: 0.6124088168144226, Entropy: 117.175048828125, Temp: 1.4914544820785522, KL: 93.72291564941406, Loss: 0.09606065601110458, Learning Rate: 0.020999999999999998\n",
      "Epoch [2789/20000], Bound: 0.6058551669120789, Entropy: 116.3347396850586, Temp: 1.4920220375061035, KL: 88.27681732177734, Loss: 0.10792507231235504, Learning Rate: 0.020999999999999998\n",
      "Epoch [2790/20000], Bound: 0.5511103868484497, Entropy: 117.08674621582031, Temp: 1.4924851655960083, KL: 75.66166687011719, Loss: 0.09865669906139374, Learning Rate: 0.020999999999999998\n",
      "Epoch [2791/20000], Bound: 0.5883455872535706, Entropy: 125.10986328125, Temp: 1.4928086996078491, KL: 81.63934326171875, Loss: 0.11330363154411316, Learning Rate: 0.020999999999999998\n",
      "Epoch [2792/20000], Bound: 0.6129512786865234, Entropy: 123.88745880126953, Temp: 1.492922067642212, KL: 82.79686737060547, Loss: 0.13335338234901428, Learning Rate: 0.020999999999999998\n",
      "Epoch [2793/20000], Bound: 0.6093975901603699, Entropy: 122.04841613769531, Temp: 1.4926562309265137, KL: 95.77143859863281, Loss: 0.08636157959699631, Learning Rate: 0.020999999999999998\n",
      "Epoch [2794/20000], Bound: 0.522621214389801, Entropy: 119.46524810791016, Temp: 1.4926695823669434, KL: 71.68402862548828, Loss: 0.08664806187152863, Learning Rate: 0.020999999999999998\n",
      "Epoch [2795/20000], Bound: 0.562199592590332, Entropy: 124.44400787353516, Temp: 1.4926687479019165, KL: 75.88231658935547, Loss: 0.1080634668469429, Learning Rate: 0.020999999999999998\n",
      "Epoch [2796/20000], Bound: 0.5798324942588806, Entropy: 123.92463684082031, Temp: 1.4924780130386353, KL: 80.220947265625, Loss: 0.10994638502597809, Learning Rate: 0.020999999999999998\n",
      "Epoch [2797/20000], Bound: 0.5429633259773254, Entropy: 124.53977966308594, Temp: 1.4921472072601318, KL: 72.0982666015625, Loss: 0.10322293639183044, Learning Rate: 0.020999999999999998\n",
      "Epoch [2798/20000], Bound: 0.5567896366119385, Entropy: 124.28673553466797, Temp: 1.491666555404663, KL: 84.71253204345703, Loss: 0.07342434674501419, Learning Rate: 0.020999999999999998\n",
      "Epoch [2799/20000], Bound: 0.6176964640617371, Entropy: 124.02386474609375, Temp: 1.491509199142456, KL: 88.34405517578125, Loss: 0.1193479597568512, Learning Rate: 0.020999999999999998\n",
      "Epoch [2800/20000], Bound: 0.5785204768180847, Entropy: 127.0111083984375, Temp: 1.491205096244812, KL: 84.19317626953125, Loss: 0.09528229385614395, Learning Rate: 0.020999999999999998\n",
      "Epoch [2801/20000], Bound: 0.6699604988098145, Entropy: 126.69853210449219, Temp: 1.4909708499908447, KL: 106.56689453125, Loss: 0.11241345852613449, Learning Rate: 0.020999999999999998\n",
      "Epoch [2802/20000], Bound: 0.5282469391822815, Entropy: 122.55601501464844, Temp: 1.4908602237701416, KL: 74.52008056640625, Loss: 0.08193337172269821, Learning Rate: 0.020999999999999998\n",
      "Epoch [2803/20000], Bound: 0.5779424905776978, Entropy: 117.44068145751953, Temp: 1.4908337593078613, KL: 90.54553985595703, Loss: 0.07339826226234436, Learning Rate: 0.020999999999999998\n",
      "Epoch [2804/20000], Bound: 0.6090719103813171, Entropy: 118.49816131591797, Temp: 1.4911493062973022, KL: 90.0356674194336, Loss: 0.1050984337925911, Learning Rate: 0.020999999999999998\n",
      "Epoch [2805/20000], Bound: 0.5751903653144836, Entropy: 119.1695785522461, Temp: 1.4914352893829346, KL: 87.0959243774414, Loss: 0.08244338631629944, Learning Rate: 0.020999999999999998\n",
      "Epoch [2806/20000], Bound: 0.5370429158210754, Entropy: 113.66948699951172, Temp: 1.4918988943099976, KL: 79.4734115600586, Loss: 0.07319814711809158, Learning Rate: 0.020999999999999998\n",
      "Epoch [2807/20000], Bound: 0.5733513832092285, Entropy: 115.66351318359375, Temp: 1.4925379753112793, KL: 83.52018737792969, Loss: 0.092812180519104, Learning Rate: 0.020999999999999998\n",
      "Epoch [2808/20000], Bound: 0.5527913570404053, Entropy: 114.28809356689453, Temp: 1.4931690692901611, KL: 84.59162139892578, Loss: 0.07033369690179825, Learning Rate: 0.020999999999999998\n",
      "Epoch [2809/20000], Bound: 0.5842477679252625, Entropy: 117.05801391601562, Temp: 1.4940438270568848, KL: 83.62620544433594, Loss: 0.1028607189655304, Learning Rate: 0.020999999999999998\n",
      "Epoch [2810/20000], Bound: 0.5303386449813843, Entropy: 118.63082885742188, Temp: 1.4947807788848877, KL: 72.8707275390625, Loss: 0.08960441499948502, Learning Rate: 0.020999999999999998\n",
      "Epoch [2811/20000], Bound: 0.5152085423469543, Entropy: 124.98334503173828, Temp: 1.4954103231430054, KL: 69.86559295654297, Loss: 0.08649323880672455, Learning Rate: 0.020999999999999998\n",
      "Epoch [2812/20000], Bound: 0.5604451894760132, Entropy: 125.35282897949219, Temp: 1.4959404468536377, KL: 79.47148132324219, Loss: 0.09470776468515396, Learning Rate: 0.020999999999999998\n",
      "Epoch [2813/20000], Bound: 0.5388847589492798, Entropy: 130.36325073242188, Temp: 1.4964027404785156, KL: 75.61972045898438, Loss: 0.08811593800783157, Learning Rate: 0.020999999999999998\n",
      "Epoch [2814/20000], Bound: 0.6119884252548218, Entropy: 132.8988037109375, Temp: 1.4968297481536865, KL: 81.23197937011719, Loss: 0.137965127825737, Learning Rate: 0.020999999999999998\n",
      "Epoch [2815/20000], Bound: 0.6112738251686096, Entropy: 134.00448608398438, Temp: 1.4967706203460693, KL: 80.6383056640625, Loss: 0.13923798501491547, Learning Rate: 0.020999999999999998\n",
      "Epoch [2816/20000], Bound: 0.52979576587677, Entropy: 131.90341186523438, Temp: 1.496254563331604, KL: 75.60185241699219, Loss: 0.08011028170585632, Learning Rate: 0.020999999999999998\n",
      "Epoch [2817/20000], Bound: 0.5346532464027405, Entropy: 131.1412811279297, Temp: 1.495887041091919, KL: 75.06398010253906, Loss: 0.08617053180932999, Learning Rate: 0.020999999999999998\n",
      "Epoch [2818/20000], Bound: 0.5132948756217957, Entropy: 132.38290405273438, Temp: 1.4955823421478271, KL: 64.69700622558594, Loss: 0.10213308781385422, Learning Rate: 0.020999999999999998\n",
      "Epoch [2819/20000], Bound: 0.5345761179924011, Entropy: 130.2154083251953, Temp: 1.4950402975082397, KL: 73.97654724121094, Loss: 0.08967182040214539, Learning Rate: 0.020999999999999998\n",
      "Epoch [2820/20000], Bound: 0.595601499080658, Entropy: 127.6718978881836, Temp: 1.4945296049118042, KL: 88.28290557861328, Loss: 0.09818948805332184, Learning Rate: 0.020999999999999998\n",
      "Epoch [2821/20000], Bound: 0.5427706241607666, Entropy: 128.557373046875, Temp: 1.4941179752349854, KL: 75.856201171875, Loss: 0.09061621129512787, Learning Rate: 0.020999999999999998\n",
      "Epoch [2822/20000], Bound: 0.5846558213233948, Entropy: 123.94451141357422, Temp: 1.493738055229187, KL: 88.5042495727539, Loss: 0.08689186722040176, Learning Rate: 0.020999999999999998\n",
      "Epoch [2823/20000], Bound: 0.5337352156639099, Entropy: 128.60499572753906, Temp: 1.4935669898986816, KL: 72.95774841308594, Loss: 0.09222347289323807, Learning Rate: 0.020999999999999998\n",
      "Epoch [2824/20000], Bound: 0.5626850128173828, Entropy: 125.13822937011719, Temp: 1.4933536052703857, KL: 79.21751403808594, Loss: 0.09739682823419571, Learning Rate: 0.020999999999999998\n",
      "Epoch [2825/20000], Bound: 0.5746646523475647, Entropy: 123.971435546875, Temp: 1.4931190013885498, KL: 81.50285339355469, Loss: 0.10085055977106094, Learning Rate: 0.020999999999999998\n",
      "Epoch [2826/20000], Bound: 0.5520818829536438, Entropy: 126.89310455322266, Temp: 1.4928556680679321, KL: 79.28687286376953, Loss: 0.08742541819810867, Learning Rate: 0.020999999999999998\n",
      "Epoch [2827/20000], Bound: 0.541547954082489, Entropy: 124.54525756835938, Temp: 1.4926841259002686, KL: 73.99485778808594, Loss: 0.09564051777124405, Learning Rate: 0.020999999999999998\n",
      "Epoch [2828/20000], Bound: 0.5705758929252625, Entropy: 129.0704803466797, Temp: 1.492447018623352, KL: 84.68051147460938, Loss: 0.08632653951644897, Learning Rate: 0.020999999999999998\n",
      "Epoch [2829/20000], Bound: 0.6326255202293396, Entropy: 126.28509521484375, Temp: 1.4923720359802246, KL: 91.96650695800781, Loss: 0.1223425641655922, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2830/20000], Bound: 0.5380424857139587, Entropy: 123.71920013427734, Temp: 1.49214768409729, KL: 79.10240936279297, Loss: 0.0753537192940712, Learning Rate: 0.020999999999999998\n",
      "Epoch [2831/20000], Bound: 0.5736613869667053, Entropy: 119.08161926269531, Temp: 1.4921414852142334, KL: 80.75689697265625, Loss: 0.10232456028461456, Learning Rate: 0.020999999999999998\n",
      "Epoch [2832/20000], Bound: 0.5919945240020752, Entropy: 118.3630142211914, Temp: 1.4920612573623657, KL: 80.78406524658203, Loss: 0.11959826201200485, Learning Rate: 0.020999999999999998\n",
      "Epoch [2833/20000], Bound: 0.5717410445213318, Entropy: 110.74219512939453, Temp: 1.491734266281128, KL: 85.76494598388672, Loss: 0.08370913565158844, Learning Rate: 0.020999999999999998\n",
      "Epoch [2834/20000], Bound: 0.5975449681282043, Entropy: 113.78640747070312, Temp: 1.4916197061538696, KL: 81.00506591796875, Loss: 0.12416984140872955, Learning Rate: 0.020999999999999998\n",
      "Epoch [2835/20000], Bound: 0.5742266774177551, Entropy: 114.200439453125, Temp: 1.4912179708480835, KL: 85.70263671875, Loss: 0.08619023859500885, Learning Rate: 0.020999999999999998\n",
      "Epoch [2836/20000], Bound: 0.622681736946106, Entropy: 116.55567932128906, Temp: 1.4910099506378174, KL: 81.93414306640625, Loss: 0.14578144252300262, Learning Rate: 0.020999999999999998\n",
      "Epoch [2837/20000], Bound: 0.541552722454071, Entropy: 117.45773315429688, Temp: 1.4903168678283691, KL: 77.88056945800781, Loss: 0.08242661505937576, Learning Rate: 0.020999999999999998\n",
      "Epoch [2838/20000], Bound: 0.5576575994491577, Entropy: 119.52637481689453, Temp: 1.4898011684417725, KL: 81.07198333740234, Loss: 0.08625243604183197, Learning Rate: 0.020999999999999998\n",
      "Epoch [2839/20000], Bound: 0.5387886762619019, Entropy: 125.3602294921875, Temp: 1.4894407987594604, KL: 75.14642333984375, Loss: 0.08905871957540512, Learning Rate: 0.020999999999999998\n",
      "Epoch [2840/20000], Bound: 0.518775224685669, Entropy: 128.2889404296875, Temp: 1.489122986793518, KL: 73.875, Loss: 0.0756751149892807, Learning Rate: 0.020999999999999998\n",
      "Epoch [2841/20000], Bound: 0.5614122748374939, Entropy: 135.20285034179688, Temp: 1.4889748096466064, KL: 79.99224853515625, Loss: 0.09324906021356583, Learning Rate: 0.020999999999999998\n",
      "Epoch [2842/20000], Bound: 0.5291559100151062, Entropy: 135.695068359375, Temp: 1.48885977268219, KL: 77.11491394042969, Loss: 0.07385898381471634, Learning Rate: 0.020999999999999998\n",
      "Epoch [2843/20000], Bound: 0.631055474281311, Entropy: 137.1198272705078, Temp: 1.4889520406723022, KL: 91.43162536621094, Loss: 0.12217893451452255, Learning Rate: 0.020999999999999998\n",
      "Epoch [2844/20000], Bound: 0.5943926572799683, Entropy: 134.60365295410156, Temp: 1.4888800382614136, KL: 77.89216613769531, Loss: 0.13134625554084778, Learning Rate: 0.020999999999999998\n",
      "Epoch [2845/20000], Bound: 0.560692310333252, Entropy: 127.0438232421875, Temp: 1.4884103536605835, KL: 81.30303955078125, Loss: 0.08813392370939255, Learning Rate: 0.020999999999999998\n",
      "Epoch [2846/20000], Bound: 0.6289507746696472, Entropy: 121.21279907226562, Temp: 1.4880763292312622, KL: 98.79623413085938, Loss: 0.0952039286494255, Learning Rate: 0.020999999999999998\n",
      "Epoch [2847/20000], Bound: 0.5655103921890259, Entropy: 115.67857360839844, Temp: 1.487980842590332, KL: 86.14703369140625, Loss: 0.07626190036535263, Learning Rate: 0.020999999999999998\n",
      "Epoch [2848/20000], Bound: 0.5772202014923096, Entropy: 113.90068817138672, Temp: 1.488166332244873, KL: 88.11284637451172, Loss: 0.08060071617364883, Learning Rate: 0.020999999999999998\n",
      "Epoch [2849/20000], Bound: 0.5122998356819153, Entropy: 109.54393005371094, Temp: 1.488579511642456, KL: 72.1551513671875, Loss: 0.07580196112394333, Learning Rate: 0.020999999999999998\n",
      "Epoch [2850/20000], Bound: 0.5399692058563232, Entropy: 112.5872802734375, Temp: 1.4890695810317993, KL: 82.11752319335938, Loss: 0.06667566299438477, Learning Rate: 0.020999999999999998\n",
      "Epoch [2851/20000], Bound: 0.5603768825531006, Entropy: 111.55928802490234, Temp: 1.4898416996002197, KL: 87.8874282836914, Loss: 0.0658765658736229, Learning Rate: 0.020999999999999998\n",
      "Epoch [2852/20000], Bound: 0.5723398923873901, Entropy: 116.52008819580078, Temp: 1.4909377098083496, KL: 85.34085845947266, Loss: 0.08561163395643234, Learning Rate: 0.020999999999999998\n",
      "Epoch [2853/20000], Bound: 0.5497292280197144, Entropy: 124.8078842163086, Temp: 1.492082118988037, KL: 81.38068389892578, Loss: 0.0782090574502945, Learning Rate: 0.020999999999999998\n",
      "Epoch [2854/20000], Bound: 0.5253634452819824, Entropy: 127.20542907714844, Temp: 1.4933043718338013, KL: 71.80319213867188, Loss: 0.08869434148073196, Learning Rate: 0.020999999999999998\n",
      "Epoch [2855/20000], Bound: 0.5805641412734985, Entropy: 128.25486755371094, Temp: 1.4943701028823853, KL: 93.60235595703125, Loss: 0.06602443754673004, Learning Rate: 0.020999999999999998\n",
      "Epoch [2856/20000], Bound: 0.569658637046814, Entropy: 132.73231506347656, Temp: 1.4957818984985352, KL: 85.91337585449219, Loss: 0.08167172968387604, Learning Rate: 0.020999999999999998\n",
      "Epoch [2857/20000], Bound: 0.5555588603019714, Entropy: 135.65802001953125, Temp: 1.4972515106201172, KL: 80.82365417480469, Loss: 0.08583465218544006, Learning Rate: 0.020999999999999998\n",
      "Epoch [2858/20000], Bound: 0.5340065956115723, Entropy: 137.31263732910156, Temp: 1.4986690282821655, KL: 79.00830078125, Loss: 0.07265805453062057, Learning Rate: 0.020999999999999998\n",
      "Epoch [2859/20000], Bound: 0.5774538516998291, Entropy: 137.4614715576172, Temp: 1.5001617670059204, KL: 85.11088562011719, Loss: 0.0920565202832222, Learning Rate: 0.020999999999999998\n",
      "Epoch [2860/20000], Bound: 0.4817858040332794, Entropy: 129.38482666015625, Temp: 1.5015758275985718, KL: 65.83248901367188, Loss: 0.07210957258939743, Learning Rate: 0.020999999999999998\n",
      "Epoch [2861/20000], Bound: 0.5770244002342224, Entropy: 128.1501007080078, Temp: 1.502915382385254, KL: 87.81314086914062, Loss: 0.08292244374752045, Learning Rate: 0.020999999999999998\n",
      "Epoch [2862/20000], Bound: 0.5612269043922424, Entropy: 124.84253692626953, Temp: 1.5043153762817383, KL: 86.34700775146484, Loss: 0.07327768951654434, Learning Rate: 0.020999999999999998\n",
      "Epoch [2863/20000], Bound: 0.5573288202285767, Entropy: 118.93431091308594, Temp: 1.5058568716049194, KL: 79.36550903320312, Loss: 0.09304361045360565, Learning Rate: 0.020999999999999998\n",
      "Epoch [2864/20000], Bound: 0.5293309092521667, Entropy: 116.16050720214844, Temp: 1.5072299242019653, KL: 77.76033020019531, Loss: 0.07341878116130829, Learning Rate: 0.020999999999999998\n",
      "Epoch [2865/20000], Bound: 0.543465793132782, Entropy: 116.17432403564453, Temp: 1.508647084236145, KL: 80.53124237060547, Loss: 0.07688576728105545, Learning Rate: 0.020999999999999998\n",
      "Epoch [2866/20000], Bound: 0.568142294883728, Entropy: 117.58671569824219, Temp: 1.5100942850112915, KL: 86.03082275390625, Loss: 0.08125987648963928, Learning Rate: 0.020999999999999998\n",
      "Epoch [2867/20000], Bound: 0.5771989822387695, Entropy: 118.04943084716797, Temp: 1.5115783214569092, KL: 83.72264862060547, Loss: 0.09747127443552017, Learning Rate: 0.020999999999999998\n",
      "Epoch [2868/20000], Bound: 0.5887969732284546, Entropy: 124.15595245361328, Temp: 1.5128906965255737, KL: 77.97713470458984, Loss: 0.1275356411933899, Learning Rate: 0.020999999999999998\n",
      "Epoch [2869/20000], Bound: 0.5921192765235901, Entropy: 119.59919738769531, Temp: 1.513658046722412, KL: 74.97286987304688, Loss: 0.14068923890590668, Learning Rate: 0.020999999999999998\n",
      "Epoch [2870/20000], Bound: 0.5542757511138916, Entropy: 121.24979400634766, Temp: 1.513759732246399, KL: 72.31957244873047, Loss: 0.11419041454792023, Learning Rate: 0.020999999999999998\n",
      "Epoch [2871/20000], Bound: 0.5770201683044434, Entropy: 122.7381820678711, Temp: 1.5135105848312378, KL: 76.2846908569336, Loss: 0.12204638868570328, Learning Rate: 0.020999999999999998\n",
      "Epoch [2872/20000], Bound: 0.619379997253418, Entropy: 120.40889739990234, Temp: 1.5129096508026123, KL: 80.53546905517578, Loss: 0.14888450503349304, Learning Rate: 0.020999999999999998\n",
      "Epoch [2873/20000], Bound: 0.5610254406929016, Entropy: 119.34725189208984, Temp: 1.5117651224136353, KL: 85.42833709716797, Loss: 0.07685773074626923, Learning Rate: 0.020999999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2874/20000], Bound: 0.6486355066299438, Entropy: 118.34086608886719, Temp: 1.5109542608261108, KL: 74.650634765625, Loss: 0.19803954660892487, Learning Rate: 0.020999999999999998\n",
      "Epoch [2875/20000], Bound: 0.5840197801589966, Entropy: 117.06808471679688, Temp: 1.5090693235397339, KL: 81.14189147949219, Loss: 0.11222602427005768, Learning Rate: 0.020999999999999998\n",
      "Epoch [2876/20000], Bound: 0.5457144975662231, Entropy: 120.138427734375, Temp: 1.5071667432785034, KL: 71.54824829101562, Loss: 0.10857365280389786, Learning Rate: 0.020999999999999998\n",
      "Epoch [2877/20000], Bound: 0.5742248892784119, Entropy: 119.48650360107422, Temp: 1.505178451538086, KL: 82.7743148803711, Loss: 0.09726496785879135, Learning Rate: 0.020999999999999998\n",
      "Epoch [2878/20000], Bound: 0.5949364304542542, Entropy: 117.12052154541016, Temp: 1.5033671855926514, KL: 81.22415924072266, Loss: 0.12190347164869308, Learning Rate: 0.020999999999999998\n",
      "Epoch [2879/20000], Bound: 0.5617358088493347, Entropy: 121.67216491699219, Temp: 1.5014407634735107, KL: 75.44898986816406, Loss: 0.10975314676761627, Learning Rate: 0.020999999999999998\n",
      "Epoch [2880/20000], Bound: 0.5704817175865173, Entropy: 121.60738372802734, Temp: 1.4994741678237915, KL: 79.69245147705078, Loss: 0.1035420224070549, Learning Rate: 0.020999999999999998\n",
      "Epoch [2881/20000], Bound: 0.6129505634307861, Entropy: 121.84362030029297, Temp: 1.4975905418395996, KL: 84.27559661865234, Loss: 0.12881675362586975, Learning Rate: 0.020999999999999998\n",
      "Epoch [2882/20000], Bound: 0.46413081884384155, Entropy: 125.61338806152344, Temp: 1.4955735206604004, KL: 59.280120849609375, Loss: 0.07914547622203827, Learning Rate: 0.020999999999999998\n",
      "Epoch [2883/20000], Bound: 0.6188848614692688, Entropy: 126.7135238647461, Temp: 1.4936730861663818, KL: 84.7952651977539, Loss: 0.13262569904327393, Learning Rate: 0.020999999999999998\n",
      "Epoch [2884/20000], Bound: 0.6137874722480774, Entropy: 129.83407592773438, Temp: 1.4916154146194458, KL: 85.06480407714844, Loss: 0.12646599113941193, Learning Rate: 0.020999999999999998\n",
      "Epoch [2885/20000], Bound: 0.5649255514144897, Entropy: 131.3397979736328, Temp: 1.4894866943359375, KL: 74.83634948730469, Loss: 0.11384178698062897, Learning Rate: 0.020999999999999998\n",
      "Epoch [2886/20000], Bound: 0.5859752297401428, Entropy: 132.24668884277344, Temp: 1.4873085021972656, KL: 79.42732238769531, Loss: 0.11799679696559906, Learning Rate: 0.020999999999999998\n",
      "Epoch [2887/20000], Bound: 0.5848871469497681, Entropy: 128.44021606445312, Temp: 1.4851012229919434, KL: 82.28559875488281, Loss: 0.10715270787477493, Learning Rate: 0.020999999999999998\n",
      "Epoch [2888/20000], Bound: 0.5736919641494751, Entropy: 125.91083526611328, Temp: 1.4830187559127808, KL: 83.6699447631836, Loss: 0.09172726422548294, Learning Rate: 0.020999999999999998\n",
      "Epoch [2889/20000], Bound: 0.6097943782806396, Entropy: 127.01625061035156, Temp: 1.481231927871704, KL: 84.62635803222656, Loss: 0.12302888929843903, Learning Rate: 0.020999999999999998\n",
      "Epoch [2890/20000], Bound: 0.594605565071106, Entropy: 127.83810424804688, Temp: 1.4793967008590698, KL: 84.96255493164062, Loss: 0.10689893364906311, Learning Rate: 0.020999999999999998\n",
      "Epoch [2891/20000], Bound: 0.5785945057868958, Entropy: 122.88185119628906, Temp: 1.47769296169281, KL: 89.84898376464844, Loss: 0.07491651177406311, Learning Rate: 0.020999999999999998\n",
      "Epoch [2892/20000], Bound: 0.6123770475387573, Entropy: 121.65216827392578, Temp: 1.476506233215332, KL: 81.33440399169922, Loss: 0.13628356158733368, Learning Rate: 0.020999999999999998\n",
      "Epoch [2893/20000], Bound: 0.6075410842895508, Entropy: 119.25025177001953, Temp: 1.475045919418335, KL: 91.0096664428711, Loss: 0.09858476370573044, Learning Rate: 0.020999999999999998\n",
      "Epoch [2894/20000], Bound: 0.5756961107254028, Entropy: 116.62767028808594, Temp: 1.4738430976867676, KL: 78.54345703125, Loss: 0.11011058837175369, Learning Rate: 0.020999999999999998\n",
      "Epoch [2895/20000], Bound: 0.5816524028778076, Entropy: 119.40292358398438, Temp: 1.472609519958496, KL: 80.91874694824219, Loss: 0.10756560415029526, Learning Rate: 0.020999999999999998\n",
      "Epoch [2896/20000], Bound: 0.5847151279449463, Entropy: 114.66233825683594, Temp: 1.4714056253433228, KL: 86.20948791503906, Loss: 0.09238889813423157, Learning Rate: 0.020999999999999998\n",
      "Epoch [2897/20000], Bound: 0.5506970882415771, Entropy: 117.19384765625, Temp: 1.4704521894454956, KL: 80.84336853027344, Loss: 0.07887034118175507, Learning Rate: 0.020999999999999998\n",
      "Epoch [2898/20000], Bound: 0.528597891330719, Entropy: 115.94651794433594, Temp: 1.4698095321655273, KL: 70.04168701171875, Loss: 0.09574303776025772, Learning Rate: 0.020999999999999998\n",
      "Epoch [2899/20000], Bound: 0.5174097418785095, Entropy: 121.16724395751953, Temp: 1.4691367149353027, KL: 72.55335235595703, Loss: 0.07732982188463211, Learning Rate: 0.020999999999999998\n",
      "Epoch [2900/20000], Bound: 0.5440412163734436, Entropy: 123.00154113769531, Temp: 1.4686683416366577, KL: 77.84133911132812, Loss: 0.08289052546024323, Learning Rate: 0.020999999999999998\n",
      "Epoch [2901/20000], Bound: 0.5723640322685242, Entropy: 130.19627380371094, Temp: 1.4683873653411865, KL: 85.93508911132812, Loss: 0.08133857697248459, Learning Rate: 0.020999999999999998\n",
      "Epoch [2902/20000], Bound: 0.5643805265426636, Entropy: 135.40283203125, Temp: 1.4683853387832642, KL: 84.98348999023438, Loss: 0.07713920623064041, Learning Rate: 0.020999999999999998\n",
      "Epoch [2903/20000], Bound: 0.5752555727958679, Entropy: 135.78787231445312, Temp: 1.4686694145202637, KL: 88.3172607421875, Loss: 0.07597270607948303, Learning Rate: 0.020999999999999998\n",
      "Epoch [2904/20000], Bound: 0.5230177044868469, Entropy: 135.5077667236328, Temp: 1.4692602157592773, KL: 75.08592224121094, Loss: 0.07362332195043564, Learning Rate: 0.020999999999999998\n",
      "Epoch [2905/20000], Bound: 0.5699076056480408, Entropy: 131.65521240234375, Temp: 1.470000982284546, KL: 83.16064453125, Loss: 0.08864743262529373, Learning Rate: 0.020999999999999998\n",
      "Epoch [2906/20000], Bound: 0.5640063881874084, Entropy: 130.72515869140625, Temp: 1.470806360244751, KL: 82.32231140136719, Loss: 0.08609022200107574, Learning Rate: 0.020999999999999998\n",
      "Epoch [2907/20000], Bound: 0.542167067527771, Entropy: 130.5490264892578, Temp: 1.4716863632202148, KL: 78.77200317382812, Loss: 0.07831290364265442, Learning Rate: 0.020999999999999998\n",
      "Epoch [2908/20000], Bound: 0.541174590587616, Entropy: 125.34452056884766, Temp: 1.4726755619049072, KL: 74.07262420654297, Loss: 0.09346748888492584, Learning Rate: 0.020999999999999998\n",
      "Epoch [2909/20000], Bound: 0.5797180533409119, Entropy: 124.0634536743164, Temp: 1.473541021347046, KL: 87.1474838256836, Loss: 0.08468299359083176, Learning Rate: 0.020999999999999998\n",
      "Epoch [2910/20000], Bound: 0.5676422715187073, Entropy: 124.89595794677734, Temp: 1.4745404720306396, KL: 80.89546966552734, Loss: 0.09466302394866943, Learning Rate: 0.020999999999999998\n",
      "Epoch [2911/20000], Bound: 0.60223388671875, Entropy: 118.84337615966797, Temp: 1.4754801988601685, KL: 84.14081573486328, Loss: 0.11670826375484467, Learning Rate: 0.020999999999999998\n",
      "Epoch [2912/20000], Bound: 0.6011505126953125, Entropy: 114.59835815429688, Temp: 1.4761687517166138, KL: 82.97097778320312, Loss: 0.11967932432889938, Learning Rate: 0.020999999999999998\n",
      "Epoch [2913/20000], Bound: 0.5596022009849548, Entropy: 117.08311462402344, Temp: 1.476584792137146, KL: 81.45675659179688, Loss: 0.0855165645480156, Learning Rate: 0.020999999999999998\n",
      "Epoch [2914/20000], Bound: 0.5756762623786926, Entropy: 116.7312240600586, Temp: 1.4771007299423218, KL: 88.56111907958984, Loss: 0.07646144926548004, Learning Rate: 0.014699999999999998\n",
      "Epoch [2915/20000], Bound: 0.5455949902534485, Entropy: 119.20823669433594, Temp: 1.4778846502304077, KL: 73.60324096679688, Loss: 0.09945151209831238, Learning Rate: 0.014699999999999998\n",
      "Epoch [2916/20000], Bound: 0.5638662576675415, Entropy: 117.45808410644531, Temp: 1.4784852266311646, KL: 86.88043212890625, Loss: 0.07128508388996124, Learning Rate: 0.014699999999999998\n",
      "Epoch [2917/20000], Bound: 0.5081344246864319, Entropy: 121.0041732788086, Temp: 1.4793809652328491, KL: 75.38611602783203, Loss: 0.06056920439004898, Learning Rate: 0.014699999999999998\n",
      "Epoch [2918/20000], Bound: 0.6010890603065491, Entropy: 120.61519622802734, Temp: 1.4805315732955933, KL: 88.32015228271484, Loss: 0.10195554792881012, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2919/20000], Bound: 0.5215700268745422, Entropy: 125.48944854736328, Temp: 1.481603980064392, KL: 72.22820281982422, Loss: 0.083058200776577, Learning Rate: 0.014699999999999998\n",
      "Epoch [2920/20000], Bound: 0.534584105014801, Entropy: 122.1716537475586, Temp: 1.4826210737228394, KL: 81.65836334228516, Loss: 0.06279975175857544, Learning Rate: 0.014699999999999998\n",
      "Epoch [2921/20000], Bound: 0.5133281946182251, Entropy: 126.02152252197266, Temp: 1.483922004699707, KL: 69.28571319580078, Loss: 0.0859936773777008, Learning Rate: 0.014699999999999998\n",
      "Epoch [2922/20000], Bound: 0.5242910385131836, Entropy: 129.7147674560547, Temp: 1.4850728511810303, KL: 69.24118041992188, Loss: 0.09576413780450821, Learning Rate: 0.014699999999999998\n",
      "Epoch [2923/20000], Bound: 0.522331714630127, Entropy: 126.177001953125, Temp: 1.485978126525879, KL: 77.09822082519531, Loss: 0.06767589598894119, Learning Rate: 0.014699999999999998\n",
      "Epoch [2924/20000], Bound: 0.52077716588974, Entropy: 131.8104248046875, Temp: 1.4870668649673462, KL: 73.48695373535156, Loss: 0.07855706661939621, Learning Rate: 0.014699999999999998\n",
      "Epoch [2925/20000], Bound: 0.5197445154190063, Entropy: 131.2384033203125, Temp: 1.4881550073623657, KL: 70.70295715332031, Loss: 0.08709800243377686, Learning Rate: 0.014699999999999998\n",
      "Epoch [2926/20000], Bound: 0.5064243674278259, Entropy: 133.77098083496094, Temp: 1.489112377166748, KL: 70.50398254394531, Loss: 0.07634138315916061, Learning Rate: 0.014699999999999998\n",
      "Epoch [2927/20000], Bound: 0.5757429003715515, Entropy: 133.56149291992188, Temp: 1.4900684356689453, KL: 88.44857788085938, Loss: 0.07828410714864731, Learning Rate: 0.014699999999999998\n",
      "Epoch [2928/20000], Bound: 0.5318581461906433, Entropy: 129.80392456054688, Temp: 1.491206407546997, KL: 80.59391784667969, Loss: 0.064780093729496, Learning Rate: 0.014699999999999998\n",
      "Epoch [2929/20000], Bound: 0.5599578022956848, Entropy: 130.54173278808594, Temp: 1.4925694465637207, KL: 73.98536682128906, Loss: 0.11234977841377258, Learning Rate: 0.014699999999999998\n",
      "Epoch [2930/20000], Bound: 0.484489768743515, Entropy: 131.36180114746094, Temp: 1.4935297966003418, KL: 62.33714294433594, Loss: 0.08551715314388275, Learning Rate: 0.014699999999999998\n",
      "Epoch [2931/20000], Bound: 0.5837620496749878, Entropy: 126.09650421142578, Temp: 1.4942772388458252, KL: 91.01874542236328, Loss: 0.07768513262271881, Learning Rate: 0.014699999999999998\n",
      "Epoch [2932/20000], Bound: 0.5169304609298706, Entropy: 123.76716613769531, Temp: 1.4952548742294312, KL: 70.96034240722656, Loss: 0.08431138098239899, Learning Rate: 0.014699999999999998\n",
      "Epoch [2933/20000], Bound: 0.5102824568748474, Entropy: 124.8302001953125, Temp: 1.4961352348327637, KL: 73.20022583007812, Loss: 0.07115679234266281, Learning Rate: 0.014699999999999998\n",
      "Epoch [2934/20000], Bound: 0.5548850893974304, Entropy: 123.97120666503906, Temp: 1.4971020221710205, KL: 85.88993835449219, Loss: 0.06828714907169342, Learning Rate: 0.014699999999999998\n",
      "Epoch [2935/20000], Bound: 0.5996710658073425, Entropy: 126.13572692871094, Temp: 1.4983214139938354, KL: 94.93270874023438, Loss: 0.08031299710273743, Learning Rate: 0.014699999999999998\n",
      "Epoch [2936/20000], Bound: 0.5197179913520813, Entropy: 121.9334945678711, Temp: 1.4997304677963257, KL: 80.66291046142578, Loss: 0.05470546707510948, Learning Rate: 0.014699999999999998\n",
      "Epoch [2937/20000], Bound: 0.5334680080413818, Entropy: 122.44005584716797, Temp: 1.5014410018920898, KL: 79.93607330322266, Loss: 0.06933385878801346, Learning Rate: 0.014699999999999998\n",
      "Epoch [2938/20000], Bound: 0.4796167314052582, Entropy: 119.30716705322266, Temp: 1.5032457113265991, KL: 66.66243743896484, Loss: 0.0676676332950592, Learning Rate: 0.014699999999999998\n",
      "Epoch [2939/20000], Bound: 0.5179063081741333, Entropy: 123.14704895019531, Temp: 1.5049973726272583, KL: 74.781982421875, Loss: 0.07316108047962189, Learning Rate: 0.014699999999999998\n",
      "Epoch [2940/20000], Bound: 0.5044631361961365, Entropy: 128.0056915283203, Temp: 1.5067315101623535, KL: 72.41921997070312, Loss: 0.06960486620664597, Learning Rate: 0.014699999999999998\n",
      "Epoch [2941/20000], Bound: 0.5102640986442566, Entropy: 127.62033081054688, Temp: 1.5084600448608398, KL: 75.1278076171875, Loss: 0.06570892781019211, Learning Rate: 0.014699999999999998\n",
      "Epoch [2942/20000], Bound: 0.4961056709289551, Entropy: 132.29115295410156, Temp: 1.5102577209472656, KL: 75.2142333984375, Loss: 0.05355219170451164, Learning Rate: 0.014699999999999998\n",
      "Epoch [2943/20000], Bound: 0.5145620703697205, Entropy: 131.36077880859375, Temp: 1.5122565031051636, KL: 73.45074462890625, Loss: 0.07524798810482025, Learning Rate: 0.014699999999999998\n",
      "Epoch [2944/20000], Bound: 0.5623648166656494, Entropy: 134.4042205810547, Temp: 1.5141630172729492, KL: 90.28981018066406, Loss: 0.06225854903459549, Learning Rate: 0.014699999999999998\n",
      "Epoch [2945/20000], Bound: 0.5364231467247009, Entropy: 134.24183654785156, Temp: 1.5163171291351318, KL: 83.53030395507812, Loss: 0.061393387615680695, Learning Rate: 0.014699999999999998\n",
      "Epoch [2946/20000], Bound: 0.5038285255432129, Entropy: 131.65536499023438, Temp: 1.5186289548873901, KL: 75.86741638183594, Loss: 0.05860351398587227, Learning Rate: 0.014699999999999998\n",
      "Epoch [2947/20000], Bound: 0.544061005115509, Entropy: 130.88650512695312, Temp: 1.5210278034210205, KL: 83.02644348144531, Loss: 0.07027777284383774, Learning Rate: 0.014699999999999998\n",
      "Epoch [2948/20000], Bound: 0.5234260559082031, Entropy: 130.194091796875, Temp: 1.523445963859558, KL: 77.10221862792969, Loss: 0.07175446301698685, Learning Rate: 0.014699999999999998\n",
      "Epoch [2949/20000], Bound: 0.5267526507377625, Entropy: 126.47337341308594, Temp: 1.5257949829101562, KL: 79.52006530761719, Loss: 0.06691412627696991, Learning Rate: 0.014699999999999998\n",
      "Epoch [2950/20000], Bound: 0.5488275289535522, Entropy: 129.1525421142578, Temp: 1.528160810470581, KL: 79.82621765136719, Loss: 0.08565252274274826, Learning Rate: 0.014699999999999998\n",
      "Epoch [2951/20000], Bound: 0.521225094795227, Entropy: 125.05416107177734, Temp: 1.5303270816802979, KL: 78.28592681884766, Loss: 0.0665227547287941, Learning Rate: 0.014699999999999998\n",
      "Epoch [2952/20000], Bound: 0.5206716656684875, Entropy: 122.41786193847656, Temp: 1.5325124263763428, KL: 80.45625305175781, Loss: 0.0591401606798172, Learning Rate: 0.014699999999999998\n",
      "Epoch [2953/20000], Bound: 0.5092080235481262, Entropy: 120.94100952148438, Temp: 1.5348215103149414, KL: 77.31520080566406, Loss: 0.05973825231194496, Learning Rate: 0.014699999999999998\n",
      "Epoch [2954/20000], Bound: 0.48177406191825867, Entropy: 121.34339904785156, Temp: 1.5371979475021362, KL: 72.56968688964844, Loss: 0.05243290960788727, Learning Rate: 0.014699999999999998\n",
      "Epoch [2955/20000], Bound: 0.5640491247177124, Entropy: 123.19512176513672, Temp: 1.539665699005127, KL: 92.02327728271484, Loss: 0.06075354292988777, Learning Rate: 0.014699999999999998\n",
      "Epoch [2956/20000], Bound: 0.46069902181625366, Entropy: 125.87096405029297, Temp: 1.542320966720581, KL: 68.95040130615234, Loss: 0.047480419278144836, Learning Rate: 0.014699999999999998\n",
      "Epoch [2957/20000], Bound: 0.4884338676929474, Entropy: 126.33602905273438, Temp: 1.5450530052185059, KL: 73.73715209960938, Loss: 0.054716143757104874, Learning Rate: 0.014699999999999998\n",
      "Epoch [2958/20000], Bound: 0.5396006107330322, Entropy: 127.63509368896484, Temp: 1.547816276550293, KL: 84.4515151977539, Loss: 0.06404278427362442, Learning Rate: 0.014699999999999998\n",
      "Epoch [2959/20000], Bound: 0.5272573232650757, Entropy: 128.2395782470703, Temp: 1.5506088733673096, KL: 79.12220764160156, Loss: 0.07065214216709137, Learning Rate: 0.014699999999999998\n",
      "Epoch [2960/20000], Bound: 0.5255613327026367, Entropy: 133.21385192871094, Temp: 1.5532891750335693, KL: 83.32258605957031, Loss: 0.05586733669042587, Learning Rate: 0.014699999999999998\n",
      "Epoch [2961/20000], Bound: 0.5148897171020508, Entropy: 136.1195831298828, Temp: 1.55608332157135, KL: 78.86582946777344, Loss: 0.06124882400035858, Learning Rate: 0.014699999999999998\n",
      "Epoch [2962/20000], Bound: 0.5253884792327881, Entropy: 132.2024688720703, Temp: 1.5588653087615967, KL: 76.39781188964844, Loss: 0.07840633392333984, Learning Rate: 0.014699999999999998\n",
      "Epoch [2963/20000], Bound: 0.5153017640113831, Entropy: 132.79031372070312, Temp: 1.5614023208618164, KL: 78.28236389160156, Loss: 0.063882015645504, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2964/20000], Bound: 0.5287811160087585, Entropy: 133.0177764892578, Temp: 1.5639070272445679, KL: 80.70045471191406, Loss: 0.06794619560241699, Learning Rate: 0.014699999999999998\n",
      "Epoch [2965/20000], Bound: 0.572692334651947, Entropy: 131.04417419433594, Temp: 1.5663570165634155, KL: 92.49418640136719, Loss: 0.06981129944324493, Learning Rate: 0.014699999999999998\n",
      "Epoch [2966/20000], Bound: 0.5416737794876099, Entropy: 127.28540802001953, Temp: 1.5688542127609253, KL: 86.80472564697266, Loss: 0.06016683578491211, Learning Rate: 0.014699999999999998\n",
      "Epoch [2967/20000], Bound: 0.53855961561203, Entropy: 123.04790496826172, Temp: 1.5714459419250488, KL: 87.01580047607422, Loss: 0.056977104395627975, Learning Rate: 0.014699999999999998\n",
      "Epoch [2968/20000], Bound: 0.5385841727256775, Entropy: 120.95048522949219, Temp: 1.574159026145935, KL: 86.9134521484375, Loss: 0.05756155028939247, Learning Rate: 0.014699999999999998\n",
      "Epoch [2969/20000], Bound: 0.5098336935043335, Entropy: 118.10167694091797, Temp: 1.5769693851470947, KL: 80.5737075805664, Loss: 0.05313887447118759, Learning Rate: 0.014699999999999998\n",
      "Epoch [2970/20000], Bound: 0.5108118653297424, Entropy: 117.29792022705078, Temp: 1.5798518657684326, KL: 81.85965728759766, Loss: 0.05012095347046852, Learning Rate: 0.014699999999999998\n",
      "Epoch [2971/20000], Bound: 0.5593213438987732, Entropy: 114.13526916503906, Temp: 1.5828450918197632, KL: 91.42942810058594, Loss: 0.06251132488250732, Learning Rate: 0.014699999999999998\n",
      "Epoch [2972/20000], Bound: 0.48748016357421875, Entropy: 119.89986419677734, Temp: 1.5858813524246216, KL: 75.95064544677734, Loss: 0.049830060452222824, Learning Rate: 0.014699999999999998\n",
      "Epoch [2973/20000], Bound: 0.5080580711364746, Entropy: 123.44123077392578, Temp: 1.5889477729797363, KL: 80.96369171142578, Loss: 0.05134313553571701, Learning Rate: 0.014699999999999998\n",
      "Epoch [2974/20000], Bound: 0.49941983819007874, Entropy: 122.37915802001953, Temp: 1.5920705795288086, KL: 74.59564971923828, Loss: 0.0643652081489563, Learning Rate: 0.014699999999999998\n",
      "Epoch [2975/20000], Bound: 0.5112709999084473, Entropy: 128.9626922607422, Temp: 1.5950144529342651, KL: 76.6627197265625, Loss: 0.06799522042274475, Learning Rate: 0.014699999999999998\n",
      "Epoch [2976/20000], Bound: 0.5496137142181396, Entropy: 128.6055450439453, Temp: 1.5977706909179688, KL: 90.92393493652344, Loss: 0.05673987418413162, Learning Rate: 0.014699999999999998\n",
      "Epoch [2977/20000], Bound: 0.4380013048648834, Entropy: 130.6232452392578, Temp: 1.600635290145874, KL: 65.45306396484375, Loss: 0.044515449553728104, Learning Rate: 0.014699999999999998\n",
      "Epoch [2978/20000], Bound: 0.4876800775527954, Entropy: 131.13475036621094, Temp: 1.603484034538269, KL: 72.82284545898438, Loss: 0.06097562611103058, Learning Rate: 0.014699999999999998\n",
      "Epoch [2979/20000], Bound: 0.4786580204963684, Entropy: 136.74346923828125, Temp: 1.6061878204345703, KL: 71.14495849609375, Loss: 0.05905104801058769, Learning Rate: 0.014699999999999998\n",
      "Epoch [2980/20000], Bound: 0.48767876625061035, Entropy: 134.94223022460938, Temp: 1.6087632179260254, KL: 72.92848205566406, Loss: 0.0609724335372448, Learning Rate: 0.014699999999999998\n",
      "Epoch [2981/20000], Bound: 0.4523603022098541, Entropy: 137.2637939453125, Temp: 1.6112147569656372, KL: 66.0711669921875, Loss: 0.054274290800094604, Learning Rate: 0.014699999999999998\n",
      "Epoch [2982/20000], Bound: 0.4608505070209503, Entropy: 138.33360290527344, Temp: 1.6135613918304443, KL: 67.21989440917969, Loss: 0.057484228163957596, Learning Rate: 0.014699999999999998\n",
      "Epoch [2983/20000], Bound: 0.5108609199523926, Entropy: 138.32525634765625, Temp: 1.615782618522644, KL: 78.39292907714844, Loss: 0.06365421414375305, Learning Rate: 0.014699999999999998\n",
      "Epoch [2984/20000], Bound: 0.473799467086792, Entropy: 137.83680725097656, Temp: 1.6179295778274536, KL: 71.81686401367188, Loss: 0.05377531796693802, Learning Rate: 0.014699999999999998\n",
      "Epoch [2985/20000], Bound: 0.472249299287796, Entropy: 138.42337036132812, Temp: 1.6200615167617798, KL: 69.85429382324219, Loss: 0.05872407928109169, Learning Rate: 0.014699999999999998\n",
      "Epoch [2986/20000], Bound: 0.49160560965538025, Entropy: 136.91259765625, Temp: 1.622093915939331, KL: 77.22628784179688, Loss: 0.051731713116168976, Learning Rate: 0.014699999999999998\n",
      "Epoch [2987/20000], Bound: 0.48188653588294983, Entropy: 135.59764099121094, Temp: 1.6241990327835083, KL: 73.99452209472656, Loss: 0.053933847695589066, Learning Rate: 0.014699999999999998\n",
      "Epoch [2988/20000], Bound: 0.475048691034317, Entropy: 134.39730834960938, Temp: 1.6263059377670288, KL: 73.7437744140625, Loss: 0.04934881255030632, Learning Rate: 0.014699999999999998\n",
      "Epoch [2989/20000], Bound: 0.46338745951652527, Entropy: 131.7335205078125, Temp: 1.6284675598144531, KL: 71.41708374023438, Loss: 0.04738646745681763, Learning Rate: 0.014699999999999998\n",
      "Epoch [2990/20000], Bound: 0.47389331459999084, Entropy: 132.07806396484375, Temp: 1.6306769847869873, KL: 74.70744323730469, Loss: 0.04574459418654442, Learning Rate: 0.014699999999999998\n",
      "Epoch [2991/20000], Bound: 0.45505404472351074, Entropy: 133.61827087402344, Temp: 1.632981300354004, KL: 70.96119689941406, Loss: 0.04253815859556198, Learning Rate: 0.014699999999999998\n",
      "Epoch [2992/20000], Bound: 0.46159887313842773, Entropy: 133.71292114257812, Temp: 1.6353719234466553, KL: 64.49983215332031, Loss: 0.06754228472709656, Learning Rate: 0.014699999999999998\n",
      "Epoch [2993/20000], Bound: 0.4413757920265198, Entropy: 137.47412109375, Temp: 1.637447476387024, KL: 58.604278564453125, Loss: 0.07000485062599182, Learning Rate: 0.014699999999999998\n",
      "Epoch [2994/20000], Bound: 0.43602776527404785, Entropy: 136.7700958251953, Temp: 1.6391386985778809, KL: 65.167724609375, Loss: 0.045981764793395996, Learning Rate: 0.014699999999999998\n",
      "Epoch [2995/20000], Bound: 0.5075015425682068, Entropy: 137.02391052246094, Temp: 1.6408644914627075, KL: 84.67071533203125, Loss: 0.043352000415325165, Learning Rate: 0.014699999999999998\n",
      "Epoch [2996/20000], Bound: 0.47633567452430725, Entropy: 136.9095458984375, Temp: 1.6428494453430176, KL: 71.22294616699219, Loss: 0.05905986949801445, Learning Rate: 0.014699999999999998\n",
      "Epoch [2997/20000], Bound: 0.5005512833595276, Entropy: 138.6969451904297, Temp: 1.6447303295135498, KL: 77.46818542480469, Loss: 0.059789028018713, Learning Rate: 0.014699999999999998\n",
      "Epoch [2998/20000], Bound: 0.5809545516967773, Entropy: 140.03982543945312, Temp: 1.6465702056884766, KL: 99.837158203125, Loss: 0.0620170421898365, Learning Rate: 0.014699999999999998\n",
      "Epoch [2999/20000], Bound: 0.5147530436515808, Entropy: 138.12994384765625, Temp: 1.648558497428894, KL: 83.54991149902344, Loss: 0.05337405949831009, Learning Rate: 0.014699999999999998\n",
      "Epoch [3000/20000], Bound: 0.5080884099006653, Entropy: 136.26649475097656, Temp: 1.6506316661834717, KL: 80.85069274902344, Loss: 0.056135211139917374, Learning Rate: 0.014699999999999998\n",
      "Epoch [3001/20000], Bound: 0.5897940993309021, Entropy: 134.37684631347656, Temp: 1.6527175903320312, KL: 98.82949829101562, Loss: 0.07382643967866898, Learning Rate: 0.014699999999999998\n",
      "Epoch [3002/20000], Bound: 0.4786393940448761, Entropy: 124.25370788574219, Temp: 1.6547646522521973, KL: 73.6414794921875, Loss: 0.054233402013778687, Learning Rate: 0.014699999999999998\n",
      "Epoch [3003/20000], Bound: 0.49556100368499756, Entropy: 125.12693786621094, Temp: 1.6567734479904175, KL: 76.64756774902344, Loss: 0.05892021954059601, Learning Rate: 0.014699999999999998\n",
      "Epoch [3004/20000], Bound: 0.5454769134521484, Entropy: 118.68851470947266, Temp: 1.658715844154358, KL: 92.4896469116211, Loss: 0.053408801555633545, Learning Rate: 0.014699999999999998\n",
      "Epoch [3005/20000], Bound: 0.4731520414352417, Entropy: 119.07962799072266, Temp: 1.6608186960220337, KL: 71.25341033935547, Loss: 0.057412754744291306, Learning Rate: 0.014699999999999998\n",
      "Epoch [3006/20000], Bound: 0.49128803610801697, Entropy: 113.25313568115234, Temp: 1.6628048419952393, KL: 74.43114471435547, Loss: 0.062472112476825714, Learning Rate: 0.014699999999999998\n",
      "Epoch [3007/20000], Bound: 0.4952656030654907, Entropy: 113.17881774902344, Temp: 1.664650797843933, KL: 80.47367858886719, Loss: 0.04765310138463974, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3008/20000], Bound: 0.4879613518714905, Entropy: 115.35336303710938, Temp: 1.6666194200515747, KL: 76.85942077636719, Loss: 0.05271213874220848, Learning Rate: 0.014699999999999998\n",
      "Epoch [3009/20000], Bound: 0.5150485634803772, Entropy: 119.7635726928711, Temp: 1.6685951948165894, KL: 83.17485809326172, Loss: 0.05612180754542351, Learning Rate: 0.014699999999999998\n",
      "Epoch [3010/20000], Bound: 0.4655708074569702, Entropy: 121.4304428100586, Temp: 1.6705929040908813, KL: 70.6952896118164, Loss: 0.053643856197595596, Learning Rate: 0.014699999999999998\n",
      "Epoch [3011/20000], Bound: 0.47889405488967896, Entropy: 128.41114807128906, Temp: 1.6725155115127563, KL: 77.3177490234375, Loss: 0.044442348182201385, Learning Rate: 0.014699999999999998\n",
      "Epoch [3012/20000], Bound: 0.5063851475715637, Entropy: 132.0150909423828, Temp: 1.6745553016662598, KL: 81.1898193359375, Loss: 0.055254705250263214, Learning Rate: 0.014699999999999998\n",
      "Epoch [3013/20000], Bound: 0.45310553908348083, Entropy: 138.95877075195312, Temp: 1.676594853401184, KL: 66.426025390625, Loss: 0.05703933537006378, Learning Rate: 0.014699999999999998\n",
      "Epoch [3014/20000], Bound: 0.5037664175033569, Entropy: 140.58616638183594, Temp: 1.6784591674804688, KL: 76.3143310546875, Loss: 0.06787050515413284, Learning Rate: 0.014699999999999998\n",
      "Epoch [3015/20000], Bound: 0.5469026565551758, Entropy: 140.76846313476562, Temp: 1.6801245212554932, KL: 91.47953796386719, Loss: 0.05932636931538582, Learning Rate: 0.014699999999999998\n",
      "Epoch [3016/20000], Bound: 0.496290922164917, Entropy: 142.14266967773438, Temp: 1.6818645000457764, KL: 80.84262084960938, Loss: 0.048491910099983215, Learning Rate: 0.014699999999999998\n",
      "Epoch [3017/20000], Bound: 0.47600558400154114, Entropy: 140.01132202148438, Temp: 1.6837095022201538, KL: 74.17843627929688, Loss: 0.05215761065483093, Learning Rate: 0.014699999999999998\n",
      "Epoch [3018/20000], Bound: 0.47397080063819885, Entropy: 136.7269744873047, Temp: 1.685533881187439, KL: 70.92216491699219, Loss: 0.060312528163194656, Learning Rate: 0.014699999999999998\n",
      "Epoch [3019/20000], Bound: 0.505237340927124, Entropy: 134.64181518554688, Temp: 1.687196969985962, KL: 79.23074340820312, Loss: 0.06090481951832771, Learning Rate: 0.014699999999999998\n",
      "Epoch [3020/20000], Bound: 0.4541246294975281, Entropy: 132.94784545898438, Temp: 1.6887882947921753, KL: 65.08085632324219, Loss: 0.06235430762171745, Learning Rate: 0.014699999999999998\n",
      "Epoch [3021/20000], Bound: 0.4683712124824524, Entropy: 128.18670654296875, Temp: 1.6901495456695557, KL: 72.10308837890625, Loss: 0.05265643820166588, Learning Rate: 0.014699999999999998\n",
      "Epoch [3022/20000], Bound: 0.45125219225883484, Entropy: 127.58789825439453, Temp: 1.6915029287338257, KL: 68.76323699951172, Loss: 0.04938646778464317, Learning Rate: 0.014699999999999998\n",
      "Epoch [3023/20000], Bound: 0.5354738235473633, Entropy: 127.9819107055664, Temp: 1.6928577423095703, KL: 87.8237075805664, Loss: 0.061185743659734726, Learning Rate: 0.014699999999999998\n",
      "Epoch [3024/20000], Bound: 0.48657840490341187, Entropy: 124.40345764160156, Temp: 1.6942429542541504, KL: 75.54083251953125, Loss: 0.0570959635078907, Learning Rate: 0.014699999999999998\n",
      "Epoch [3025/20000], Bound: 0.4463018774986267, Entropy: 128.15267944335938, Temp: 1.6955885887145996, KL: 68.42314147949219, Loss: 0.04682426527142525, Learning Rate: 0.014699999999999998\n",
      "Epoch [3026/20000], Bound: 0.49363988637924194, Entropy: 124.13111114501953, Temp: 1.6969630718231201, KL: 78.1274185180664, Loss: 0.05528821051120758, Learning Rate: 0.014699999999999998\n",
      "Epoch [3027/20000], Bound: 0.481005996465683, Entropy: 126.9323959350586, Temp: 1.6983449459075928, KL: 74.31627655029297, Loss: 0.05648801103234291, Learning Rate: 0.014699999999999998\n",
      "Epoch [3028/20000], Bound: 0.458873987197876, Entropy: 133.74574279785156, Temp: 1.6996783018112183, KL: 68.50498962402344, Loss: 0.05637402459979057, Learning Rate: 0.014699999999999998\n",
      "Epoch [3029/20000], Bound: 0.47278648614883423, Entropy: 129.73399353027344, Temp: 1.7009092569351196, KL: 73.846435546875, Loss: 0.0515277273952961, Learning Rate: 0.014699999999999998\n",
      "Epoch [3030/20000], Bound: 0.4378521144390106, Entropy: 136.54530334472656, Temp: 1.702165126800537, KL: 64.81742858886719, Loss: 0.05137389525771141, Learning Rate: 0.014699999999999998\n",
      "Epoch [3031/20000], Bound: 0.4639314115047455, Entropy: 134.41781616210938, Temp: 1.703352928161621, KL: 67.98165893554688, Loss: 0.06197825446724892, Learning Rate: 0.014699999999999998\n",
      "Epoch [3032/20000], Bound: 0.5108801126480103, Entropy: 137.09735107421875, Temp: 1.704367756843567, KL: 72.85684204101562, Loss: 0.08521482348442078, Learning Rate: 0.014699999999999998\n",
      "Epoch [3033/20000], Bound: 0.5066412687301636, Entropy: 136.77308654785156, Temp: 1.704971432685852, KL: 72.9443359375, Loss: 0.0814996138215065, Learning Rate: 0.014699999999999998\n",
      "Epoch [3034/20000], Bound: 0.516840398311615, Entropy: 139.18759155273438, Temp: 1.705252766609192, KL: 78.15437316894531, Loss: 0.0746556967496872, Learning Rate: 0.014699999999999998\n",
      "Epoch [3035/20000], Bound: 0.48945894837379456, Entropy: 138.2313995361328, Temp: 1.7053855657577515, KL: 72.82862854003906, Loss: 0.0679403692483902, Learning Rate: 0.014699999999999998\n",
      "Epoch [3036/20000], Bound: 0.4798972010612488, Entropy: 139.76702880859375, Temp: 1.705418348312378, KL: 71.37202453613281, Loss: 0.0646086186170578, Learning Rate: 0.014699999999999998\n",
      "Epoch [3037/20000], Bound: 0.4724756181240082, Entropy: 134.17967224121094, Temp: 1.7053898572921753, KL: 74.597900390625, Loss: 0.04931475594639778, Learning Rate: 0.014699999999999998\n",
      "Epoch [3038/20000], Bound: 0.46995893120765686, Entropy: 132.09922790527344, Temp: 1.7055439949035645, KL: 68.30343627929688, Loss: 0.06581047922372818, Learning Rate: 0.014699999999999998\n",
      "Epoch [3039/20000], Bound: 0.5558933615684509, Entropy: 133.90382385253906, Temp: 1.705576777458191, KL: 86.71363830566406, Loss: 0.08298536390066147, Learning Rate: 0.014699999999999998\n",
      "Epoch [3040/20000], Bound: 0.4988560974597931, Entropy: 126.97866821289062, Temp: 1.7054630517959595, KL: 76.76882934570312, Loss: 0.06396244466304779, Learning Rate: 0.014699999999999998\n",
      "Epoch [3041/20000], Bound: 0.4992333948612213, Entropy: 124.80854034423828, Temp: 1.7053658962249756, KL: 77.06958770751953, Loss: 0.0633816346526146, Learning Rate: 0.014699999999999998\n",
      "Epoch [3042/20000], Bound: 0.5288257002830505, Entropy: 123.44017028808594, Temp: 1.7052946090698242, KL: 81.70623779296875, Loss: 0.07429519295692444, Learning Rate: 0.014699999999999998\n",
      "Epoch [3043/20000], Bound: 0.49264052510261536, Entropy: 120.1064224243164, Temp: 1.705149531364441, KL: 79.4409408569336, Loss: 0.05109227076172829, Learning Rate: 0.014699999999999998\n",
      "Epoch [3044/20000], Bound: 0.5092626214027405, Entropy: 123.30719757080078, Temp: 1.7052223682403564, KL: 81.64629364013672, Loss: 0.05814776197075844, Learning Rate: 0.014699999999999998\n",
      "Epoch [3045/20000], Bound: 0.48457738757133484, Entropy: 123.85729217529297, Temp: 1.705418586730957, KL: 76.62366485595703, Loss: 0.05292125418782234, Learning Rate: 0.014699999999999998\n",
      "Epoch [3046/20000], Bound: 0.48677581548690796, Entropy: 124.54722595214844, Temp: 1.7057466506958008, KL: 71.43913269042969, Loss: 0.06988685578107834, Learning Rate: 0.014699999999999998\n",
      "Epoch [3047/20000], Bound: 0.5374784469604492, Entropy: 125.4660873413086, Temp: 1.7059142589569092, KL: 86.60379791259766, Loss: 0.0673394650220871, Learning Rate: 0.014699999999999998\n",
      "Epoch [3048/20000], Bound: 0.5039478540420532, Entropy: 127.40943908691406, Temp: 1.7061214447021484, KL: 82.62422180175781, Loss: 0.0509793646633625, Learning Rate: 0.014699999999999998\n",
      "Epoch [3049/20000], Bound: 0.4556499421596527, Entropy: 131.42333984375, Temp: 1.7065428495407104, KL: 71.49641418457031, Loss: 0.045449595898389816, Learning Rate: 0.014699999999999998\n",
      "Epoch [3050/20000], Bound: 0.48826101422309875, Entropy: 133.64956665039062, Temp: 1.7071235179901123, KL: 75.41436767578125, Loss: 0.059491049498319626, Learning Rate: 0.014699999999999998\n",
      "Epoch [3051/20000], Bound: 0.5071696043014526, Entropy: 133.9069366455078, Temp: 1.7076958417892456, KL: 80.26667785644531, Loss: 0.06061398237943649, Learning Rate: 0.014699999999999998\n",
      "Epoch [3052/20000], Bound: 0.4967131018638611, Entropy: 132.60037231445312, Temp: 1.7082927227020264, KL: 79.07913208007812, Loss: 0.05561399459838867, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3053/20000], Bound: 0.5128359794616699, Entropy: 131.77182006835938, Temp: 1.7089662551879883, KL: 81.80197143554688, Loss: 0.06085683032870293, Learning Rate: 0.014699999999999998\n",
      "Epoch [3054/20000], Bound: 0.4834362268447876, Entropy: 130.99209594726562, Temp: 1.7096643447875977, KL: 67.30180358886719, Loss: 0.07950788736343384, Learning Rate: 0.014699999999999998\n",
      "Epoch [3055/20000], Bound: 0.47612807154655457, Entropy: 132.1890869140625, Temp: 1.709987759590149, KL: 73.02493286132812, Loss: 0.057018402963876724, Learning Rate: 0.014699999999999998\n",
      "Epoch [3056/20000], Bound: 0.5113747119903564, Entropy: 129.3549041748047, Temp: 1.710334062576294, KL: 79.30384826660156, Loss: 0.0670333281159401, Learning Rate: 0.014699999999999998\n",
      "Epoch [3057/20000], Bound: 0.46484673023223877, Entropy: 131.42431640625, Temp: 1.7106292247772217, KL: 70.369140625, Loss: 0.05602092295885086, Learning Rate: 0.014699999999999998\n",
      "Epoch [3058/20000], Bound: 0.45246249437332153, Entropy: 129.85482788085938, Temp: 1.7109360694885254, KL: 68.26243591308594, Loss: 0.05269122123718262, Learning Rate: 0.014699999999999998\n",
      "Epoch [3059/20000], Bound: 0.5109249949455261, Entropy: 132.0424041748047, Temp: 1.7112767696380615, KL: 80.78520202636719, Loss: 0.06238541379570961, Learning Rate: 0.014699999999999998\n",
      "Epoch [3060/20000], Bound: 0.5092294812202454, Entropy: 127.98653411865234, Temp: 1.7116422653198242, KL: 81.1298599243164, Loss: 0.060004543513059616, Learning Rate: 0.014699999999999998\n",
      "Epoch [3061/20000], Bound: 0.5056192278862, Entropy: 125.3945541381836, Temp: 1.712064504623413, KL: 79.55538177490234, Loss: 0.061668526381254196, Learning Rate: 0.014699999999999998\n",
      "Epoch [3062/20000], Bound: 0.486887663602829, Entropy: 129.48158264160156, Temp: 1.712499737739563, KL: 76.10899353027344, Loss: 0.05664350092411041, Learning Rate: 0.014699999999999998\n",
      "Epoch [3063/20000], Bound: 0.46830493211746216, Entropy: 130.37123107910156, Temp: 1.7129796743392944, KL: 74.08634948730469, Loss: 0.04795943945646286, Learning Rate: 0.014699999999999998\n",
      "Epoch [3064/20000], Bound: 0.502777636051178, Entropy: 130.09725952148438, Temp: 1.7135982513427734, KL: 80.44976806640625, Loss: 0.0568259097635746, Learning Rate: 0.014699999999999998\n",
      "Epoch [3065/20000], Bound: 0.4972493350505829, Entropy: 130.38967895507812, Temp: 1.7142820358276367, KL: 80.94866943359375, Loss: 0.050928305834531784, Learning Rate: 0.014699999999999998\n",
      "Epoch [3066/20000], Bound: 0.5100604891777039, Entropy: 131.7672119140625, Temp: 1.7151085138320923, KL: 79.83702087402344, Loss: 0.06465324014425278, Learning Rate: 0.014699999999999998\n",
      "Epoch [3067/20000], Bound: 0.49925491213798523, Entropy: 131.56251525878906, Temp: 1.7158668041229248, KL: 81.95669555664062, Loss: 0.049704644829034805, Learning Rate: 0.014699999999999998\n",
      "Epoch [3068/20000], Bound: 0.5167651772499084, Entropy: 132.91607666015625, Temp: 1.7167847156524658, KL: 82.15733337402344, Loss: 0.06352148950099945, Learning Rate: 0.014699999999999998\n",
      "Epoch [3069/20000], Bound: 0.46026864647865295, Entropy: 130.0087127685547, Temp: 1.7176610231399536, KL: 72.22430419921875, Loss: 0.047420501708984375, Learning Rate: 0.014699999999999998\n",
      "Epoch [3070/20000], Bound: 0.4656217396259308, Entropy: 128.01495361328125, Temp: 1.7186203002929688, KL: 68.83998107910156, Loss: 0.06143670901656151, Learning Rate: 0.014699999999999998\n",
      "Epoch [3071/20000], Bound: 0.4655054807662964, Entropy: 130.882568359375, Temp: 1.7194256782531738, KL: 74.1358642578125, Loss: 0.045981183648109436, Learning Rate: 0.014699999999999998\n",
      "Epoch [3072/20000], Bound: 0.5215832591056824, Entropy: 131.4611053466797, Temp: 1.720357894897461, KL: 85.83885192871094, Loss: 0.05703137442469597, Learning Rate: 0.014699999999999998\n",
      "Epoch [3073/20000], Bound: 0.48749423027038574, Entropy: 131.69906616210938, Temp: 1.7213647365570068, KL: 71.5679931640625, Loss: 0.07077418267726898, Learning Rate: 0.014699999999999998\n",
      "Epoch [3074/20000], Bound: 0.4560371935367584, Entropy: 131.45297241210938, Temp: 1.722110629081726, KL: 68.70205688476562, Loss: 0.054631248116493225, Learning Rate: 0.014699999999999998\n",
      "Epoch [3075/20000], Bound: 0.5069126486778259, Entropy: 132.47996520996094, Temp: 1.7228119373321533, KL: 79.89686584472656, Loss: 0.062316231429576874, Learning Rate: 0.014699999999999998\n",
      "Epoch [3076/20000], Bound: 0.4395715594291687, Entropy: 134.9201202392578, Temp: 1.7234792709350586, KL: 65.02546691894531, Loss: 0.05293986201286316, Learning Rate: 0.014699999999999998\n",
      "Epoch [3077/20000], Bound: 0.4759075939655304, Entropy: 132.64808654785156, Temp: 1.7240935564041138, KL: 75.17347717285156, Loss: 0.05129645764827728, Learning Rate: 0.014699999999999998\n",
      "Epoch [3078/20000], Bound: 0.5111398100852966, Entropy: 130.64974975585938, Temp: 1.7247852087020874, KL: 79.97834777832031, Loss: 0.06564848870038986, Learning Rate: 0.014699999999999998\n",
      "Epoch [3079/20000], Bound: 0.5118849873542786, Entropy: 128.49517822265625, Temp: 1.725396752357483, KL: 85.74874877929688, Loss: 0.04957140237092972, Learning Rate: 0.014699999999999998\n",
      "Epoch [3080/20000], Bound: 0.44284164905548096, Entropy: 130.55726623535156, Temp: 1.7262095212936401, KL: 66.32289123535156, Loss: 0.05173560604453087, Learning Rate: 0.014699999999999998\n",
      "Epoch [3081/20000], Bound: 0.5184316635131836, Entropy: 132.31353759765625, Temp: 1.7269822359085083, KL: 86.56436157226562, Loss: 0.05271575227379799, Learning Rate: 0.014699999999999998\n",
      "Epoch [3082/20000], Bound: 0.43711957335472107, Entropy: 133.8578643798828, Temp: 1.7279026508331299, KL: 65.97308349609375, Loss: 0.048551250249147415, Learning Rate: 0.014699999999999998\n",
      "Epoch [3083/20000], Bound: 0.45277857780456543, Entropy: 133.22950744628906, Temp: 1.7288117408752441, KL: 69.49665832519531, Loss: 0.05014937371015549, Learning Rate: 0.014699999999999998\n",
      "Epoch [3084/20000], Bound: 0.4863031506538391, Entropy: 134.0377655029297, Temp: 1.7297229766845703, KL: 79.96652221679688, Loss: 0.045910049229860306, Learning Rate: 0.014699999999999998\n",
      "Epoch [3085/20000], Bound: 0.5129368901252747, Entropy: 131.27655029296875, Temp: 1.7307968139648438, KL: 85.4962158203125, Loss: 0.051500868052244186, Learning Rate: 0.014699999999999998\n",
      "Epoch [3086/20000], Bound: 0.4726327955722809, Entropy: 132.51791381835938, Temp: 1.7319905757904053, KL: 75.46882629394531, Loss: 0.04829050973057747, Learning Rate: 0.014699999999999998\n",
      "Epoch [3087/20000], Bound: 0.4828360378742218, Entropy: 133.76046752929688, Temp: 1.733239769935608, KL: 77.15377807617188, Loss: 0.051482271403074265, Learning Rate: 0.014699999999999998\n",
      "Epoch [3088/20000], Bound: 0.48940643668174744, Entropy: 135.0036163330078, Temp: 1.7345093488693237, KL: 80.11505126953125, Loss: 0.04821388050913811, Learning Rate: 0.014699999999999998\n",
      "Epoch [3089/20000], Bound: 0.5157870650291443, Entropy: 133.34727478027344, Temp: 1.7358697652816772, KL: 87.83193969726562, Loss: 0.04742801561951637, Learning Rate: 0.014699999999999998\n",
      "Epoch [3090/20000], Bound: 0.4951673150062561, Entropy: 132.16519165039062, Temp: 1.7373931407928467, KL: 79.36038208007812, Loss: 0.05514281243085861, Learning Rate: 0.014699999999999998\n",
      "Epoch [3091/20000], Bound: 0.5242702960968018, Entropy: 131.49441528320312, Temp: 1.7388752698898315, KL: 90.26251220703125, Loss: 0.047668274492025375, Learning Rate: 0.014699999999999998\n",
      "Epoch [3092/20000], Bound: 0.4799218475818634, Entropy: 130.050537109375, Temp: 1.7405232191085815, KL: 73.53079223632812, Loss: 0.059976086020469666, Learning Rate: 0.014699999999999998\n",
      "Epoch [3093/20000], Bound: 0.481222003698349, Entropy: 124.95867156982422, Temp: 1.7419891357421875, KL: 75.08562469482422, Loss: 0.05659985542297363, Learning Rate: 0.014699999999999998\n",
      "Epoch [3094/20000], Bound: 0.4752434194087982, Entropy: 124.6751937866211, Temp: 1.7433520555496216, KL: 76.51483917236328, Loss: 0.04789230599999428, Learning Rate: 0.014699999999999998\n",
      "Epoch [3095/20000], Bound: 0.5436732172966003, Entropy: 123.6911849975586, Temp: 1.7447562217712402, KL: 81.5391616821289, Loss: 0.0894627571105957, Learning Rate: 0.014699999999999998\n",
      "Epoch [3096/20000], Bound: 0.49461281299591064, Entropy: 124.77066040039062, Temp: 1.7456769943237305, KL: 75.23271179199219, Loss: 0.06695637851953506, Learning Rate: 0.014699999999999998\n",
      "Epoch [3097/20000], Bound: 0.4919852316379547, Entropy: 122.02379608154297, Temp: 1.7464015483856201, KL: 80.34027862548828, Loss: 0.05027034133672714, Learning Rate: 0.014699999999999998\n",
      "Epoch [3098/20000], Bound: 0.5017715692520142, Entropy: 122.8096923828125, Temp: 1.7472305297851562, KL: 79.59175109863281, Loss: 0.060300104320049286, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3099/20000], Bound: 0.48137497901916504, Entropy: 129.15118408203125, Temp: 1.7480056285858154, KL: 74.4761962890625, Loss: 0.05874808505177498, Learning Rate: 0.014699999999999998\n",
      "Epoch [3100/20000], Bound: 0.4553923010826111, Entropy: 124.36238098144531, Temp: 1.7487025260925293, KL: 67.15821838378906, Loss: 0.05968470498919487, Learning Rate: 0.014699999999999998\n",
      "Epoch [3101/20000], Bound: 0.48400089144706726, Entropy: 130.8925323486328, Temp: 1.7492408752441406, KL: 75.72842407226562, Loss: 0.05728796869516373, Learning Rate: 0.014699999999999998\n",
      "Epoch [3102/20000], Bound: 0.4583374559879303, Entropy: 131.93833923339844, Temp: 1.749755859375, KL: 66.9730224609375, Loss: 0.06248818337917328, Learning Rate: 0.014699999999999998\n",
      "Epoch [3103/20000], Bound: 0.4964440166950226, Entropy: 128.88372802734375, Temp: 1.7500874996185303, KL: 74.46064758300781, Loss: 0.07082431763410568, Learning Rate: 0.014699999999999998\n",
      "Epoch [3104/20000], Bound: 0.4942656457424164, Entropy: 133.53915405273438, Temp: 1.7502142190933228, KL: 80.6273193359375, Loss: 0.051471903920173645, Learning Rate: 0.014699999999999998\n",
      "Epoch [3105/20000], Bound: 0.44763314723968506, Entropy: 135.3196258544922, Temp: 1.7504863739013672, KL: 67.12916564941406, Loss: 0.05399377644062042, Learning Rate: 0.014699999999999998\n",
      "Epoch [3106/20000], Bound: 0.48592135310173035, Entropy: 133.64068603515625, Temp: 1.7507206201553345, KL: 73.91352844238281, Loss: 0.0640539899468422, Learning Rate: 0.014699999999999998\n",
      "Epoch [3107/20000], Bound: 0.5565968155860901, Entropy: 134.2478485107422, Temp: 1.7508469820022583, KL: 99.48757934570312, Loss: 0.04967759549617767, Learning Rate: 0.014699999999999998\n",
      "Epoch [3108/20000], Bound: 0.46718645095825195, Entropy: 132.8346405029297, Temp: 1.7513136863708496, KL: 71.14163208007812, Loss: 0.05740736424922943, Learning Rate: 0.014699999999999998\n",
      "Epoch [3109/20000], Bound: 0.47972890734672546, Entropy: 136.79248046875, Temp: 1.7517143487930298, KL: 74.8802490234375, Loss: 0.05647603049874306, Learning Rate: 0.014699999999999998\n",
      "Epoch [3110/20000], Bound: 0.5102499723434448, Entropy: 132.909912109375, Temp: 1.7521052360534668, KL: 84.68161010742188, Loss: 0.05289773270487785, Learning Rate: 0.014699999999999998\n",
      "Epoch [3111/20000], Bound: 0.5272035002708435, Entropy: 133.412841796875, Temp: 1.7526315450668335, KL: 87.52552795410156, Loss: 0.05881547927856445, Learning Rate: 0.014699999999999998\n",
      "Epoch [3112/20000], Bound: 0.5407946705818176, Entropy: 133.43002319335938, Temp: 1.7532230615615845, KL: 91.06211853027344, Loss: 0.06024068221449852, Learning Rate: 0.014699999999999998\n",
      "Epoch [3113/20000], Bound: 0.531565248966217, Entropy: 131.65443420410156, Temp: 1.7538859844207764, KL: 80.36555480957031, Loss: 0.08295843750238419, Learning Rate: 0.014699999999999998\n",
      "Epoch [3114/20000], Bound: 0.513595461845398, Entropy: 128.16636657714844, Temp: 1.754198431968689, KL: 81.91053771972656, Loss: 0.06365033984184265, Learning Rate: 0.014699999999999998\n",
      "Epoch [3115/20000], Bound: 0.49469685554504395, Entropy: 126.0130844116211, Temp: 1.7544753551483154, KL: 72.3803482055664, Loss: 0.07554534077644348, Learning Rate: 0.014699999999999998\n",
      "Epoch [3116/20000], Bound: 0.5113748908042908, Entropy: 120.59146881103516, Temp: 1.7544591426849365, KL: 80.21797943115234, Loss: 0.06667076051235199, Learning Rate: 0.014699999999999998\n",
      "Epoch [3117/20000], Bound: 0.5042209625244141, Entropy: 121.99970245361328, Temp: 1.7543811798095703, KL: 81.4480209350586, Loss: 0.057347603142261505, Learning Rate: 0.014699999999999998\n",
      "Epoch [3118/20000], Bound: 0.4908488988876343, Entropy: 122.38780975341797, Temp: 1.7543896436691284, KL: 77.19121551513672, Loss: 0.0587695874273777, Learning Rate: 0.014699999999999998\n",
      "Epoch [3119/20000], Bound: 0.48983362317085266, Entropy: 122.61772155761719, Temp: 1.7544145584106445, KL: 79.30662536621094, Loss: 0.05193690210580826, Learning Rate: 0.014699999999999998\n",
      "Epoch [3120/20000], Bound: 0.48789551854133606, Entropy: 122.14830017089844, Temp: 1.7545716762542725, KL: 76.94560241699219, Loss: 0.05713972449302673, Learning Rate: 0.014699999999999998\n",
      "Epoch [3121/20000], Bound: 0.4406135082244873, Entropy: 128.20274353027344, Temp: 1.7547508478164673, KL: 64.31025695800781, Loss: 0.056964777410030365, Learning Rate: 0.014699999999999998\n",
      "Epoch [3122/20000], Bound: 0.4505619406700134, Entropy: 129.21534729003906, Temp: 1.7548242807388306, KL: 67.53482055664062, Loss: 0.05520496144890785, Learning Rate: 0.014699999999999998\n",
      "Epoch [3123/20000], Bound: 0.4928627610206604, Entropy: 134.34796142578125, Temp: 1.7548611164093018, KL: 77.26873779296875, Loss: 0.06017131730914116, Learning Rate: 0.014699999999999998\n",
      "Epoch [3124/20000], Bound: 0.4476955831050873, Entropy: 136.41429138183594, Temp: 1.7548919916152954, KL: 67.87910461425781, Loss: 0.052077364176511765, Learning Rate: 0.014699999999999998\n",
      "Epoch [3125/20000], Bound: 0.4862995445728302, Entropy: 140.46669006347656, Temp: 1.7549389600753784, KL: 76.93385314941406, Loss: 0.055931128561496735, Learning Rate: 0.014699999999999998\n",
      "Epoch [3126/20000], Bound: 0.4864264726638794, Entropy: 139.82400512695312, Temp: 1.7550357580184937, KL: 74.37284851074219, Loss: 0.06333208829164505, Learning Rate: 0.014699999999999998\n",
      "Epoch [3127/20000], Bound: 0.4739905297756195, Entropy: 141.44683837890625, Temp: 1.7550468444824219, KL: 73.53567504882812, Loss: 0.055995285511016846, Learning Rate: 0.014699999999999998\n",
      "Epoch [3128/20000], Bound: 0.4378170967102051, Entropy: 146.60968017578125, Temp: 1.7550768852233887, KL: 64.47236633300781, Loss: 0.054444849491119385, Learning Rate: 0.014699999999999998\n",
      "Epoch [3129/20000], Bound: 0.5160850882530212, Entropy: 141.05044555664062, Temp: 1.7550532817840576, KL: 80.80267333984375, Loss: 0.06889412552118301, Learning Rate: 0.014699999999999998\n",
      "Epoch [3130/20000], Bound: 0.49004241824150085, Entropy: 141.04710388183594, Temp: 1.7549422979354858, KL: 80.2509765625, Loss: 0.04943898692727089, Learning Rate: 0.014699999999999998\n",
      "Epoch [3131/20000], Bound: 0.4937613308429718, Entropy: 138.28773498535156, Temp: 1.755021572113037, KL: 76.30155944824219, Loss: 0.06364978849887848, Learning Rate: 0.014699999999999998\n",
      "Epoch [3132/20000], Bound: 0.44852450489997864, Entropy: 134.85736083984375, Temp: 1.7550318241119385, KL: 64.2620849609375, Loss: 0.06300844997167587, Learning Rate: 0.014699999999999998\n",
      "Epoch [3133/20000], Bound: 0.4770379960536957, Entropy: 134.8900146484375, Temp: 1.7548646926879883, KL: 68.33587646484375, Loss: 0.07316971570253372, Learning Rate: 0.014699999999999998\n",
      "Epoch [3134/20000], Bound: 0.48974624276161194, Entropy: 130.37109375, Temp: 1.7544364929199219, KL: 78.34881591796875, Loss: 0.054598528891801834, Learning Rate: 0.014699999999999998\n",
      "Epoch [3135/20000], Bound: 0.4461107850074768, Entropy: 133.05239868164062, Temp: 1.7541393041610718, KL: 69.28068542480469, Loss: 0.04686683043837547, Learning Rate: 0.014699999999999998\n",
      "Epoch [3136/20000], Bound: 0.529204785823822, Entropy: 129.94491577148438, Temp: 1.7539819478988647, KL: 82.66909790039062, Loss: 0.07441400736570358, Learning Rate: 0.014699999999999998\n",
      "Epoch [3137/20000], Bound: 0.5109230875968933, Entropy: 127.09326171875, Temp: 1.753693699836731, KL: 82.06784057617188, Loss: 0.06098982319235802, Learning Rate: 0.014699999999999998\n",
      "Epoch [3138/20000], Bound: 0.49563682079315186, Entropy: 124.30624389648438, Temp: 1.7534689903259277, KL: 79.18984985351562, Loss: 0.056838810443878174, Learning Rate: 0.014699999999999998\n",
      "Epoch [3139/20000], Bound: 0.4363294243812561, Entropy: 127.24716186523438, Temp: 1.7533326148986816, KL: 62.56965637207031, Loss: 0.05870838090777397, Learning Rate: 0.014699999999999998\n",
      "Epoch [3140/20000], Bound: 0.4538594186306, Entropy: 128.76918029785156, Temp: 1.753078818321228, KL: 71.45309448242188, Loss: 0.046444397419691086, Learning Rate: 0.014699999999999998\n",
      "Epoch [3141/20000], Bound: 0.481465607881546, Entropy: 132.3502960205078, Temp: 1.7529902458190918, KL: 73.98818969726562, Loss: 0.06043868511915207, Learning Rate: 0.014699999999999998\n",
      "Epoch [3142/20000], Bound: 0.5129696130752563, Entropy: 130.82083129882812, Temp: 1.7528738975524902, KL: 87.25199890136719, Loss: 0.04783254489302635, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3143/20000], Bound: 0.528586208820343, Entropy: 134.6458740234375, Temp: 1.7530410289764404, KL: 87.44329833984375, Loss: 0.0602312795817852, Learning Rate: 0.014699999999999998\n",
      "Epoch [3144/20000], Bound: 0.4730162024497986, Entropy: 131.20985412597656, Temp: 1.7532899379730225, KL: 77.35877990722656, Loss: 0.04425846040248871, Learning Rate: 0.014699999999999998\n",
      "Epoch [3145/20000], Bound: 0.47723618149757385, Entropy: 134.46780395507812, Temp: 1.7537437677383423, KL: 78.95034790039062, Loss: 0.04302141070365906, Learning Rate: 0.014699999999999998\n",
      "Epoch [3146/20000], Bound: 0.48868805170059204, Entropy: 130.3716583251953, Temp: 1.7544147968292236, KL: 76.19454956054688, Loss: 0.059899214655160904, Learning Rate: 0.014699999999999998\n",
      "Epoch [3147/20000], Bound: 0.48140665888786316, Entropy: 135.5463104248047, Temp: 1.755010724067688, KL: 75.41883850097656, Loss: 0.05640653148293495, Learning Rate: 0.014699999999999998\n",
      "Epoch [3148/20000], Bound: 0.49374502897262573, Entropy: 134.0286407470703, Temp: 1.7555804252624512, KL: 75.84260559082031, Loss: 0.06496961414813995, Learning Rate: 0.014699999999999998\n",
      "Epoch [3149/20000], Bound: 0.43516266345977783, Entropy: 132.55035400390625, Temp: 1.7560075521469116, KL: 67.46614074707031, Loss: 0.043997202068567276, Learning Rate: 0.014699999999999998\n",
      "Epoch [3150/20000], Bound: 0.49304497241973877, Entropy: 133.44317626953125, Temp: 1.7565250396728516, KL: 78.55061340332031, Loss: 0.05674656108021736, Learning Rate: 0.014699999999999998\n",
      "Epoch [3151/20000], Bound: 0.47615405917167664, Entropy: 136.738037109375, Temp: 1.7570486068725586, KL: 71.44430541992188, Loss: 0.06371504813432693, Learning Rate: 0.014699999999999998\n",
      "Epoch [3152/20000], Bound: 0.5042626857757568, Entropy: 136.8998260498047, Temp: 1.7574048042297363, KL: 77.447509765625, Loss: 0.06892167776823044, Learning Rate: 0.014699999999999998\n",
      "Epoch [3153/20000], Bound: 0.5287289023399353, Entropy: 135.26226806640625, Temp: 1.7575974464416504, KL: 80.01109313964844, Loss: 0.08175843954086304, Learning Rate: 0.014699999999999998\n",
      "Epoch [3154/20000], Bound: 0.4832001328468323, Entropy: 134.36912536621094, Temp: 1.7574889659881592, KL: 73.54803466796875, Loss: 0.06324954330921173, Learning Rate: 0.014699999999999998\n",
      "Epoch [3155/20000], Bound: 0.461099773645401, Entropy: 129.86257934570312, Temp: 1.7573037147521973, KL: 70.50599670410156, Loss: 0.05481582507491112, Learning Rate: 0.014699999999999998\n",
      "Epoch [3156/20000], Bound: 0.5112780928611755, Entropy: 130.97052001953125, Temp: 1.7571406364440918, KL: 81.57351684570312, Loss: 0.06286647915840149, Learning Rate: 0.014699999999999998\n",
      "Epoch [3157/20000], Bound: 0.4889538586139679, Entropy: 127.35379791259766, Temp: 1.7569928169250488, KL: 76.85111236572266, Loss: 0.05836104601621628, Learning Rate: 0.014699999999999998\n",
      "Epoch [3158/20000], Bound: 0.49379488825798035, Entropy: 126.3167724609375, Temp: 1.7568764686584473, KL: 79.55168151855469, Loss: 0.05451182648539543, Learning Rate: 0.014699999999999998\n",
      "Epoch [3159/20000], Bound: 0.47948333621025085, Entropy: 128.8746795654297, Temp: 1.7568708658218384, KL: 71.35379028320312, Loss: 0.06655861437320709, Learning Rate: 0.014699999999999998\n",
      "Epoch [3160/20000], Bound: 0.49478036165237427, Entropy: 128.782470703125, Temp: 1.7567087411880493, KL: 76.50076293945312, Loss: 0.06397218257188797, Learning Rate: 0.014699999999999998\n",
      "Epoch [3161/20000], Bound: 0.5430265665054321, Entropy: 126.87271881103516, Temp: 1.7564959526062012, KL: 84.0184555053711, Loss: 0.08239981532096863, Learning Rate: 0.014699999999999998\n",
      "Epoch [3162/20000], Bound: 0.4787440299987793, Entropy: 124.35005950927734, Temp: 1.7560549974441528, KL: 75.56856536865234, Loss: 0.053948089480400085, Learning Rate: 0.014699999999999998\n",
      "Epoch [3163/20000], Bound: 0.49747422337532043, Entropy: 126.32284545898438, Temp: 1.7557268142700195, KL: 82.63827514648438, Loss: 0.048601217567920685, Learning Rate: 0.014699999999999998\n",
      "Epoch [3164/20000], Bound: 0.5245620608329773, Entropy: 124.7340316772461, Temp: 1.7556477785110474, KL: 78.91358184814453, Loss: 0.08131593465805054, Learning Rate: 0.014699999999999998\n",
      "Epoch [3165/20000], Bound: 0.4642157256603241, Entropy: 125.98468780517578, Temp: 1.7552908658981323, KL: 70.35961151123047, Loss: 0.057526595890522, Learning Rate: 0.014699999999999998\n",
      "Epoch [3166/20000], Bound: 0.577879786491394, Entropy: 126.21556091308594, Temp: 1.754934310913086, KL: 99.3802490234375, Loss: 0.06918057054281235, Learning Rate: 0.014699999999999998\n",
      "Epoch [3167/20000], Bound: 0.5422338247299194, Entropy: 127.03556060791016, Temp: 1.7546966075897217, KL: 80.33539581298828, Loss: 0.09212664514780045, Learning Rate: 0.014699999999999998\n",
      "Epoch [3168/20000], Bound: 0.4538829028606415, Entropy: 123.21585845947266, Temp: 1.754063367843628, KL: 71.00016021728516, Loss: 0.04779777303338051, Learning Rate: 0.014699999999999998\n",
      "Epoch [3169/20000], Bound: 0.49411091208457947, Entropy: 122.71484375, Temp: 1.7536091804504395, KL: 76.74024963378906, Loss: 0.06261254101991653, Learning Rate: 0.014699999999999998\n",
      "Epoch [3170/20000], Bound: 0.49983271956443787, Entropy: 126.89386749267578, Temp: 1.7531590461730957, KL: 76.90245819091797, Loss: 0.06670919060707092, Learning Rate: 0.014699999999999998\n",
      "Epoch [3171/20000], Bound: 0.46539267897605896, Entropy: 122.69573974609375, Temp: 1.75265634059906, KL: 70.82948303222656, Loss: 0.05697814002633095, Learning Rate: 0.014699999999999998\n",
      "Epoch [3172/20000], Bound: 0.4514748752117157, Entropy: 123.36935424804688, Temp: 1.7521848678588867, KL: 65.71298217773438, Loss: 0.06098622828722, Learning Rate: 0.014699999999999998\n",
      "Epoch [3173/20000], Bound: 0.5374361276626587, Entropy: 126.17316436767578, Temp: 1.7516292333602905, KL: 82.91490936279297, Loss: 0.08053895831108093, Learning Rate: 0.014699999999999998\n",
      "Epoch [3174/20000], Bound: 0.47970107197761536, Entropy: 130.51937866210938, Temp: 1.750900387763977, KL: 73.93528747558594, Loss: 0.05911490321159363, Learning Rate: 0.014699999999999998\n",
      "Epoch [3175/20000], Bound: 0.5228444933891296, Entropy: 131.16275024414062, Temp: 1.7502284049987793, KL: 84.302734375, Loss: 0.06424588710069656, Learning Rate: 0.014699999999999998\n",
      "Epoch [3176/20000], Bound: 0.5177099108695984, Entropy: 130.55551147460938, Temp: 1.7496392726898193, KL: 82.94253540039062, Loss: 0.06384842842817307, Learning Rate: 0.014699999999999998\n",
      "Epoch [3177/20000], Bound: 0.4844921827316284, Entropy: 130.99449157714844, Temp: 1.7491178512573242, KL: 75.16957092285156, Loss: 0.05926642194390297, Learning Rate: 0.014699999999999998\n",
      "Epoch [3178/20000], Bound: 0.4374960660934448, Entropy: 128.64439392089844, Temp: 1.748645305633545, KL: 67.66763305664062, Loss: 0.04483351483941078, Learning Rate: 0.014699999999999998\n",
      "Epoch [3179/20000], Bound: 0.4959598779678345, Entropy: 132.17042541503906, Temp: 1.7483524084091187, KL: 77.58171081542969, Loss: 0.06143611669540405, Learning Rate: 0.014699999999999998\n",
      "Epoch [3180/20000], Bound: 0.45173802971839905, Entropy: 129.1512451171875, Temp: 1.7480800151824951, KL: 67.93409729003906, Loss: 0.05468093603849411, Learning Rate: 0.014699999999999998\n",
      "Epoch [3181/20000], Bound: 0.45422565937042236, Entropy: 130.16624450683594, Temp: 1.747824788093567, KL: 69.94961547851562, Loss: 0.050782669335603714, Learning Rate: 0.014699999999999998\n",
      "Epoch [3182/20000], Bound: 0.5080503821372986, Entropy: 128.9484100341797, Temp: 1.747663974761963, KL: 88.33625793457031, Loss: 0.04039328545331955, Learning Rate: 0.014699999999999998\n",
      "Epoch [3183/20000], Bound: 0.49958857893943787, Entropy: 131.67474365234375, Temp: 1.747921347618103, KL: 81.39337158203125, Loss: 0.053423069417476654, Learning Rate: 0.014699999999999998\n",
      "Epoch [3184/20000], Bound: 0.4728458523750305, Entropy: 131.97216796875, Temp: 1.7482991218566895, KL: 76.56448364257812, Loss: 0.04613947123289108, Learning Rate: 0.014699999999999998\n",
      "Epoch [3185/20000], Bound: 0.4439275860786438, Entropy: 134.958251953125, Temp: 1.7488435506820679, KL: 70.81930541992188, Loss: 0.040607914328575134, Learning Rate: 0.014699999999999998\n",
      "Epoch [3186/20000], Bound: 0.5145208239555359, Entropy: 132.7220916748047, Temp: 1.7495622634887695, KL: 86.62594604492188, Loss: 0.05069238692522049, Learning Rate: 0.014699999999999998\n",
      "Epoch [3187/20000], Bound: 0.5195656418800354, Entropy: 130.81190490722656, Temp: 1.750443696975708, KL: 90.7408447265625, Loss: 0.04314908757805824, Learning Rate: 0.014699999999999998\n",
      "Epoch [3188/20000], Bound: 0.49593284726142883, Entropy: 132.30918884277344, Temp: 1.7516183853149414, KL: 78.691162109375, Loss: 0.058405227959156036, Learning Rate: 0.014699999999999998\n",
      "Epoch [3189/20000], Bound: 0.4732950031757355, Entropy: 134.4914093017578, Temp: 1.7527185678482056, KL: 73.80497741699219, Loss: 0.05458294227719307, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3190/20000], Bound: 0.49349772930145264, Entropy: 131.27081298828125, Temp: 1.7537562847137451, KL: 82.21250915527344, Loss: 0.04652928560972214, Learning Rate: 0.014699999999999998\n",
      "Epoch [3191/20000], Bound: 0.4907505214214325, Entropy: 128.29002380371094, Temp: 1.7549381256103516, KL: 81.19784545898438, Loss: 0.0473024845123291, Learning Rate: 0.014699999999999998\n",
      "Epoch [3192/20000], Bound: 0.4780609905719757, Entropy: 131.50965881347656, Temp: 1.7562274932861328, KL: 75.44490051269531, Loss: 0.05377579107880592, Learning Rate: 0.014699999999999998\n",
      "Epoch [3193/20000], Bound: 0.437507688999176, Entropy: 130.19827270507812, Temp: 1.7574597597122192, KL: 66.03065490722656, Loss: 0.04987000301480293, Learning Rate: 0.014699999999999998\n",
      "Epoch [3194/20000], Bound: 0.4904305040836334, Entropy: 128.7621612548828, Temp: 1.7585999965667725, KL: 79.48748779296875, Loss: 0.05211026966571808, Learning Rate: 0.014699999999999998\n",
      "Epoch [3195/20000], Bound: 0.45944640040397644, Entropy: 129.29063415527344, Temp: 1.7597601413726807, KL: 73.4620361328125, Loss: 0.045262932777404785, Learning Rate: 0.014699999999999998\n",
      "Epoch [3196/20000], Bound: 0.4808237552642822, Entropy: 131.46002197265625, Temp: 1.7609779834747314, KL: 71.82821655273438, Loss: 0.06642211228609085, Learning Rate: 0.014699999999999998\n",
      "Epoch [3197/20000], Bound: 0.48362410068511963, Entropy: 127.29732513427734, Temp: 1.7619171142578125, KL: 80.45560455322266, Loss: 0.044168900698423386, Learning Rate: 0.014699999999999998\n",
      "Epoch [3198/20000], Bound: 0.4688001275062561, Entropy: 133.14276123046875, Temp: 1.7630186080932617, KL: 74.3458251953125, Loss: 0.05004985257983208, Learning Rate: 0.014699999999999998\n",
      "Epoch [3199/20000], Bound: 0.4924589991569519, Entropy: 132.42355346679688, Temp: 1.7641173601150513, KL: 81.64288330078125, Loss: 0.047889504581689835, Learning Rate: 0.014699999999999998\n",
      "Epoch [3200/20000], Bound: 0.47373345494270325, Entropy: 131.04937744140625, Temp: 1.7653167247772217, KL: 72.05256652832031, Loss: 0.06045214459300041, Learning Rate: 0.014699999999999998\n",
      "Epoch [3201/20000], Bound: 0.5083558559417725, Entropy: 131.16314697265625, Temp: 1.7663228511810303, KL: 77.41157531738281, Loss: 0.07273712009191513, Learning Rate: 0.014699999999999998\n",
      "Epoch [3202/20000], Bound: 0.5104255676269531, Entropy: 131.20921325683594, Temp: 1.7670303583145142, KL: 88.26374816894531, Loss: 0.0437421016395092, Learning Rate: 0.014699999999999998\n",
      "Epoch [3203/20000], Bound: 0.47530224919319153, Entropy: 132.4732208251953, Temp: 1.7679975032806396, KL: 72.71690368652344, Loss: 0.059897229075431824, Learning Rate: 0.014699999999999998\n",
      "Epoch [3204/20000], Bound: 0.5383760929107666, Entropy: 127.85140228271484, Temp: 1.7688062191009521, KL: 85.60674285888672, Loss: 0.07456112653017044, Learning Rate: 0.014699999999999998\n",
      "Epoch [3205/20000], Bound: 0.44606444239616394, Entropy: 126.63794708251953, Temp: 1.7693891525268555, KL: 69.0101089477539, Loss: 0.048240289092063904, Learning Rate: 0.014699999999999998\n",
      "Epoch [3206/20000], Bound: 0.5090557336807251, Entropy: 128.2576446533203, Temp: 1.769986629486084, KL: 78.74444580078125, Loss: 0.06969941407442093, Learning Rate: 0.014699999999999998\n",
      "Epoch [3207/20000], Bound: 0.45325392484664917, Entropy: 127.67790985107422, Temp: 1.7703779935836792, KL: 69.8641128540039, Loss: 0.05124371498823166, Learning Rate: 0.014699999999999998\n",
      "Epoch [3208/20000], Bound: 0.5109171271324158, Entropy: 127.1423110961914, Temp: 1.7707650661468506, KL: 79.75067901611328, Loss: 0.06840575486421585, Learning Rate: 0.014699999999999998\n",
      "Epoch [3209/20000], Bound: 0.4799979031085968, Entropy: 125.57091522216797, Temp: 1.7709949016571045, KL: 68.81937408447266, Loss: 0.07467103004455566, Learning Rate: 0.014699999999999998\n",
      "Epoch [3210/20000], Bound: 0.5032230019569397, Entropy: 130.69931030273438, Temp: 1.7708766460418701, KL: 78.53083801269531, Loss: 0.06563052535057068, Learning Rate: 0.014699999999999998\n",
      "Epoch [3211/20000], Bound: 0.4786376357078552, Entropy: 125.7910385131836, Temp: 1.7706793546676636, KL: 75.7635269165039, Loss: 0.053993020206689835, Learning Rate: 0.014699999999999998\n",
      "Epoch [3212/20000], Bound: 0.5021414756774902, Entropy: 127.9757080078125, Temp: 1.770554780960083, KL: 75.46089172363281, Loss: 0.07341568171977997, Learning Rate: 0.014699999999999998\n",
      "Epoch [3213/20000], Bound: 0.5117040276527405, Entropy: 127.73410034179688, Temp: 1.7702069282531738, KL: 72.50968933105469, Loss: 0.08947265148162842, Learning Rate: 0.014699999999999998\n",
      "Epoch [3214/20000], Bound: 0.5049715042114258, Entropy: 132.15939331054688, Temp: 1.7693960666656494, KL: 78.34646606445312, Loss: 0.06749231368303299, Learning Rate: 0.014699999999999998\n",
      "Epoch [3215/20000], Bound: 0.4751304090023041, Entropy: 130.13099670410156, Temp: 1.7685480117797852, KL: 72.10430908203125, Loss: 0.061519134789705276, Learning Rate: 0.014699999999999998\n",
      "Epoch [3216/20000], Bound: 0.4762503206729889, Entropy: 130.7259521484375, Temp: 1.767690896987915, KL: 76.3785400390625, Loss: 0.05026187002658844, Learning Rate: 0.014699999999999998\n",
      "Epoch [3217/20000], Bound: 0.5037631392478943, Entropy: 133.5329132080078, Temp: 1.7670382261276245, KL: 80.24288940429688, Loss: 0.06104384362697601, Learning Rate: 0.014699999999999998\n",
      "Epoch [3218/20000], Bound: 0.5110488533973694, Entropy: 134.66221618652344, Temp: 1.7664494514465332, KL: 78.78382873535156, Loss: 0.0710473582148552, Learning Rate: 0.014699999999999998\n",
      "Epoch [3219/20000], Bound: 0.47198718786239624, Entropy: 130.7104034423828, Temp: 1.7657577991485596, KL: 74.43537902832031, Loss: 0.05237479880452156, Learning Rate: 0.014699999999999998\n",
      "Epoch [3220/20000], Bound: 0.5289036631584167, Entropy: 135.60446166992188, Temp: 1.7652053833007812, KL: 89.99069213867188, Loss: 0.05398117005825043, Learning Rate: 0.014699999999999998\n",
      "Epoch [3221/20000], Bound: 0.5088788270950317, Entropy: 135.0387420654297, Temp: 1.7649081945419312, KL: 76.64418029785156, Loss: 0.07527358829975128, Learning Rate: 0.014699999999999998\n",
      "Epoch [3222/20000], Bound: 0.5111668109893799, Entropy: 130.9569549560547, Temp: 1.7643975019454956, KL: 86.99755859375, Loss: 0.04777337610721588, Learning Rate: 0.014699999999999998\n",
      "Epoch [3223/20000], Bound: 0.5263014435768127, Entropy: 129.35287475585938, Temp: 1.7642021179199219, KL: 85.76907348632812, Loss: 0.06371301412582397, Learning Rate: 0.014699999999999998\n",
      "Epoch [3224/20000], Bound: 0.4795643091201782, Entropy: 129.26419067382812, Temp: 1.764045000076294, KL: 74.69276428222656, Loss: 0.057443153113126755, Learning Rate: 0.014699999999999998\n",
      "Epoch [3225/20000], Bound: 0.5695943236351013, Entropy: 129.034912109375, Temp: 1.7639029026031494, KL: 92.39285278320312, Loss: 0.08218545466661453, Learning Rate: 0.014699999999999998\n",
      "Epoch [3226/20000], Bound: 0.4986116588115692, Entropy: 125.73208618164062, Temp: 1.7635951042175293, KL: 75.74736022949219, Loss: 0.06948026269674301, Learning Rate: 0.014699999999999998\n",
      "Epoch [3227/20000], Bound: 0.5207669138908386, Entropy: 123.92369079589844, Temp: 1.7631511688232422, KL: 76.02462768554688, Loss: 0.08669646829366684, Learning Rate: 0.014699999999999998\n",
      "Epoch [3228/20000], Bound: 0.5829454660415649, Entropy: 123.70355224609375, Temp: 1.7623393535614014, KL: 84.8992919921875, Loss: 0.11536213010549545, Learning Rate: 0.014699999999999998\n",
      "Epoch [3229/20000], Bound: 0.5095884799957275, Entropy: 119.50569915771484, Temp: 1.7608957290649414, KL: 80.87097930908203, Loss: 0.06367646157741547, Learning Rate: 0.014699999999999998\n",
      "Epoch [3230/20000], Bound: 0.5520283579826355, Entropy: 118.4222640991211, Temp: 1.759570837020874, KL: 82.22209930419922, Loss: 0.0954044982790947, Learning Rate: 0.014699999999999998\n",
      "Epoch [3231/20000], Bound: 0.5018664598464966, Entropy: 116.54447937011719, Temp: 1.7579162120819092, KL: 78.090087890625, Loss: 0.0651845633983612, Learning Rate: 0.014699999999999998\n",
      "Epoch [3232/20000], Bound: 0.5103421211242676, Entropy: 116.0790786743164, Temp: 1.7563551664352417, KL: 80.09810638427734, Loss: 0.06626280397176743, Learning Rate: 0.014699999999999998\n",
      "Epoch [3233/20000], Bound: 0.5452930927276611, Entropy: 120.22847747802734, Temp: 1.7548853158950806, KL: 80.9965591430664, Loss: 0.09287143498659134, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3234/20000], Bound: 0.5082994103431702, Entropy: 121.0849838256836, Temp: 1.753129482269287, KL: 77.58684539794922, Loss: 0.07160191982984543, Learning Rate: 0.014699999999999998\n",
      "Epoch [3235/20000], Bound: 0.5692797899246216, Entropy: 118.6081314086914, Temp: 1.7513853311538696, KL: 87.64315032958984, Loss: 0.09472429007291794, Learning Rate: 0.014699999999999998\n",
      "Epoch [3236/20000], Bound: 0.5781838297843933, Entropy: 120.96261596679688, Temp: 1.7494324445724487, KL: 90.02975463867188, Loss: 0.09579406678676605, Learning Rate: 0.014699999999999998\n",
      "Epoch [3237/20000], Bound: 0.5074656009674072, Entropy: 123.0898208618164, Temp: 1.747304916381836, KL: 78.0799331665039, Loss: 0.06924335658550262, Learning Rate: 0.014699999999999998\n",
      "Epoch [3238/20000], Bound: 0.49638718366622925, Entropy: 118.27958679199219, Temp: 1.7452738285064697, KL: 74.73606872558594, Loss: 0.06978092342615128, Learning Rate: 0.014699999999999998\n",
      "Epoch [3239/20000], Bound: 0.5171439051628113, Entropy: 121.83252716064453, Temp: 1.7432891130447388, KL: 81.97595977783203, Loss: 0.06581147015094757, Learning Rate: 0.014699999999999998\n",
      "Epoch [3240/20000], Bound: 0.5381920337677002, Entropy: 120.37238311767578, Temp: 1.7414824962615967, KL: 78.43163299560547, Loss: 0.09354197233915329, Learning Rate: 0.014699999999999998\n",
      "Epoch [3241/20000], Bound: 0.5053448677062988, Entropy: 122.81079864501953, Temp: 1.739406943321228, KL: 73.82251739501953, Loss: 0.07937706261873245, Learning Rate: 0.014699999999999998\n",
      "Epoch [3242/20000], Bound: 0.5037515163421631, Entropy: 122.88433074951172, Temp: 1.7372419834136963, KL: 73.99988555908203, Loss: 0.0774821937084198, Learning Rate: 0.014699999999999998\n",
      "Epoch [3243/20000], Bound: 0.5309993624687195, Entropy: 124.27812194824219, Temp: 1.7350291013717651, KL: 92.22268676757812, Loss: 0.047414157539606094, Learning Rate: 0.014699999999999998\n",
      "Epoch [3244/20000], Bound: 0.5039569139480591, Entropy: 125.62245178222656, Temp: 1.7333989143371582, KL: 73.15414428710938, Loss: 0.07992366701364517, Learning Rate: 0.014699999999999998\n",
      "Epoch [3245/20000], Bound: 0.5115130543708801, Entropy: 126.88414001464844, Temp: 1.7316280603408813, KL: 70.73912048339844, Loss: 0.0929921492934227, Learning Rate: 0.014699999999999998\n",
      "Epoch [3246/20000], Bound: 0.5482539534568787, Entropy: 129.75096130371094, Temp: 1.72952139377594, KL: 80.87348937988281, Loss: 0.0946032702922821, Learning Rate: 0.014699999999999998\n",
      "Epoch [3247/20000], Bound: 0.5258691310882568, Entropy: 125.06864929199219, Temp: 1.7272065877914429, KL: 81.55497741699219, Loss: 0.07343531399965286, Learning Rate: 0.014699999999999998\n",
      "Epoch [3248/20000], Bound: 0.5232041478157043, Entropy: 126.57105255126953, Temp: 1.7250126600265503, KL: 76.86457061767578, Loss: 0.08468260616064072, Learning Rate: 0.014699999999999998\n",
      "Epoch [3249/20000], Bound: 0.528489887714386, Entropy: 129.24143981933594, Temp: 1.722720980644226, KL: 76.96174621582031, Loss: 0.08873225003480911, Learning Rate: 0.014699999999999998\n",
      "Epoch [3250/20000], Bound: 0.4876384735107422, Entropy: 124.83609771728516, Temp: 1.7202889919281006, KL: 71.33985137939453, Loss: 0.07150533050298691, Learning Rate: 0.014699999999999998\n",
      "Epoch [3251/20000], Bound: 0.4804907739162445, Entropy: 129.67483520507812, Temp: 1.7179162502288818, KL: 67.57450866699219, Loss: 0.07669811695814133, Learning Rate: 0.014699999999999998\n",
      "Epoch [3252/20000], Bound: 0.5271636247634888, Entropy: 127.13811492919922, Temp: 1.7154827117919922, KL: 83.7839126586914, Loss: 0.06739869713783264, Learning Rate: 0.014699999999999998\n",
      "Epoch [3253/20000], Bound: 0.476220041513443, Entropy: 130.81687927246094, Temp: 1.7133092880249023, KL: 74.32827758789062, Loss: 0.05344972014427185, Learning Rate: 0.014699999999999998\n",
      "Epoch [3254/20000], Bound: 0.489096075296402, Entropy: 129.59432983398438, Temp: 1.7114747762680054, KL: 74.55253601074219, Loss: 0.06289879232645035, Learning Rate: 0.014699999999999998\n",
      "Epoch [3255/20000], Bound: 0.4757349193096161, Entropy: 131.01332092285156, Temp: 1.709811806678772, KL: 72.39015197753906, Loss: 0.05855733901262283, Learning Rate: 0.014699999999999998\n",
      "Epoch [3256/20000], Bound: 0.4977048933506012, Entropy: 134.75233459472656, Temp: 1.7083454132080078, KL: 72.25590515136719, Loss: 0.07638905942440033, Learning Rate: 0.014699999999999998\n",
      "Epoch [3257/20000], Bound: 0.5078924298286438, Entropy: 133.11863708496094, Temp: 1.7067980766296387, KL: 79.68974304199219, Loss: 0.06284631788730621, Learning Rate: 0.014699999999999998\n",
      "Epoch [3258/20000], Bound: 0.5345891118049622, Entropy: 133.4047088623047, Temp: 1.7054567337036133, KL: 87.89505004882812, Loss: 0.06105548143386841, Learning Rate: 0.014699999999999998\n",
      "Epoch [3259/20000], Bound: 0.5106019973754883, Entropy: 137.5431365966797, Temp: 1.7044148445129395, KL: 76.74882507324219, Loss: 0.07357022911310196, Learning Rate: 0.014699999999999998\n",
      "Epoch [3260/20000], Bound: 0.4648177921772003, Entropy: 130.8309326171875, Temp: 1.703345775604248, KL: 67.44329833984375, Loss: 0.06424448639154434, Learning Rate: 0.014699999999999998\n",
      "Epoch [3261/20000], Bound: 0.5344598889350891, Entropy: 132.8911895751953, Temp: 1.7022833824157715, KL: 81.39244079589844, Loss: 0.07983638346195221, Learning Rate: 0.014699999999999998\n",
      "Epoch [3262/20000], Bound: 0.4845161736011505, Entropy: 131.45521545410156, Temp: 1.7011618614196777, KL: 72.08123779296875, Loss: 0.065990149974823, Learning Rate: 0.014699999999999998\n",
      "Epoch [3263/20000], Bound: 0.5126153230667114, Entropy: 131.6396484375, Temp: 1.700082540512085, KL: 80.62356567382812, Loss: 0.06361830979585648, Learning Rate: 0.014699999999999998\n",
      "Epoch [3264/20000], Bound: 0.5101211667060852, Entropy: 133.43606567382812, Temp: 1.6991714239120483, KL: 75.53170776367188, Loss: 0.07648883014917374, Learning Rate: 0.014699999999999998\n",
      "Epoch [3265/20000], Bound: 0.4846805930137634, Entropy: 131.1559295654297, Temp: 1.6981724500656128, KL: 72.962646484375, Loss: 0.06338410079479218, Learning Rate: 0.014699999999999998\n",
      "Epoch [3266/20000], Bound: 0.4846589267253876, Entropy: 130.97589111328125, Temp: 1.6972554922103882, KL: 71.45355224609375, Loss: 0.06776738911867142, Learning Rate: 0.014699999999999998\n",
      "Epoch [3267/20000], Bound: 0.5276734828948975, Entropy: 132.26373291015625, Temp: 1.6963329315185547, KL: 82.24415588378906, Loss: 0.07122837752103806, Learning Rate: 0.014699999999999998\n",
      "Epoch [3268/20000], Bound: 0.5314967036247253, Entropy: 128.2228546142578, Temp: 1.69547700881958, KL: 87.47409057617188, Loss: 0.058998953551054, Learning Rate: 0.014699999999999998\n",
      "Epoch [3269/20000], Bound: 0.524986207485199, Entropy: 123.00772857666016, Temp: 1.6949131488800049, KL: 84.0114517211914, Loss: 0.06366237252950668, Learning Rate: 0.014699999999999998\n",
      "Epoch [3270/20000], Bound: 0.4930780231952667, Entropy: 121.99691772460938, Temp: 1.694509506225586, KL: 73.90733337402344, Loss: 0.06714760512113571, Learning Rate: 0.014699999999999998\n",
      "Epoch [3271/20000], Bound: 0.5141152143478394, Entropy: 122.66837310791016, Temp: 1.694089651107788, KL: 80.74565887451172, Loss: 0.06415540724992752, Learning Rate: 0.014699999999999998\n",
      "Epoch [3272/20000], Bound: 0.49603164196014404, Entropy: 124.5263671875, Temp: 1.6937745809555054, KL: 75.65011596679688, Loss: 0.06435286998748779, Learning Rate: 0.014699999999999998\n",
      "Epoch [3273/20000], Bound: 0.4589866101741791, Entropy: 126.56982421875, Temp: 1.693495750427246, KL: 64.90220642089844, Loss: 0.06681262701749802, Learning Rate: 0.014699999999999998\n",
      "Epoch [3274/20000], Bound: 0.4455849528312683, Entropy: 128.88629150390625, Temp: 1.6930902004241943, KL: 67.08901977539062, Loss: 0.050099752843379974, Learning Rate: 0.014699999999999998\n",
      "Epoch [3275/20000], Bound: 0.5241473317146301, Entropy: 128.41259765625, Temp: 1.6928443908691406, KL: 83.1005859375, Loss: 0.06551714986562729, Learning Rate: 0.014699999999999998\n",
      "Epoch [3276/20000], Bound: 0.5235216617584229, Entropy: 128.43240356445312, Temp: 1.6926941871643066, KL: 88.82719421386719, Loss: 0.04806558042764664, Learning Rate: 0.014699999999999998\n",
      "Epoch [3277/20000], Bound: 0.5052788853645325, Entropy: 129.50979614257812, Temp: 1.6929426193237305, KL: 76.3896484375, Loss: 0.06966132670640945, Learning Rate: 0.014699999999999998\n",
      "Epoch [3278/20000], Bound: 0.4651753902435303, Entropy: 130.33489990234375, Temp: 1.6931036710739136, KL: 67.3623046875, Loss: 0.06432044506072998, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3279/20000], Bound: 0.4486297070980072, Entropy: 134.91311645507812, Temp: 1.693160057067871, KL: 64.93861389160156, Loss: 0.05876384675502777, Learning Rate: 0.014699999999999998\n",
      "Epoch [3280/20000], Bound: 0.5460056066513062, Entropy: 129.40696716308594, Temp: 1.693176031112671, KL: 87.60693359375, Loss: 0.07093255966901779, Learning Rate: 0.014699999999999998\n",
      "Epoch [3281/20000], Bound: 0.4862020015716553, Entropy: 132.5379638671875, Temp: 1.6932324171066284, KL: 77.02690124511719, Loss: 0.05235246568918228, Learning Rate: 0.014699999999999998\n",
      "Epoch [3282/20000], Bound: 0.4872574210166931, Entropy: 131.32566833496094, Temp: 1.6934795379638672, KL: 75.298583984375, Loss: 0.05831311270594597, Learning Rate: 0.014699999999999998\n",
      "Epoch [3283/20000], Bound: 0.45314377546310425, Entropy: 133.85366821289062, Temp: 1.6937916278839111, KL: 69.4742431640625, Loss: 0.04884318262338638, Learning Rate: 0.014699999999999998\n",
      "Epoch [3284/20000], Bound: 0.5225896835327148, Entropy: 130.52940368652344, Temp: 1.694237470626831, KL: 80.74458312988281, Loss: 0.07124458998441696, Learning Rate: 0.014699999999999998\n",
      "Epoch [3285/20000], Bound: 0.5133613348007202, Entropy: 130.7923583984375, Temp: 1.6945998668670654, KL: 80.2806396484375, Loss: 0.06493160128593445, Learning Rate: 0.014699999999999998\n",
      "Epoch [3286/20000], Bound: 0.5291128158569336, Entropy: 131.99658203125, Temp: 1.6949727535247803, KL: 80.52696228027344, Loss: 0.07743372023105621, Learning Rate: 0.014699999999999998\n",
      "Epoch [3287/20000], Bound: 0.5051312446594238, Entropy: 131.99156188964844, Temp: 1.695177435874939, KL: 73.28866577148438, Loss: 0.07880325615406036, Learning Rate: 0.014699999999999998\n",
      "Epoch [3288/20000], Bound: 0.5062431693077087, Entropy: 130.20262145996094, Temp: 1.6951279640197754, KL: 74.11273193359375, Loss: 0.07728234678506851, Learning Rate: 0.014699999999999998\n",
      "Epoch [3289/20000], Bound: 0.49634894728660583, Entropy: 127.88086700439453, Temp: 1.6948813199996948, KL: 80.2525405883789, Loss: 0.051090266555547714, Learning Rate: 0.014699999999999998\n",
      "Epoch [3290/20000], Bound: 0.47500959038734436, Entropy: 130.55569458007812, Temp: 1.6949076652526855, KL: 72.12519836425781, Loss: 0.058035027235746384, Learning Rate: 0.014699999999999998\n",
      "Epoch [3291/20000], Bound: 0.5077801942825317, Entropy: 129.55258178710938, Temp: 1.6949878931045532, KL: 82.33982849121094, Loss: 0.054269932210445404, Learning Rate: 0.014699999999999998\n",
      "Epoch [3292/20000], Bound: 0.4900272786617279, Entropy: 126.7792739868164, Temp: 1.6952840089797974, KL: 77.51847076416016, Loss: 0.05408145859837532, Learning Rate: 0.014699999999999998\n",
      "Epoch [3293/20000], Bound: 0.4821232259273529, Entropy: 127.41278839111328, Temp: 1.6957249641418457, KL: 73.91785430908203, Loss: 0.05841228365898132, Learning Rate: 0.014699999999999998\n",
      "Epoch [3294/20000], Bound: 0.4941723644733429, Entropy: 127.05673217773438, Temp: 1.6961920261383057, KL: 78.08689880371094, Loss: 0.05579349771142006, Learning Rate: 0.014699999999999998\n",
      "Epoch [3295/20000], Bound: 0.5373386740684509, Entropy: 131.34506225585938, Temp: 1.69676673412323, KL: 84.87557983398438, Loss: 0.071732297539711, Learning Rate: 0.014699999999999998\n",
      "Epoch [3296/20000], Bound: 0.4964728355407715, Entropy: 131.306884765625, Temp: 1.6972795724868774, KL: 78.49949645996094, Loss: 0.05649900436401367, Learning Rate: 0.014699999999999998\n",
      "Epoch [3297/20000], Bound: 0.47696614265441895, Entropy: 129.77816772460938, Temp: 1.6978883743286133, KL: 76.38555908203125, Loss: 0.04717715084552765, Learning Rate: 0.014699999999999998\n",
      "Epoch [3298/20000], Bound: 0.5201835036277771, Entropy: 131.8956756591797, Temp: 1.6986984014511108, KL: 81.42900085449219, Loss: 0.06746554374694824, Learning Rate: 0.014699999999999998\n",
      "Epoch [3299/20000], Bound: 0.49067872762680054, Entropy: 130.8563995361328, Temp: 1.6994439363479614, KL: 74.42460632324219, Loss: 0.06394073367118835, Learning Rate: 0.014699999999999998\n",
      "Epoch [3300/20000], Bound: 0.48832717537879944, Entropy: 131.6525115966797, Temp: 1.7001042366027832, KL: 71.53382873535156, Loss: 0.07059124112129211, Learning Rate: 0.014699999999999998\n",
      "Epoch [3301/20000], Bound: 0.49844467639923096, Entropy: 131.4590606689453, Temp: 1.7005555629730225, KL: 71.90232849121094, Loss: 0.07767893373966217, Learning Rate: 0.014699999999999998\n",
      "Epoch [3302/20000], Bound: 0.4816075265407562, Entropy: 133.67237854003906, Temp: 1.7007185220718384, KL: 69.79855346679688, Loss: 0.07036980241537094, Learning Rate: 0.014699999999999998\n",
      "Epoch [3303/20000], Bound: 0.5102880001068115, Entropy: 130.7807159423828, Temp: 1.7007039785385132, KL: 82.22433471679688, Loss: 0.05702534690499306, Learning Rate: 0.014699999999999998\n",
      "Epoch [3304/20000], Bound: 0.5798006057739258, Entropy: 133.0526580810547, Temp: 1.7008658647537231, KL: 98.44378662109375, Loss: 0.06966584175825119, Learning Rate: 0.014699999999999998\n",
      "Epoch [3305/20000], Bound: 0.5282742381095886, Entropy: 129.7543182373047, Temp: 1.7011725902557373, KL: 80.67813110351562, Loss: 0.07661911845207214, Learning Rate: 0.014699999999999998\n",
      "Epoch [3306/20000], Bound: 0.5127072334289551, Entropy: 127.7953109741211, Temp: 1.7013200521469116, KL: 80.21480560302734, Loss: 0.06496657431125641, Learning Rate: 0.014699999999999998\n",
      "Epoch [3307/20000], Bound: 0.45213279128074646, Entropy: 124.82875061035156, Temp: 1.7014882564544678, KL: 65.53538513183594, Loss: 0.060021717101335526, Learning Rate: 0.014699999999999998\n",
      "Epoch [3308/20000], Bound: 0.5110085010528564, Entropy: 124.20523834228516, Temp: 1.7015812397003174, KL: 74.62993621826172, Loss: 0.07998847961425781, Learning Rate: 0.014699999999999998\n",
      "Epoch [3309/20000], Bound: 0.4904705584049225, Entropy: 121.9169692993164, Temp: 1.7014179229736328, KL: 74.20917510986328, Loss: 0.0645061731338501, Learning Rate: 0.014699999999999998\n",
      "Epoch [3310/20000], Bound: 0.5058279633522034, Entropy: 118.80147552490234, Temp: 1.7012455463409424, KL: 82.15081024169922, Loss: 0.05360821634531021, Learning Rate: 0.014699999999999998\n",
      "Epoch [3311/20000], Bound: 0.534502387046814, Entropy: 121.40583038330078, Temp: 1.7013150453567505, KL: 79.5914535522461, Loss: 0.08511249721050262, Learning Rate: 0.014699999999999998\n",
      "Epoch [3312/20000], Bound: 0.5033098459243774, Entropy: 123.65059661865234, Temp: 1.7011138200759888, KL: 82.21932220458984, Loss: 0.05133894458413124, Learning Rate: 0.014699999999999998\n",
      "Epoch [3313/20000], Bound: 0.48981109261512756, Entropy: 123.28726196289062, Temp: 1.7011921405792236, KL: 78.80741882324219, Loss: 0.05045133829116821, Learning Rate: 0.014699999999999998\n",
      "Epoch [3314/20000], Bound: 0.4823318123817444, Entropy: 127.32542419433594, Temp: 1.7014987468719482, KL: 73.67048645019531, Loss: 0.05960029363632202, Learning Rate: 0.014699999999999998\n",
      "Epoch [3315/20000], Bound: 0.49759984016418457, Entropy: 129.90492248535156, Temp: 1.7018165588378906, KL: 80.68331909179688, Loss: 0.05125150457024574, Learning Rate: 0.014699999999999998\n",
      "Epoch [3316/20000], Bound: 0.5055676102638245, Entropy: 129.68287658691406, Temp: 1.7023463249206543, KL: 78.61820983886719, Loss: 0.06383766978979111, Learning Rate: 0.014699999999999998\n",
      "Epoch [3317/20000], Bound: 0.5511690378189087, Entropy: 131.61654663085938, Temp: 1.702856421470642, KL: 96.49359130859375, Loss: 0.04995951056480408, Learning Rate: 0.014699999999999998\n",
      "Epoch [3318/20000], Bound: 0.4320223331451416, Entropy: 132.32473754882812, Temp: 1.703740119934082, KL: 63.19970703125, Loss: 0.0518440417945385, Learning Rate: 0.014699999999999998\n",
      "Epoch [3319/20000], Bound: 0.5098063945770264, Entropy: 130.1759490966797, Temp: 1.7045718431472778, KL: 85.1051025390625, Loss: 0.048411257565021515, Learning Rate: 0.014699999999999998\n",
      "Epoch [3320/20000], Bound: 0.47576525807380676, Entropy: 134.40359497070312, Temp: 1.7056500911712646, KL: 74.8944091796875, Loss: 0.05103830248117447, Learning Rate: 0.014699999999999998\n",
      "Epoch [3321/20000], Bound: 0.46518319845199585, Entropy: 132.279296875, Temp: 1.7067997455596924, KL: 68.29063415527344, Loss: 0.06219109892845154, Learning Rate: 0.014699999999999998\n",
      "Epoch [3322/20000], Bound: 0.49520763754844666, Entropy: 134.90550231933594, Temp: 1.7077687978744507, KL: 77.14836120605469, Loss: 0.06002223864197731, Learning Rate: 0.014699999999999998\n",
      "Epoch [3323/20000], Bound: 0.4445669651031494, Entropy: 134.83798217773438, Temp: 1.7087072134017944, KL: 70.22648620605469, Loss: 0.0408625453710556, Learning Rate: 0.014699999999999998\n",
      "Epoch [3324/20000], Bound: 0.46887943148612976, Entropy: 136.96156311035156, Temp: 1.709831714630127, KL: 69.38807678222656, Loss: 0.061980731785297394, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3325/20000], Bound: 0.47581782937049866, Entropy: 136.4750213623047, Temp: 1.710789680480957, KL: 69.78715515136719, Loss: 0.06627708673477173, Learning Rate: 0.014699999999999998\n",
      "Epoch [3326/20000], Bound: 0.5077303647994995, Entropy: 134.5182647705078, Temp: 1.711536169052124, KL: 78.97868347167969, Loss: 0.06505244970321655, Learning Rate: 0.014699999999999998\n",
      "Epoch [3327/20000], Bound: 0.5050882697105408, Entropy: 131.00619506835938, Temp: 1.7122137546539307, KL: 80.76046752929688, Loss: 0.057723864912986755, Learning Rate: 0.014699999999999998\n",
      "Epoch [3328/20000], Bound: 0.5279637575149536, Entropy: 134.09982299804688, Temp: 1.7129572629928589, KL: 89.276123046875, Loss: 0.05189470946788788, Learning Rate: 0.014699999999999998\n",
      "Epoch [3329/20000], Bound: 0.49442052841186523, Entropy: 130.32920837402344, Temp: 1.7139360904693604, KL: 76.25593566894531, Loss: 0.062318067997694016, Learning Rate: 0.014699999999999998\n",
      "Epoch [3330/20000], Bound: 0.47018393874168396, Entropy: 127.04312133789062, Temp: 1.714829921722412, KL: 71.89859008789062, Loss: 0.0558948889374733, Learning Rate: 0.014699999999999998\n",
      "Epoch [3331/20000], Bound: 0.47823360562324524, Entropy: 124.90348052978516, Temp: 1.7156943082809448, KL: 74.62700653076172, Loss: 0.05428203567862511, Learning Rate: 0.014699999999999998\n",
      "Epoch [3332/20000], Bound: 0.5128739476203918, Entropy: 123.9540786743164, Temp: 1.7165862321853638, KL: 87.10454559326172, Loss: 0.0458812415599823, Learning Rate: 0.014699999999999998\n",
      "Epoch [3333/20000], Bound: 0.5103159546852112, Entropy: 128.69097900390625, Temp: 1.7177619934082031, KL: 84.93359375, Loss: 0.050171833485364914, Learning Rate: 0.014699999999999998\n",
      "Epoch [3334/20000], Bound: 0.5053874850273132, Entropy: 127.80018615722656, Temp: 1.7191046476364136, KL: 81.40199279785156, Loss: 0.05649228021502495, Learning Rate: 0.014699999999999998\n",
      "Epoch [3335/20000], Bound: 0.48824530839920044, Entropy: 129.94903564453125, Temp: 1.7204630374908447, KL: 74.17544555664062, Loss: 0.06375521421432495, Learning Rate: 0.014699999999999998\n",
      "Epoch [3336/20000], Bound: 0.519322395324707, Entropy: 128.7549285888672, Temp: 1.721644401550293, KL: 86.54884338378906, Loss: 0.05316533148288727, Learning Rate: 0.014699999999999998\n",
      "Epoch [3337/20000], Bound: 0.5135653018951416, Entropy: 126.26294708251953, Temp: 1.7229582071304321, KL: 80.3516616821289, Loss: 0.06646494567394257, Learning Rate: 0.014699999999999998\n",
      "Epoch [3338/20000], Bound: 0.5189001560211182, Entropy: 127.26406860351562, Temp: 1.7241238355636597, KL: 80.09471130371094, Loss: 0.07168768346309662, Learning Rate: 0.014699999999999998\n",
      "Epoch [3339/20000], Bound: 0.5081822872161865, Entropy: 127.35338592529297, Temp: 1.725073218345642, KL: 80.00731658935547, Loss: 0.06315527111291885, Learning Rate: 0.014699999999999998\n",
      "Epoch [3340/20000], Bound: 0.5107300281524658, Entropy: 123.82472229003906, Temp: 1.7259531021118164, KL: 74.00668334960938, Loss: 0.08267351239919662, Learning Rate: 0.014699999999999998\n",
      "Epoch [3341/20000], Bound: 0.5072305798530579, Entropy: 126.4033432006836, Temp: 1.726409673690796, KL: 82.2603988647461, Loss: 0.05592348426580429, Learning Rate: 0.014699999999999998\n",
      "Epoch [3342/20000], Bound: 0.5598329901695251, Entropy: 125.11052703857422, Temp: 1.7269774675369263, KL: 91.59465789794922, Loss: 0.07356595247983932, Learning Rate: 0.014699999999999998\n",
      "Epoch [3343/20000], Bound: 0.5265055298805237, Entropy: 122.24972534179688, Temp: 1.7274802923202515, KL: 78.80381774902344, Loss: 0.08194658160209656, Learning Rate: 0.014699999999999998\n",
      "Epoch [3344/20000], Bound: 0.4739374816417694, Entropy: 120.9290771484375, Temp: 1.7276604175567627, KL: 74.45486450195312, Loss: 0.05201900377869606, Learning Rate: 0.014699999999999998\n",
      "Epoch [3345/20000], Bound: 0.5082904696464539, Entropy: 123.09825897216797, Temp: 1.7279530763626099, KL: 74.4369888305664, Loss: 0.07951559871435165, Learning Rate: 0.014699999999999998\n",
      "Epoch [3346/20000], Bound: 0.5066514611244202, Entropy: 123.88592529296875, Temp: 1.7279291152954102, KL: 79.90643310546875, Loss: 0.062349241226911545, Learning Rate: 0.014699999999999998\n",
      "Epoch [3347/20000], Bound: 0.49689579010009766, Entropy: 126.21405792236328, Temp: 1.7279393672943115, KL: 74.71199798583984, Loss: 0.06947579234838486, Learning Rate: 0.014699999999999998\n",
      "Epoch [3348/20000], Bound: 0.5286935567855835, Entropy: 126.24146270751953, Temp: 1.7278146743774414, KL: 86.54532623291016, Loss: 0.0613994337618351, Learning Rate: 0.014699999999999998\n",
      "Epoch [3349/20000], Bound: 0.48709118366241455, Entropy: 126.99913787841797, Temp: 1.7278200387954712, KL: 78.86981964111328, Loss: 0.04960109665989876, Learning Rate: 0.014699999999999998\n",
      "Epoch [3350/20000], Bound: 0.5096260905265808, Entropy: 124.52996826171875, Temp: 1.7280406951904297, KL: 80.71176147460938, Loss: 0.06245681643486023, Learning Rate: 0.014699999999999998\n",
      "Epoch [3351/20000], Bound: 0.5189369320869446, Entropy: 128.44593811035156, Temp: 1.728278398513794, KL: 80.10269165039062, Loss: 0.07190989702939987, Learning Rate: 0.014699999999999998\n",
      "Epoch [3352/20000], Bound: 0.4915156066417694, Entropy: 126.83594512939453, Temp: 1.728381633758545, KL: 81.63736724853516, Loss: 0.045148756355047226, Learning Rate: 0.014699999999999998\n",
      "Epoch [3353/20000], Bound: 0.46690481901168823, Entropy: 129.10189819335938, Temp: 1.7287883758544922, KL: 73.59794616699219, Loss: 0.04909580573439598, Learning Rate: 0.014699999999999998\n",
      "Epoch [3354/20000], Bound: 0.49185889959335327, Entropy: 131.23399353027344, Temp: 1.7293199300765991, KL: 77.40234375, Loss: 0.057722754776477814, Learning Rate: 0.014699999999999998\n",
      "Epoch [3355/20000], Bound: 0.47576743364334106, Entropy: 131.7161407470703, Temp: 1.7298717498779297, KL: 71.44096374511719, Loss: 0.06226835027337074, Learning Rate: 0.014699999999999998\n",
      "Epoch [3356/20000], Bound: 0.4864378571510315, Entropy: 129.797119140625, Temp: 1.73030424118042, KL: 77.61070251464844, Loss: 0.052857186645269394, Learning Rate: 0.014699999999999998\n",
      "Epoch [3357/20000], Bound: 0.49488645792007446, Entropy: 134.22640991210938, Temp: 1.7308428287506104, KL: 80.39894104003906, Loss: 0.05156736075878143, Learning Rate: 0.014699999999999998\n",
      "Epoch [3358/20000], Bound: 0.4803251624107361, Entropy: 134.98806762695312, Temp: 1.7315261363983154, KL: 74.05766296386719, Loss: 0.05835647135972977, Learning Rate: 0.014699999999999998\n",
      "Epoch [3359/20000], Bound: 0.4920179545879364, Entropy: 135.14727783203125, Temp: 1.7321646213531494, KL: 76.76771545410156, Loss: 0.05982724204659462, Learning Rate: 0.014699999999999998\n",
      "Epoch [3360/20000], Bound: 0.5131013989448547, Entropy: 138.09161376953125, Temp: 1.7327693700790405, KL: 85.80300903320312, Loss: 0.05087106674909592, Learning Rate: 0.014699999999999998\n",
      "Epoch [3361/20000], Bound: 0.5008814334869385, Entropy: 136.0143585205078, Temp: 1.7335776090621948, KL: 81.66450500488281, Loss: 0.052894227206707, Learning Rate: 0.014699999999999998\n",
      "Epoch [3362/20000], Bound: 0.4451834261417389, Entropy: 130.20013427734375, Temp: 1.7344931364059448, KL: 65.84175109863281, Loss: 0.055217865854501724, Learning Rate: 0.014699999999999998\n",
      "Epoch [3363/20000], Bound: 0.5257869362831116, Entropy: 129.1178436279297, Temp: 1.7352924346923828, KL: 84.8409423828125, Loss: 0.06432103365659714, Learning Rate: 0.014699999999999998\n",
      "Epoch [3364/20000], Bound: 0.49995648860931396, Entropy: 129.44456481933594, Temp: 1.7360565662384033, KL: 82.28311157226562, Loss: 0.050505101680755615, Learning Rate: 0.014699999999999998\n",
      "Epoch [3365/20000], Bound: 0.4396061897277832, Entropy: 130.07347106933594, Temp: 1.7369729280471802, KL: 69.2923583984375, Loss: 0.041219983249902725, Learning Rate: 0.014699999999999998\n",
      "Epoch [3366/20000], Bound: 0.4668082594871521, Entropy: 128.97982788085938, Temp: 1.738031029701233, KL: 70.67134094238281, Loss: 0.05789223685860634, Learning Rate: 0.014699999999999998\n",
      "Epoch [3367/20000], Bound: 0.486745148897171, Entropy: 128.82952880859375, Temp: 1.7389672994613647, KL: 75.64175415039062, Loss: 0.059211790561676025, Learning Rate: 0.014699999999999998\n",
      "Epoch [3368/20000], Bound: 0.43807992339134216, Entropy: 132.29449462890625, Temp: 1.7398277521133423, KL: 66.65081787109375, Loss: 0.04780936986207962, Learning Rate: 0.014699999999999998\n",
      "Epoch [3369/20000], Bound: 0.4802553057670593, Entropy: 130.898193359375, Temp: 1.7406972646713257, KL: 78.10598754882812, Loss: 0.047103628516197205, Learning Rate: 0.014699999999999998\n",
      "Epoch [3370/20000], Bound: 0.4624662399291992, Entropy: 131.9556427001953, Temp: 1.7417112588882446, KL: 65.44091796875, Loss: 0.0697329193353653, Learning Rate: 0.014699999999999998\n",
      "Epoch [3371/20000], Bound: 0.5014235973358154, Entropy: 135.201416015625, Temp: 1.742356777191162, KL: 79.01873779296875, Loss: 0.06141367182135582, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3372/20000], Bound: 0.5061298608779907, Entropy: 134.99658203125, Temp: 1.7429535388946533, KL: 80.61286926269531, Loss: 0.060682762414216995, Learning Rate: 0.014699999999999998\n",
      "Epoch [3373/20000], Bound: 0.4878450632095337, Entropy: 134.8498077392578, Temp: 1.7435342073440552, KL: 75.49906921386719, Loss: 0.06070975959300995, Learning Rate: 0.014699999999999998\n",
      "Epoch [3374/20000], Bound: 0.48653891682624817, Entropy: 136.9295654296875, Temp: 1.744042992591858, KL: 77.32777404785156, Loss: 0.054457519203424454, Learning Rate: 0.014699999999999998\n",
      "Epoch [3375/20000], Bound: 0.4571039080619812, Entropy: 136.708251953125, Temp: 1.7446038722991943, KL: 72.54951477050781, Loss: 0.04537328705191612, Learning Rate: 0.014699999999999998\n",
      "Epoch [3376/20000], Bound: 0.46652740240097046, Entropy: 138.67535400390625, Temp: 1.745302677154541, KL: 69.42111206054688, Loss: 0.06157088279724121, Learning Rate: 0.014699999999999998\n",
      "Epoch [3377/20000], Bound: 0.4811536371707916, Entropy: 133.26380920410156, Temp: 1.7458330392837524, KL: 75.05622863769531, Loss: 0.05681300163269043, Learning Rate: 0.014699999999999998\n",
      "Epoch [3378/20000], Bound: 0.4697800278663635, Entropy: 130.57969665527344, Temp: 1.7463492155075073, KL: 73.35372924804688, Loss: 0.05286029726266861, Learning Rate: 0.014699999999999998\n",
      "Epoch [3379/20000], Bound: 0.44629958271980286, Entropy: 128.82150268554688, Temp: 1.746895432472229, KL: 66.904541015625, Loss: 0.05349460616707802, Learning Rate: 0.014699999999999998\n",
      "Epoch [3380/20000], Bound: 0.4939199388027191, Entropy: 129.13465881347656, Temp: 1.7473851442337036, KL: 83.62751770019531, Loss: 0.04245917871594429, Learning Rate: 0.014699999999999998\n",
      "Epoch [3381/20000], Bound: 0.4592575132846832, Entropy: 135.06690979003906, Temp: 1.748180627822876, KL: 68.14462280273438, Loss: 0.05977784842252731, Learning Rate: 0.014699999999999998\n",
      "Epoch [3382/20000], Bound: 0.4691944122314453, Entropy: 135.37954711914062, Temp: 1.7488075494766235, KL: 73.77565002441406, Loss: 0.05131572484970093, Learning Rate: 0.014699999999999998\n",
      "Epoch [3383/20000], Bound: 0.46854692697525024, Entropy: 134.9406280517578, Temp: 1.7494794130325317, KL: 76.49136352539062, Loss: 0.043086640536785126, Learning Rate: 0.014699999999999998\n",
      "Epoch [3384/20000], Bound: 0.4750964343547821, Entropy: 136.5413360595703, Temp: 1.7503516674041748, KL: 75.59481811523438, Loss: 0.050759878009557724, Learning Rate: 0.014699999999999998\n",
      "Epoch [3385/20000], Bound: 0.4378896951675415, Entropy: 135.37644958496094, Temp: 1.7512716054916382, KL: 67.13641357421875, Loss: 0.04675256088376045, Learning Rate: 0.014699999999999998\n",
      "Epoch [3386/20000], Bound: 0.4941719174385071, Entropy: 135.59829711914062, Temp: 1.7522034645080566, KL: 78.9874267578125, Loss: 0.05618287995457649, Learning Rate: 0.014699999999999998\n",
      "Epoch [3387/20000], Bound: 0.45282843708992004, Entropy: 136.62342834472656, Temp: 1.7531254291534424, KL: 70.32073974609375, Loss: 0.0488983578979969, Learning Rate: 0.014699999999999998\n",
      "Epoch [3388/20000], Bound: 0.4270594120025635, Entropy: 133.17686462402344, Temp: 1.7540581226348877, KL: 65.41848754882812, Loss: 0.04381575435400009, Learning Rate: 0.014699999999999998\n",
      "Epoch [3389/20000], Bound: 0.45413917303085327, Entropy: 137.89637756347656, Temp: 1.7550268173217773, KL: 73.59988403320312, Loss: 0.040627434849739075, Learning Rate: 0.014699999999999998\n",
      "Epoch [3390/20000], Bound: 0.4720993638038635, Entropy: 136.38282775878906, Temp: 1.7561688423156738, KL: 73.30133056640625, Loss: 0.05524905025959015, Learning Rate: 0.014699999999999998\n",
      "Epoch [3391/20000], Bound: 0.4517897665500641, Entropy: 139.96665954589844, Temp: 1.7572270631790161, KL: 70.18856811523438, Loss: 0.04867078736424446, Learning Rate: 0.014699999999999998\n",
      "Epoch [3392/20000], Bound: 0.45022934675216675, Entropy: 138.521728515625, Temp: 1.7582796812057495, KL: 70.66281127929688, Loss: 0.04619552567601204, Learning Rate: 0.014699999999999998\n",
      "Epoch [3393/20000], Bound: 0.45344579219818115, Entropy: 138.3120574951172, Temp: 1.759371042251587, KL: 69.80580139160156, Loss: 0.05109691992402077, Learning Rate: 0.014699999999999998\n",
      "Epoch [3394/20000], Bound: 0.47188490629196167, Entropy: 137.77862548828125, Temp: 1.7604072093963623, KL: 71.79762268066406, Loss: 0.059542879462242126, Learning Rate: 0.014699999999999998\n",
      "Epoch [3395/20000], Bound: 0.5152959227561951, Entropy: 138.41595458984375, Temp: 1.7612789869308472, KL: 84.90011596679688, Loss: 0.05691986531019211, Learning Rate: 0.014699999999999998\n",
      "Epoch [3396/20000], Bound: 0.4873203635215759, Entropy: 140.75546264648438, Temp: 1.7621856927871704, KL: 71.87620544433594, Loss: 0.07143241912126541, Learning Rate: 0.014699999999999998\n",
      "Epoch [3397/20000], Bound: 0.4418033957481384, Entropy: 137.62448120117188, Temp: 1.7627511024475098, KL: 67.18955993652344, Loss: 0.04996271803975105, Learning Rate: 0.014699999999999998\n",
      "Epoch [3398/20000], Bound: 0.4943929612636566, Entropy: 140.2645263671875, Temp: 1.7632982730865479, KL: 83.43553161621094, Loss: 0.04429937154054642, Learning Rate: 0.014699999999999998\n",
      "Epoch [3399/20000], Bound: 0.5134448409080505, Entropy: 133.0890655517578, Temp: 1.7640949487686157, KL: 87.46342468261719, Loss: 0.04829459264874458, Learning Rate: 0.014699999999999998\n",
      "Epoch [3400/20000], Bound: 0.49685752391815186, Entropy: 133.15847778320312, Temp: 1.7650928497314453, KL: 82.32269287109375, Loss: 0.04951666668057442, Learning Rate: 0.014699999999999998\n",
      "Epoch [3401/20000], Bound: 0.4813624620437622, Entropy: 127.88907623291016, Temp: 1.7661980390548706, KL: 79.04976654052734, Loss: 0.046607606112957, Learning Rate: 0.014699999999999998\n",
      "Epoch [3402/20000], Bound: 0.47926080226898193, Entropy: 130.64852905273438, Temp: 1.7674111127853394, KL: 78.39857482910156, Loss: 0.0468730628490448, Learning Rate: 0.014699999999999998\n",
      "Epoch [3403/20000], Bound: 0.43460842967033386, Entropy: 127.32941436767578, Temp: 1.7687082290649414, KL: 66.41284942626953, Loss: 0.04709097743034363, Learning Rate: 0.014699999999999998\n",
      "Epoch [3404/20000], Bound: 0.5005280375480652, Entropy: 127.7802963256836, Temp: 1.7699453830718994, KL: 82.24669647216797, Loss: 0.0529259592294693, Learning Rate: 0.014699999999999998\n",
      "Epoch [3405/20000], Bound: 0.4914364814758301, Entropy: 122.42826843261719, Temp: 1.7712044715881348, KL: 78.93678283691406, Loss: 0.0550960898399353, Learning Rate: 0.014699999999999998\n",
      "Epoch [3406/20000], Bound: 0.47579771280288696, Entropy: 126.0614242553711, Temp: 1.7724119424819946, KL: 76.33263397216797, Loss: 0.050264306366443634, Learning Rate: 0.014699999999999998\n",
      "Epoch [3407/20000], Bound: 0.4464222192764282, Entropy: 125.59495544433594, Temp: 1.773620843887329, KL: 69.54000854492188, Loss: 0.047184817492961884, Learning Rate: 0.014699999999999998\n",
      "Epoch [3408/20000], Bound: 0.4657925069332123, Entropy: 127.83773803710938, Temp: 1.7748057842254639, KL: 70.73159790039062, Loss: 0.05846831575036049, Learning Rate: 0.014699999999999998\n",
      "Epoch [3409/20000], Bound: 0.4750819504261017, Entropy: 131.0571746826172, Temp: 1.7757970094680786, KL: 78.95407104492188, Loss: 0.04248840734362602, Learning Rate: 0.014699999999999998\n",
      "Epoch [3410/20000], Bound: 0.4569146931171417, Entropy: 130.75961303710938, Temp: 1.776961088180542, KL: 67.72071838378906, Loss: 0.06029420718550682, Learning Rate: 0.014699999999999998\n",
      "Epoch [3411/20000], Bound: 0.44530901312828064, Entropy: 133.87278747558594, Temp: 1.7778668403625488, KL: 67.84544372558594, Loss: 0.05129825323820114, Learning Rate: 0.014699999999999998\n",
      "Epoch [3412/20000], Bound: 0.47784900665283203, Entropy: 136.82505798339844, Temp: 1.778686761856079, KL: 76.11306762695312, Loss: 0.05276203900575638, Learning Rate: 0.014699999999999998\n",
      "Epoch [3413/20000], Bound: 0.47062671184539795, Entropy: 138.20472717285156, Temp: 1.7794957160949707, KL: 72.32235717773438, Loss: 0.0578804612159729, Learning Rate: 0.014699999999999998\n",
      "Epoch [3414/20000], Bound: 0.46046629548072815, Entropy: 138.5331268310547, Temp: 1.780168890953064, KL: 72.06282043457031, Loss: 0.05089511722326279, Learning Rate: 0.014699999999999998\n",
      "Epoch [3415/20000], Bound: 0.4664888083934784, Entropy: 139.27066040039062, Temp: 1.7808302640914917, KL: 71.25656127929688, Loss: 0.05776192620396614, Learning Rate: 0.014699999999999998\n",
      "Epoch [3416/20000], Bound: 0.48966705799102783, Entropy: 136.88687133789062, Temp: 1.7813584804534912, KL: 80.67433166503906, Loss: 0.04931113123893738, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3417/20000], Bound: 0.4773263931274414, Entropy: 140.13909912109375, Temp: 1.7820059061050415, KL: 75.001708984375, Loss: 0.055626340210437775, Learning Rate: 0.014699999999999998\n",
      "Epoch [3418/20000], Bound: 0.47518664598464966, Entropy: 136.60989379882812, Temp: 1.7825963497161865, KL: 73.08432006835938, Loss: 0.059376657009124756, Learning Rate: 0.014699999999999998\n",
      "Epoch [3419/20000], Bound: 0.4642135798931122, Entropy: 136.8542022705078, Temp: 1.7830525636672974, KL: 74.11177062988281, Loss: 0.048111360520124435, Learning Rate: 0.014699999999999998\n",
      "Epoch [3420/20000], Bound: 0.48776692152023315, Entropy: 132.6352081298828, Temp: 1.7835826873779297, KL: 73.79740905761719, Loss: 0.06720774620771408, Learning Rate: 0.014699999999999998\n",
      "Epoch [3421/20000], Bound: 0.46842071413993835, Entropy: 132.14256286621094, Temp: 1.7838647365570068, KL: 68.2769775390625, Loss: 0.06770788878202438, Learning Rate: 0.014699999999999998\n",
      "Epoch [3422/20000], Bound: 0.49091607332229614, Entropy: 126.81513214111328, Temp: 1.7838517427444458, KL: 79.43584442138672, Loss: 0.053891703486442566, Learning Rate: 0.014699999999999998\n",
      "Epoch [3423/20000], Bound: 0.48985645174980164, Entropy: 126.1926040649414, Temp: 1.7839211225509644, KL: 74.42249298095703, Loss: 0.06711168587207794, Learning Rate: 0.014699999999999998\n",
      "Epoch [3424/20000], Bound: 0.46700918674468994, Entropy: 124.17362976074219, Temp: 1.783795952796936, KL: 72.40254211425781, Loss: 0.05506370589137077, Learning Rate: 0.014699999999999998\n",
      "Epoch [3425/20000], Bound: 0.4974833130836487, Entropy: 125.41361236572266, Temp: 1.7836689949035645, KL: 80.22942352294922, Loss: 0.05686063691973686, Learning Rate: 0.014699999999999998\n",
      "Epoch [3426/20000], Bound: 0.5016849637031555, Entropy: 123.61924743652344, Temp: 1.783596158027649, KL: 79.406982421875, Loss: 0.06251660734415054, Learning Rate: 0.014699999999999998\n",
      "Epoch [3427/20000], Bound: 0.4747292995452881, Entropy: 127.58878326416016, Temp: 1.7834722995758057, KL: 78.98316192626953, Loss: 0.04252214729785919, Learning Rate: 0.014699999999999998\n",
      "Epoch [3428/20000], Bound: 0.5077731609344482, Entropy: 126.52703857421875, Temp: 1.7836244106292725, KL: 77.12419128417969, Loss: 0.07381284236907959, Learning Rate: 0.014699999999999998\n",
      "Epoch [3429/20000], Bound: 0.4851146340370178, Entropy: 129.0338897705078, Temp: 1.7834970951080322, KL: 75.22895812988281, Loss: 0.0611126683652401, Learning Rate: 0.014699999999999998\n",
      "Epoch [3430/20000], Bound: 0.5280569791793823, Entropy: 126.71505737304688, Temp: 1.7833008766174316, KL: 89.75038146972656, Loss: 0.05502547323703766, Learning Rate: 0.014699999999999998\n",
      "Epoch [3431/20000], Bound: 0.47443822026252747, Entropy: 130.55812072753906, Temp: 1.7832956314086914, KL: 74.61872863769531, Loss: 0.05452592670917511, Learning Rate: 0.014699999999999998\n",
      "Epoch [3432/20000], Bound: 0.5154040455818176, Entropy: 129.22569274902344, Temp: 1.7833104133605957, KL: 81.72430419921875, Loss: 0.06709970533847809, Learning Rate: 0.014699999999999998\n",
      "Epoch [3433/20000], Bound: 0.49484655261039734, Entropy: 132.29916381835938, Temp: 1.7832173109054565, KL: 77.59519958496094, Loss: 0.06213034316897392, Learning Rate: 0.014699999999999998\n",
      "Epoch [3434/20000], Bound: 0.4907965660095215, Entropy: 127.7095947265625, Temp: 1.7830618619918823, KL: 76.59999084472656, Loss: 0.06171176955103874, Learning Rate: 0.014699999999999998\n",
      "Epoch [3435/20000], Bound: 0.4247787296772003, Entropy: 130.22921752929688, Temp: 1.7828463315963745, KL: 63.810089111328125, Loss: 0.0477927140891552, Learning Rate: 0.014699999999999998\n",
      "Epoch [3436/20000], Bound: 0.5327284932136536, Entropy: 129.4271240234375, Temp: 1.7826632261276245, KL: 88.80278015136719, Loss: 0.06154705211520195, Learning Rate: 0.014699999999999998\n",
      "Epoch [3437/20000], Bound: 0.5195461511611938, Entropy: 127.33667755126953, Temp: 1.7825567722320557, KL: 84.23734283447266, Loss: 0.06340848654508591, Learning Rate: 0.014699999999999998\n",
      "Epoch [3438/20000], Bound: 0.48444509506225586, Entropy: 126.40786743164062, Temp: 1.7824418544769287, KL: 75.85623168945312, Loss: 0.05878482758998871, Learning Rate: 0.014699999999999998\n",
      "Epoch [3439/20000], Bound: 0.4963415563106537, Entropy: 130.15853881835938, Temp: 1.782302737236023, KL: 77.52400207519531, Loss: 0.06347633898258209, Learning Rate: 0.014699999999999998\n",
      "Epoch [3440/20000], Bound: 0.45369043946266174, Entropy: 129.579833984375, Temp: 1.7820844650268555, KL: 67.14274597167969, Loss: 0.05967792496085167, Learning Rate: 0.014699999999999998\n",
      "Epoch [3441/20000], Bound: 0.5067798495292664, Entropy: 127.71346282958984, Temp: 1.7817399501800537, KL: 80.2622299194336, Loss: 0.06412641704082489, Learning Rate: 0.014699999999999998\n",
      "Epoch [3442/20000], Bound: 0.5267989635467529, Entropy: 127.50770568847656, Temp: 1.781356930732727, KL: 87.2110595703125, Loss: 0.060994021594524384, Learning Rate: 0.014699999999999998\n",
      "Epoch [3443/20000], Bound: 0.498416930437088, Entropy: 126.22389221191406, Temp: 1.7810649871826172, KL: 79.16374206542969, Loss: 0.06047063693404198, Learning Rate: 0.014699999999999998\n",
      "Epoch [3444/20000], Bound: 0.49424514174461365, Entropy: 124.96552276611328, Temp: 1.7807774543762207, KL: 73.64888763427734, Loss: 0.07262492924928665, Learning Rate: 0.014699999999999998\n",
      "Epoch [3445/20000], Bound: 0.47487837076187134, Entropy: 128.9454345703125, Temp: 1.7802361249923706, KL: 75.31948852539062, Loss: 0.05276428163051605, Learning Rate: 0.014699999999999998\n",
      "Epoch [3446/20000], Bound: 0.5019458532333374, Entropy: 126.82341766357422, Temp: 1.7798092365264893, KL: 76.4888687133789, Loss: 0.07074820250272751, Learning Rate: 0.014699999999999998\n",
      "Epoch [3447/20000], Bound: 0.5306316018104553, Entropy: 126.31626892089844, Temp: 1.7792062759399414, KL: 87.80978393554688, Loss: 0.062388647347688675, Learning Rate: 0.014699999999999998\n",
      "Epoch [3448/20000], Bound: 0.5025299191474915, Entropy: 127.7538070678711, Temp: 1.7787035703659058, KL: 81.93824005126953, Loss: 0.05585186928510666, Learning Rate: 0.014699999999999998\n",
      "Epoch [3449/20000], Bound: 0.513989269733429, Entropy: 127.85871887207031, Temp: 1.7783352136611938, KL: 84.73573303222656, Loss: 0.05723980814218521, Learning Rate: 0.014699999999999998\n",
      "Epoch [3450/20000], Bound: 0.4778284430503845, Entropy: 126.45539093017578, Temp: 1.7780954837799072, KL: 75.27542877197266, Loss: 0.05507457256317139, Learning Rate: 0.014699999999999998\n",
      "Epoch [3451/20000], Bound: 0.5424314737319946, Entropy: 124.4763412475586, Temp: 1.7779046297073364, KL: 93.22537994384766, Loss: 0.057040002197027206, Learning Rate: 0.014699999999999998\n",
      "Epoch [3452/20000], Bound: 0.5170864462852478, Entropy: 127.0995101928711, Temp: 1.7779172658920288, KL: 81.57494354248047, Loss: 0.0686393678188324, Learning Rate: 0.014699999999999998\n",
      "Epoch [3453/20000], Bound: 0.4590895473957062, Entropy: 124.00859069824219, Temp: 1.7778033018112183, KL: 71.54074096679688, Loss: 0.05122222751379013, Learning Rate: 0.014699999999999998\n",
      "Epoch [3454/20000], Bound: 0.4775475263595581, Entropy: 127.74410247802734, Temp: 1.7777481079101562, KL: 75.78321075439453, Loss: 0.05341323837637901, Learning Rate: 0.014699999999999998\n",
      "Epoch [3455/20000], Bound: 0.5057028532028198, Entropy: 125.5017318725586, Temp: 1.7777572870254517, KL: 88.1552505493164, Loss: 0.040870752185583115, Learning Rate: 0.014699999999999998\n",
      "Epoch [3456/20000], Bound: 0.5072553157806396, Entropy: 127.36165618896484, Temp: 1.7781624794006348, KL: 82.93286895751953, Loss: 0.05683283507823944, Learning Rate: 0.014699999999999998\n",
      "Epoch [3457/20000], Bound: 0.5129156112670898, Entropy: 126.20997619628906, Temp: 1.7786073684692383, KL: 84.75724792480469, Loss: 0.056318558752536774, Learning Rate: 0.014699999999999998\n",
      "Epoch [3458/20000], Bound: 0.5216711163520813, Entropy: 126.28024291992188, Temp: 1.7791152000427246, KL: 82.30056762695312, Loss: 0.07042618840932846, Learning Rate: 0.014699999999999998\n",
      "Epoch [3459/20000], Bound: 0.46578365564346313, Entropy: 128.05113220214844, Temp: 1.7794240713119507, KL: 71.51423645019531, Loss: 0.05644422397017479, Learning Rate: 0.014699999999999998\n",
      "Epoch [3460/20000], Bound: 0.47499221563339233, Entropy: 129.23768615722656, Temp: 1.7796604633331299, KL: 74.02291870117188, Loss: 0.05646910518407822, Learning Rate: 0.014699999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3461/20000], Bound: 0.4814692437648773, Entropy: 131.3662567138672, Temp: 1.7798593044281006, KL: 75.25013732910156, Loss: 0.058052029460668564, Learning Rate: 0.014699999999999998\n",
      "Epoch [3462/20000], Bound: 0.47597262263298035, Entropy: 132.7396697998047, Temp: 1.7800114154815674, KL: 70.73747253417969, Loss: 0.06647010147571564, Learning Rate: 0.014699999999999998\n",
      "Epoch [3463/20000], Bound: 0.5203752517700195, Entropy: 129.48081970214844, Temp: 1.7799313068389893, KL: 86.21316528320312, Loss: 0.058406513184309006, Learning Rate: 0.014699999999999998\n",
      "Epoch [3464/20000], Bound: 0.4948132038116455, Entropy: 129.691162109375, Temp: 1.7799460887908936, KL: 75.53562927246094, Loss: 0.06774334609508514, Learning Rate: 0.014699999999999998\n",
      "Epoch [3465/20000], Bound: 0.5442027449607849, Entropy: 124.8428955078125, Temp: 1.7797764539718628, KL: 87.32827758789062, Loss: 0.07523129880428314, Learning Rate: 0.014699999999999998\n",
      "Epoch [3466/20000], Bound: 0.5450422167778015, Entropy: 125.03965759277344, Temp: 1.7794524431228638, KL: 78.17131042480469, Loss: 0.10166071355342865, Learning Rate: 0.014699999999999998\n",
      "Epoch [3467/20000], Bound: 0.5170069932937622, Entropy: 125.19136810302734, Temp: 1.7784712314605713, KL: 78.97429656982422, Loss: 0.07591204345226288, Learning Rate: 0.014699999999999998\n",
      "Epoch [3468/20000], Bound: 0.5312484502792358, Entropy: 122.45769500732422, Temp: 1.7773147821426392, KL: 79.2093734741211, Loss: 0.08699539303779602, Learning Rate: 0.014699999999999998\n",
      "Epoch [3469/20000], Bound: 0.49970173835754395, Entropy: 120.91089630126953, Temp: 1.7758294343948364, KL: 80.55440521240234, Loss: 0.05733616650104523, Learning Rate: 0.014699999999999998\n",
      "Epoch [3470/20000], Bound: 0.4965151846408844, Entropy: 122.10934448242188, Temp: 1.7745420932769775, KL: 82.19035339355469, Loss: 0.05012010410428047, Learning Rate: 0.014699999999999998\n",
      "Epoch [3471/20000], Bound: 0.49449384212493896, Entropy: 120.11129760742188, Temp: 1.7735710144042969, KL: 79.09043884277344, Loss: 0.057200558483600616, Learning Rate: 0.014699999999999998\n",
      "Epoch [3472/20000], Bound: 0.5274099111557007, Entropy: 121.87651824951172, Temp: 1.772735834121704, KL: 81.36331939697266, Loss: 0.07752221822738647, Learning Rate: 0.014699999999999998\n",
      "Epoch [3473/20000], Bound: 0.48534005880355835, Entropy: 123.19589233398438, Temp: 1.7717210054397583, KL: 75.70941162109375, Loss: 0.05942661315202713, Learning Rate: 0.014699999999999998\n",
      "Epoch [3474/20000], Bound: 0.5183480381965637, Entropy: 124.63742065429688, Temp: 1.7707746028900146, KL: 88.25112915039062, Loss: 0.050481781363487244, Learning Rate: 0.014699999999999998\n",
      "Epoch [3475/20000], Bound: 0.5173375606536865, Entropy: 124.20281219482422, Temp: 1.7701747417449951, KL: 76.37816619873047, Loss: 0.08315204083919525, Learning Rate: 0.014699999999999998\n",
      "Epoch [3476/20000], Bound: 0.5031788945198059, Entropy: 126.81233215332031, Temp: 1.769228219985962, KL: 82.87155151367188, Loss: 0.05325186625123024, Learning Rate: 0.014699999999999998\n",
      "Epoch [3477/20000], Bound: 0.472830206155777, Entropy: 125.53038787841797, Temp: 1.7685279846191406, KL: 68.28577423095703, Loss: 0.0705365464091301, Learning Rate: 0.014699999999999998\n",
      "Epoch [3478/20000], Bound: 0.4658874571323395, Entropy: 131.34913635253906, Temp: 1.767600417137146, KL: 72.71664428710938, Loss: 0.05263759195804596, Learning Rate: 0.014699999999999998\n",
      "Epoch [3479/20000], Bound: 0.4838883876800537, Entropy: 129.6277618408203, Temp: 1.7668168544769287, KL: 70.83729553222656, Loss: 0.07185617089271545, Learning Rate: 0.010289999999999997\n",
      "Epoch [3480/20000], Bound: 0.5039870142936707, Entropy: 130.57470703125, Temp: 1.765825629234314, KL: 81.17524719238281, Loss: 0.0585247203707695, Learning Rate: 0.010289999999999997\n",
      "Epoch [3481/20000], Bound: 0.5246320366859436, Entropy: 131.4708709716797, Temp: 1.7649853229522705, KL: 86.198974609375, Loss: 0.061149027198553085, Learning Rate: 0.010289999999999997\n",
      "Epoch [3482/20000], Bound: 0.40542054176330566, Entropy: 132.1927490234375, Temp: 1.7642942667007446, KL: 60.33992004394531, Loss: 0.04309743642807007, Learning Rate: 0.010289999999999997\n",
      "Epoch [3483/20000], Bound: 0.47434061765670776, Entropy: 131.5642547607422, Temp: 1.7637457847595215, KL: 74.34367370605469, Loss: 0.05436177924275398, Learning Rate: 0.010289999999999997\n",
      "Epoch [3484/20000], Bound: 0.47459274530410767, Entropy: 130.87501525878906, Temp: 1.763298749923706, KL: 76.18309020996094, Loss: 0.04932093992829323, Learning Rate: 0.010289999999999997\n",
      "Epoch [3485/20000], Bound: 0.4736407995223999, Entropy: 134.91705322265625, Temp: 1.7630486488342285, KL: 69.01992797851562, Loss: 0.06888693571090698, Learning Rate: 0.010289999999999997\n",
      "Epoch [3486/20000], Bound: 0.48645931482315063, Entropy: 135.122802734375, Temp: 1.7625690698623657, KL: 80.79338073730469, Loss: 0.045473165810108185, Learning Rate: 0.010289999999999997\n",
      "Epoch [3487/20000], Bound: 0.49015823006629944, Entropy: 134.82345581054688, Temp: 1.7624058723449707, KL: 79.82423400878906, Loss: 0.051133520901203156, Learning Rate: 0.010289999999999997\n",
      "Epoch [3488/20000], Bound: 0.4906454086303711, Entropy: 131.5438995361328, Temp: 1.7624229192733765, KL: 82.15789794921875, Loss: 0.04489939659833908, Learning Rate: 0.010289999999999997\n",
      "Epoch [3489/20000], Bound: 0.47686171531677246, Entropy: 136.1275177001953, Temp: 1.7627315521240234, KL: 74.051025390625, Loss: 0.057101186364889145, Learning Rate: 0.010289999999999997\n",
      "Epoch [3490/20000], Bound: 0.49566420912742615, Entropy: 131.51852416992188, Temp: 1.763008952140808, KL: 85.63027954101562, Loss: 0.03907139599323273, Learning Rate: 0.010289999999999997\n",
      "Epoch [3491/20000], Bound: 0.51617830991745, Entropy: 135.75242614746094, Temp: 1.7636854648590088, KL: 84.94393920898438, Loss: 0.057652365416288376, Learning Rate: 0.010289999999999997\n",
      "Epoch [3492/20000], Bound: 0.47658583521842957, Entropy: 132.99978637695312, Temp: 1.7644058465957642, KL: 76.22148132324219, Loss: 0.05081039294600487, Learning Rate: 0.010289999999999997\n",
      "Epoch [3493/20000], Bound: 0.45247289538383484, Entropy: 132.90736389160156, Temp: 1.765181303024292, KL: 68.20022583007812, Loss: 0.05515558272600174, Learning Rate: 0.010289999999999997\n",
      "Epoch [3494/20000], Bound: 0.5004293918609619, Entropy: 132.89602661132812, Temp: 1.7658400535583496, KL: 84.12797546386719, Loss: 0.04730347543954849, Learning Rate: 0.010289999999999997\n",
      "Epoch [3495/20000], Bound: 0.4351964294910431, Entropy: 131.2562713623047, Temp: 1.7667033672332764, KL: 68.03797912597656, Loss: 0.04284494370222092, Learning Rate: 0.010289999999999997\n",
      "Epoch [3496/20000], Bound: 0.47597089409828186, Entropy: 130.46482849121094, Temp: 1.7676464319229126, KL: 75.69125366210938, Loss: 0.05198720842599869, Learning Rate: 0.010289999999999997\n",
      "Epoch [3497/20000], Bound: 0.4907725751399994, Entropy: 128.15658569335938, Temp: 1.7685924768447876, KL: 84.02911376953125, Loss: 0.040046803653240204, Learning Rate: 0.010289999999999997\n",
      "Epoch [3498/20000], Bound: 0.4964248239994049, Entropy: 130.1951141357422, Temp: 1.7698310613632202, KL: 84.38577270507812, Loss: 0.04359698295593262, Learning Rate: 0.010289999999999997\n",
      "Epoch [3499/20000], Bound: 0.49557241797447205, Entropy: 129.02748107910156, Temp: 1.771275520324707, KL: 82.10243225097656, Loss: 0.04944537952542305, Learning Rate: 0.010289999999999997\n",
      "Epoch [3500/20000], Bound: 0.4723297655582428, Entropy: 128.01072692871094, Temp: 1.7727816104888916, KL: 77.03614807128906, Loss: 0.045619718730449677, Learning Rate: 0.010289999999999997\n",
      "Epoch [3501/20000], Bound: 0.4752548933029175, Entropy: 125.3234634399414, Temp: 1.7743499279022217, KL: 76.31925201416016, Loss: 0.04997308552265167, Learning Rate: 0.010289999999999997\n",
      "Epoch [3502/20000], Bound: 0.4839326739311218, Entropy: 126.052001953125, Temp: 1.775890827178955, KL: 79.71327209472656, Loss: 0.04723628982901573, Learning Rate: 0.010289999999999997\n",
      "Epoch [3503/20000], Bound: 0.48468589782714844, Entropy: 125.74803161621094, Temp: 1.7774884700775146, KL: 82.55458068847656, Loss: 0.03991365805268288, Learning Rate: 0.010289999999999997\n",
      "Epoch [3504/20000], Bound: 0.46440383791923523, Entropy: 123.74308013916016, Temp: 1.7792890071868896, KL: 77.71514129638672, Loss: 0.037962574511766434, Learning Rate: 0.010289999999999997\n",
      "Epoch [3505/20000], Bound: 0.44681891798973083, Entropy: 125.10407257080078, Temp: 1.7812519073486328, KL: 70.84183502197266, Loss: 0.0441371351480484, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3506/20000], Bound: 0.46711307764053345, Entropy: 127.37117004394531, Temp: 1.783177137374878, KL: 76.50628662109375, Loss: 0.04361073300242424, Learning Rate: 0.010289999999999997\n",
      "Epoch [3507/20000], Bound: 0.45445019006729126, Entropy: 127.2126693725586, Temp: 1.7851378917694092, KL: 74.5605697631836, Loss: 0.039578042924404144, Learning Rate: 0.010289999999999997\n",
      "Epoch [3508/20000], Bound: 0.410645067691803, Entropy: 132.1289825439453, Temp: 1.7871758937835693, KL: 64.55009460449219, Loss: 0.03577837720513344, Learning Rate: 0.010289999999999997\n",
      "Epoch [3509/20000], Bound: 0.4552818238735199, Entropy: 134.3593292236328, Temp: 1.7892361879348755, KL: 73.01657104492188, Loss: 0.04470576345920563, Learning Rate: 0.010289999999999997\n",
      "Epoch [3510/20000], Bound: 0.47356924414634705, Entropy: 137.63916015625, Temp: 1.7912538051605225, KL: 77.45005798339844, Loss: 0.04629563167691231, Learning Rate: 0.010289999999999997\n",
      "Epoch [3511/20000], Bound: 0.42570436000823975, Entropy: 138.35455322265625, Temp: 1.793251395225525, KL: 65.58042907714844, Loss: 0.0438837856054306, Learning Rate: 0.010289999999999997\n",
      "Epoch [3512/20000], Bound: 0.48425009846687317, Entropy: 137.50755310058594, Temp: 1.7951377630233765, KL: 80.84544372558594, Loss: 0.04528067633509636, Learning Rate: 0.010289999999999997\n",
      "Epoch [3513/20000], Bound: 0.5011920928955078, Entropy: 140.15391540527344, Temp: 1.7970662117004395, KL: 88.17622375488281, Loss: 0.038334231823682785, Learning Rate: 0.010289999999999997\n",
      "Epoch [3514/20000], Bound: 0.41087421774864197, Entropy: 142.58572387695312, Temp: 1.7992234230041504, KL: 65.013916015625, Loss: 0.03510732203722, Learning Rate: 0.010289999999999997\n",
      "Epoch [3515/20000], Bound: 0.44087883830070496, Entropy: 142.52468872070312, Temp: 1.8013947010040283, KL: 71.21324157714844, Loss: 0.03956571966409683, Learning Rate: 0.010289999999999997\n",
      "Epoch [3516/20000], Bound: 0.39070406556129456, Entropy: 144.82772827148438, Temp: 1.803566575050354, KL: 58.7716064453125, Loss: 0.038533616811037064, Learning Rate: 0.010289999999999997\n",
      "Epoch [3517/20000], Bound: 0.4366076588630676, Entropy: 143.77659606933594, Temp: 1.8056138753890991, KL: 67.09916687011719, Loss: 0.04801346734166145, Learning Rate: 0.010289999999999997\n",
      "Epoch [3518/20000], Bound: 0.4356090724468231, Entropy: 143.7218475341797, Temp: 1.8074750900268555, KL: 67.93464660644531, Loss: 0.04504230245947838, Learning Rate: 0.010289999999999997\n",
      "Epoch [3519/20000], Bound: 0.4075929522514343, Entropy: 146.6704864501953, Temp: 1.809227466583252, KL: 61.719268798828125, Loss: 0.04228219389915466, Learning Rate: 0.010289999999999997\n",
      "Epoch [3520/20000], Bound: 0.49735185503959656, Entropy: 145.531982421875, Temp: 1.8108580112457275, KL: 85.67254638671875, Loss: 0.04299026355147362, Learning Rate: 0.010289999999999997\n",
      "Epoch [3521/20000], Bound: 0.46067968010902405, Entropy: 143.82505798339844, Temp: 1.8126239776611328, KL: 72.38142395019531, Loss: 0.051486048847436905, Learning Rate: 0.010289999999999997\n",
      "Epoch [3522/20000], Bound: 0.43651697039604187, Entropy: 140.00218200683594, Temp: 1.814221739768982, KL: 69.99624633789062, Loss: 0.04026942327618599, Learning Rate: 0.010289999999999997\n",
      "Epoch [3523/20000], Bound: 0.49467557668685913, Entropy: 136.82357788085938, Temp: 1.81583571434021, KL: 83.63053894042969, Loss: 0.04676550626754761, Learning Rate: 0.010289999999999997\n",
      "Epoch [3524/20000], Bound: 0.4453527629375458, Entropy: 137.04443359375, Temp: 1.8174947500228882, KL: 71.23722839355469, Loss: 0.04344077408313751, Learning Rate: 0.010289999999999997\n",
      "Epoch [3525/20000], Bound: 0.46347132325172424, Entropy: 135.43031311035156, Temp: 1.819117546081543, KL: 77.85249328613281, Loss: 0.038798049092292786, Learning Rate: 0.010289999999999997\n",
      "Epoch [3526/20000], Bound: 0.43815433979034424, Entropy: 134.7638702392578, Temp: 1.820858359336853, KL: 69.784912109375, Loss: 0.042300060391426086, Learning Rate: 0.010289999999999997\n",
      "Epoch [3527/20000], Bound: 0.4803553819656372, Entropy: 129.4789581298828, Temp: 1.8225548267364502, KL: 80.45828247070312, Loss: 0.044659510254859924, Learning Rate: 0.010289999999999997\n",
      "Epoch [3528/20000], Bound: 0.4703839123249054, Entropy: 129.9398651123047, Temp: 1.8242826461791992, KL: 77.17916870117188, Loss: 0.046104174107313156, Learning Rate: 0.010289999999999997\n",
      "Epoch [3529/20000], Bound: 0.43299946188926697, Entropy: 132.61424255371094, Temp: 1.8259766101837158, KL: 67.98655700683594, Loss: 0.04368705675005913, Learning Rate: 0.010289999999999997\n",
      "Epoch [3530/20000], Bound: 0.4428417682647705, Entropy: 130.26548767089844, Temp: 1.827580451965332, KL: 72.50350952148438, Loss: 0.038532551378011703, Learning Rate: 0.010289999999999997\n",
      "Epoch [3531/20000], Bound: 0.46635040640830994, Entropy: 132.55328369140625, Temp: 1.8292421102523804, KL: 78.98135375976562, Loss: 0.03834172338247299, Learning Rate: 0.010289999999999997\n",
      "Epoch [3532/20000], Bound: 0.44967707991600037, Entropy: 128.36062622070312, Temp: 1.831025242805481, KL: 74.56866455078125, Loss: 0.03804728761315346, Learning Rate: 0.010289999999999997\n",
      "Epoch [3533/20000], Bound: 0.42405492067337036, Entropy: 131.86683654785156, Temp: 1.8328748941421509, KL: 65.90011596679688, Loss: 0.04321999475359917, Learning Rate: 0.010289999999999997\n",
      "Epoch [3534/20000], Bound: 0.4339287579059601, Entropy: 135.23158264160156, Temp: 1.834596037864685, KL: 70.08255004882812, Loss: 0.03895159438252449, Learning Rate: 0.010289999999999997\n",
      "Epoch [3535/20000], Bound: 0.4477119445800781, Entropy: 136.68316650390625, Temp: 1.8363217115402222, KL: 72.88288879394531, Loss: 0.04141648858785629, Learning Rate: 0.010289999999999997\n",
      "Epoch [3536/20000], Bound: 0.4384772479534149, Entropy: 137.03196716308594, Temp: 1.8380353450775146, KL: 67.49618530273438, Loss: 0.04940435290336609, Learning Rate: 0.010289999999999997\n",
      "Epoch [3537/20000], Bound: 0.4903668463230133, Entropy: 135.44505310058594, Temp: 1.839534878730774, KL: 82.04409790039062, Loss: 0.04886361584067345, Learning Rate: 0.010289999999999997\n",
      "Epoch [3538/20000], Bound: 0.44111984968185425, Entropy: 136.3948211669922, Temp: 1.8410061597824097, KL: 70.21127319335938, Loss: 0.0440458208322525, Learning Rate: 0.010289999999999997\n",
      "Epoch [3539/20000], Bound: 0.4474019408226013, Entropy: 139.35252380371094, Temp: 1.842409372329712, KL: 69.31314086914062, Loss: 0.05111732333898544, Learning Rate: 0.010289999999999997\n",
      "Epoch [3540/20000], Bound: 0.4625624120235443, Entropy: 138.74993896484375, Temp: 1.8436135053634644, KL: 78.41142272949219, Loss: 0.037701837718486786, Learning Rate: 0.010289999999999997\n",
      "Epoch [3541/20000], Bound: 0.4794013798236847, Entropy: 139.40281677246094, Temp: 1.8449729681015015, KL: 81.12727355957031, Loss: 0.04313727840781212, Learning Rate: 0.010289999999999997\n",
      "Epoch [3542/20000], Bound: 0.4891451299190521, Entropy: 136.9816131591797, Temp: 1.8464020490646362, KL: 83.43157958984375, Loss: 0.04446791112422943, Learning Rate: 0.010289999999999997\n",
      "Epoch [3543/20000], Bound: 0.43655720353126526, Entropy: 135.62954711914062, Temp: 1.847892165184021, KL: 71.24713134765625, Loss: 0.038192279636859894, Learning Rate: 0.010289999999999997\n",
      "Epoch [3544/20000], Bound: 0.4869391620159149, Entropy: 136.69573974609375, Temp: 1.8494206666946411, KL: 81.12394714355469, Loss: 0.04914086312055588, Learning Rate: 0.010289999999999997\n",
      "Epoch [3545/20000], Bound: 0.421892374753952, Entropy: 132.97486877441406, Temp: 1.8508903980255127, KL: 66.21778869628906, Loss: 0.04142792522907257, Learning Rate: 0.010289999999999997\n",
      "Epoch [3546/20000], Bound: 0.4029918313026428, Entropy: 135.44198608398438, Temp: 1.8522839546203613, KL: 62.206878662109375, Loss: 0.0391106978058815, Learning Rate: 0.010289999999999997\n",
      "Epoch [3547/20000], Bound: 0.44899794459342957, Entropy: 135.2565460205078, Temp: 1.8536055088043213, KL: 72.97370910644531, Loss: 0.04278506711125374, Learning Rate: 0.010289999999999997\n",
      "Epoch [3548/20000], Bound: 0.46533888578414917, Entropy: 131.67733764648438, Temp: 1.854910969734192, KL: 76.90385437011719, Loss: 0.044336918741464615, Learning Rate: 0.010289999999999997\n",
      "Epoch [3549/20000], Bound: 0.4590776264667511, Entropy: 135.8772430419922, Temp: 1.8562140464782715, KL: 74.92291259765625, Loss: 0.045060329139232635, Learning Rate: 0.010289999999999997\n",
      "Epoch [3550/20000], Bound: 0.4164983332157135, Entropy: 134.28445434570312, Temp: 1.8574793338775635, KL: 64.92596435546875, Loss: 0.04132974147796631, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3551/20000], Bound: 0.4233543872833252, Entropy: 134.7941436767578, Temp: 1.8586688041687012, KL: 66.59587097167969, Loss: 0.04169866070151329, Learning Rate: 0.010289999999999997\n",
      "Epoch [3552/20000], Bound: 0.47731247544288635, Entropy: 135.51358032226562, Temp: 1.8598002195358276, KL: 81.05415344238281, Loss: 0.042405448853969574, Learning Rate: 0.010289999999999997\n",
      "Epoch [3553/20000], Bound: 0.45044925808906555, Entropy: 139.96044921875, Temp: 1.8610178232192993, KL: 76.39227294921875, Loss: 0.03494131565093994, Learning Rate: 0.010289999999999997\n",
      "Epoch [3554/20000], Bound: 0.47255852818489075, Entropy: 142.07884216308594, Temp: 1.8623988628387451, KL: 81.70712280273438, Loss: 0.037166059017181396, Learning Rate: 0.010289999999999997\n",
      "Epoch [3555/20000], Bound: 0.507472038269043, Entropy: 137.0155029296875, Temp: 1.863938570022583, KL: 92.75849914550781, Loss: 0.03460603579878807, Learning Rate: 0.010289999999999997\n",
      "Epoch [3556/20000], Bound: 0.4582555592060089, Entropy: 135.37889099121094, Temp: 1.865772008895874, KL: 76.82305908203125, Loss: 0.03972644358873367, Learning Rate: 0.010289999999999997\n",
      "Epoch [3557/20000], Bound: 0.4987352788448334, Entropy: 135.81333923339844, Temp: 1.867619514465332, KL: 87.57513427734375, Loss: 0.041802406311035156, Learning Rate: 0.010289999999999997\n",
      "Epoch [3558/20000], Bound: 0.4269483685493469, Entropy: 136.6483612060547, Temp: 1.8695478439331055, KL: 70.33029174804688, Loss: 0.03460216149687767, Learning Rate: 0.010289999999999997\n",
      "Epoch [3559/20000], Bound: 0.46635866165161133, Entropy: 131.78915405273438, Temp: 1.8715031147003174, KL: 78.80113220214844, Loss: 0.04069088399410248, Learning Rate: 0.010289999999999997\n",
      "Epoch [3560/20000], Bound: 0.45438721776008606, Entropy: 133.46145629882812, Temp: 1.8734561204910278, KL: 76.77349853515625, Loss: 0.037321534007787704, Learning Rate: 0.010289999999999997\n",
      "Epoch [3561/20000], Bound: 0.47561246156692505, Entropy: 130.65277099609375, Temp: 1.8754454851150513, KL: 82.52784729003906, Loss: 0.03786764666438103, Learning Rate: 0.010289999999999997\n",
      "Epoch [3562/20000], Bound: 0.45709124207496643, Entropy: 130.4882049560547, Temp: 1.8775129318237305, KL: 77.504638671875, Loss: 0.03752533718943596, Learning Rate: 0.010289999999999997\n",
      "Epoch [3563/20000], Bound: 0.4215506613254547, Entropy: 132.751953125, Temp: 1.8796043395996094, KL: 68.49574279785156, Loss: 0.03604688495397568, Learning Rate: 0.010289999999999997\n",
      "Epoch [3564/20000], Bound: 0.43320193886756897, Entropy: 130.96189880371094, Temp: 1.8816492557525635, KL: 71.86817932128906, Loss: 0.03538556024432182, Learning Rate: 0.010289999999999997\n",
      "Epoch [3565/20000], Bound: 0.4125760793685913, Entropy: 130.87460327148438, Temp: 1.883697509765625, KL: 64.9930419921875, Loss: 0.03923239931464195, Learning Rate: 0.010289999999999997\n",
      "Epoch [3566/20000], Bound: 0.4055598974227905, Entropy: 130.79934692382812, Temp: 1.8856024742126465, KL: 63.48329162597656, Loss: 0.038459938019514084, Learning Rate: 0.010289999999999997\n",
      "Epoch [3567/20000], Bound: 0.4714040160179138, Entropy: 132.4315948486328, Temp: 1.8873744010925293, KL: 78.79156494140625, Loss: 0.04513104259967804, Learning Rate: 0.010289999999999997\n",
      "Epoch [3568/20000], Bound: 0.46942007541656494, Entropy: 132.78985595703125, Temp: 1.8890620470046997, KL: 79.70074462890625, Loss: 0.04130657762289047, Learning Rate: 0.010289999999999997\n",
      "Epoch [3569/20000], Bound: 0.48180508613586426, Entropy: 133.75579833984375, Temp: 1.890750527381897, KL: 78.59967041015625, Loss: 0.05361543595790863, Learning Rate: 0.010289999999999997\n",
      "Epoch [3570/20000], Bound: 0.47879666090011597, Entropy: 135.900146484375, Temp: 1.8922032117843628, KL: 82.86820983886719, Loss: 0.04010903835296631, Learning Rate: 0.010289999999999997\n",
      "Epoch [3571/20000], Bound: 0.4794047474861145, Entropy: 134.73683166503906, Temp: 1.8937296867370605, KL: 81.3258056640625, Loss: 0.04470701888203621, Learning Rate: 0.010289999999999997\n",
      "Epoch [3572/20000], Bound: 0.4999256134033203, Entropy: 134.32601928710938, Temp: 1.895221471786499, KL: 86.9588623046875, Loss: 0.045663803815841675, Learning Rate: 0.010289999999999997\n",
      "Epoch [3573/20000], Bound: 0.42462465167045593, Entropy: 133.84640502929688, Temp: 1.8967187404632568, KL: 65.91398620605469, Loss: 0.045574892312288284, Learning Rate: 0.010289999999999997\n",
      "Epoch [3574/20000], Bound: 0.3997560143470764, Entropy: 133.10169982910156, Temp: 1.8980039358139038, KL: 63.416961669921875, Loss: 0.035040274262428284, Learning Rate: 0.010289999999999997\n",
      "Epoch [3575/20000], Bound: 0.43649959564208984, Entropy: 136.21890258789062, Temp: 1.899268388748169, KL: 72.10456848144531, Loss: 0.037737488746643066, Learning Rate: 0.010289999999999997\n",
      "Epoch [3576/20000], Bound: 0.4649589955806732, Entropy: 136.01177978515625, Temp: 1.9005526304244995, KL: 74.72607421875, Loss: 0.05154143646359444, Learning Rate: 0.010289999999999997\n",
      "Epoch [3577/20000], Bound: 0.47378289699554443, Entropy: 134.4393768310547, Temp: 1.901625156402588, KL: 79.03431701660156, Loss: 0.046820394694805145, Learning Rate: 0.010289999999999997\n",
      "Epoch [3578/20000], Bound: 0.4748912751674652, Entropy: 137.87826538085938, Temp: 1.9026366472244263, KL: 82.67317199707031, Loss: 0.03812681511044502, Learning Rate: 0.010289999999999997\n",
      "Epoch [3579/20000], Bound: 0.4693126678466797, Entropy: 135.745849609375, Temp: 1.9037877321243286, KL: 78.44001770019531, Loss: 0.045125048607587814, Learning Rate: 0.010289999999999997\n",
      "Epoch [3580/20000], Bound: 0.4415968656539917, Entropy: 134.41322326660156, Temp: 1.9048922061920166, KL: 73.48478698730469, Loss: 0.03794584050774574, Learning Rate: 0.010289999999999997\n",
      "Epoch [3581/20000], Bound: 0.4528588652610779, Entropy: 135.33975219726562, Temp: 1.9060364961624146, KL: 74.15634155273438, Loss: 0.04434764385223389, Learning Rate: 0.010289999999999997\n",
      "Epoch [3582/20000], Bound: 0.4847162067890167, Entropy: 133.92092895507812, Temp: 1.9071027040481567, KL: 83.92825317382812, Loss: 0.04244174435734749, Learning Rate: 0.010289999999999997\n",
      "Epoch [3583/20000], Bound: 0.5311442017555237, Entropy: 138.0511932373047, Temp: 1.9082307815551758, KL: 100.64547729492188, Loss: 0.03521931916475296, Learning Rate: 0.010289999999999997\n",
      "Epoch [3584/20000], Bound: 0.40103068947792053, Entropy: 136.62884521484375, Temp: 1.9096996784210205, KL: 60.43183898925781, Loss: 0.04405711963772774, Learning Rate: 0.010289999999999997\n",
      "Epoch [3585/20000], Bound: 0.45888131856918335, Entropy: 132.87451171875, Temp: 1.9109127521514893, KL: 77.85508728027344, Loss: 0.039226386696100235, Learning Rate: 0.010289999999999997\n",
      "Epoch [3586/20000], Bound: 0.4339292645454407, Entropy: 132.41297912597656, Temp: 1.9121681451797485, KL: 71.86122131347656, Loss: 0.03699510172009468, Learning Rate: 0.010289999999999997\n",
      "Epoch [3587/20000], Bound: 0.469481885433197, Entropy: 132.93043518066406, Temp: 1.9134414196014404, KL: 80.17593383789062, Loss: 0.041071824729442596, Learning Rate: 0.010289999999999997\n",
      "Epoch [3588/20000], Bound: 0.4371330440044403, Entropy: 132.2496795654297, Temp: 1.9147366285324097, KL: 73.07545471191406, Loss: 0.03618035838007927, Learning Rate: 0.010289999999999997\n",
      "Epoch [3589/20000], Bound: 0.44730350375175476, Entropy: 130.49066162109375, Temp: 1.9160709381103516, KL: 74.04742431640625, Loss: 0.04095791280269623, Learning Rate: 0.010289999999999997\n",
      "Epoch [3590/20000], Bound: 0.4602104127407074, Entropy: 132.34439086914062, Temp: 1.917358636856079, KL: 76.50439453125, Loss: 0.04396301507949829, Learning Rate: 0.010289999999999997\n",
      "Epoch [3591/20000], Bound: 0.4669190049171448, Entropy: 132.58819580078125, Temp: 1.9185715913772583, KL: 79.23776245117188, Loss: 0.041817378252744675, Learning Rate: 0.010289999999999997\n",
      "Epoch [3592/20000], Bound: 0.42939209938049316, Entropy: 132.84434509277344, Temp: 1.919783115386963, KL: 72.21733093261719, Loss: 0.033130109310150146, Learning Rate: 0.010289999999999997\n",
      "Epoch [3593/20000], Bound: 0.46719083189964294, Entropy: 133.27505493164062, Temp: 1.921086072921753, KL: 82.67115783691406, Loss: 0.03317644074559212, Learning Rate: 0.010289999999999997\n",
      "Epoch [3594/20000], Bound: 0.46961578726768494, Entropy: 133.3043975830078, Temp: 1.922571063041687, KL: 78.08345031738281, Loss: 0.04696425795555115, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3595/20000], Bound: 0.44865262508392334, Entropy: 134.4151611328125, Temp: 1.9239153861999512, KL: 71.76065063476562, Loss: 0.048136260360479355, Learning Rate: 0.010289999999999997\n",
      "Epoch [3596/20000], Bound: 0.44028010964393616, Entropy: 135.23573303222656, Temp: 1.925044298171997, KL: 71.59664916992188, Loss: 0.042608097195625305, Learning Rate: 0.010289999999999997\n",
      "Epoch [3597/20000], Bound: 0.44768616557121277, Entropy: 137.63180541992188, Temp: 1.9260802268981934, KL: 72.87405395507812, Loss: 0.04461650922894478, Learning Rate: 0.010289999999999997\n",
      "Epoch [3598/20000], Bound: 0.4303751587867737, Entropy: 136.61175537109375, Temp: 1.927006721496582, KL: 65.67259216308594, Loss: 0.05104777216911316, Learning Rate: 0.010289999999999997\n",
      "Epoch [3599/20000], Bound: 0.4563539922237396, Entropy: 138.3958282470703, Temp: 1.9276353120803833, KL: 77.05166625976562, Loss: 0.04009018838405609, Learning Rate: 0.010289999999999997\n",
      "Epoch [3600/20000], Bound: 0.43814393877983093, Entropy: 135.80902099609375, Temp: 1.9283208847045898, KL: 71.34538269042969, Loss: 0.04184625670313835, Learning Rate: 0.010289999999999997\n",
      "Epoch [3601/20000], Bound: 0.4593444764614105, Entropy: 132.6984100341797, Temp: 1.92896568775177, KL: 77.582763671875, Loss: 0.04093785211443901, Learning Rate: 0.010289999999999997\n",
      "Epoch [3602/20000], Bound: 0.44284334778785706, Entropy: 132.44239807128906, Temp: 1.929653286933899, KL: 68.3341064453125, Loss: 0.053029246628284454, Learning Rate: 0.010289999999999997\n",
      "Epoch [3603/20000], Bound: 0.4354356527328491, Entropy: 134.5404510498047, Temp: 1.9300549030303955, KL: 70.36447143554688, Loss: 0.04252687469124794, Learning Rate: 0.010289999999999997\n",
      "Epoch [3604/20000], Bound: 0.4524781405925751, Entropy: 134.8581085205078, Temp: 1.930418848991394, KL: 73.23489379882812, Loss: 0.04726805537939072, Learning Rate: 0.010289999999999997\n",
      "Epoch [3605/20000], Bound: 0.43874531984329224, Entropy: 134.05252075195312, Temp: 1.9306883811950684, KL: 69.15538024902344, Loss: 0.048017196357250214, Learning Rate: 0.010289999999999997\n",
      "Epoch [3606/20000], Bound: 0.45260289311408997, Entropy: 137.79507446289062, Temp: 1.9308154582977295, KL: 71.94105529785156, Loss: 0.0507209375500679, Learning Rate: 0.010289999999999997\n",
      "Epoch [3607/20000], Bound: 0.4445970952510834, Entropy: 137.1195526123047, Temp: 1.9307924509048462, KL: 74.38833618164062, Loss: 0.03863216191530228, Learning Rate: 0.010289999999999997\n",
      "Epoch [3608/20000], Bound: 0.4792456030845642, Entropy: 137.73452758789062, Temp: 1.9308886528015137, KL: 80.48452758789062, Loss: 0.04821048676967621, Learning Rate: 0.010289999999999997\n",
      "Epoch [3609/20000], Bound: 0.47445258498191833, Entropy: 134.30519104003906, Temp: 1.930972695350647, KL: 82.52247619628906, Loss: 0.03935345262289047, Learning Rate: 0.010289999999999997\n",
      "Epoch [3610/20000], Bound: 0.4817756116390228, Entropy: 132.06723022460938, Temp: 1.9312318563461304, KL: 83.20001220703125, Loss: 0.04309368133544922, Learning Rate: 0.010289999999999997\n",
      "Epoch [3611/20000], Bound: 0.4623512327671051, Entropy: 131.4840850830078, Temp: 1.9315849542617798, KL: 75.26841735839844, Loss: 0.04921866953372955, Learning Rate: 0.010289999999999997\n",
      "Epoch [3612/20000], Bound: 0.43258434534072876, Entropy: 130.3365936279297, Temp: 1.9318270683288574, KL: 71.56825256347656, Loss: 0.03745747357606888, Learning Rate: 0.010289999999999997\n",
      "Epoch [3613/20000], Bound: 0.484884649515152, Entropy: 128.09457397460938, Temp: 1.9321552515029907, KL: 85.17405700683594, Loss: 0.040366895496845245, Learning Rate: 0.010289999999999997\n",
      "Epoch [3614/20000], Bound: 0.4790412485599518, Entropy: 128.36825561523438, Temp: 1.9326395988464355, KL: 81.66868591308594, Loss: 0.04505663737654686, Learning Rate: 0.010289999999999997\n",
      "Epoch [3615/20000], Bound: 0.43163084983825684, Entropy: 131.93673706054688, Temp: 1.9331419467926025, KL: 68.29423522949219, Loss: 0.045298513025045395, Learning Rate: 0.010289999999999997\n",
      "Epoch [3616/20000], Bound: 0.42764100432395935, Entropy: 126.60896301269531, Temp: 1.9335181713104248, KL: 70.66378784179688, Loss: 0.03639087826013565, Learning Rate: 0.010289999999999997\n",
      "Epoch [3617/20000], Bound: 0.44027313590049744, Entropy: 126.75408172607422, Temp: 1.933976650238037, KL: 72.67826080322266, Loss: 0.040083494037389755, Learning Rate: 0.010289999999999997\n",
      "Epoch [3618/20000], Bound: 0.47630104422569275, Entropy: 128.46340942382812, Temp: 1.9344582557678223, KL: 83.33183288574219, Loss: 0.03877662867307663, Learning Rate: 0.010289999999999997\n",
      "Epoch [3619/20000], Bound: 0.4593849778175354, Entropy: 131.1761016845703, Temp: 1.9350906610488892, KL: 78.76531982421875, Loss: 0.03812910616397858, Learning Rate: 0.010289999999999997\n",
      "Epoch [3620/20000], Bound: 0.4677049219608307, Entropy: 132.28221130371094, Temp: 1.9358261823654175, KL: 80.22813415527344, Loss: 0.04047339782118797, Learning Rate: 0.010289999999999997\n",
      "Epoch [3621/20000], Bound: 0.46106216311454773, Entropy: 132.9291229248047, Temp: 1.9366233348846436, KL: 74.03062438964844, Loss: 0.051632270216941833, Learning Rate: 0.010289999999999997\n",
      "Epoch [3622/20000], Bound: 0.4564419388771057, Entropy: 136.06893920898438, Temp: 1.9372003078460693, KL: 73.49606323242188, Loss: 0.04966828227043152, Learning Rate: 0.010289999999999997\n",
      "Epoch [3623/20000], Bound: 0.4605705738067627, Entropy: 137.72715759277344, Temp: 1.9376097917556763, KL: 77.26658630371094, Loss: 0.04295281320810318, Learning Rate: 0.010289999999999997\n",
      "Epoch [3624/20000], Bound: 0.4693641662597656, Entropy: 135.95896911621094, Temp: 1.9380345344543457, KL: 80.65180969238281, Loss: 0.04068591445684433, Learning Rate: 0.010289999999999997\n",
      "Epoch [3625/20000], Bound: 0.43615615367889404, Entropy: 135.5244903564453, Temp: 1.938549518585205, KL: 72.618896484375, Loss: 0.03747277706861496, Learning Rate: 0.010289999999999997\n",
      "Epoch [3626/20000], Bound: 0.44021016359329224, Entropy: 137.31678771972656, Temp: 1.93912672996521, KL: 69.08926391601562, Loss: 0.0494571216404438, Learning Rate: 0.010289999999999997\n",
      "Epoch [3627/20000], Bound: 0.4340001046657562, Entropy: 136.23443603515625, Temp: 1.9394915103912354, KL: 71.52166748046875, Loss: 0.03881492093205452, Learning Rate: 0.010289999999999997\n",
      "Epoch [3628/20000], Bound: 0.444670170545578, Entropy: 135.989013671875, Temp: 1.9398950338363647, KL: 74.79615783691406, Loss: 0.037937939167022705, Learning Rate: 0.010289999999999997\n",
      "Epoch [3629/20000], Bound: 0.4759911894798279, Entropy: 135.68849182128906, Temp: 1.9403836727142334, KL: 76.07731628417969, Loss: 0.05747456103563309, Learning Rate: 0.010289999999999997\n",
      "Epoch [3630/20000], Bound: 0.44314154982566833, Entropy: 133.0468292236328, Temp: 1.9405884742736816, KL: 70.93605041503906, Loss: 0.046818654984235764, Learning Rate: 0.010289999999999997\n",
      "Epoch [3631/20000], Bound: 0.4808964431285858, Entropy: 135.8255615234375, Temp: 1.9406861066818237, KL: 83.39852905273438, Loss: 0.042288389056921005, Learning Rate: 0.010289999999999997\n",
      "Epoch [3632/20000], Bound: 0.4262341260910034, Entropy: 137.04827880859375, Temp: 1.9408998489379883, KL: 65.62033081054688, Loss: 0.04863610118627548, Learning Rate: 0.010289999999999997\n",
      "Epoch [3633/20000], Bound: 0.4345744848251343, Entropy: 135.16297912597656, Temp: 1.940913200378418, KL: 69.62928771972656, Loss: 0.04413769766688347, Learning Rate: 0.010289999999999997\n",
      "Epoch [3634/20000], Bound: 0.4669378399848938, Entropy: 134.50486755371094, Temp: 1.9408760070800781, KL: 79.24632263183594, Loss: 0.04262517765164375, Learning Rate: 0.010289999999999997\n",
      "Epoch [3635/20000], Bound: 0.4356718063354492, Entropy: 135.46360778808594, Temp: 1.9409208297729492, KL: 69.07545471191406, Loss: 0.046336472034454346, Learning Rate: 0.010289999999999997\n",
      "Epoch [3636/20000], Bound: 0.4445765018463135, Entropy: 135.57769775390625, Temp: 1.9408632516860962, KL: 73.4912109375, Loss: 0.041265495121479034, Learning Rate: 0.010289999999999997\n",
      "Epoch [3637/20000], Bound: 0.4402596056461334, Entropy: 135.0968475341797, Temp: 1.9408581256866455, KL: 72.37199401855469, Loss: 0.041081707924604416, Learning Rate: 0.010289999999999997\n",
      "Epoch [3638/20000], Bound: 0.4487340748310089, Entropy: 138.1796112060547, Temp: 1.940891981124878, KL: 68.3240966796875, Loss: 0.0575486458837986, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3639/20000], Bound: 0.48409467935562134, Entropy: 136.96946716308594, Temp: 1.940600872039795, KL: 85.46768188476562, Loss: 0.03935911878943443, Learning Rate: 0.010289999999999997\n",
      "Epoch [3640/20000], Bound: 0.49481064081192017, Entropy: 139.16619873046875, Temp: 1.940541386604309, KL: 85.31082153320312, Loss: 0.04790263622999191, Learning Rate: 0.010289999999999997\n",
      "Epoch [3641/20000], Bound: 0.4752042591571808, Entropy: 135.20404052734375, Temp: 1.9405266046524048, KL: 81.55525207519531, Loss: 0.042778413742780685, Learning Rate: 0.010289999999999997\n",
      "Epoch [3642/20000], Bound: 0.44522005319595337, Entropy: 134.310302734375, Temp: 1.9406123161315918, KL: 73.41349792480469, Loss: 0.04191651567816734, Learning Rate: 0.010289999999999997\n",
      "Epoch [3643/20000], Bound: 0.4702985882759094, Entropy: 135.1992645263672, Temp: 1.9407230615615845, KL: 77.08757019042969, Loss: 0.05065860599279404, Learning Rate: 0.010289999999999997\n",
      "Epoch [3644/20000], Bound: 0.4327574670314789, Entropy: 134.52818298339844, Temp: 1.9407258033752441, KL: 70.33767700195312, Loss: 0.041031744331121445, Learning Rate: 0.010289999999999997\n",
      "Epoch [3645/20000], Bound: 0.47078368067741394, Entropy: 132.9504852294922, Temp: 1.9407470226287842, KL: 72.03208923339844, Loss: 0.06404247879981995, Learning Rate: 0.010289999999999997\n",
      "Epoch [3646/20000], Bound: 0.4146667420864105, Entropy: 131.62799072265625, Temp: 1.940361499786377, KL: 66.06118774414062, Loss: 0.039509356021881104, Learning Rate: 0.010289999999999997\n",
      "Epoch [3647/20000], Bound: 0.47106683254241943, Entropy: 131.21026611328125, Temp: 1.9400182962417603, KL: 80.69972229003906, Loss: 0.041894350200891495, Learning Rate: 0.010289999999999997\n",
      "Epoch [3648/20000], Bound: 0.4846170246601105, Entropy: 131.71934509277344, Temp: 1.9398177862167358, KL: 79.23088073730469, Loss: 0.055796749889850616, Learning Rate: 0.010289999999999997\n",
      "Epoch [3649/20000], Bound: 0.43732312321662903, Entropy: 134.4038543701172, Temp: 1.939466118812561, KL: 74.12477111816406, Loss: 0.034443069249391556, Learning Rate: 0.010289999999999997\n",
      "Epoch [3650/20000], Bound: 0.45551538467407227, Entropy: 130.39227294921875, Temp: 1.939337968826294, KL: 76.02452087402344, Loss: 0.04254251345992088, Learning Rate: 0.010289999999999997\n",
      "Epoch [3651/20000], Bound: 0.48938965797424316, Entropy: 133.08865356445312, Temp: 1.9392722845077515, KL: 83.26528930664062, Loss: 0.04899120330810547, Learning Rate: 0.010289999999999997\n",
      "Epoch [3652/20000], Bound: 0.4903048574924469, Entropy: 132.3742218017578, Temp: 1.9392127990722656, KL: 86.61163330078125, Loss: 0.041056860238313675, Learning Rate: 0.010289999999999997\n",
      "Epoch [3653/20000], Bound: 0.49315860867500305, Entropy: 131.56463623046875, Temp: 1.939342975616455, KL: 80.19171142578125, Loss: 0.05979010462760925, Learning Rate: 0.010289999999999997\n",
      "Epoch [3654/20000], Bound: 0.43428298830986023, Entropy: 130.17807006835938, Temp: 1.9392240047454834, KL: 69.48281860351562, Loss: 0.044262271374464035, Learning Rate: 0.010289999999999997\n",
      "Epoch [3655/20000], Bound: 0.4537508487701416, Entropy: 129.31285095214844, Temp: 1.9390650987625122, KL: 75.30795288085938, Loss: 0.04310549795627594, Learning Rate: 0.010289999999999997\n",
      "Epoch [3656/20000], Bound: 0.4238460063934326, Entropy: 131.60360717773438, Temp: 1.9389539957046509, KL: 67.73727416992188, Loss: 0.04147142916917801, Learning Rate: 0.010289999999999997\n",
      "Epoch [3657/20000], Bound: 0.4449372887611389, Entropy: 130.2836151123047, Temp: 1.9388386011123657, KL: 73.72688293457031, Loss: 0.04085038974881172, Learning Rate: 0.010289999999999997\n",
      "Epoch [3658/20000], Bound: 0.45658835768699646, Entropy: 131.93882751464844, Temp: 1.9387946128845215, KL: 76.71295166015625, Loss: 0.041526198387145996, Learning Rate: 0.010289999999999997\n",
      "Epoch [3659/20000], Bound: 0.4228469431400299, Entropy: 132.66029357910156, Temp: 1.9388326406478882, KL: 66.89369201660156, Loss: 0.04295157268643379, Learning Rate: 0.010289999999999997\n",
      "Epoch [3660/20000], Bound: 0.45964741706848145, Entropy: 131.4176788330078, Temp: 1.9388132095336914, KL: 75.45462036132812, Loss: 0.04699438065290451, Learning Rate: 0.010289999999999997\n",
      "Epoch [3661/20000], Bound: 0.4567145109176636, Entropy: 134.39854431152344, Temp: 1.9387544393539429, KL: 75.41384887695312, Loss: 0.044966649264097214, Learning Rate: 0.010289999999999997\n",
      "Epoch [3662/20000], Bound: 0.45842409133911133, Entropy: 135.8662567138672, Temp: 1.9386985301971436, KL: 75.45623779296875, Loss: 0.04609660431742668, Learning Rate: 0.010289999999999997\n",
      "Epoch [3663/20000], Bound: 0.47889450192451477, Entropy: 135.65530395507812, Temp: 1.9386241436004639, KL: 77.69465637207031, Loss: 0.055418968200683594, Learning Rate: 0.010289999999999997\n",
      "Epoch [3664/20000], Bound: 0.4819537103176117, Entropy: 138.118408203125, Temp: 1.9383772611618042, KL: 81.42643737792969, Loss: 0.048079974949359894, Learning Rate: 0.010289999999999997\n",
      "Epoch [3665/20000], Bound: 0.43319958448410034, Entropy: 139.59019470214844, Temp: 1.9381544589996338, KL: 71.69296264648438, Loss: 0.03776898235082626, Learning Rate: 0.010289999999999997\n",
      "Epoch [3666/20000], Bound: 0.4773530960083008, Entropy: 137.99185180664062, Temp: 1.9380544424057007, KL: 81.85171508789062, Loss: 0.04352359101176262, Learning Rate: 0.010289999999999997\n",
      "Epoch [3667/20000], Bound: 0.43328002095222473, Entropy: 140.27474975585938, Temp: 1.938056468963623, KL: 68.32781982421875, Loss: 0.04650405794382095, Learning Rate: 0.010289999999999997\n",
      "Epoch [3668/20000], Bound: 0.454293817281723, Entropy: 134.35842895507812, Temp: 1.9379509687423706, KL: 73.67739868164062, Loss: 0.047668054699897766, Learning Rate: 0.010289999999999997\n",
      "Epoch [3669/20000], Bound: 0.46363964676856995, Entropy: 138.85525512695312, Temp: 1.9377834796905518, KL: 77.13150024414062, Loss: 0.04555016756057739, Learning Rate: 0.010289999999999997\n",
      "Epoch [3670/20000], Bound: 0.4626013934612274, Entropy: 132.57398986816406, Temp: 1.9376376867294312, KL: 76.28901672363281, Loss: 0.04695942625403404, Learning Rate: 0.010289999999999997\n",
      "Epoch [3671/20000], Bound: 0.41715535521507263, Entropy: 132.556884765625, Temp: 1.9374754428863525, KL: 65.08082580566406, Loss: 0.04366756230592728, Learning Rate: 0.010289999999999997\n",
      "Epoch [3672/20000], Bound: 0.4679436981678009, Entropy: 131.70533752441406, Temp: 1.9372432231903076, KL: 79.9913330078125, Loss: 0.04131317138671875, Learning Rate: 0.010289999999999997\n",
      "Epoch [3673/20000], Bound: 0.46376463770866394, Entropy: 130.7233123779297, Temp: 1.9371516704559326, KL: 76.98289489746094, Loss: 0.04600406438112259, Learning Rate: 0.010289999999999997\n",
      "Epoch [3674/20000], Bound: 0.4603542685508728, Entropy: 132.36085510253906, Temp: 1.937064528465271, KL: 77.37820434570312, Loss: 0.0424882210791111, Learning Rate: 0.010289999999999997\n",
      "Epoch [3675/20000], Bound: 0.45538586378097534, Entropy: 131.73046875, Temp: 1.9370545148849487, KL: 74.96192932128906, Loss: 0.04511500149965286, Learning Rate: 0.010289999999999997\n",
      "Epoch [3676/20000], Bound: 0.48241931200027466, Entropy: 130.20408630371094, Temp: 1.9370372295379639, KL: 83.212646484375, Loss: 0.04377113655209541, Learning Rate: 0.010289999999999997\n",
      "Epoch [3677/20000], Bound: 0.4483397305011749, Entropy: 126.20206451416016, Temp: 1.9371243715286255, KL: 76.27571868896484, Loss: 0.03664960712194443, Learning Rate: 0.010289999999999997\n",
      "Epoch [3678/20000], Bound: 0.49334684014320374, Entropy: 132.20970153808594, Temp: 1.9373748302459717, KL: 82.77845764160156, Loss: 0.05319403111934662, Learning Rate: 0.010289999999999997\n",
      "Epoch [3679/20000], Bound: 0.4400659203529358, Entropy: 129.7798614501953, Temp: 1.937516689300537, KL: 72.52095031738281, Loss: 0.04045556113123894, Learning Rate: 0.010289999999999997\n",
      "Epoch [3680/20000], Bound: 0.4810219407081604, Entropy: 130.62025451660156, Temp: 1.9377018213272095, KL: 82.00032043457031, Loss: 0.045874614268541336, Learning Rate: 0.010289999999999997\n",
      "Epoch [3681/20000], Bound: 0.4729056656360626, Entropy: 131.31553649902344, Temp: 1.9379175901412964, KL: 80.20399475097656, Loss: 0.04445856809616089, Learning Rate: 0.010289999999999997\n",
      "Epoch [3682/20000], Bound: 0.4463512897491455, Entropy: 130.98277282714844, Temp: 1.9381697177886963, KL: 69.33642578125, Loss: 0.05316474288702011, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3683/20000], Bound: 0.5047709941864014, Entropy: 130.62619018554688, Temp: 1.9381686449050903, KL: 84.97946166992188, Loss: 0.05635383352637291, Learning Rate: 0.010289999999999997\n",
      "Epoch [3684/20000], Bound: 0.47763076424598694, Entropy: 131.39720153808594, Temp: 1.9380451440811157, KL: 80.90301513671875, Loss: 0.04617824777960777, Learning Rate: 0.010289999999999997\n",
      "Epoch [3685/20000], Bound: 0.4634923040866852, Entropy: 131.9499053955078, Temp: 1.9379655122756958, KL: 73.98919677734375, Loss: 0.053555648773908615, Learning Rate: 0.010289999999999997\n",
      "Epoch [3686/20000], Bound: 0.4342266917228699, Entropy: 130.1344451904297, Temp: 1.9377089738845825, KL: 69.63320922851562, Loss: 0.043791383504867554, Learning Rate: 0.010289999999999997\n",
      "Epoch [3687/20000], Bound: 0.41260066628456116, Entropy: 131.076171875, Temp: 1.9374381303787231, KL: 63.06193542480469, Loss: 0.04575661942362785, Learning Rate: 0.010289999999999997\n",
      "Epoch [3688/20000], Bound: 0.46980807185173035, Entropy: 128.1184539794922, Temp: 1.937043309211731, KL: 78.46095275878906, Loss: 0.04663204401731491, Learning Rate: 0.010289999999999997\n",
      "Epoch [3689/20000], Bound: 0.4505840539932251, Entropy: 134.2596893310547, Temp: 1.9366865158081055, KL: 72.62403869628906, Loss: 0.047673549503088, Learning Rate: 0.010289999999999997\n",
      "Epoch [3690/20000], Bound: 0.47896620631217957, Entropy: 134.5131072998047, Temp: 1.9362821578979492, KL: 81.18780517578125, Loss: 0.046378087252378464, Learning Rate: 0.010289999999999997\n",
      "Epoch [3691/20000], Bound: 0.4374972879886627, Entropy: 134.06947326660156, Temp: 1.9359509944915771, KL: 70.32191467285156, Loss: 0.04426750913262367, Learning Rate: 0.010289999999999997\n",
      "Epoch [3692/20000], Bound: 0.47887662053108215, Entropy: 136.6571807861328, Temp: 1.9356130361557007, KL: 79.84794616699219, Loss: 0.04974757879972458, Learning Rate: 0.010289999999999997\n",
      "Epoch [3693/20000], Bound: 0.43372899293899536, Entropy: 136.02084350585938, Temp: 1.9352630376815796, KL: 69.2767333984375, Loss: 0.04429180175065994, Learning Rate: 0.010289999999999997\n",
      "Epoch [3694/20000], Bound: 0.48802322149276733, Entropy: 134.36122131347656, Temp: 1.9348971843719482, KL: 85.97186279296875, Loss: 0.040795087814331055, Learning Rate: 0.010289999999999997\n",
      "Epoch [3695/20000], Bound: 0.42692533135414124, Entropy: 135.5554962158203, Temp: 1.9347598552703857, KL: 66.1578369140625, Loss: 0.047575920820236206, Learning Rate: 0.010289999999999997\n",
      "Epoch [3696/20000], Bound: 0.45697709918022156, Entropy: 133.1937713623047, Temp: 1.9344862699508667, KL: 75.98291015625, Loss: 0.04354710876941681, Learning Rate: 0.010289999999999997\n",
      "Epoch [3697/20000], Bound: 0.4170229136943817, Entropy: 138.0485382080078, Temp: 1.9342765808105469, KL: 60.5069580078125, Loss: 0.05531708523631096, Learning Rate: 0.010289999999999997\n",
      "Epoch [3698/20000], Bound: 0.45331108570098877, Entropy: 137.67544555664062, Temp: 1.933719515800476, KL: 74.36407470703125, Loss: 0.04505164176225662, Learning Rate: 0.010289999999999997\n",
      "Epoch [3699/20000], Bound: 0.4216509759426117, Entropy: 136.93206787109375, Temp: 1.9332088232040405, KL: 68.18138122558594, Loss: 0.03863969072699547, Learning Rate: 0.010289999999999997\n",
      "Epoch [3700/20000], Bound: 0.3666383922100067, Entropy: 138.18701171875, Temp: 1.9328019618988037, KL: 53.864776611328125, Loss: 0.0389266274869442, Learning Rate: 0.010289999999999997\n",
      "Epoch [3701/20000], Bound: 0.4227530360221863, Entropy: 139.16116333007812, Temp: 1.9323222637176514, KL: 67.43241882324219, Loss: 0.04131496325135231, Learning Rate: 0.010289999999999997\n",
      "Epoch [3702/20000], Bound: 0.48417600989341736, Entropy: 141.75881958007812, Temp: 1.9318825006484985, KL: 85.63954162597656, Loss: 0.03861534968018532, Learning Rate: 0.010289999999999997\n",
      "Epoch [3703/20000], Bound: 0.4673728942871094, Entropy: 142.54075622558594, Temp: 1.9317231178283691, KL: 72.60482788085938, Loss: 0.05980687960982323, Learning Rate: 0.010289999999999997\n",
      "Epoch [3704/20000], Bound: 0.434560626745224, Entropy: 146.105712890625, Temp: 1.9312642812728882, KL: 71.92869567871094, Loss: 0.03789663314819336, Learning Rate: 0.010289999999999997\n",
      "Epoch [3705/20000], Bound: 0.4297790825366974, Entropy: 142.72225952148438, Temp: 1.9309618473052979, KL: 64.90660095214844, Loss: 0.05271182581782341, Learning Rate: 0.010289999999999997\n",
      "Epoch [3706/20000], Bound: 0.44707217812538147, Entropy: 143.38731384277344, Temp: 1.9304269552230835, KL: 76.55967712402344, Loss: 0.03476696461439133, Learning Rate: 0.010289999999999997\n",
      "Epoch [3707/20000], Bound: 0.45319682359695435, Entropy: 140.53819274902344, Temp: 1.9301685094833374, KL: 76.01274108886719, Loss: 0.04058380052447319, Learning Rate: 0.010289999999999997\n",
      "Epoch [3708/20000], Bound: 0.44109582901000977, Entropy: 141.68414306640625, Temp: 1.930037260055542, KL: 72.85333251953125, Loss: 0.040087614208459854, Learning Rate: 0.010289999999999997\n",
      "Epoch [3709/20000], Bound: 0.5039896965026855, Entropy: 137.0902099609375, Temp: 1.929997205734253, KL: 90.60542297363281, Loss: 0.0408652238547802, Learning Rate: 0.010289999999999997\n",
      "Epoch [3710/20000], Bound: 0.46702703833580017, Entropy: 136.88047790527344, Temp: 1.9302057027816772, KL: 79.73336791992188, Loss: 0.041044387966394424, Learning Rate: 0.010289999999999997\n",
      "Epoch [3711/20000], Bound: 0.4610409736633301, Entropy: 133.02842712402344, Temp: 1.9305247068405151, KL: 81.807373046875, Loss: 0.031290505081415176, Learning Rate: 0.010289999999999997\n",
      "Epoch [3712/20000], Bound: 0.44542133808135986, Entropy: 135.21469116210938, Temp: 1.931158185005188, KL: 74.75491333007812, Loss: 0.03828447312116623, Learning Rate: 0.010289999999999997\n",
      "Epoch [3713/20000], Bound: 0.4475482404232025, Entropy: 132.06643676757812, Temp: 1.9318618774414062, KL: 76.130859375, Loss: 0.036270227283239365, Learning Rate: 0.010289999999999997\n",
      "Epoch [3714/20000], Bound: 0.4049319922924042, Entropy: 130.56590270996094, Temp: 1.932682991027832, KL: 66.66632080078125, Loss: 0.03110024333000183, Learning Rate: 0.010289999999999997\n",
      "Epoch [3715/20000], Bound: 0.41749322414398193, Entropy: 128.76531982421875, Temp: 1.933613657951355, KL: 68.73733520507812, Loss: 0.03434506431221962, Learning Rate: 0.010289999999999997\n",
      "Epoch [3716/20000], Bound: 0.44155094027519226, Entropy: 130.10671997070312, Temp: 1.9345982074737549, KL: 71.89460754394531, Loss: 0.043036088347435, Learning Rate: 0.010289999999999997\n",
      "Epoch [3717/20000], Bound: 0.4199792742729187, Entropy: 129.94700622558594, Temp: 1.9354881048202515, KL: 67.15341186523438, Loss: 0.040207184851169586, Learning Rate: 0.010289999999999997\n",
      "Epoch [3718/20000], Bound: 0.46848946809768677, Entropy: 133.00466918945312, Temp: 1.936297059059143, KL: 81.23075866699219, Loss: 0.03848036378622055, Learning Rate: 0.010289999999999997\n",
      "Epoch [3719/20000], Bound: 0.4626886546611786, Entropy: 133.2465057373047, Temp: 1.93721604347229, KL: 78.17018127441406, Loss: 0.04215410724282265, Learning Rate: 0.010289999999999997\n",
      "Epoch [3720/20000], Bound: 0.4664306938648224, Entropy: 133.36013793945312, Temp: 1.9381279945373535, KL: 78.73190307617188, Loss: 0.043481044471263885, Learning Rate: 0.010289999999999997\n",
      "Epoch [3721/20000], Bound: 0.4774056077003479, Entropy: 135.6082305908203, Temp: 1.9390119314193726, KL: 82.67556762695312, Loss: 0.041474245488643646, Learning Rate: 0.010289999999999997\n",
      "Epoch [3722/20000], Bound: 0.39816907048225403, Entropy: 132.39463806152344, Temp: 1.9399502277374268, KL: 63.49371337890625, Loss: 0.03494613245129585, Learning Rate: 0.010289999999999997\n",
      "Epoch [3723/20000], Bound: 0.4579509198665619, Entropy: 134.39950561523438, Temp: 1.9408656358718872, KL: 79.22892761230469, Loss: 0.0361035019159317, Learning Rate: 0.010289999999999997\n",
      "Epoch [3724/20000], Bound: 0.44522154331207275, Entropy: 135.09548950195312, Temp: 1.9419020414352417, KL: 77.95326232910156, Loss: 0.030269360169768333, Learning Rate: 0.010289999999999997\n",
      "Epoch [3725/20000], Bound: 0.45937034487724304, Entropy: 137.2841033935547, Temp: 1.9431519508361816, KL: 77.10746765136719, Loss: 0.04267798364162445, Learning Rate: 0.010289999999999997\n",
      "Epoch [3726/20000], Bound: 0.4341316223144531, Entropy: 132.347412109375, Temp: 1.9443330764770508, KL: 73.98399353027344, Loss: 0.03272520378232002, Learning Rate: 0.010289999999999997\n",
      "Epoch [3727/20000], Bound: 0.4485262930393219, Entropy: 134.7416534423828, Temp: 1.945619821548462, KL: 73.62954711914062, Loss: 0.043879445642232895, Learning Rate: 0.010289999999999997\n",
      "Epoch [3728/20000], Bound: 0.450000137090683, Entropy: 134.78309631347656, Temp: 1.9467699527740479, KL: 76.55937194824219, Loss: 0.037445735186338425, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3729/20000], Bound: 0.4289926290512085, Entropy: 139.6610107421875, Temp: 1.9479566812515259, KL: 70.67791748046875, Loss: 0.03774330019950867, Learning Rate: 0.010289999999999997\n",
      "Epoch [3730/20000], Bound: 0.44064760208129883, Entropy: 137.31019592285156, Temp: 1.949107050895691, KL: 70.08168029785156, Loss: 0.0474853515625, Learning Rate: 0.010289999999999997\n",
      "Epoch [3731/20000], Bound: 0.4412975311279297, Entropy: 135.52203369140625, Temp: 1.9500187635421753, KL: 74.97468566894531, Loss: 0.03542403504252434, Learning Rate: 0.010289999999999997\n",
      "Epoch [3732/20000], Bound: 0.40855616331100464, Entropy: 136.90679931640625, Temp: 1.9510116577148438, KL: 66.76358032226562, Loss: 0.03384184464812279, Learning Rate: 0.010289999999999997\n",
      "Epoch [3733/20000], Bound: 0.44397032260894775, Entropy: 136.42762756347656, Temp: 1.9520227909088135, KL: 74.68197631835938, Loss: 0.03813525289297104, Learning Rate: 0.010289999999999997\n",
      "Epoch [3734/20000], Bound: 0.4151221513748169, Entropy: 136.7877960205078, Temp: 1.9530445337295532, KL: 66.75349426269531, Loss: 0.03838750347495079, Learning Rate: 0.010289999999999997\n",
      "Epoch [3735/20000], Bound: 0.40560004115104675, Entropy: 139.6999053955078, Temp: 1.9539852142333984, KL: 64.83413696289062, Loss: 0.036867622286081314, Learning Rate: 0.010289999999999997\n",
      "Epoch [3736/20000], Bound: 0.42061692476272583, Entropy: 139.63174438476562, Temp: 1.954862117767334, KL: 67.10939025878906, Loss: 0.04128899425268173, Learning Rate: 0.010289999999999997\n",
      "Epoch [3737/20000], Bound: 0.45001375675201416, Entropy: 138.73529052734375, Temp: 1.9556143283843994, KL: 75.30294799804688, Loss: 0.040969524532556534, Learning Rate: 0.010289999999999997\n",
      "Epoch [3738/20000], Bound: 0.4265519678592682, Entropy: 143.37774658203125, Temp: 1.9563472270965576, KL: 72.66209411621094, Loss: 0.03123064711689949, Learning Rate: 0.010289999999999997\n",
      "Epoch [3739/20000], Bound: 0.4542441666126251, Entropy: 142.23812866210938, Temp: 1.9572352170944214, KL: 80.54795837402344, Loss: 0.030656665563583374, Learning Rate: 0.010289999999999997\n",
      "Epoch [3740/20000], Bound: 0.4181312918663025, Entropy: 141.1796112060547, Temp: 1.9583529233932495, KL: 66.88206481933594, Loss: 0.040257714688777924, Learning Rate: 0.010289999999999997\n",
      "Epoch [3741/20000], Bound: 0.4563634693622589, Entropy: 142.13250732421875, Temp: 1.9593364000320435, KL: 79.71340942382812, Loss: 0.03439224138855934, Learning Rate: 0.010289999999999997\n",
      "Epoch [3742/20000], Bound: 0.4419342875480652, Entropy: 139.96780395507812, Temp: 1.9604532718658447, KL: 73.6719970703125, Loss: 0.03954280912876129, Learning Rate: 0.010289999999999997\n",
      "Epoch [3743/20000], Bound: 0.4473590552806854, Entropy: 140.53759765625, Temp: 1.961521029472351, KL: 76.40087890625, Loss: 0.036466196179389954, Learning Rate: 0.010289999999999997\n",
      "Epoch [3744/20000], Bound: 0.4125254154205322, Entropy: 142.6643524169922, Temp: 1.9626349210739136, KL: 65.06321716308594, Loss: 0.041186075657606125, Learning Rate: 0.010289999999999997\n",
      "Epoch [3745/20000], Bound: 0.48804306983947754, Entropy: 139.99485778808594, Temp: 1.9635709524154663, KL: 87.78919982910156, Loss: 0.03733178600668907, Learning Rate: 0.010289999999999997\n",
      "Epoch [3746/20000], Bound: 0.4662063419818878, Entropy: 137.55154418945312, Temp: 1.9646598100662231, KL: 81.70564270019531, Loss: 0.036653902381658554, Learning Rate: 0.010289999999999997\n",
      "Epoch [3747/20000], Bound: 0.46121928095817566, Entropy: 134.4951934814453, Temp: 1.9658390283584595, KL: 80.63609313964844, Loss: 0.03578903526067734, Learning Rate: 0.010289999999999997\n",
      "Epoch [3748/20000], Bound: 0.48592787981033325, Entropy: 131.39373779296875, Temp: 1.9671052694320679, KL: 88.609619140625, Loss: 0.03379951789975166, Learning Rate: 0.010289999999999997\n",
      "Epoch [3749/20000], Bound: 0.43842974305152893, Entropy: 131.2995147705078, Temp: 1.968566656112671, KL: 72.83309936523438, Loss: 0.03945724293589592, Learning Rate: 0.010289999999999997\n",
      "Epoch [3750/20000], Bound: 0.4373110830783844, Entropy: 132.431396484375, Temp: 1.96992826461792, KL: 74.26393127441406, Loss: 0.03508198633790016, Learning Rate: 0.010289999999999997\n",
      "Epoch [3751/20000], Bound: 0.45313864946365356, Entropy: 127.89557647705078, Temp: 1.971303939819336, KL: 79.10555267333984, Loss: 0.03405041992664337, Learning Rate: 0.010289999999999997\n",
      "Epoch [3752/20000], Bound: 0.4848191440105438, Entropy: 130.9457550048828, Temp: 1.9727612733840942, KL: 89.54838562011719, Loss: 0.03082602471113205, Learning Rate: 0.010289999999999997\n",
      "Epoch [3753/20000], Bound: 0.4705834984779358, Entropy: 128.5567626953125, Temp: 1.9744577407836914, KL: 81.30537414550781, Loss: 0.0412244088947773, Learning Rate: 0.010289999999999997\n",
      "Epoch [3754/20000], Bound: 0.45652562379837036, Entropy: 128.42105102539062, Temp: 1.9760748147964478, KL: 80.74493408203125, Loss: 0.03249689191579819, Learning Rate: 0.010289999999999997\n",
      "Epoch [3755/20000], Bound: 0.4535923898220062, Entropy: 128.75315856933594, Temp: 1.9777922630310059, KL: 78.55195617675781, Loss: 0.03600110113620758, Learning Rate: 0.010289999999999997\n",
      "Epoch [3756/20000], Bound: 0.40359601378440857, Entropy: 130.6656951904297, Temp: 1.9795042276382446, KL: 63.08738708496094, Loss: 0.04058827459812164, Learning Rate: 0.010289999999999997\n",
      "Epoch [3757/20000], Bound: 0.47769784927368164, Entropy: 130.6414031982422, Temp: 1.980950117111206, KL: 86.25224304199219, Loss: 0.03419848904013634, Learning Rate: 0.010289999999999997\n",
      "Epoch [3758/20000], Bound: 0.44711998105049133, Entropy: 131.8126983642578, Temp: 1.9825263023376465, KL: 77.13436889648438, Loss: 0.03513349965214729, Learning Rate: 0.010289999999999997\n",
      "Epoch [3759/20000], Bound: 0.43973901867866516, Entropy: 131.45440673828125, Temp: 1.9841090440750122, KL: 73.296630859375, Loss: 0.039660852402448654, Learning Rate: 0.010289999999999997\n",
      "Epoch [3760/20000], Bound: 0.45934855937957764, Entropy: 132.42001342773438, Temp: 1.9855620861053467, KL: 78.77006530761719, Loss: 0.03983841463923454, Learning Rate: 0.010289999999999997\n",
      "Epoch [3761/20000], Bound: 0.42373764514923096, Entropy: 132.46157836914062, Temp: 1.9869493246078491, KL: 71.61296081542969, Loss: 0.03290475159883499, Learning Rate: 0.010289999999999997\n",
      "Epoch [3762/20000], Bound: 0.4392552077770233, Entropy: 133.75830078125, Temp: 1.9883475303649902, KL: 74.59864807128906, Loss: 0.03617113456130028, Learning Rate: 0.010289999999999997\n",
      "Epoch [3763/20000], Bound: 0.4236069917678833, Entropy: 134.97879028320312, Temp: 1.9897160530090332, KL: 69.97366333007812, Loss: 0.037016209214925766, Learning Rate: 0.010289999999999997\n",
      "Epoch [3764/20000], Bound: 0.41658827662467957, Entropy: 137.7930908203125, Temp: 1.9909906387329102, KL: 68.52757263183594, Loss: 0.035906169563531876, Learning Rate: 0.010289999999999997\n",
      "Epoch [3765/20000], Bound: 0.40609943866729736, Entropy: 138.2422332763672, Temp: 1.9921873807907104, KL: 66.66529846191406, Loss: 0.03355969861149788, Learning Rate: 0.010289999999999997\n",
      "Epoch [3766/20000], Bound: 0.43910354375839233, Entropy: 140.4427032470703, Temp: 1.9933433532714844, KL: 70.61979675292969, Loss: 0.04619714617729187, Learning Rate: 0.010289999999999997\n",
      "Epoch [3767/20000], Bound: 0.4929037392139435, Entropy: 140.23191833496094, Temp: 1.9942361116409302, KL: 88.49575805664062, Loss: 0.04043027013540268, Learning Rate: 0.010289999999999997\n",
      "Epoch [3768/20000], Bound: 0.4686850607395172, Entropy: 140.73780822753906, Temp: 1.9951918125152588, KL: 82.15994262695312, Loss: 0.03840208053588867, Learning Rate: 0.010289999999999997\n",
      "Epoch [3769/20000], Bound: 0.3954916298389435, Entropy: 140.63099670410156, Temp: 1.996182918548584, KL: 63.27032470703125, Loss: 0.035141244530677795, Learning Rate: 0.010289999999999997\n",
      "Epoch [3770/20000], Bound: 0.42700281739234924, Entropy: 140.54946899414062, Temp: 1.9970794916152954, KL: 68.89085388183594, Loss: 0.042250026017427444, Learning Rate: 0.010289999999999997\n",
      "Epoch [3771/20000], Bound: 0.4837739169597626, Entropy: 139.02537536621094, Temp: 1.9977983236312866, KL: 89.01512145996094, Loss: 0.03243093565106392, Learning Rate: 0.010289999999999997\n",
      "Epoch [3772/20000], Bound: 0.4524171054363251, Entropy: 137.66500854492188, Temp: 1.9987632036209106, KL: 74.1094970703125, Loss: 0.04697175323963165, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3773/20000], Bound: 0.47708168625831604, Entropy: 137.68624877929688, Temp: 1.9994977712631226, KL: 84.77935791015625, Loss: 0.03814134746789932, Learning Rate: 0.010289999999999997\n",
      "Epoch [3774/20000], Bound: 0.44625476002693176, Entropy: 133.45855712890625, Temp: 2.0003159046173096, KL: 77.54327392578125, Loss: 0.03407364711165428, Learning Rate: 0.010289999999999997\n",
      "Epoch [3775/20000], Bound: 0.4331832230091095, Entropy: 135.7362518310547, Temp: 2.0012223720550537, KL: 72.1630859375, Loss: 0.038429491221904755, Learning Rate: 0.010289999999999997\n",
      "Epoch [3776/20000], Bound: 0.4172021150588989, Entropy: 133.52928161621094, Temp: 2.002060651779175, KL: 70.10209655761719, Loss: 0.03267891705036163, Learning Rate: 0.010289999999999997\n",
      "Epoch [3777/20000], Bound: 0.4255756437778473, Entropy: 127.87962341308594, Temp: 2.0029382705688477, KL: 71.79045104980469, Loss: 0.03417961671948433, Learning Rate: 0.010289999999999997\n",
      "Epoch [3778/20000], Bound: 0.44617247581481934, Entropy: 134.89353942871094, Temp: 2.0038349628448486, KL: 74.77415466308594, Loss: 0.0410379022359848, Learning Rate: 0.010289999999999997\n",
      "Epoch [3779/20000], Bound: 0.43488940596580505, Entropy: 135.2703094482422, Temp: 2.0046331882476807, KL: 74.29299926757812, Loss: 0.03439010679721832, Learning Rate: 0.010289999999999997\n",
      "Epoch [3780/20000], Bound: 0.4742899239063263, Entropy: 131.3855743408203, Temp: 2.0054779052734375, KL: 80.16110229492188, Loss: 0.04782109335064888, Learning Rate: 0.010289999999999997\n",
      "Epoch [3781/20000], Bound: 0.44158226251602173, Entropy: 132.40296936035156, Temp: 2.00614070892334, KL: 73.79765319824219, Loss: 0.040321316570043564, Learning Rate: 0.010289999999999997\n",
      "Epoch [3782/20000], Bound: 0.4896058142185211, Entropy: 133.65286254882812, Temp: 2.006730318069458, KL: 85.645263671875, Loss: 0.045529525727033615, Learning Rate: 0.010289999999999997\n",
      "Epoch [3783/20000], Bound: 0.4704735279083252, Entropy: 134.5533905029297, Temp: 2.0072646141052246, KL: 82.462158203125, Loss: 0.039358578622341156, Learning Rate: 0.010289999999999997\n",
      "Epoch [3784/20000], Bound: 0.4177227318286896, Entropy: 133.05154418945312, Temp: 2.007844924926758, KL: 68.61589050292969, Loss: 0.03689056262373924, Learning Rate: 0.010289999999999997\n",
      "Epoch [3785/20000], Bound: 0.4026618003845215, Entropy: 134.9598846435547, Temp: 2.0083775520324707, KL: 62.629913330078125, Loss: 0.041737932711839676, Learning Rate: 0.010289999999999997\n",
      "Epoch [3786/20000], Bound: 0.42983493208885193, Entropy: 135.0837860107422, Temp: 2.0086987018585205, KL: 72.12313842773438, Loss: 0.03642914071679115, Learning Rate: 0.010289999999999997\n",
      "Epoch [3787/20000], Bound: 0.4737457036972046, Entropy: 136.66392517089844, Temp: 2.0090434551239014, KL: 79.24591064453125, Loss: 0.04980788007378578, Learning Rate: 0.010289999999999997\n",
      "Epoch [3788/20000], Bound: 0.4706921875476837, Entropy: 133.8507537841797, Temp: 2.009200096130371, KL: 79.73603820800781, Loss: 0.046366434544324875, Learning Rate: 0.010289999999999997\n",
      "Epoch [3789/20000], Bound: 0.3987725079059601, Entropy: 135.398193359375, Temp: 2.0092642307281494, KL: 64.26373291015625, Loss: 0.03512360155582428, Learning Rate: 0.010289999999999997\n",
      "Epoch [3790/20000], Bound: 0.47888436913490295, Entropy: 137.03564453125, Temp: 2.0093231201171875, KL: 83.49983215332031, Loss: 0.04299961030483246, Learning Rate: 0.010289999999999997\n",
      "Epoch [3791/20000], Bound: 0.4313758611679077, Entropy: 133.96873474121094, Temp: 2.009406805038452, KL: 72.83721923828125, Loss: 0.03573061525821686, Learning Rate: 0.010289999999999997\n",
      "Epoch [3792/20000], Bound: 0.4485240876674652, Entropy: 138.11846923828125, Temp: 2.0095598697662354, KL: 75.965087890625, Loss: 0.03989032283425331, Learning Rate: 0.010289999999999997\n",
      "Epoch [3793/20000], Bound: 0.4216957092285156, Entropy: 137.15219116210938, Temp: 2.0097179412841797, KL: 70.87800598144531, Loss: 0.033999472856521606, Learning Rate: 0.010289999999999997\n",
      "Epoch [3794/20000], Bound: 0.4630977511405945, Entropy: 138.21624755859375, Temp: 2.0099546909332275, KL: 78.50689697265625, Loss: 0.04395335912704468, Learning Rate: 0.010289999999999997\n",
      "Epoch [3795/20000], Bound: 0.42721638083457947, Entropy: 137.2449188232422, Temp: 2.010127544403076, KL: 68.29811096191406, Loss: 0.04418889060616493, Learning Rate: 0.010289999999999997\n",
      "Epoch [3796/20000], Bound: 0.4193287193775177, Entropy: 134.63926696777344, Temp: 2.0101306438446045, KL: 67.88040161132812, Loss: 0.03986318036913872, Learning Rate: 0.010289999999999997\n",
      "Epoch [3797/20000], Bound: 0.4332273006439209, Entropy: 135.20504760742188, Temp: 2.0100691318511963, KL: 71.56246948242188, Loss: 0.04019523039460182, Learning Rate: 0.010289999999999997\n",
      "Epoch [3798/20000], Bound: 0.458987295627594, Entropy: 136.25636291503906, Temp: 2.009981393814087, KL: 77.74217224121094, Loss: 0.042908381670713425, Learning Rate: 0.010289999999999997\n",
      "Epoch [3799/20000], Bound: 0.4665967524051666, Entropy: 136.89906311035156, Temp: 2.0098764896392822, KL: 77.94981384277344, Loss: 0.047860369086265564, Learning Rate: 0.010289999999999997\n",
      "Epoch [3800/20000], Bound: 0.46200400590896606, Entropy: 132.15966796875, Temp: 2.0096535682678223, KL: 80.52180480957031, Loss: 0.03814510628581047, Learning Rate: 0.010289999999999997\n",
      "Epoch [3801/20000], Bound: 0.4913631081581116, Entropy: 133.13186645507812, Temp: 2.009556531906128, KL: 88.55511474609375, Loss: 0.03970222547650337, Learning Rate: 0.010289999999999997\n",
      "Epoch [3802/20000], Bound: 0.470855712890625, Entropy: 132.76756286621094, Temp: 2.0096187591552734, KL: 83.65219116210938, Loss: 0.036754436790943146, Learning Rate: 0.010289999999999997\n",
      "Epoch [3803/20000], Bound: 0.4734358489513397, Entropy: 134.8762969970703, Temp: 2.0098392963409424, KL: 82.52890014648438, Loss: 0.04143718630075455, Learning Rate: 0.010289999999999997\n",
      "Epoch [3804/20000], Bound: 0.4326012432575226, Entropy: 130.9381561279297, Temp: 2.0100913047790527, KL: 70.76200866699219, Loss: 0.041755352169275284, Learning Rate: 0.010289999999999997\n",
      "Epoch [3805/20000], Bound: 0.4015660285949707, Entropy: 131.17576599121094, Temp: 2.0102438926696777, KL: 63.04637145996094, Loss: 0.04001596197485924, Learning Rate: 0.010289999999999997\n",
      "Epoch [3806/20000], Bound: 0.44251590967178345, Entropy: 132.77024841308594, Temp: 2.010261058807373, KL: 70.51974487304688, Loss: 0.04923996329307556, Learning Rate: 0.010289999999999997\n",
      "Epoch [3807/20000], Bound: 0.475149542093277, Entropy: 132.3150177001953, Temp: 2.0100386142730713, KL: 83.78398132324219, Loss: 0.03957474231719971, Learning Rate: 0.010289999999999997\n",
      "Epoch [3808/20000], Bound: 0.436588853597641, Entropy: 131.76953125, Temp: 2.009944200515747, KL: 74.85792541503906, Loss: 0.034317515790462494, Learning Rate: 0.010289999999999997\n",
      "Epoch [3809/20000], Bound: 0.4604998528957367, Entropy: 128.56427001953125, Temp: 2.0099878311157227, KL: 79.42045593261719, Loss: 0.03981649875640869, Learning Rate: 0.010289999999999997\n",
      "Epoch [3810/20000], Bound: 0.4298079311847687, Entropy: 131.85801696777344, Temp: 2.0100839138031006, KL: 69.68034362792969, Loss: 0.04252507910132408, Learning Rate: 0.010289999999999997\n",
      "Epoch [3811/20000], Bound: 0.45561668276786804, Entropy: 132.5688018798828, Temp: 2.0100674629211426, KL: 79.01283264160156, Loss: 0.0373457595705986, Learning Rate: 0.010289999999999997\n",
      "Epoch [3812/20000], Bound: 0.4042758047580719, Entropy: 129.5689697265625, Temp: 2.0101583003997803, KL: 64.86068725585938, Loss: 0.03729446604847908, Learning Rate: 0.010289999999999997\n",
      "Epoch [3813/20000], Bound: 0.4534766972064972, Entropy: 129.72596740722656, Temp: 2.0101990699768066, KL: 76.87347412109375, Loss: 0.041150663048028946, Learning Rate: 0.010289999999999997\n",
      "Epoch [3814/20000], Bound: 0.5205656886100769, Entropy: 132.86874389648438, Temp: 2.010237693786621, KL: 95.37843322753906, Loss: 0.0451534129679203, Learning Rate: 0.010289999999999997\n",
      "Epoch [3815/20000], Bound: 0.4830892086029053, Entropy: 135.2241668701172, Temp: 2.010373830795288, KL: 85.34530639648438, Loss: 0.04154929891228676, Learning Rate: 0.010289999999999997\n",
      "Epoch [3816/20000], Bound: 0.4116264879703522, Entropy: 135.39306640625, Temp: 2.010575294494629, KL: 69.455322265625, Loss: 0.030774347484111786, Learning Rate: 0.010289999999999997\n",
      "Epoch [3817/20000], Bound: 0.4613117575645447, Entropy: 134.09950256347656, Temp: 2.0109076499938965, KL: 78.17770385742188, Loss: 0.043517377227544785, Learning Rate: 0.010289999999999997\n",
      "Epoch [3818/20000], Bound: 0.4659900367259979, Entropy: 135.92071533203125, Temp: 2.0111708641052246, KL: 82.37580871582031, Loss: 0.0364551804959774, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3819/20000], Bound: 0.4317978620529175, Entropy: 133.53465270996094, Temp: 2.0115652084350586, KL: 71.19204711914062, Loss: 0.04017077386379242, Learning Rate: 0.010289999999999997\n",
      "Epoch [3820/20000], Bound: 0.45281296968460083, Entropy: 136.34422302246094, Temp: 2.0118823051452637, KL: 79.15797424316406, Loss: 0.035051848739385605, Learning Rate: 0.010289999999999997\n",
      "Epoch [3821/20000], Bound: 0.416875958442688, Entropy: 138.4940643310547, Temp: 2.0123229026794434, KL: 69.80023193359375, Loss: 0.03348993882536888, Learning Rate: 0.010289999999999997\n",
      "Epoch [3822/20000], Bound: 0.4720339775085449, Entropy: 135.83624267578125, Temp: 2.0128121376037598, KL: 78.3187255859375, Loss: 0.050971075892448425, Learning Rate: 0.010289999999999997\n",
      "Epoch [3823/20000], Bound: 0.4237651824951172, Entropy: 135.70791625976562, Temp: 2.013057231903076, KL: 70.57894897460938, Loss: 0.036238837987184525, Learning Rate: 0.010289999999999997\n",
      "Epoch [3824/20000], Bound: 0.43532660603523254, Entropy: 132.79066467285156, Temp: 2.013317584991455, KL: 68.93394470214844, Loss: 0.04825635626912117, Learning Rate: 0.010289999999999997\n",
      "Epoch [3825/20000], Bound: 0.42016977071762085, Entropy: 134.23487854003906, Temp: 2.013312339782715, KL: 68.18785095214844, Loss: 0.03974463418126106, Learning Rate: 0.010289999999999997\n",
      "Epoch [3826/20000], Bound: 0.42204487323760986, Entropy: 132.42739868164062, Temp: 2.013244867324829, KL: 65.81173706054688, Loss: 0.046914804726839066, Learning Rate: 0.010289999999999997\n",
      "Epoch [3827/20000], Bound: 0.4599204957485199, Entropy: 134.03863525390625, Temp: 2.0129387378692627, KL: 76.25816345214844, Loss: 0.04734865203499794, Learning Rate: 0.010289999999999997\n",
      "Epoch [3828/20000], Bound: 0.46018120646476746, Entropy: 134.3507080078125, Temp: 2.0125224590301514, KL: 79.01210021972656, Loss: 0.04068189114332199, Learning Rate: 0.010289999999999997\n",
      "Epoch [3829/20000], Bound: 0.43968984484672546, Entropy: 134.88645935058594, Temp: 2.012179136276245, KL: 73.71231079101562, Loss: 0.03938228636980057, Learning Rate: 0.010289999999999997\n",
      "Epoch [3830/20000], Bound: 0.4334140419960022, Entropy: 134.61485290527344, Temp: 2.0118746757507324, KL: 69.37348937988281, Loss: 0.04581131041049957, Learning Rate: 0.010289999999999997\n",
      "Epoch [3831/20000], Bound: 0.4388093054294586, Entropy: 133.59791564941406, Temp: 2.0114200115203857, KL: 74.45391845703125, Loss: 0.036905739456415176, Learning Rate: 0.010289999999999997\n",
      "Epoch [3832/20000], Bound: 0.4444020390510559, Entropy: 136.74029541015625, Temp: 2.011078357696533, KL: 71.76033020019531, Loss: 0.04749361053109169, Learning Rate: 0.010289999999999997\n",
      "Epoch [3833/20000], Bound: 0.4457011818885803, Entropy: 137.04299926757812, Temp: 2.01058030128479, KL: 76.61570739746094, Loss: 0.03631746396422386, Learning Rate: 0.010289999999999997\n",
      "Epoch [3834/20000], Bound: 0.42343494296073914, Entropy: 135.71026611328125, Temp: 2.0102360248565674, KL: 70.03179931640625, Loss: 0.03730021417140961, Learning Rate: 0.010289999999999997\n",
      "Epoch [3835/20000], Bound: 0.41176220774650574, Entropy: 134.90126037597656, Temp: 2.009939670562744, KL: 69.74760437011719, Loss: 0.030120735988020897, Learning Rate: 0.010289999999999997\n",
      "Epoch [3836/20000], Bound: 0.4241527020931244, Entropy: 136.18307495117188, Temp: 2.009843349456787, KL: 69.67134094238281, Loss: 0.03867526352405548, Learning Rate: 0.010289999999999997\n",
      "Epoch [3837/20000], Bound: 0.4108157455921173, Entropy: 138.0102081298828, Temp: 2.0097367763519287, KL: 64.66993713378906, Loss: 0.04211437702178955, Learning Rate: 0.010289999999999997\n",
      "Epoch [3838/20000], Bound: 0.47087275981903076, Entropy: 143.887939453125, Temp: 2.009490966796875, KL: 81.55802917480469, Loss: 0.0419730469584465, Learning Rate: 0.010289999999999997\n",
      "Epoch [3839/20000], Bound: 0.46776267886161804, Entropy: 139.66790771484375, Temp: 2.009303331375122, KL: 82.81283569335938, Loss: 0.036586616188287735, Learning Rate: 0.010289999999999997\n",
      "Epoch [3840/20000], Bound: 0.41481247544288635, Entropy: 140.79490661621094, Temp: 2.0092971324920654, KL: 69.48957824707031, Loss: 0.032792191952466965, Learning Rate: 0.010289999999999997\n",
      "Epoch [3841/20000], Bound: 0.4045253396034241, Entropy: 138.14218139648438, Temp: 2.0094006061553955, KL: 64.65373229980469, Loss: 0.03795750439167023, Learning Rate: 0.010289999999999997\n",
      "Epoch [3842/20000], Bound: 0.43201231956481934, Entropy: 138.29275512695312, Temp: 2.0094358921051025, KL: 73.30792236328125, Loss: 0.03499826043844223, Learning Rate: 0.010289999999999997\n",
      "Epoch [3843/20000], Bound: 0.45315662026405334, Entropy: 139.55593872070312, Temp: 2.009567975997925, KL: 79.60089111328125, Loss: 0.034119024872779846, Learning Rate: 0.010289999999999997\n",
      "Epoch [3844/20000], Bound: 0.42366746068000793, Entropy: 139.12435913085938, Temp: 2.009871244430542, KL: 70.47831726074219, Loss: 0.03633815050125122, Learning Rate: 0.010289999999999997\n",
      "Epoch [3845/20000], Bound: 0.4565705955028534, Entropy: 136.493896484375, Temp: 2.0101847648620605, KL: 78.14564514160156, Loss: 0.04018570855259895, Learning Rate: 0.010289999999999997\n",
      "Epoch [3846/20000], Bound: 0.4973822236061096, Entropy: 137.31809997558594, Temp: 2.0105035305023193, KL: 90.4554443359375, Loss: 0.03954588249325752, Learning Rate: 0.010289999999999997\n",
      "Epoch [3847/20000], Bound: 0.4173473119735718, Entropy: 135.18870544433594, Temp: 2.0109641551971436, KL: 68.30647277832031, Loss: 0.037485454231500626, Learning Rate: 0.010289999999999997\n",
      "Epoch [3848/20000], Bound: 0.45735782384872437, Entropy: 136.18450927734375, Temp: 2.011369228363037, KL: 79.31369018554688, Loss: 0.03787906467914581, Learning Rate: 0.010289999999999997\n",
      "Epoch [3849/20000], Bound: 0.4422062635421753, Entropy: 133.50428771972656, Temp: 2.011831521987915, KL: 74.52578735351562, Loss: 0.03910413756966591, Learning Rate: 0.010289999999999997\n",
      "Epoch [3850/20000], Bound: 0.459601491689682, Entropy: 131.4140625, Temp: 2.01226806640625, KL: 80.81735229492188, Loss: 0.03577360138297081, Learning Rate: 0.010289999999999997\n",
      "Epoch [3851/20000], Bound: 0.43638303875923157, Entropy: 132.26495361328125, Temp: 2.0128183364868164, KL: 72.25645446777344, Loss: 0.04072241857647896, Learning Rate: 0.010289999999999997\n",
      "Epoch [3852/20000], Bound: 0.4377685487270355, Entropy: 132.15208435058594, Temp: 2.0132734775543213, KL: 72.96266174316406, Loss: 0.03994030877947807, Learning Rate: 0.010289999999999997\n",
      "Epoch [3853/20000], Bound: 0.46066153049468994, Entropy: 133.1812286376953, Temp: 2.013666868209839, KL: 74.89814758300781, Loss: 0.051275771111249924, Learning Rate: 0.010289999999999997\n",
      "Epoch [3854/20000], Bound: 0.4512070119380951, Entropy: 132.76283264160156, Temp: 2.013777732849121, KL: 74.04133605957031, Loss: 0.04668067768216133, Learning Rate: 0.010289999999999997\n",
      "Epoch [3855/20000], Bound: 0.40193575620651245, Entropy: 134.42050170898438, Temp: 2.0137248039245605, KL: 65.12692260742188, Loss: 0.035167258232831955, Learning Rate: 0.010289999999999997\n",
      "Epoch [3856/20000], Bound: 0.4535330533981323, Entropy: 131.24612426757812, Temp: 2.013681650161743, KL: 75.94590759277344, Loss: 0.043595317751169205, Learning Rate: 0.010289999999999997\n",
      "Epoch [3857/20000], Bound: 0.4391866624355316, Entropy: 135.29185485839844, Temp: 2.0135772228240967, KL: 74.46890258789062, Loss: 0.03719243034720421, Learning Rate: 0.010289999999999997\n",
      "Epoch [3858/20000], Bound: 0.432428777217865, Entropy: 139.23593139648438, Temp: 2.013543128967285, KL: 71.9832763671875, Loss: 0.03869114816188812, Learning Rate: 0.010289999999999997\n",
      "Epoch [3859/20000], Bound: 0.4474201202392578, Entropy: 139.24951171875, Temp: 2.01351261138916, KL: 75.55520629882812, Loss: 0.040246035903692245, Learning Rate: 0.010289999999999997\n",
      "Epoch [3860/20000], Bound: 0.47957146167755127, Entropy: 140.0813446044922, Temp: 2.013489246368408, KL: 83.67337036132812, Loss: 0.04321189969778061, Learning Rate: 0.010289999999999997\n",
      "Epoch [3861/20000], Bound: 0.4531426429748535, Entropy: 140.31996154785156, Temp: 2.013491630554199, KL: 74.50718688964844, Loss: 0.046886052936315536, Learning Rate: 0.010289999999999997\n",
      "Epoch [3862/20000], Bound: 0.4357365071773529, Entropy: 138.99012756347656, Temp: 2.0133414268493652, KL: 70.17767333984375, Loss: 0.04545150697231293, Learning Rate: 0.010289999999999997\n",
      "Epoch [3863/20000], Bound: 0.4780886769294739, Entropy: 139.3413848876953, Temp: 2.0130374431610107, KL: 80.39566040039062, Loss: 0.05024750530719757, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3864/20000], Bound: 0.44406041502952576, Entropy: 135.54515075683594, Temp: 2.0126020908355713, KL: 78.95286560058594, Loss: 0.02942298911511898, Learning Rate: 0.010289999999999997\n",
      "Epoch [3865/20000], Bound: 0.4248243272304535, Entropy: 134.2371826171875, Temp: 2.0124900341033936, KL: 70.11550903320312, Loss: 0.03809618577361107, Learning Rate: 0.010289999999999997\n",
      "Epoch [3866/20000], Bound: 0.46738287806510925, Entropy: 135.36279296875, Temp: 2.0123839378356934, KL: 82.64845275878906, Loss: 0.03682539984583855, Learning Rate: 0.010289999999999997\n",
      "Epoch [3867/20000], Bound: 0.4662153422832489, Entropy: 133.8622283935547, Temp: 2.0124423503875732, KL: 74.35537719726562, Loss: 0.05658797174692154, Learning Rate: 0.010289999999999997\n",
      "Epoch [3868/20000], Bound: 0.4528881013393402, Entropy: 131.2961883544922, Temp: 2.0121312141418457, KL: 74.94512939453125, Loss: 0.04558167979121208, Learning Rate: 0.010289999999999997\n",
      "Epoch [3869/20000], Bound: 0.42950016260147095, Entropy: 130.61424255371094, Temp: 2.011733055114746, KL: 71.32296752929688, Loss: 0.03827163577079773, Learning Rate: 0.010289999999999997\n",
      "Epoch [3870/20000], Bound: 0.4531107246875763, Entropy: 131.53012084960938, Temp: 2.0113790035247803, KL: 78.968017578125, Loss: 0.03571894019842148, Learning Rate: 0.010289999999999997\n",
      "Epoch [3871/20000], Bound: 0.5021930932998657, Entropy: 129.4056396484375, Temp: 2.011202812194824, KL: 93.8026123046875, Loss: 0.0349050909280777, Learning Rate: 0.010289999999999997\n",
      "Epoch [3872/20000], Bound: 0.4538649916648865, Entropy: 129.53562927246094, Temp: 2.0113515853881836, KL: 76.06768798828125, Loss: 0.043462932109832764, Learning Rate: 0.010289999999999997\n",
      "Epoch [3873/20000], Bound: 0.4592636227607727, Entropy: 132.25828552246094, Temp: 2.0114269256591797, KL: 78.57615661621094, Loss: 0.041075315326452255, Learning Rate: 0.010289999999999997\n",
      "Epoch [3874/20000], Bound: 0.45419490337371826, Entropy: 130.4720458984375, Temp: 2.011514902114868, KL: 77.34864807128906, Loss: 0.040517617017030716, Learning Rate: 0.010289999999999997\n",
      "Epoch [3875/20000], Bound: 0.42542243003845215, Entropy: 129.5897979736328, Temp: 2.011613607406616, KL: 69.21882629394531, Loss: 0.04071008041501045, Learning Rate: 0.010289999999999997\n",
      "Epoch [3876/20000], Bound: 0.4250432848930359, Entropy: 133.85580444335938, Temp: 2.011630058288574, KL: 71.28196716308594, Loss: 0.03532394394278526, Learning Rate: 0.010289999999999997\n",
      "Epoch [3877/20000], Bound: 0.4700263440608978, Entropy: 133.89744567871094, Temp: 2.0117149353027344, KL: 83.17352294921875, Loss: 0.03741402551531792, Learning Rate: 0.010289999999999997\n",
      "Epoch [3878/20000], Bound: 0.4304317831993103, Entropy: 135.083251953125, Temp: 2.011939764022827, KL: 72.05404663085938, Loss: 0.037099484354257584, Learning Rate: 0.010289999999999997\n",
      "Epoch [3879/20000], Bound: 0.4802336096763611, Entropy: 134.90396118164062, Temp: 2.012180805206299, KL: 83.27391052246094, Loss: 0.04464923217892647, Learning Rate: 0.010289999999999997\n",
      "Epoch [3880/20000], Bound: 0.43466129899024963, Entropy: 136.63343811035156, Temp: 2.012387752532959, KL: 67.92640686035156, Loss: 0.050279226154088974, Learning Rate: 0.010289999999999997\n",
      "Epoch [3881/20000], Bound: 0.42422229051589966, Entropy: 138.906982421875, Temp: 2.0122733116149902, KL: 68.87261962890625, Loss: 0.04076891764998436, Learning Rate: 0.010289999999999997\n",
      "Epoch [3882/20000], Bound: 0.45749905705451965, Entropy: 137.40283203125, Temp: 2.012091636657715, KL: 79.24267578125, Loss: 0.03817908465862274, Learning Rate: 0.010289999999999997\n",
      "Epoch [3883/20000], Bound: 0.46293142437934875, Entropy: 139.1656036376953, Temp: 2.0120186805725098, KL: 81.78221130371094, Loss: 0.035755500197410583, Learning Rate: 0.010289999999999997\n",
      "Epoch [3884/20000], Bound: 0.4388098418712616, Entropy: 137.91136169433594, Temp: 2.0121233463287354, KL: 72.73237609863281, Loss: 0.04120422899723053, Learning Rate: 0.010289999999999997\n",
      "Epoch [3885/20000], Bound: 0.4173527657985687, Entropy: 138.61570739746094, Temp: 2.012171745300293, KL: 67.20993041992188, Loss: 0.04024389758706093, Learning Rate: 0.010289999999999997\n",
      "Epoch [3886/20000], Bound: 0.4653875231742859, Entropy: 138.89527893066406, Temp: 2.012129783630371, KL: 78.88615417480469, Loss: 0.044724367558956146, Learning Rate: 0.010289999999999997\n",
      "Epoch [3887/20000], Bound: 0.44758883118629456, Entropy: 135.96435546875, Temp: 2.0120344161987305, KL: 76.95245361328125, Loss: 0.036850251257419586, Learning Rate: 0.010289999999999997\n",
      "Epoch [3888/20000], Bound: 0.4712204039096832, Entropy: 137.4038543701172, Temp: 2.012044906616211, KL: 79.89860534667969, Loss: 0.04643147438764572, Learning Rate: 0.010289999999999997\n",
      "Epoch [3889/20000], Bound: 0.4461820125579834, Entropy: 134.827880859375, Temp: 2.011969804763794, KL: 73.74710083007812, Loss: 0.043825700879096985, Learning Rate: 0.010289999999999997\n",
      "Epoch [3890/20000], Bound: 0.43094009160995483, Entropy: 135.48544311523438, Temp: 2.0118091106414795, KL: 72.99336242675781, Loss: 0.035110510885715485, Learning Rate: 0.010289999999999997\n",
      "Epoch [3891/20000], Bound: 0.43131640553474426, Entropy: 133.5724639892578, Temp: 2.011758327484131, KL: 74.04788208007812, Loss: 0.03274684399366379, Learning Rate: 0.010289999999999997\n",
      "Epoch [3892/20000], Bound: 0.4801661968231201, Entropy: 132.29554748535156, Temp: 2.011871337890625, KL: 81.24261474609375, Loss: 0.04963776469230652, Learning Rate: 0.010289999999999997\n",
      "Epoch [3893/20000], Bound: 0.4145529866218567, Entropy: 135.69097900390625, Temp: 2.0118327140808105, KL: 66.69831848144531, Loss: 0.039622850716114044, Learning Rate: 0.010289999999999997\n",
      "Epoch [3894/20000], Bound: 0.4526387155056, Entropy: 133.9308319091797, Temp: 2.011721134185791, KL: 75.09320068359375, Loss: 0.04502594843506813, Learning Rate: 0.010289999999999997\n",
      "Epoch [3895/20000], Bound: 0.4447324872016907, Entropy: 132.1424560546875, Temp: 2.0115158557891846, KL: 72.71925354003906, Loss: 0.045352108776569366, Learning Rate: 0.010289999999999997\n",
      "Epoch [3896/20000], Bound: 0.4251805245876312, Entropy: 134.90899658203125, Temp: 2.011192560195923, KL: 69.93099975585938, Loss: 0.038764212280511856, Learning Rate: 0.010289999999999997\n",
      "Epoch [3897/20000], Bound: 0.45065298676490784, Entropy: 133.53736877441406, Temp: 2.010880470275879, KL: 74.65745544433594, Loss: 0.04468178749084473, Learning Rate: 0.010289999999999997\n",
      "Epoch [3898/20000], Bound: 0.4105806052684784, Entropy: 135.3751678466797, Temp: 2.010498285293579, KL: 65.47496032714844, Loss: 0.039971545338630676, Learning Rate: 0.010289999999999997\n",
      "Epoch [3899/20000], Bound: 0.441388338804245, Entropy: 137.09852600097656, Temp: 2.010056734085083, KL: 73.91217041015625, Loss: 0.040009573101997375, Learning Rate: 0.010289999999999997\n",
      "Epoch [3900/20000], Bound: 0.45717853307724, Entropy: 136.10169982910156, Temp: 2.009655237197876, KL: 73.61631774902344, Loss: 0.05187198892235756, Learning Rate: 0.010289999999999997\n",
      "Epoch [3901/20000], Bound: 0.43801170587539673, Entropy: 136.49349975585938, Temp: 2.0090231895446777, KL: 69.26344299316406, Loss: 0.04920044168829918, Learning Rate: 0.010289999999999997\n",
      "Epoch [3902/20000], Bound: 0.45326733589172363, Entropy: 136.3289337158203, Temp: 2.0081942081451416, KL: 71.96946716308594, Loss: 0.05315319076180458, Learning Rate: 0.010289999999999997\n",
      "Epoch [3903/20000], Bound: 0.4678511321544647, Entropy: 138.42230224609375, Temp: 2.0071322917938232, KL: 75.98681640625, Loss: 0.05358058959245682, Learning Rate: 0.010289999999999997\n",
      "Epoch [3904/20000], Bound: 0.412858247756958, Entropy: 138.74224853515625, Temp: 2.0058984756469727, KL: 61.77790832519531, Loss: 0.050609245896339417, Learning Rate: 0.010289999999999997\n",
      "Epoch [3905/20000], Bound: 0.47197720408439636, Entropy: 134.8523712158203, Temp: 2.0044124126434326, KL: 76.32810974121094, Loss: 0.0556599497795105, Learning Rate: 0.010289999999999997\n",
      "Epoch [3906/20000], Bound: 0.39856988191604614, Entropy: 140.58448791503906, Temp: 2.0027592182159424, KL: 60.425872802734375, Loss: 0.044419486075639725, Learning Rate: 0.010289999999999997\n",
      "Epoch [3907/20000], Bound: 0.46156877279281616, Entropy: 135.41099548339844, Temp: 2.0010228157043457, KL: 77.01266479492188, Loss: 0.04631794989109039, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3908/20000], Bound: 0.43373793363571167, Entropy: 138.22938537597656, Temp: 1.9993606805801392, KL: 69.02871704101562, Loss: 0.0466001071035862, Learning Rate: 0.010289999999999997\n",
      "Epoch [3909/20000], Bound: 0.4793468415737152, Entropy: 137.0428466796875, Temp: 1.9976716041564941, KL: 80.83056640625, Loss: 0.049629755318164825, Learning Rate: 0.010289999999999997\n",
      "Epoch [3910/20000], Bound: 0.4581087827682495, Entropy: 133.19131469726562, Temp: 1.996025562286377, KL: 76.989501953125, Loss: 0.04374568909406662, Learning Rate: 0.010289999999999997\n",
      "Epoch [3911/20000], Bound: 0.4433469772338867, Entropy: 135.0391082763672, Temp: 1.994507908821106, KL: 75.07182312011719, Loss: 0.03803595155477524, Learning Rate: 0.010289999999999997\n",
      "Epoch [3912/20000], Bound: 0.4737103283405304, Entropy: 136.7179718017578, Temp: 1.9932140111923218, KL: 78.50546264648438, Loss: 0.05117331072688103, Learning Rate: 0.010289999999999997\n",
      "Epoch [3913/20000], Bound: 0.46717292070388794, Entropy: 136.74496459960938, Temp: 1.9918702840805054, KL: 79.72477722167969, Loss: 0.04329986497759819, Learning Rate: 0.010289999999999997\n",
      "Epoch [3914/20000], Bound: 0.4715189039707184, Entropy: 135.2346954345703, Temp: 1.9906699657440186, KL: 81.81645202636719, Loss: 0.04117705672979355, Learning Rate: 0.010289999999999997\n",
      "Epoch [3915/20000], Bound: 0.48416614532470703, Entropy: 131.5986328125, Temp: 1.9896697998046875, KL: 87.60269165039062, Loss: 0.03594278544187546, Learning Rate: 0.010289999999999997\n",
      "Epoch [3916/20000], Bound: 0.49109235405921936, Entropy: 133.55381774902344, Temp: 1.9890276193618774, KL: 84.91006469726562, Loss: 0.04787677526473999, Learning Rate: 0.010289999999999997\n",
      "Epoch [3917/20000], Bound: 0.5019938945770264, Entropy: 128.69984436035156, Temp: 1.9884185791015625, KL: 85.73292541503906, Loss: 0.05406741052865982, Learning Rate: 0.010289999999999997\n",
      "Epoch [3918/20000], Bound: 0.44851475954055786, Entropy: 131.33334350585938, Temp: 1.9877146482467651, KL: 75.28910827636719, Loss: 0.04093140363693237, Learning Rate: 0.010289999999999997\n",
      "Epoch [3919/20000], Bound: 0.43310579657554626, Entropy: 128.41737365722656, Temp: 1.9870996475219727, KL: 71.94314575195312, Loss: 0.03853264078497887, Learning Rate: 0.010289999999999997\n",
      "Epoch [3920/20000], Bound: 0.4484749734401703, Entropy: 128.20896911621094, Temp: 1.9865821599960327, KL: 72.27365112304688, Loss: 0.04845896735787392, Learning Rate: 0.010289999999999997\n",
      "Epoch [3921/20000], Bound: 0.46433961391448975, Entropy: 127.8771743774414, Temp: 1.9859347343444824, KL: 78.40950775146484, Loss: 0.044362783432006836, Learning Rate: 0.010289999999999997\n",
      "Epoch [3922/20000], Bound: 0.432696133852005, Entropy: 125.92638397216797, Temp: 1.9853311777114868, KL: 71.32807159423828, Loss: 0.03974738344550133, Learning Rate: 0.010289999999999997\n",
      "Epoch [3923/20000], Bound: 0.45756083726882935, Entropy: 127.09806060791016, Temp: 1.9847919940948486, KL: 80.73021697998047, Loss: 0.03359019756317139, Learning Rate: 0.010289999999999997\n",
      "Epoch [3924/20000], Bound: 0.4506608545780182, Entropy: 130.5074462890625, Temp: 1.9845528602600098, KL: 75.3660888671875, Loss: 0.04216635599732399, Learning Rate: 0.010289999999999997\n",
      "Epoch [3925/20000], Bound: 0.452266126871109, Entropy: 131.61500549316406, Temp: 1.984333872795105, KL: 75.79878234863281, Loss: 0.04221228137612343, Learning Rate: 0.010289999999999997\n",
      "Epoch [3926/20000], Bound: 0.4074665904045105, Entropy: 132.13291931152344, Temp: 1.984136939048767, KL: 67.27157592773438, Loss: 0.032736193388700485, Learning Rate: 0.010289999999999997\n",
      "Epoch [3927/20000], Bound: 0.4466724991798401, Entropy: 134.18637084960938, Temp: 1.9840788841247559, KL: 75.45077514648438, Loss: 0.039110567420721054, Learning Rate: 0.010289999999999997\n",
      "Epoch [3928/20000], Bound: 0.45900577306747437, Entropy: 136.23104858398438, Temp: 1.9840930700302124, KL: 74.37669372558594, Loss: 0.05061526224017143, Learning Rate: 0.010289999999999997\n",
      "Epoch [3929/20000], Bound: 0.4529588222503662, Entropy: 135.09820556640625, Temp: 1.9839035272598267, KL: 75.99668884277344, Loss: 0.04219447076320648, Learning Rate: 0.010289999999999997\n",
      "Epoch [3930/20000], Bound: 0.4954547882080078, Entropy: 136.78627014160156, Temp: 1.9837361574172974, KL: 84.43191528320312, Loss: 0.05219872295856476, Learning Rate: 0.010289999999999997\n",
      "Epoch [3931/20000], Bound: 0.45132386684417725, Entropy: 138.16844177246094, Temp: 1.9834624528884888, KL: 77.58100891113281, Loss: 0.03702206909656525, Learning Rate: 0.010289999999999997\n",
      "Epoch [3932/20000], Bound: 0.4334646165370941, Entropy: 138.33116149902344, Temp: 1.9833537340164185, KL: 72.57164001464844, Loss: 0.0370907336473465, Learning Rate: 0.010289999999999997\n",
      "Epoch [3933/20000], Bound: 0.46366479992866516, Entropy: 137.48004150390625, Temp: 1.9833368062973022, KL: 76.75813293457031, Loss: 0.0479554645717144, Learning Rate: 0.010289999999999997\n",
      "Epoch [3934/20000], Bound: 0.4441501498222351, Entropy: 140.2518768310547, Temp: 1.9832059144973755, KL: 73.532958984375, Loss: 0.04213806986808777, Learning Rate: 0.010289999999999997\n",
      "Epoch [3935/20000], Bound: 0.44731366634368896, Entropy: 137.78988647460938, Temp: 1.9830659627914429, KL: 73.69354248046875, Loss: 0.04396384581923485, Learning Rate: 0.010289999999999997\n",
      "Epoch [3936/20000], Bound: 0.4518211781978607, Entropy: 136.05140686035156, Temp: 1.9828789234161377, KL: 75.79917907714844, Loss: 0.04185028001666069, Learning Rate: 0.010289999999999997\n",
      "Epoch [3937/20000], Bound: 0.4546661972999573, Entropy: 133.02439880371094, Temp: 1.9827208518981934, KL: 74.91403198242188, Loss: 0.04610885679721832, Learning Rate: 0.010289999999999997\n",
      "Epoch [3938/20000], Bound: 0.43238401412963867, Entropy: 132.92079162597656, Temp: 1.9824838638305664, KL: 70.70191955566406, Loss: 0.04103086516261101, Learning Rate: 0.010289999999999997\n",
      "Epoch [3939/20000], Bound: 0.4003499746322632, Entropy: 133.3317413330078, Temp: 1.9822417497634888, KL: 64.002685546875, Loss: 0.03618043288588524, Learning Rate: 0.010289999999999997\n",
      "Epoch [3940/20000], Bound: 0.4501868784427643, Entropy: 134.49093627929688, Temp: 1.9820287227630615, KL: 75.28700256347656, Loss: 0.04195321351289749, Learning Rate: 0.010289999999999997\n",
      "Epoch [3941/20000], Bound: 0.4948081076145172, Entropy: 132.4806671142578, Temp: 1.9818403720855713, KL: 86.227294921875, Loss: 0.047114208340644836, Learning Rate: 0.010289999999999997\n",
      "Epoch [3942/20000], Bound: 0.4653269052505493, Entropy: 134.0106964111328, Temp: 1.9816808700561523, KL: 77.77285766601562, Loss: 0.04655253142118454, Learning Rate: 0.010289999999999997\n",
      "Epoch [3943/20000], Bound: 0.45318546891212463, Entropy: 135.10037231445312, Temp: 1.9814664125442505, KL: 79.94598388671875, Loss: 0.032316721975803375, Learning Rate: 0.010289999999999997\n",
      "Epoch [3944/20000], Bound: 0.48781928420066833, Entropy: 132.926025390625, Temp: 1.9815468788146973, KL: 85.79405212402344, Loss: 0.042921505868434906, Learning Rate: 0.010289999999999997\n",
      "Epoch [3945/20000], Bound: 0.43018656969070435, Entropy: 134.02847290039062, Temp: 1.9817179441452026, KL: 70.83659362792969, Loss: 0.0391482375562191, Learning Rate: 0.010289999999999997\n",
      "Epoch [3946/20000], Bound: 0.406647264957428, Entropy: 132.8125457763672, Temp: 1.9818886518478394, KL: 67.86697387695312, Loss: 0.030623989179730415, Learning Rate: 0.010289999999999997\n",
      "Epoch [3947/20000], Bound: 0.4358367919921875, Entropy: 134.65316772460938, Temp: 1.982221245765686, KL: 71.2000732421875, Loss: 0.04216749966144562, Learning Rate: 0.010289999999999997\n",
      "Epoch [3948/20000], Bound: 0.42231717705726624, Entropy: 133.33644104003906, Temp: 1.98247230052948, KL: 70.89410400390625, Loss: 0.03361218050122261, Learning Rate: 0.010289999999999997\n",
      "Epoch [3949/20000], Bound: 0.4782455861568451, Entropy: 131.3715362548828, Temp: 1.982841968536377, KL: 84.65960693359375, Loss: 0.03869391232728958, Learning Rate: 0.010289999999999997\n",
      "Epoch [3950/20000], Bound: 0.4665578305721283, Entropy: 134.15966796875, Temp: 1.983353853225708, KL: 82.99957275390625, Loss: 0.034321852028369904, Learning Rate: 0.010289999999999997\n",
      "Epoch [3951/20000], Bound: 0.45491650700569153, Entropy: 132.22705078125, Temp: 1.9840738773345947, KL: 75.50251770019531, Loss: 0.04484359547495842, Learning Rate: 0.010289999999999997\n",
      "Epoch [3952/20000], Bound: 0.4604235589504242, Entropy: 134.7261505126953, Temp: 1.984660267829895, KL: 77.26097106933594, Loss: 0.044385332614183426, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3953/20000], Bound: 0.41588497161865234, Entropy: 134.2902374267578, Temp: 1.985155701637268, KL: 67.86959838867188, Loss: 0.03693026304244995, Learning Rate: 0.010289999999999997\n",
      "Epoch [3954/20000], Bound: 0.42635011672973633, Entropy: 134.5387725830078, Temp: 1.9856311082839966, KL: 70.968017578125, Loss: 0.036282017827034, Learning Rate: 0.010289999999999997\n",
      "Epoch [3955/20000], Bound: 0.42136505246162415, Entropy: 134.6164093017578, Temp: 1.9861384630203247, KL: 70.08180236816406, Loss: 0.03511336073279381, Learning Rate: 0.010289999999999997\n",
      "Epoch [3956/20000], Bound: 0.43598228693008423, Entropy: 136.8141632080078, Temp: 1.986690878868103, KL: 72.81929016113281, Loss: 0.038314759731292725, Learning Rate: 0.010289999999999997\n",
      "Epoch [3957/20000], Bound: 0.4294840693473816, Entropy: 135.99398803710938, Temp: 1.9872406721115112, KL: 72.73703002929688, Loss: 0.03403420373797417, Learning Rate: 0.010289999999999997\n",
      "Epoch [3958/20000], Bound: 0.4304291903972626, Entropy: 135.96067810058594, Temp: 1.9878848791122437, KL: 71.01048278808594, Loss: 0.03904854133725166, Learning Rate: 0.010289999999999997\n",
      "Epoch [3959/20000], Bound: 0.4444337785243988, Entropy: 137.53831481933594, Temp: 1.988478422164917, KL: 75.92558288574219, Loss: 0.03647185489535332, Learning Rate: 0.010289999999999997\n",
      "Epoch [3960/20000], Bound: 0.44944700598716736, Entropy: 137.46463012695312, Temp: 1.9891400337219238, KL: 78.93923950195312, Loss: 0.03245924785733223, Learning Rate: 0.010289999999999997\n",
      "Epoch [3961/20000], Bound: 0.4052882194519043, Entropy: 138.78887939453125, Temp: 1.9899874925613403, KL: 67.07939147949219, Loss: 0.031920380890369415, Learning Rate: 0.010289999999999997\n",
      "Epoch [3962/20000], Bound: 0.4795383810997009, Entropy: 137.40516662597656, Temp: 1.9908819198608398, KL: 83.71760559082031, Loss: 0.042310308665037155, Learning Rate: 0.010289999999999997\n",
      "Epoch [3963/20000], Bound: 0.43491846323013306, Entropy: 135.217529296875, Temp: 1.9917645454406738, KL: 75.03094482421875, Loss: 0.03216955065727234, Learning Rate: 0.010289999999999997\n",
      "Epoch [3964/20000], Bound: 0.46378108859062195, Entropy: 137.39190673828125, Temp: 1.992772102355957, KL: 79.98410034179688, Loss: 0.04021952673792839, Learning Rate: 0.010289999999999997\n",
      "Epoch [3965/20000], Bound: 0.46810194849967957, Entropy: 137.3773651123047, Temp: 1.993760585784912, KL: 83.00552368164062, Loss: 0.035807788372039795, Learning Rate: 0.010289999999999997\n",
      "Epoch [3966/20000], Bound: 0.42866602540016174, Entropy: 136.3315887451172, Temp: 1.994863748550415, KL: 73.64906311035156, Loss: 0.03141139820218086, Learning Rate: 0.010289999999999997\n",
      "Epoch [3967/20000], Bound: 0.471825510263443, Entropy: 134.9534149169922, Temp: 1.9960685968399048, KL: 84.19009399414062, Loss: 0.0356370210647583, Learning Rate: 0.010289999999999997\n",
      "Epoch [3968/20000], Bound: 0.4077361822128296, Entropy: 135.11814880371094, Temp: 1.997380256652832, KL: 66.38613891601562, Loss: 0.03548506274819374, Learning Rate: 0.010289999999999997\n",
      "Epoch [3969/20000], Bound: 0.4633873403072357, Entropy: 135.55328369140625, Temp: 1.9985928535461426, KL: 83.30636596679688, Loss: 0.03181308135390282, Learning Rate: 0.010289999999999997\n",
      "Epoch [3970/20000], Bound: 0.4429129660129547, Entropy: 134.80667114257812, Temp: 1.9999864101409912, KL: 77.90950012207031, Loss: 0.03080049157142639, Learning Rate: 0.010289999999999997\n",
      "Epoch [3971/20000], Bound: 0.4519543945789337, Entropy: 134.49017333984375, Temp: 2.00150728225708, KL: 78.66261291503906, Loss: 0.035342659801244736, Learning Rate: 0.010289999999999997\n",
      "Epoch [3972/20000], Bound: 0.4793141782283783, Entropy: 130.6591339111328, Temp: 2.0030441284179688, KL: 87.86921691894531, Loss: 0.032199714332818985, Learning Rate: 0.010289999999999997\n",
      "Epoch [3973/20000], Bound: 0.4253445863723755, Entropy: 132.03016662597656, Temp: 2.004762649536133, KL: 71.86491394042969, Loss: 0.0338875949382782, Learning Rate: 0.010289999999999997\n",
      "Epoch [3974/20000], Bound: 0.4299670457839966, Entropy: 132.73129272460938, Temp: 2.00643253326416, KL: 73.57768249511719, Loss: 0.032832443714141846, Learning Rate: 0.010289999999999997\n",
      "Epoch [3975/20000], Bound: 0.49630966782569885, Entropy: 128.95960998535156, Temp: 2.0081002712249756, KL: 91.11436462402344, Loss: 0.03700090944766998, Learning Rate: 0.010289999999999997\n",
      "Epoch [3976/20000], Bound: 0.45313453674316406, Entropy: 130.30783081054688, Temp: 2.0098538398742676, KL: 80.54595947265625, Loss: 0.03176160529255867, Learning Rate: 0.010289999999999997\n",
      "Epoch [3977/20000], Bound: 0.3886341452598572, Entropy: 130.92417907714844, Temp: 2.011693239212036, KL: 61.65898132324219, Loss: 0.03502866253256798, Learning Rate: 0.010289999999999997\n",
      "Epoch [3978/20000], Bound: 0.44403424859046936, Entropy: 130.701171875, Temp: 2.013319969177246, KL: 73.39605712890625, Loss: 0.04322870075702667, Learning Rate: 0.010289999999999997\n",
      "Epoch [3979/20000], Bound: 0.41259774565696716, Entropy: 132.57330322265625, Temp: 2.0146961212158203, KL: 68.67630004882812, Loss: 0.03346952423453331, Learning Rate: 0.010289999999999997\n",
      "Epoch [3980/20000], Bound: 0.44218897819519043, Entropy: 135.50169372558594, Temp: 2.0160200595855713, KL: 76.01553344726562, Loss: 0.03551485016942024, Learning Rate: 0.010289999999999997\n",
      "Epoch [3981/20000], Bound: 0.4260692596435547, Entropy: 133.57583618164062, Temp: 2.0173285007476807, KL: 71.07986450195312, Loss: 0.03667736053466797, Learning Rate: 0.010289999999999997\n",
      "Epoch [3982/20000], Bound: 0.43766024708747864, Entropy: 138.0530242919922, Temp: 2.0185396671295166, KL: 75.1649169921875, Loss: 0.03455118462443352, Learning Rate: 0.010289999999999997\n",
      "Epoch [3983/20000], Bound: 0.4743204116821289, Entropy: 136.1498565673828, Temp: 2.019757032394409, KL: 86.17890930175781, Loss: 0.03336857259273529, Learning Rate: 0.010289999999999997\n",
      "Epoch [3984/20000], Bound: 0.418322890996933, Entropy: 138.37136840820312, Temp: 2.0211222171783447, KL: 71.17182922363281, Loss: 0.031303029507398605, Learning Rate: 0.010289999999999997\n",
      "Epoch [3985/20000], Bound: 0.4490165710449219, Entropy: 135.98541259765625, Temp: 2.022508382797241, KL: 75.19410705566406, Loss: 0.04251265898346901, Learning Rate: 0.010289999999999997\n",
      "Epoch [3986/20000], Bound: 0.4313732087612152, Entropy: 135.49537658691406, Temp: 2.0236923694610596, KL: 69.59858703613281, Loss: 0.04412584379315376, Learning Rate: 0.010289999999999997\n",
      "Epoch [3987/20000], Bound: 0.42432209849357605, Entropy: 139.18882751464844, Temp: 2.0245914459228516, KL: 70.53608703613281, Loss: 0.03702294081449509, Learning Rate: 0.010289999999999997\n",
      "Epoch [3988/20000], Bound: 0.4870637059211731, Entropy: 135.3383026123047, Temp: 2.0254111289978027, KL: 85.72517395019531, Loss: 0.044065918773412704, Learning Rate: 0.010289999999999997\n",
      "Epoch [3989/20000], Bound: 0.40175721049308777, Entropy: 134.6949005126953, Temp: 2.0261597633361816, KL: 66.61257934570312, Loss: 0.031670134514570236, Learning Rate: 0.010289999999999997\n",
      "Epoch [3990/20000], Bound: 0.46651530265808105, Entropy: 139.67431640625, Temp: 2.0269253253936768, KL: 81.75650024414062, Loss: 0.03888464719057083, Learning Rate: 0.010289999999999997\n",
      "Epoch [3991/20000], Bound: 0.4755738377571106, Entropy: 134.11244201660156, Temp: 2.027700662612915, KL: 86.60493469238281, Loss: 0.0335182324051857, Learning Rate: 0.010289999999999997\n",
      "Epoch [3992/20000], Bound: 0.45015016198158264, Entropy: 134.2660369873047, Temp: 2.0286593437194824, KL: 76.1468505859375, Loss: 0.04112603887915611, Learning Rate: 0.010289999999999997\n",
      "Epoch [3993/20000], Bound: 0.4226970374584198, Entropy: 134.169921875, Temp: 2.0294933319091797, KL: 67.93449401855469, Loss: 0.04245650768280029, Learning Rate: 0.010289999999999997\n",
      "Epoch [3994/20000], Bound: 0.46453016996383667, Entropy: 134.3301544189453, Temp: 2.0300893783569336, KL: 82.35304260253906, Loss: 0.036089394241571426, Learning Rate: 0.010289999999999997\n",
      "Epoch [3995/20000], Bound: 0.46234655380249023, Entropy: 133.80892944335938, Temp: 2.0307796001434326, KL: 81.00247192382812, Loss: 0.03787345439195633, Learning Rate: 0.010289999999999997\n",
      "Epoch [3996/20000], Bound: 0.42907604575157166, Entropy: 137.69415283203125, Temp: 2.0314977169036865, KL: 69.98455810546875, Loss: 0.04178629815578461, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3997/20000], Bound: 0.4659661650657654, Entropy: 132.01443481445312, Temp: 2.032026529312134, KL: 82.1917724609375, Loss: 0.03758002817630768, Learning Rate: 0.010289999999999997\n",
      "Epoch [3998/20000], Bound: 0.45351362228393555, Entropy: 132.7444610595703, Temp: 2.0326170921325684, KL: 80.81929016113281, Loss: 0.0321088545024395, Learning Rate: 0.010289999999999997\n",
      "Epoch [3999/20000], Bound: 0.4673745036125183, Entropy: 131.54507446289062, Temp: 2.033376932144165, KL: 84.51051330566406, Loss: 0.03293415531516075, Learning Rate: 0.010289999999999997\n",
      "Epoch [4000/20000], Bound: 0.4494342803955078, Entropy: 130.07687377929688, Temp: 2.0343070030212402, KL: 76.85050964355469, Loss: 0.039049286395311356, Learning Rate: 0.010289999999999997\n",
      "Epoch [4001/20000], Bound: 0.4363577365875244, Entropy: 135.41542053222656, Temp: 2.0351645946502686, KL: 74.1728515625, Loss: 0.036568719893693924, Learning Rate: 0.010289999999999997\n",
      "Epoch [4002/20000], Bound: 0.43448299169540405, Entropy: 132.95738220214844, Temp: 2.035984516143799, KL: 75.34046936035156, Loss: 0.03243516758084297, Learning Rate: 0.010289999999999997\n",
      "Epoch [4003/20000], Bound: 0.42592599987983704, Entropy: 133.56419372558594, Temp: 2.0368812084198, KL: 73.72193908691406, Loss: 0.03059755265712738, Learning Rate: 0.010289999999999997\n",
      "Epoch [4004/20000], Bound: 0.42987021803855896, Entropy: 134.67654418945312, Temp: 2.037872314453125, KL: 70.59040832519531, Loss: 0.0409899465739727, Learning Rate: 0.010289999999999997\n",
      "Epoch [4005/20000], Bound: 0.4438319504261017, Entropy: 135.6385498046875, Temp: 2.0386643409729004, KL: 71.55136108398438, Loss: 0.04825948551297188, Learning Rate: 0.010289999999999997\n",
      "Epoch [4006/20000], Bound: 0.4413559138774872, Entropy: 137.56277465820312, Temp: 2.0391149520874023, KL: 76.22769165039062, Loss: 0.03508559241890907, Learning Rate: 0.010289999999999997\n",
      "Epoch [4007/20000], Bound: 0.46565115451812744, Entropy: 134.34263610839844, Temp: 2.039621591567993, KL: 82.33262634277344, Loss: 0.03724977746605873, Learning Rate: 0.010289999999999997\n",
      "Epoch [4008/20000], Bound: 0.44356510043144226, Entropy: 135.28367614746094, Temp: 2.040191888809204, KL: 78.09408569335938, Loss: 0.03207385167479515, Learning Rate: 0.010289999999999997\n",
      "Epoch [4009/20000], Bound: 0.40962135791778564, Entropy: 136.97406005859375, Temp: 2.0408971309661865, KL: 68.83952331542969, Loss: 0.0317508839070797, Learning Rate: 0.010289999999999997\n",
      "Epoch [4010/20000], Bound: 0.48407042026519775, Entropy: 135.82186889648438, Temp: 2.041630268096924, KL: 86.24853515625, Loss: 0.041102997958660126, Learning Rate: 0.010289999999999997\n",
      "Epoch [4011/20000], Bound: 0.4378954768180847, Entropy: 135.45187377929688, Temp: 2.0423531532287598, KL: 74.593505859375, Loss: 0.03679002821445465, Learning Rate: 0.010289999999999997\n",
      "Epoch [4012/20000], Bound: 0.44479259848594666, Entropy: 132.10922241210938, Temp: 2.0430424213409424, KL: 76.79249572753906, Loss: 0.03619790077209473, Learning Rate: 0.010289999999999997\n",
      "Epoch [4013/20000], Bound: 0.49870288372039795, Entropy: 134.81248474121094, Temp: 2.043739080429077, KL: 90.4796142578125, Loss: 0.04171185940504074, Learning Rate: 0.010289999999999997\n",
      "Epoch [4014/20000], Bound: 0.4808383285999298, Entropy: 129.56349182128906, Temp: 2.0444560050964355, KL: 87.93434143066406, Loss: 0.03470028191804886, Learning Rate: 0.010289999999999997\n",
      "Epoch [4015/20000], Bound: 0.4379121661186218, Entropy: 131.60806274414062, Temp: 2.04532790184021, KL: 74.21861267089844, Loss: 0.03779776394367218, Learning Rate: 0.010289999999999997\n",
      "Epoch [4016/20000], Bound: 0.42801591753959656, Entropy: 133.04190063476562, Temp: 2.0461196899414062, KL: 72.89067077636719, Loss: 0.034303270280361176, Learning Rate: 0.010289999999999997\n",
      "Epoch [4017/20000], Bound: 0.47167906165122986, Entropy: 129.4160919189453, Temp: 2.046907901763916, KL: 85.4490966796875, Loss: 0.03419896587729454, Learning Rate: 0.010289999999999997\n",
      "Epoch [4018/20000], Bound: 0.4658540189266205, Entropy: 129.1742706298828, Temp: 2.0478274822235107, KL: 83.75276184082031, Loss: 0.0341847687959671, Learning Rate: 0.010289999999999997\n",
      "Epoch [4019/20000], Bound: 0.438739150762558, Entropy: 127.36532592773438, Temp: 2.048847198486328, KL: 75.34019470214844, Loss: 0.0357215479016304, Learning Rate: 0.010289999999999997\n",
      "Epoch [4020/20000], Bound: 0.4335517883300781, Entropy: 130.11631774902344, Temp: 2.0498299598693848, KL: 76.93278503417969, Loss: 0.028307465836405754, Learning Rate: 0.010289999999999997\n",
      "Epoch [4021/20000], Bound: 0.41840940713882446, Entropy: 131.4839324951172, Temp: 2.0509743690490723, KL: 69.37281799316406, Loss: 0.0365401990711689, Learning Rate: 0.010289999999999997\n",
      "Epoch [4022/20000], Bound: 0.43585798144340515, Entropy: 130.16476440429688, Temp: 2.051981210708618, KL: 74.06758117675781, Loss: 0.03693028911948204, Learning Rate: 0.010289999999999997\n",
      "Epoch [4023/20000], Bound: 0.44233813881874084, Entropy: 129.89833068847656, Temp: 2.052905321121216, KL: 73.46372985839844, Loss: 0.042882371693849564, Learning Rate: 0.010289999999999997\n",
      "Epoch [4024/20000], Bound: 0.46172961592674255, Entropy: 132.46762084960938, Temp: 2.0536043643951416, KL: 79.76881408691406, Loss: 0.04113321378827095, Learning Rate: 0.010289999999999997\n",
      "Epoch [4025/20000], Bound: 0.44299301505088806, Entropy: 132.47230529785156, Temp: 2.0542104244232178, KL: 77.0057373046875, Loss: 0.034745071083307266, Learning Rate: 0.010289999999999997\n",
      "Epoch [4026/20000], Bound: 0.453424334526062, Entropy: 135.6244659423828, Temp: 2.0548558235168457, KL: 79.44271850585938, Loss: 0.03609877824783325, Learning Rate: 0.010289999999999997\n",
      "Epoch [4027/20000], Bound: 0.4368557631969452, Entropy: 135.6543731689453, Temp: 2.0555288791656494, KL: 74.21287536621094, Loss: 0.03735191375017166, Learning Rate: 0.010289999999999997\n",
      "Epoch [4028/20000], Bound: 0.43027183413505554, Entropy: 138.7358856201172, Temp: 2.0561397075653076, KL: 69.58786010742188, Loss: 0.04412441328167915, Learning Rate: 0.010289999999999997\n",
      "Epoch [4029/20000], Bound: 0.43584755063056946, Entropy: 136.4864959716797, Temp: 2.0564777851104736, KL: 72.94747924804688, Loss: 0.039762746542692184, Learning Rate: 0.010289999999999997\n",
      "Epoch [4030/20000], Bound: 0.4363609552383423, Entropy: 141.1417999267578, Temp: 2.056713342666626, KL: 70.21858215332031, Loss: 0.04675396904349327, Learning Rate: 0.010289999999999997\n",
      "Epoch [4031/20000], Bound: 0.46503809094429016, Entropy: 141.9998779296875, Temp: 2.056657075881958, KL: 76.92066955566406, Loss: 0.050497740507125854, Learning Rate: 0.010289999999999997\n",
      "Epoch [4032/20000], Bound: 0.4678826928138733, Entropy: 138.5764923095703, Temp: 2.0563249588012695, KL: 80.663330078125, Loss: 0.043423037976026535, Learning Rate: 0.010289999999999997\n",
      "Epoch [4033/20000], Bound: 0.45946675539016724, Entropy: 137.02207946777344, Temp: 2.0559544563293457, KL: 83.40434265136719, Loss: 0.030753852799534798, Learning Rate: 0.010289999999999997\n",
      "Epoch [4034/20000], Bound: 0.4273300766944885, Entropy: 137.15969848632812, Temp: 2.055881977081299, KL: 71.29353332519531, Loss: 0.03797765448689461, Learning Rate: 0.010289999999999997\n",
      "Epoch [4035/20000], Bound: 0.4473169147968292, Entropy: 136.64553833007812, Temp: 2.0557732582092285, KL: 73.01126098632812, Loss: 0.04750196263194084, Learning Rate: 0.010289999999999997\n",
      "Epoch [4036/20000], Bound: 0.44891756772994995, Entropy: 134.55947875976562, Temp: 2.055422067642212, KL: 77.89459228515625, Loss: 0.036729421466588974, Learning Rate: 0.010289999999999997\n",
      "Epoch [4037/20000], Bound: 0.44918060302734375, Entropy: 131.84048461914062, Temp: 2.0551652908325195, KL: 77.81642150878906, Loss: 0.037095800042152405, Learning Rate: 0.010289999999999997\n",
      "Epoch [4038/20000], Bound: 0.441072016954422, Entropy: 130.52574157714844, Temp: 2.0549848079681396, KL: 75.27761840820312, Loss: 0.037645358592271805, Learning Rate: 0.010289999999999997\n",
      "Epoch [4039/20000], Bound: 0.46132129430770874, Entropy: 129.5461883544922, Temp: 2.0548319816589355, KL: 78.89166259765625, Loss: 0.04301229864358902, Learning Rate: 0.010289999999999997\n",
      "Epoch [4040/20000], Bound: 0.4623502194881439, Entropy: 130.64195251464844, Temp: 2.054615020751953, KL: 81.39427185058594, Loss: 0.03764709085226059, Learning Rate: 0.010289999999999997\n",
      "Epoch [4041/20000], Bound: 0.4325925409793854, Entropy: 131.53158569335938, Temp: 2.0544960498809814, KL: 68.96360778808594, Loss: 0.047187019139528275, Learning Rate: 0.010289999999999997\n",
      "Epoch [4042/20000], Bound: 0.43253904581069946, Entropy: 129.71043395996094, Temp: 2.0540969371795654, KL: 71.20736694335938, Loss: 0.0416807159781456, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4043/20000], Bound: 0.4644417464733124, Entropy: 132.0056610107422, Temp: 2.053605079650879, KL: 79.84817504882812, Loss: 0.0428706631064415, Learning Rate: 0.010289999999999997\n",
      "Epoch [4044/20000], Bound: 0.4360694885253906, Entropy: 131.38587951660156, Temp: 2.053098201751709, KL: 74.75802612304688, Loss: 0.03542264178395271, Learning Rate: 0.010289999999999997\n",
      "Epoch [4045/20000], Bound: 0.4601736068725586, Entropy: 131.68362426757812, Temp: 2.0527021884918213, KL: 80.85917663574219, Loss: 0.03734736517071724, Learning Rate: 0.010289999999999997\n",
      "Epoch [4046/20000], Bound: 0.4175885021686554, Entropy: 133.52772521972656, Temp: 2.05242657661438, KL: 66.71687316894531, Loss: 0.042496100068092346, Learning Rate: 0.010289999999999997\n",
      "Epoch [4047/20000], Bound: 0.47241348028182983, Entropy: 134.6849365234375, Temp: 2.051975727081299, KL: 82.6795654296875, Loss: 0.04164762422442436, Learning Rate: 0.010289999999999997\n",
      "Epoch [4048/20000], Bound: 0.4910692870616913, Entropy: 134.04347229003906, Temp: 2.051567792892456, KL: 83.82115173339844, Loss: 0.05251297354698181, Learning Rate: 0.010289999999999997\n",
      "Epoch [4049/20000], Bound: 0.4490933418273926, Entropy: 135.6703338623047, Temp: 2.050954580307007, KL: 74.85498046875, Loss: 0.04413621872663498, Learning Rate: 0.010289999999999997\n",
      "Epoch [4050/20000], Bound: 0.444423645734787, Entropy: 135.74688720703125, Temp: 2.050255537033081, KL: 73.50674438476562, Loss: 0.04415757581591606, Learning Rate: 0.010289999999999997\n",
      "Epoch [4051/20000], Bound: 0.5096689462661743, Entropy: 134.75018310546875, Temp: 2.049464225769043, KL: 89.29048156738281, Loss: 0.05312764644622803, Learning Rate: 0.010289999999999997\n",
      "Epoch [4052/20000], Bound: 0.43950650095939636, Entropy: 133.03729248046875, Temp: 2.0485541820526123, KL: 72.58976745605469, Loss: 0.04295501112937927, Learning Rate: 0.010289999999999997\n",
      "Epoch [4053/20000], Bound: 0.46972954273223877, Entropy: 136.0828094482422, Temp: 2.0475940704345703, KL: 75.78585815429688, Loss: 0.05641325190663338, Learning Rate: 0.010289999999999997\n",
      "Epoch [4054/20000], Bound: 0.4868360161781311, Entropy: 132.94857788085938, Temp: 2.0463054180145264, KL: 86.82904052734375, Loss: 0.0418768972158432, Learning Rate: 0.010289999999999997\n",
      "Epoch [4055/20000], Bound: 0.46915528178215027, Entropy: 130.47276306152344, Temp: 2.0451900959014893, KL: 79.88406372070312, Loss: 0.04592626914381981, Learning Rate: 0.010289999999999997\n",
      "Epoch [4056/20000], Bound: 0.4490584433078766, Entropy: 131.17308044433594, Temp: 2.044060468673706, KL: 75.31906127929688, Loss: 0.04280439391732216, Learning Rate: 0.010289999999999997\n",
      "Epoch [4057/20000], Bound: 0.44603586196899414, Entropy: 131.54261779785156, Temp: 2.042942762374878, KL: 73.59944152832031, Loss: 0.04487493261694908, Learning Rate: 0.010289999999999997\n",
      "Epoch [4058/20000], Bound: 0.42863729596138, Entropy: 130.36387634277344, Temp: 2.0417675971984863, KL: 71.76490783691406, Loss: 0.03736710548400879, Learning Rate: 0.010289999999999997\n",
      "Epoch [4059/20000], Bound: 0.4507773816585541, Entropy: 129.4119873046875, Temp: 2.040703535079956, KL: 74.17816162109375, Loss: 0.04671718552708626, Learning Rate: 0.010289999999999997\n",
      "Epoch [4060/20000], Bound: 0.41266775131225586, Entropy: 134.3639373779297, Temp: 2.0395419597625732, KL: 65.29885864257812, Loss: 0.042419951409101486, Learning Rate: 0.010289999999999997\n",
      "Epoch [4061/20000], Bound: 0.4489153027534485, Entropy: 134.87245178222656, Temp: 2.0382938385009766, KL: 75.40980529785156, Loss: 0.04233189672231674, Learning Rate: 0.010289999999999997\n",
      "Epoch [4062/20000], Bound: 0.43699681758880615, Entropy: 134.2522430419922, Temp: 2.0370891094207764, KL: 68.70437622070312, Loss: 0.05048300698399544, Learning Rate: 0.010289999999999997\n",
      "Epoch [4063/20000], Bound: 0.43040338158607483, Entropy: 134.21792602539062, Temp: 2.0356504917144775, KL: 71.53036499023438, Loss: 0.03899189829826355, Learning Rate: 0.010289999999999997\n",
      "Epoch [4064/20000], Bound: 0.4137841463088989, Entropy: 136.6029815673828, Temp: 2.0343143939971924, KL: 68.40081787109375, Loss: 0.03543350100517273, Learning Rate: 0.010289999999999997\n",
      "Epoch [4065/20000], Bound: 0.43476077914237976, Entropy: 139.24517822265625, Temp: 2.0331239700317383, KL: 70.80787658691406, Loss: 0.0436900332570076, Learning Rate: 0.010289999999999997\n",
      "Epoch [4066/20000], Bound: 0.43308553099632263, Entropy: 140.6458740234375, Temp: 2.0318915843963623, KL: 69.91302490234375, Loss: 0.044713035225868225, Learning Rate: 0.010289999999999997\n",
      "Epoch [4067/20000], Bound: 0.4345322847366333, Entropy: 138.58377075195312, Temp: 2.030588150024414, KL: 71.88766479492188, Loss: 0.04081438109278679, Learning Rate: 0.010289999999999997\n",
      "Epoch [4068/20000], Bound: 0.4394165575504303, Entropy: 140.6275634765625, Temp: 2.0293397903442383, KL: 72.05934143066406, Loss: 0.04372945800423622, Learning Rate: 0.010289999999999997\n",
      "Epoch [4069/20000], Bound: 0.4384806156158447, Entropy: 138.5685577392578, Temp: 2.028074264526367, KL: 72.84574890136719, Loss: 0.04111200571060181, Learning Rate: 0.010289999999999997\n",
      "Epoch [4070/20000], Bound: 0.43432366847991943, Entropy: 138.33534240722656, Temp: 2.026867389678955, KL: 71.93287658691406, Loss: 0.04046577960252762, Learning Rate: 0.010289999999999997\n",
      "Epoch [4071/20000], Bound: 0.4822653830051422, Entropy: 137.20712280273438, Temp: 2.0257201194763184, KL: 82.30891418457031, Loss: 0.048958566039800644, Learning Rate: 0.010289999999999997\n",
      "Epoch [4072/20000], Bound: 0.4084267318248749, Entropy: 137.0065460205078, Temp: 2.0245437622070312, KL: 65.2833251953125, Loss: 0.0393231101334095, Learning Rate: 0.010289999999999997\n",
      "Epoch [4073/20000], Bound: 0.44989147782325745, Entropy: 133.85357666015625, Temp: 2.0233757495880127, KL: 76.97074890136719, Loss: 0.03876097500324249, Learning Rate: 0.010289999999999997\n",
      "Epoch [4074/20000], Bound: 0.4652838110923767, Entropy: 133.08267211914062, Temp: 2.022367238998413, KL: 78.57333374023438, Loss: 0.04572208225727081, Learning Rate: 0.010289999999999997\n",
      "Epoch [4075/20000], Bound: 0.4646255671977997, Entropy: 131.4484100341797, Temp: 2.021354913711548, KL: 81.87861633300781, Loss: 0.03704380244016647, Learning Rate: 0.010289999999999997\n",
      "Epoch [4076/20000], Bound: 0.46331164240837097, Entropy: 132.77981567382812, Temp: 2.0205860137939453, KL: 77.572998046875, Loss: 0.04672909528017044, Learning Rate: 0.010289999999999997\n",
      "Epoch [4077/20000], Bound: 0.4462692439556122, Entropy: 132.3502960205078, Temp: 2.0197558403015137, KL: 69.63777160644531, Loss: 0.054264288395643234, Learning Rate: 0.010289999999999997\n",
      "Epoch [4078/20000], Bound: 0.4689437448978424, Entropy: 131.79383850097656, Temp: 2.0185961723327637, KL: 79.73228454589844, Loss: 0.04538625478744507, Learning Rate: 0.010289999999999997\n",
      "Epoch [4079/20000], Bound: 0.45477765798568726, Entropy: 131.82827758789062, Temp: 2.0174739360809326, KL: 74.12590026855469, Loss: 0.04909346625208855, Learning Rate: 0.010289999999999997\n",
      "Epoch [4080/20000], Bound: 0.44381919503211975, Entropy: 134.90444946289062, Temp: 2.01623272895813, KL: 73.25405883789062, Loss: 0.04350714012980461, Learning Rate: 0.010289999999999997\n",
      "Epoch [4081/20000], Bound: 0.45108911395072937, Entropy: 133.86073303222656, Temp: 2.015009880065918, KL: 73.27897644042969, Loss: 0.04852103441953659, Learning Rate: 0.010289999999999997\n",
      "Epoch [4082/20000], Bound: 0.4482728838920593, Entropy: 133.89976501464844, Temp: 2.0136847496032715, KL: 75.59182739257812, Loss: 0.04075963795185089, Learning Rate: 0.010289999999999997\n",
      "Epoch [4083/20000], Bound: 0.484834223985672, Entropy: 135.8854217529297, Temp: 2.012483835220337, KL: 82.03056335449219, Loss: 0.0511513352394104, Learning Rate: 0.010289999999999997\n",
      "Epoch [4084/20000], Bound: 0.5093933939933777, Entropy: 134.3345489501953, Temp: 2.011223077774048, KL: 91.41847229003906, Loss: 0.04635177180171013, Learning Rate: 0.010289999999999997\n",
      "Epoch [4085/20000], Bound: 0.43841296434402466, Entropy: 132.1294403076172, Temp: 2.010129451751709, KL: 74.51332092285156, Loss: 0.036445461213588715, Learning Rate: 0.010289999999999997\n",
      "Epoch [4086/20000], Bound: 0.41582241654396057, Entropy: 133.47216796875, Temp: 2.0092344284057617, KL: 67.697265625, Loss: 0.037930361926555634, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4087/20000], Bound: 0.4505467712879181, Entropy: 129.80740356445312, Temp: 2.008401870727539, KL: 77.19212341308594, Loss: 0.03822942078113556, Learning Rate: 0.010289999999999997\n",
      "Epoch [4088/20000], Bound: 0.46512851119041443, Entropy: 129.0990447998047, Temp: 2.0077316761016846, KL: 79.96484375, Loss: 0.041720133274793625, Learning Rate: 0.010289999999999997\n",
      "Epoch [4089/20000], Bound: 0.4554349482059479, Entropy: 131.28416442871094, Temp: 2.0071563720703125, KL: 78.73391723632812, Loss: 0.03781886026263237, Learning Rate: 0.010289999999999997\n",
      "Epoch [4090/20000], Bound: 0.4893406331539154, Entropy: 133.107666015625, Temp: 2.006747245788574, KL: 85.16787719726562, Loss: 0.046521060168743134, Learning Rate: 0.010289999999999997\n",
      "Epoch [4091/20000], Bound: 0.48466452956199646, Entropy: 132.8551483154297, Temp: 2.0063531398773193, KL: 82.69354248046875, Loss: 0.04918694868683815, Learning Rate: 0.010289999999999997\n",
      "Epoch [4092/20000], Bound: 0.45398032665252686, Entropy: 132.7803955078125, Temp: 2.0058815479278564, KL: 77.52754211425781, Loss: 0.039750903844833374, Learning Rate: 0.010289999999999997\n",
      "Epoch [4093/20000], Bound: 0.4741859436035156, Entropy: 131.2196502685547, Temp: 2.005506753921509, KL: 81.82159423828125, Loss: 0.04360601305961609, Learning Rate: 0.010289999999999997\n",
      "Epoch [4094/20000], Bound: 0.42537954449653625, Entropy: 134.63134765625, Temp: 2.0051767826080322, KL: 66.32763671875, Loss: 0.04773060232400894, Learning Rate: 0.010289999999999997\n",
      "Epoch [4095/20000], Bound: 0.43405237793922424, Entropy: 131.10150146484375, Temp: 2.004600763320923, KL: 70.2532958984375, Loss: 0.04388625919818878, Learning Rate: 0.010289999999999997\n",
      "Epoch [4096/20000], Bound: 0.44684869050979614, Entropy: 128.89715576171875, Temp: 2.00394606590271, KL: 73.71835327148438, Loss: 0.044151123613119125, Learning Rate: 0.010289999999999997\n",
      "Epoch [4097/20000], Bound: 0.4575066566467285, Entropy: 132.44949340820312, Temp: 2.0032575130462646, KL: 78.77471923828125, Loss: 0.03907221555709839, Learning Rate: 0.010289999999999997\n",
      "Epoch [4098/20000], Bound: 0.47329989075660706, Entropy: 131.38473510742188, Temp: 2.0027222633361816, KL: 81.17138671875, Loss: 0.044490840286016464, Learning Rate: 0.010289999999999997\n",
      "Epoch [4099/20000], Bound: 0.4638884365558624, Entropy: 130.64251708984375, Temp: 2.002223253250122, KL: 78.46012878417969, Loss: 0.044408585876226425, Learning Rate: 0.010289999999999997\n",
      "Epoch [4100/20000], Bound: 0.45602285861968994, Entropy: 130.93966674804688, Temp: 2.001727342605591, KL: 75.82666015625, Loss: 0.045327961444854736, Learning Rate: 0.010289999999999997\n",
      "Epoch [4101/20000], Bound: 0.4583646059036255, Entropy: 130.0576629638672, Temp: 2.001181125640869, KL: 74.79412841796875, Loss: 0.04956618696451187, Learning Rate: 0.010289999999999997\n",
      "Epoch [4102/20000], Bound: 0.4022393524646759, Entropy: 133.67689514160156, Temp: 2.000476360321045, KL: 64.25398254394531, Loss: 0.03723462298512459, Learning Rate: 0.010289999999999997\n",
      "Epoch [4103/20000], Bound: 0.43551406264305115, Entropy: 134.18984985351562, Temp: 1.9998008012771606, KL: 70.25445556640625, Loss: 0.04477611556649208, Learning Rate: 0.010289999999999997\n",
      "Epoch [4104/20000], Bound: 0.46081632375717163, Entropy: 133.18699645996094, Temp: 1.999041199684143, KL: 79.17744445800781, Loss: 0.040305059403181076, Learning Rate: 0.010289999999999997\n",
      "Epoch [4105/20000], Bound: 0.46384188532829285, Entropy: 133.16893005371094, Temp: 1.9984230995178223, KL: 80.23330688476562, Loss: 0.03982372209429741, Learning Rate: 0.010289999999999997\n",
      "Epoch [4106/20000], Bound: 0.4350516200065613, Entropy: 134.37400817871094, Temp: 1.9979572296142578, KL: 72.2127685546875, Loss: 0.03950854763388634, Learning Rate: 0.010289999999999997\n",
      "Epoch [4107/20000], Bound: 0.45846420526504517, Entropy: 133.23663330078125, Temp: 1.9975415468215942, KL: 76.9854736328125, Loss: 0.04405539855360985, Learning Rate: 0.010289999999999997\n",
      "Epoch [4108/20000], Bound: 0.4406222701072693, Entropy: 138.76695251464844, Temp: 1.997118353843689, KL: 72.46284484863281, Loss: 0.04273774474859238, Learning Rate: 0.010289999999999997\n",
      "Epoch [4109/20000], Bound: 0.40157878398895264, Entropy: 136.35829162597656, Temp: 1.9966663122177124, KL: 64.46009826660156, Loss: 0.03619198128581047, Learning Rate: 0.010289999999999997\n",
      "Epoch [4110/20000], Bound: 0.4386044144630432, Entropy: 138.16973876953125, Temp: 1.9962517023086548, KL: 75.14404296875, Loss: 0.03459012880921364, Learning Rate: 0.010289999999999997\n",
      "Epoch [4111/20000], Bound: 0.4473820626735687, Entropy: 136.26223754882812, Temp: 1.996040940284729, KL: 77.64962768554688, Loss: 0.034464240074157715, Learning Rate: 0.010289999999999997\n",
      "Epoch [4112/20000], Bound: 0.4285421073436737, Entropy: 137.62913513183594, Temp: 1.9960463047027588, KL: 69.86248779296875, Loss: 0.040847621858119965, Learning Rate: 0.010289999999999997\n",
      "Epoch [4113/20000], Bound: 0.4504472017288208, Entropy: 135.47982788085938, Temp: 1.9959959983825684, KL: 78.25741577148438, Loss: 0.03510826453566551, Learning Rate: 0.010289999999999997\n",
      "Epoch [4114/20000], Bound: 0.5175959467887878, Entropy: 136.7169647216797, Temp: 1.9961373805999756, KL: 98.78330993652344, Loss: 0.033709872514009476, Learning Rate: 0.010289999999999997\n",
      "Epoch [4115/20000], Bound: 0.40732088685035706, Entropy: 138.2763671875, Temp: 1.9967114925384521, KL: 66.85874938964844, Loss: 0.03400736302137375, Learning Rate: 0.010289999999999997\n",
      "Epoch [4116/20000], Bound: 0.4442213773727417, Entropy: 137.73721313476562, Temp: 1.9973053932189941, KL: 74.67970275878906, Loss: 0.03971487283706665, Learning Rate: 0.010289999999999997\n",
      "Epoch [4117/20000], Bound: 0.4285115897655487, Entropy: 135.754638671875, Temp: 1.9978697299957275, KL: 73.86688232421875, Loss: 0.03085201233625412, Learning Rate: 0.010289999999999997\n",
      "Epoch [4118/20000], Bound: 0.42147815227508545, Entropy: 133.11062622070312, Temp: 1.9986164569854736, KL: 70.418701171875, Loss: 0.03469228371977806, Learning Rate: 0.010289999999999997\n",
      "Epoch [4119/20000], Bound: 0.4708983600139618, Entropy: 135.7001190185547, Temp: 1.9993897676467896, KL: 82.953857421875, Loss: 0.038172218948602676, Learning Rate: 0.010289999999999997\n",
      "Epoch [4120/20000], Bound: 0.45069289207458496, Entropy: 134.77967834472656, Temp: 2.0002477169036865, KL: 77.32518005371094, Loss: 0.037750523537397385, Learning Rate: 0.010289999999999997\n",
      "Epoch [4121/20000], Bound: 0.4690592586994171, Entropy: 132.79029846191406, Temp: 2.0011258125305176, KL: 83.51559448242188, Loss: 0.03548978641629219, Learning Rate: 0.010289999999999997\n",
      "Epoch [4122/20000], Bound: 0.4249974191188812, Entropy: 133.42994689941406, Temp: 2.0021474361419678, KL: 73.43934631347656, Loss: 0.02964422106742859, Learning Rate: 0.010289999999999997\n",
      "Epoch [4123/20000], Bound: 0.47287043929100037, Entropy: 133.18756103515625, Temp: 2.003326177597046, KL: 85.23391723632812, Loss: 0.034056663513183594, Learning Rate: 0.010289999999999997\n",
      "Epoch [4124/20000], Bound: 0.4145160913467407, Entropy: 130.948486328125, Temp: 2.0046703815460205, KL: 68.60549926757812, Loss: 0.03467264026403427, Learning Rate: 0.010289999999999997\n",
      "Epoch [4125/20000], Bound: 0.4252236783504486, Entropy: 132.88365173339844, Temp: 2.0059523582458496, KL: 70.34014892578125, Loss: 0.037639301270246506, Learning Rate: 0.010289999999999997\n",
      "Epoch [4126/20000], Bound: 0.41556867957115173, Entropy: 134.99893188476562, Temp: 2.0071239471435547, KL: 66.7930908203125, Loss: 0.03996000811457634, Learning Rate: 0.010289999999999997\n",
      "Epoch [4127/20000], Bound: 0.45707249641418457, Entropy: 131.88987731933594, Temp: 2.0080926418304443, KL: 80.62660217285156, Loss: 0.03430259972810745, Learning Rate: 0.010289999999999997\n",
      "Epoch [4128/20000], Bound: 0.4335910975933075, Entropy: 136.4250030517578, Temp: 2.009183168411255, KL: 73.28497314453125, Loss: 0.03613634407520294, Learning Rate: 0.010289999999999997\n",
      "Epoch [4129/20000], Bound: 0.4607694745063782, Entropy: 135.76583862304688, Temp: 2.0102508068084717, KL: 79.05085754394531, Loss: 0.040937233716249466, Learning Rate: 0.010289999999999997\n",
      "Epoch [4130/20000], Bound: 0.46110090613365173, Entropy: 137.7273712158203, Temp: 2.0112457275390625, KL: 80.23960876464844, Loss: 0.038250140845775604, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4131/20000], Bound: 0.4540170133113861, Entropy: 136.72457885742188, Temp: 2.012253522872925, KL: 77.65000915527344, Loss: 0.03966439515352249, Learning Rate: 0.010289999999999997\n",
      "Epoch [4132/20000], Bound: 0.45710158348083496, Entropy: 140.56912231445312, Temp: 2.013206720352173, KL: 78.392578125, Loss: 0.04004201665520668, Learning Rate: 0.010289999999999997\n",
      "Epoch [4133/20000], Bound: 0.4477410912513733, Entropy: 140.42919921875, Temp: 2.014108657836914, KL: 72.87583923339844, Loss: 0.04714003577828407, Learning Rate: 0.010289999999999997\n",
      "Epoch [4134/20000], Bound: 0.4122292101383209, Entropy: 138.51805114746094, Temp: 2.014721393585205, KL: 65.83441162109375, Loss: 0.04027630388736725, Learning Rate: 0.010289999999999997\n",
      "Epoch [4135/20000], Bound: 0.4250923693180084, Entropy: 138.1841583251953, Temp: 2.0151565074920654, KL: 66.48466491699219, Loss: 0.047355812042951584, Learning Rate: 0.010289999999999997\n",
      "Epoch [4136/20000], Bound: 0.4357335567474365, Entropy: 135.29058837890625, Temp: 2.0152618885040283, KL: 74.89979553222656, Loss: 0.03377925604581833, Learning Rate: 0.010289999999999997\n",
      "Epoch [4137/20000], Bound: 0.47076931595802307, Entropy: 135.86178588867188, Temp: 2.015512704849243, KL: 81.23394775390625, Loss: 0.042894039303064346, Learning Rate: 0.010289999999999997\n",
      "Epoch [4138/20000], Bound: 0.4048607647418976, Entropy: 135.6974639892578, Temp: 2.0157418251037598, KL: 65.23716735839844, Loss: 0.0368749238550663, Learning Rate: 0.010289999999999997\n",
      "Epoch [4139/20000], Bound: 0.4232122600078583, Entropy: 136.8681640625, Temp: 2.015908718109131, KL: 67.62887573242188, Loss: 0.04325525090098381, Learning Rate: 0.010289999999999997\n",
      "Epoch [4140/20000], Bound: 0.4523155391216278, Entropy: 136.65560913085938, Temp: 2.015887975692749, KL: 76.79765319824219, Loss: 0.04068223014473915, Learning Rate: 0.010289999999999997\n",
      "Epoch [4141/20000], Bound: 0.44171199202537537, Entropy: 136.68865966796875, Temp: 2.0158743858337402, KL: 74.86129760742188, Loss: 0.038040969520807266, Learning Rate: 0.010289999999999997\n",
      "Epoch [4142/20000], Bound: 0.45047062635421753, Entropy: 135.748046875, Temp: 2.0159103870391846, KL: 77.91238403320312, Loss: 0.03661486878991127, Learning Rate: 0.010289999999999997\n",
      "Epoch [4143/20000], Bound: 0.4285353124141693, Entropy: 134.8825225830078, Temp: 2.0160622596740723, KL: 72.90939331054688, Loss: 0.033790599554777145, Learning Rate: 0.010289999999999997\n",
      "Epoch [4144/20000], Bound: 0.4692421853542328, Entropy: 138.26272583007812, Temp: 2.0163307189941406, KL: 79.42018127441406, Loss: 0.04630934074521065, Learning Rate: 0.010289999999999997\n",
      "Epoch [4145/20000], Bound: 0.49679479002952576, Entropy: 135.45668029785156, Temp: 2.0164690017700195, KL: 90.48051452636719, Loss: 0.03926824778318405, Learning Rate: 0.010289999999999997\n",
      "Epoch [4146/20000], Bound: 0.4419364929199219, Entropy: 139.15139770507812, Temp: 2.016789674758911, KL: 71.31535339355469, Loss: 0.04701441153883934, Learning Rate: 0.010289999999999997\n",
      "Epoch [4147/20000], Bound: 0.4648747742176056, Entropy: 136.7526092529297, Temp: 2.016857862472534, KL: 82.87896728515625, Loss: 0.03459543734788895, Learning Rate: 0.010289999999999997\n",
      "Epoch [4148/20000], Bound: 0.4758071005344391, Entropy: 136.34815979003906, Temp: 2.0171453952789307, KL: 82.04710388183594, Loss: 0.04460132494568825, Learning Rate: 0.010289999999999997\n",
      "Epoch [4149/20000], Bound: 0.43785741925239563, Entropy: 135.04237365722656, Temp: 2.0173726081848145, KL: 72.01785278320312, Loss: 0.042453400790691376, Learning Rate: 0.010289999999999997\n",
      "Epoch [4150/20000], Bound: 0.4746176302433014, Entropy: 134.78965759277344, Temp: 2.0174779891967773, KL: 83.92041015625, Loss: 0.039099693298339844, Learning Rate: 0.010289999999999997\n",
      "Epoch [4151/20000], Bound: 0.4333522617816925, Entropy: 133.4555206298828, Temp: 2.0176985263824463, KL: 73.59722900390625, Loss: 0.03543701022863388, Learning Rate: 0.010289999999999997\n",
      "Epoch [4152/20000], Bound: 0.46513819694519043, Entropy: 131.49891662597656, Temp: 2.017993450164795, KL: 77.82272338867188, Loss: 0.04735184460878372, Learning Rate: 0.010289999999999997\n",
      "Epoch [4153/20000], Bound: 0.4413793683052063, Entropy: 129.030517578125, Temp: 2.0181076526641846, KL: 76.80670166015625, Loss: 0.033052828162908554, Learning Rate: 0.010289999999999997\n",
      "Epoch [4154/20000], Bound: 0.4439540505409241, Entropy: 131.39871215820312, Temp: 2.018404483795166, KL: 75.83561706542969, Loss: 0.0372626967728138, Learning Rate: 0.010289999999999997\n",
      "Epoch [4155/20000], Bound: 0.44779857993125916, Entropy: 129.986328125, Temp: 2.0187480449676514, KL: 74.12159729003906, Loss: 0.044210489839315414, Learning Rate: 0.010289999999999997\n",
      "Epoch [4156/20000], Bound: 0.47967687249183655, Entropy: 131.0689239501953, Temp: 2.018937826156616, KL: 80.71180725097656, Loss: 0.05080223083496094, Learning Rate: 0.010289999999999997\n",
      "Epoch [4157/20000], Bound: 0.4455728530883789, Entropy: 130.23635864257812, Temp: 2.0189056396484375, KL: 77.85655212402344, Loss: 0.033404137939214706, Learning Rate: 0.010289999999999997\n",
      "Epoch [4158/20000], Bound: 0.4454535245895386, Entropy: 131.99884033203125, Temp: 2.019073247909546, KL: 72.18852233886719, Loss: 0.047362059354782104, Learning Rate: 0.010289999999999997\n",
      "Epoch [4159/20000], Bound: 0.4559424817562103, Entropy: 129.96353149414062, Temp: 2.0190014839172363, KL: 79.11370849609375, Loss: 0.03760644420981407, Learning Rate: 0.010289999999999997\n",
      "Epoch [4160/20000], Bound: 0.45631346106529236, Entropy: 132.3516082763672, Temp: 2.0190420150756836, KL: 76.0064697265625, Loss: 0.045566167682409286, Learning Rate: 0.010289999999999997\n",
      "Epoch [4161/20000], Bound: 0.4580182433128357, Entropy: 129.52297973632812, Temp: 2.0189476013183594, KL: 75.38017272949219, Loss: 0.04832792282104492, Learning Rate: 0.010289999999999997\n",
      "Epoch [4162/20000], Bound: 0.4370358884334564, Entropy: 131.96310424804688, Temp: 2.0186550617218018, KL: 74.78672790527344, Loss: 0.035059213638305664, Learning Rate: 0.010289999999999997\n",
      "Epoch [4163/20000], Bound: 0.4150482714176178, Entropy: 131.25099182128906, Temp: 2.0185110569000244, KL: 66.1966552734375, Loss: 0.041352108120918274, Learning Rate: 0.010289999999999997\n",
      "Epoch [4164/20000], Bound: 0.4082135856151581, Entropy: 135.14727783203125, Temp: 2.018235445022583, KL: 66.16004943847656, Loss: 0.03687035292387009, Learning Rate: 0.010289999999999997\n",
      "Epoch [4165/20000], Bound: 0.45604076981544495, Entropy: 133.547119140625, Temp: 2.017955780029297, KL: 78.37269592285156, Loss: 0.039479900151491165, Learning Rate: 0.010289999999999997\n",
      "Epoch [4166/20000], Bound: 0.4261898696422577, Entropy: 138.07672119140625, Temp: 2.0177552700042725, KL: 71.4500732421875, Loss: 0.03585343062877655, Learning Rate: 0.010289999999999997\n",
      "Epoch [4167/20000], Bound: 0.39815640449523926, Entropy: 139.10401916503906, Temp: 2.0176351070404053, KL: 64.9312744140625, Loss: 0.033256646245718, Learning Rate: 0.010289999999999997\n",
      "Epoch [4168/20000], Bound: 0.4628131687641144, Entropy: 135.7194061279297, Temp: 2.0175745487213135, KL: 80.64913940429688, Loss: 0.0386640727519989, Learning Rate: 0.010289999999999997\n",
      "Epoch [4169/20000], Bound: 0.43629005551338196, Entropy: 138.50498962402344, Temp: 2.0176191329956055, KL: 71.53352355957031, Loss: 0.042575377970933914, Learning Rate: 0.010289999999999997\n",
      "Epoch [4170/20000], Bound: 0.39769047498703003, Entropy: 138.89627075195312, Temp: 2.017549753189087, KL: 63.31211853027344, Loss: 0.03696165233850479, Learning Rate: 0.010289999999999997\n",
      "Epoch [4171/20000], Bound: 0.44478079676628113, Entropy: 137.9043731689453, Temp: 2.01741886138916, KL: 74.84573364257812, Loss: 0.040265414863824844, Learning Rate: 0.010289999999999997\n",
      "Epoch [4172/20000], Bound: 0.4905981123447418, Entropy: 138.7864227294922, Temp: 2.0172905921936035, KL: 88.13551330566406, Loss: 0.04045562818646431, Learning Rate: 0.010289999999999997\n",
      "Epoch [4173/20000], Bound: 0.4217609763145447, Entropy: 138.2445526123047, Temp: 2.0173165798187256, KL: 67.45016479492188, Loss: 0.04274587705731392, Learning Rate: 0.010289999999999997\n",
      "Epoch [4174/20000], Bound: 0.46376049518585205, Entropy: 138.68128967285156, Temp: 2.0171751976013184, KL: 77.35330200195312, Loss: 0.04750128090381622, Learning Rate: 0.010289999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4175/20000], Bound: 0.4137170612812042, Entropy: 133.662109375, Temp: 2.0168867111206055, KL: 67.55343627929688, Loss: 0.03705938905477524, Learning Rate: 0.010289999999999997\n",
      "Epoch [4176/20000], Bound: 0.4804510474205017, Entropy: 136.89759826660156, Temp: 2.0166099071502686, KL: 85.97840881347656, Loss: 0.03824678808450699, Learning Rate: 0.010289999999999997\n",
      "Epoch [4177/20000], Bound: 0.4171263873577118, Entropy: 133.29225158691406, Temp: 2.016533851623535, KL: 67.57594299316406, Loss: 0.03928552195429802, Learning Rate: 0.010289999999999997\n",
      "Epoch [4178/20000], Bound: 0.45765605568885803, Entropy: 133.52972412109375, Temp: 2.0163915157318115, KL: 80.46894836425781, Loss: 0.03538442403078079, Learning Rate: 0.010289999999999997\n",
      "Epoch [4179/20000], Bound: 0.49688929319381714, Entropy: 132.8456573486328, Temp: 2.0164458751678467, KL: 90.95623779296875, Loss: 0.038159146904945374, Learning Rate: 0.010289999999999997\n",
      "Epoch [4180/20000], Bound: 0.42065665125846863, Entropy: 133.5388641357422, Temp: 2.0167267322540283, KL: 69.61195373535156, Loss: 0.03662508726119995, Learning Rate: 0.010289999999999997\n",
      "Epoch [4181/20000], Bound: 0.45311832427978516, Entropy: 133.31202697753906, Temp: 2.0169994831085205, KL: 76.95329284667969, Loss: 0.040896520018577576, Learning Rate: 0.010289999999999997\n",
      "Epoch [4182/20000], Bound: 0.45643773674964905, Entropy: 130.5113983154297, Temp: 2.017245054244995, KL: 81.02207946777344, Loss: 0.033173900097608566, Learning Rate: 0.010289999999999997\n",
      "Epoch [4183/20000], Bound: 0.46614885330200195, Entropy: 133.05877685546875, Temp: 2.0177104473114014, KL: 77.38584899902344, Loss: 0.04915488511323929, Learning Rate: 0.010289999999999997\n",
      "Epoch [4184/20000], Bound: 0.49601587653160095, Entropy: 131.89791870117188, Temp: 2.017925977706909, KL: 84.30197143554688, Loss: 0.054045286029577255, Learning Rate: 0.010289999999999997\n",
      "Epoch [4185/20000], Bound: 0.41502004861831665, Entropy: 131.272705078125, Temp: 2.0178792476654053, KL: 67.88099670410156, Loss: 0.03714556619524956, Learning Rate: 0.010289999999999997\n",
      "Epoch [4186/20000], Bound: 0.5121347904205322, Entropy: 128.1285400390625, Temp: 2.017820358276367, KL: 80.27159118652344, Loss: 0.07633338868618011, Learning Rate: 0.010289999999999997\n",
      "Epoch [4187/20000], Bound: 0.46445801854133606, Entropy: 127.2571029663086, Temp: 2.016937255859375, KL: 73.92183685302734, Loss: 0.05650297552347183, Learning Rate: 0.010289999999999997\n",
      "Epoch [4188/20000], Bound: 0.48397544026374817, Entropy: 126.5360107421875, Temp: 2.0157134532928467, KL: 78.36369323730469, Loss: 0.059707269072532654, Learning Rate: 0.010289999999999997\n",
      "Epoch [4189/20000], Bound: 0.4619496464729309, Entropy: 125.7336654663086, Temp: 2.0141632556915283, KL: 72.8971176147461, Loss: 0.05717901885509491, Learning Rate: 0.010289999999999997\n",
      "Epoch [4190/20000], Bound: 0.4684201776981354, Entropy: 125.95198822021484, Temp: 2.0123140811920166, KL: 78.7385482788086, Loss: 0.0472889207303524, Learning Rate: 0.010289999999999997\n",
      "Epoch [4191/20000], Bound: 0.5004998445510864, Entropy: 126.41636657714844, Temp: 2.0105173587799072, KL: 77.93890380859375, Loss: 0.07303901761770248, Learning Rate: 0.010289999999999997\n",
      "Epoch [4192/20000], Bound: 0.4868903160095215, Entropy: 125.50147247314453, Temp: 2.00813364982605, KL: 83.9626235961914, Loss: 0.04773907735943794, Learning Rate: 0.010289999999999997\n",
      "Epoch [4193/20000], Bound: 0.46789148449897766, Entropy: 125.74525451660156, Temp: 2.0059149265289307, KL: 77.94900512695312, Loss: 0.04868719354271889, Learning Rate: 0.010289999999999997\n",
      "Epoch [4194/20000], Bound: 0.5014307498931885, Entropy: 123.73640441894531, Temp: 2.003751039505005, KL: 82.06527709960938, Loss: 0.06329776346683502, Learning Rate: 0.010289999999999997\n",
      "Epoch [4195/20000], Bound: 0.4609087407588959, Entropy: 126.51888275146484, Temp: 2.0013344287872314, KL: 72.59825897216797, Loss: 0.05688101798295975, Learning Rate: 0.010289999999999997\n",
      "Epoch [4196/20000], Bound: 0.4868146479129791, Entropy: 128.73953247070312, Temp: 1.9987289905548096, KL: 84.75886535644531, Loss: 0.04538146033883095, Learning Rate: 0.010289999999999997\n",
      "Epoch [4197/20000], Bound: 0.48255518078804016, Entropy: 129.7936553955078, Temp: 1.9963929653167725, KL: 81.5098876953125, Loss: 0.050265587866306305, Learning Rate: 0.010289999999999997\n",
      "Epoch [4198/20000], Bound: 0.4615633487701416, Entropy: 129.50418090820312, Temp: 1.9941433668136597, KL: 76.21830749511719, Loss: 0.04810614883899689, Learning Rate: 0.010289999999999997\n",
      "Epoch [4199/20000], Bound: 0.4694230258464813, Entropy: 126.50257873535156, Temp: 1.991962194442749, KL: 75.90675354003906, Loss: 0.0545242503285408, Learning Rate: 0.010289999999999997\n",
      "Epoch [4200/20000], Bound: 0.44969597458839417, Entropy: 133.578125, Temp: 1.9896833896636963, KL: 72.91984558105469, Loss: 0.047780852764844894, Learning Rate: 0.010289999999999997\n",
      "Epoch [4201/20000], Bound: 0.4744583070278168, Entropy: 132.59359741210938, Temp: 1.9874486923217773, KL: 76.50149536132812, Loss: 0.05659528449177742, Learning Rate: 0.010289999999999997\n",
      "Epoch [4202/20000], Bound: 0.42610442638397217, Entropy: 134.8092498779297, Temp: 1.9857943058013916, KL: 67.79161071777344, Loss: 0.044115617871284485, Learning Rate: 0.007202999999999998\n",
      "Epoch [4203/20000], Bound: 0.447950154542923, Entropy: 136.40481567382812, Temp: 1.9841983318328857, KL: 74.80972290039062, Loss: 0.04163389280438423, Learning Rate: 0.007202999999999998\n",
      "Epoch [4204/20000], Bound: 0.46443548798561096, Entropy: 135.64328002929688, Temp: 1.9827635288238525, KL: 80.59223937988281, Loss: 0.03882874548435211, Learning Rate: 0.007202999999999998\n",
      "Epoch [4205/20000], Bound: 0.41775399446487427, Entropy: 136.0152130126953, Temp: 1.9815746545791626, KL: 69.28852844238281, Loss: 0.03452473506331444, Learning Rate: 0.007202999999999998\n",
      "Epoch [4206/20000], Bound: 0.43332380056381226, Entropy: 137.35435485839844, Temp: 1.9805864095687866, KL: 70.77980041503906, Loss: 0.04143496975302696, Learning Rate: 0.007202999999999998\n",
      "Epoch [4207/20000], Bound: 0.46165168285369873, Entropy: 136.76937866210938, Temp: 1.9796698093414307, KL: 75.861083984375, Loss: 0.0486549474298954, Learning Rate: 0.007202999999999998\n",
      "Epoch [4208/20000], Bound: 0.44120946526527405, Entropy: 137.74876403808594, Temp: 1.9787375926971436, KL: 69.70130920410156, Loss: 0.04962054640054703, Learning Rate: 0.007202999999999998\n",
      "Epoch [4209/20000], Bound: 0.45083165168762207, Entropy: 137.1737518310547, Temp: 1.9777190685272217, KL: 76.79716491699219, Loss: 0.0384645089507103, Learning Rate: 0.007202999999999998\n",
      "Epoch [4210/20000], Bound: 0.44648686051368713, Entropy: 137.32595825195312, Temp: 1.976884126663208, KL: 77.64369201660156, Loss: 0.03321022912859917, Learning Rate: 0.007202999999999998\n",
      "Epoch [4211/20000], Bound: 0.432033509016037, Entropy: 141.19509887695312, Temp: 1.9763158559799194, KL: 67.54139709472656, Loss: 0.048614297062158585, Learning Rate: 0.007202999999999998\n",
      "Epoch [4212/20000], Bound: 0.4324401021003723, Entropy: 138.97900390625, Temp: 1.9756247997283936, KL: 74.78384399414062, Loss: 0.030550876632332802, Learning Rate: 0.007202999999999998\n",
      "Epoch [4213/20000], Bound: 0.4344823658466339, Entropy: 137.87734985351562, Temp: 1.9752103090286255, KL: 72.2806396484375, Loss: 0.038294706493616104, Learning Rate: 0.007202999999999998\n",
      "Epoch [4214/20000], Bound: 0.46079084277153015, Entropy: 138.24386596679688, Temp: 1.9748849868774414, KL: 78.59942626953125, Loss: 0.04096002131700516, Learning Rate: 0.007202999999999998\n",
      "Epoch [4215/20000], Bound: 0.4594193696975708, Entropy: 137.93801879882812, Temp: 1.9746489524841309, KL: 78.83563232421875, Loss: 0.03936348855495453, Learning Rate: 0.007202999999999998\n",
      "Epoch [4216/20000], Bound: 0.44147881865501404, Entropy: 137.24642944335938, Temp: 1.9745237827301025, KL: 74.85649108886719, Loss: 0.03665126860141754, Learning Rate: 0.007202999999999998\n",
      "Epoch [4217/20000], Bound: 0.47280383110046387, Entropy: 138.4340057373047, Temp: 1.9745118618011475, KL: 83.73753356933594, Loss: 0.03669830411672592, Learning Rate: 0.007202999999999998\n",
      "Epoch [4218/20000], Bound: 0.42896899580955505, Entropy: 135.66864013671875, Temp: 1.9746774435043335, KL: 72.0836181640625, Loss: 0.03495034947991371, Learning Rate: 0.007202999999999998\n",
      "Epoch [4219/20000], Bound: 0.4874223470687866, Entropy: 133.1933135986328, Temp: 1.9749332666397095, KL: 88.60093688964844, Loss: 0.03527069091796875, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4220/20000], Bound: 0.4541892111301422, Entropy: 136.10633850097656, Temp: 1.975404977798462, KL: 78.88923645019531, Loss: 0.03549351543188095, Learning Rate: 0.007202999999999998\n",
      "Epoch [4221/20000], Bound: 0.4203735291957855, Entropy: 133.21156311035156, Temp: 1.9759855270385742, KL: 68.36418151855469, Loss: 0.0384930782020092, Learning Rate: 0.007202999999999998\n",
      "Epoch [4222/20000], Bound: 0.4515892565250397, Entropy: 133.00115966796875, Temp: 1.9765162467956543, KL: 78.53662109375, Loss: 0.03456566855311394, Learning Rate: 0.007202999999999998\n",
      "Epoch [4223/20000], Bound: 0.38983914256095886, Entropy: 134.81793212890625, Temp: 1.9771623611450195, KL: 62.060028076171875, Loss: 0.03402554988861084, Learning Rate: 0.007202999999999998\n",
      "Epoch [4224/20000], Bound: 0.453199177980423, Entropy: 135.3995361328125, Temp: 1.9777740240097046, KL: 77.07363891601562, Loss: 0.03945674002170563, Learning Rate: 0.007202999999999998\n",
      "Epoch [4225/20000], Bound: 0.4658429026603699, Entropy: 131.29139709472656, Temp: 1.9783920049667358, KL: 83.73667907714844, Loss: 0.031754568219184875, Learning Rate: 0.007202999999999998\n",
      "Epoch [4226/20000], Bound: 0.44869476556777954, Entropy: 132.75425720214844, Temp: 1.9792087078094482, KL: 76.22311401367188, Loss: 0.038443051278591156, Learning Rate: 0.007202999999999998\n",
      "Epoch [4227/20000], Bound: 0.3968524932861328, Entropy: 135.45986938476562, Temp: 1.9800206422805786, KL: 65.21629333496094, Loss: 0.03074246272444725, Learning Rate: 0.007202999999999998\n",
      "Epoch [4228/20000], Bound: 0.41319379210472107, Entropy: 137.5837860107422, Temp: 1.9808682203292847, KL: 70.96664428710938, Loss: 0.02717715874314308, Learning Rate: 0.007202999999999998\n",
      "Epoch [4229/20000], Bound: 0.4546017050743103, Entropy: 135.36593627929688, Temp: 1.9818634986877441, KL: 80.67510986328125, Loss: 0.03150356188416481, Learning Rate: 0.007202999999999998\n",
      "Epoch [4230/20000], Bound: 0.49377891421318054, Entropy: 138.7574005126953, Temp: 1.9829957485198975, KL: 90.15802001953125, Loss: 0.03646490350365639, Learning Rate: 0.007202999999999998\n",
      "Epoch [4231/20000], Bound: 0.43260133266448975, Entropy: 136.58543395996094, Temp: 1.9842404127120972, KL: 68.92913818359375, Loss: 0.045696284621953964, Learning Rate: 0.007202999999999998\n",
      "Epoch [4232/20000], Bound: 0.38175055384635925, Entropy: 137.8415069580078, Temp: 1.9852371215820312, KL: 61.86918640136719, Loss: 0.029429133981466293, Learning Rate: 0.007202999999999998\n",
      "Epoch [4233/20000], Bound: 0.4369257688522339, Entropy: 138.79344177246094, Temp: 1.9862405061721802, KL: 74.18963623046875, Loss: 0.03550947085022926, Learning Rate: 0.007202999999999998\n",
      "Epoch [4234/20000], Bound: 0.4521624743938446, Entropy: 135.66189575195312, Temp: 1.987248182296753, KL: 77.12274169921875, Loss: 0.03889475390315056, Learning Rate: 0.007202999999999998\n",
      "Epoch [4235/20000], Bound: 0.41092649102211, Entropy: 138.4871826171875, Temp: 1.9882237911224365, KL: 68.60977172851562, Loss: 0.03180275112390518, Learning Rate: 0.007202999999999998\n",
      "Epoch [4236/20000], Bound: 0.4335705041885376, Entropy: 136.46925354003906, Temp: 1.9892226457595825, KL: 74.43354797363281, Loss: 0.0326550230383873, Learning Rate: 0.007202999999999998\n",
      "Epoch [4237/20000], Bound: 0.4234621226787567, Entropy: 140.15965270996094, Temp: 1.9902772903442383, KL: 71.50546264648438, Loss: 0.03308424726128578, Learning Rate: 0.007202999999999998\n",
      "Epoch [4238/20000], Bound: 0.44611799716949463, Entropy: 137.2213592529297, Temp: 1.9913477897644043, KL: 75.79754638671875, Loss: 0.038069356232881546, Learning Rate: 0.007202999999999998\n",
      "Epoch [4239/20000], Bound: 0.41250526905059814, Entropy: 142.27072143554688, Temp: 1.9923789501190186, KL: 68.15579223632812, Loss: 0.03411917760968208, Learning Rate: 0.007202999999999998\n",
      "Epoch [4240/20000], Bound: 0.43227264285087585, Entropy: 140.2999725341797, Temp: 1.9933778047561646, KL: 72.39102172851562, Loss: 0.03700914606451988, Learning Rate: 0.007202999999999998\n",
      "Epoch [4241/20000], Bound: 0.44834426045417786, Entropy: 138.3424835205078, Temp: 1.9943315982818604, KL: 78.93934631347656, Loss: 0.031855180859565735, Learning Rate: 0.007202999999999998\n",
      "Epoch [4242/20000], Bound: 0.38209599256515503, Entropy: 141.4503631591797, Temp: 1.9953937530517578, KL: 63.40580749511719, Loss: 0.026048246771097183, Learning Rate: 0.007202999999999998\n",
      "Epoch [4243/20000], Bound: 0.4262361526489258, Entropy: 138.065185546875, Temp: 1.9965249300003052, KL: 71.96286010742188, Loss: 0.03401719406247139, Learning Rate: 0.007202999999999998\n",
      "Epoch [4244/20000], Bound: 0.45388808846473694, Entropy: 140.5690460205078, Temp: 1.997645616531372, KL: 79.94200134277344, Loss: 0.033389490097761154, Learning Rate: 0.007202999999999998\n",
      "Epoch [4245/20000], Bound: 0.46505260467529297, Entropy: 140.48422241210938, Temp: 1.9988356828689575, KL: 82.29095458984375, Loss: 0.03556498512625694, Learning Rate: 0.007202999999999998\n",
      "Epoch [4246/20000], Bound: 0.3602043092250824, Entropy: 141.09019470214844, Temp: 2.000067710876465, KL: 55.28999328613281, Loss: 0.032519206404685974, Learning Rate: 0.007202999999999998\n",
      "Epoch [4247/20000], Bound: 0.436382919549942, Entropy: 139.31985473632812, Temp: 2.0011518001556396, KL: 76.09410095214844, Loss: 0.030821671709418297, Learning Rate: 0.007202999999999998\n",
      "Epoch [4248/20000], Bound: 0.44242945313453674, Entropy: 138.9938201904297, Temp: 2.002319574356079, KL: 77.12786865234375, Loss: 0.03249099478125572, Learning Rate: 0.007202999999999998\n",
      "Epoch [4249/20000], Bound: 0.44195276498794556, Entropy: 138.82188415527344, Temp: 2.003540277481079, KL: 77.84678649902344, Loss: 0.03040267899632454, Learning Rate: 0.007202999999999998\n",
      "Epoch [4250/20000], Bound: 0.4220569133758545, Entropy: 136.9410858154297, Temp: 2.004851818084717, KL: 71.10406494140625, Loss: 0.03354716673493385, Learning Rate: 0.007202999999999998\n",
      "Epoch [4251/20000], Bound: 0.42728960514068604, Entropy: 137.57675170898438, Temp: 2.0061280727386475, KL: 73.77766418457031, Loss: 0.03048865683376789, Learning Rate: 0.007202999999999998\n",
      "Epoch [4252/20000], Bound: 0.4789946973323822, Entropy: 134.67971801757812, Temp: 2.007451057434082, KL: 88.96615600585938, Loss: 0.029403528198599815, Learning Rate: 0.007202999999999998\n",
      "Epoch [4253/20000], Bound: 0.4195958077907562, Entropy: 133.60916137695312, Temp: 2.0089592933654785, KL: 71.57962036132812, Loss: 0.03080892562866211, Learning Rate: 0.007202999999999998\n",
      "Epoch [4254/20000], Bound: 0.4544764757156372, Entropy: 132.62059020996094, Temp: 2.0104637145996094, KL: 80.96315002441406, Loss: 0.031697168946266174, Learning Rate: 0.007202999999999998\n",
      "Epoch [4255/20000], Bound: 0.42314913868904114, Entropy: 133.96456909179688, Temp: 2.012026309967041, KL: 70.55747985839844, Loss: 0.035845980048179626, Learning Rate: 0.007202999999999998\n",
      "Epoch [4256/20000], Bound: 0.4511622190475464, Entropy: 135.71466064453125, Temp: 2.013474941253662, KL: 80.95916748046875, Loss: 0.02946239709854126, Learning Rate: 0.007202999999999998\n",
      "Epoch [4257/20000], Bound: 0.47012418508529663, Entropy: 131.0790557861328, Temp: 2.0150253772735596, KL: 84.78617858886719, Loss: 0.03359593451023102, Learning Rate: 0.007202999999999998\n",
      "Epoch [4258/20000], Bound: 0.4245014190673828, Entropy: 132.34292602539062, Temp: 2.0166215896606445, KL: 74.03988647460938, Loss: 0.02825208567082882, Learning Rate: 0.007202999999999998\n",
      "Epoch [4259/20000], Bound: 0.4169035255908966, Entropy: 131.4366912841797, Temp: 2.0182669162750244, KL: 73.19000244140625, Loss: 0.025268346071243286, Learning Rate: 0.007202999999999998\n",
      "Epoch [4260/20000], Bound: 0.4390840530395508, Entropy: 133.23782348632812, Temp: 2.020003080368042, KL: 75.66244506835938, Loss: 0.03434887155890465, Learning Rate: 0.007202999999999998\n",
      "Epoch [4261/20000], Bound: 0.45277139544487, Entropy: 134.330078125, Temp: 2.021672248840332, KL: 81.03022766113281, Loss: 0.03070269525051117, Learning Rate: 0.007202999999999998\n",
      "Epoch [4262/20000], Bound: 0.40245577692985535, Entropy: 134.81179809570312, Temp: 2.0233914852142334, KL: 67.08973693847656, Loss: 0.03088313341140747, Learning Rate: 0.007202999999999998\n",
      "Epoch [4263/20000], Bound: 0.45868587493896484, Entropy: 134.12779235839844, Temp: 2.0250325202941895, KL: 81.36721801757812, Loss: 0.034177735447883606, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4264/20000], Bound: 0.43883267045021057, Entropy: 134.44017028808594, Temp: 2.0266621112823486, KL: 75.96771240234375, Loss: 0.033617053180933, Learning Rate: 0.007202999999999998\n",
      "Epoch [4265/20000], Bound: 0.43078354001045227, Entropy: 135.387939453125, Temp: 2.028245210647583, KL: 73.42463684082031, Loss: 0.0343954972922802, Learning Rate: 0.007202999999999998\n",
      "Epoch [4266/20000], Bound: 0.42797189950942993, Entropy: 137.52772521972656, Temp: 2.0297489166259766, KL: 71.46524047851562, Loss: 0.03734476864337921, Learning Rate: 0.007202999999999998\n",
      "Epoch [4267/20000], Bound: 0.4497878849506378, Entropy: 134.68508911132812, Temp: 2.0311076641082764, KL: 80.8282470703125, Loss: 0.029414841905236244, Learning Rate: 0.007202999999999998\n",
      "Epoch [4268/20000], Bound: 0.40113377571105957, Entropy: 138.81060791015625, Temp: 2.0325605869293213, KL: 65.08024597167969, Loss: 0.035185910761356354, Learning Rate: 0.007202999999999998\n",
      "Epoch [4269/20000], Bound: 0.45237210392951965, Entropy: 136.9396514892578, Temp: 2.0338551998138428, KL: 80.84817504882812, Loss: 0.03127333149313927, Learning Rate: 0.007202999999999998\n",
      "Epoch [4270/20000], Bound: 0.44172072410583496, Entropy: 137.1637725830078, Temp: 2.0352132320404053, KL: 78.80235290527344, Loss: 0.028901390731334686, Learning Rate: 0.007202999999999998\n",
      "Epoch [4271/20000], Bound: 0.3980081081390381, Entropy: 137.3031463623047, Temp: 2.036654472351074, KL: 66.63925170898438, Loss: 0.029404690489172935, Learning Rate: 0.007202999999999998\n",
      "Epoch [4272/20000], Bound: 0.4561426341533661, Entropy: 136.94334411621094, Temp: 2.038058280944824, KL: 82.09539794921875, Loss: 0.03100965917110443, Learning Rate: 0.007202999999999998\n",
      "Epoch [4273/20000], Bound: 0.44348233938217163, Entropy: 138.9004364013672, Temp: 2.0395257472991943, KL: 79.92298889160156, Loss: 0.02751254104077816, Learning Rate: 0.007202999999999998\n",
      "Epoch [4274/20000], Bound: 0.43701842427253723, Entropy: 138.7957000732422, Temp: 2.0410966873168945, KL: 76.26579284667969, Loss: 0.03205597400665283, Learning Rate: 0.007202999999999998\n",
      "Epoch [4275/20000], Bound: 0.3936782777309418, Entropy: 136.60548400878906, Temp: 2.042644500732422, KL: 64.82693481445312, Loss: 0.031172459945082664, Learning Rate: 0.007202999999999998\n",
      "Epoch [4276/20000], Bound: 0.4213394522666931, Entropy: 135.32223510742188, Temp: 2.0440897941589355, KL: 72.75213623046875, Loss: 0.03007768839597702, Learning Rate: 0.007202999999999998\n",
      "Epoch [4277/20000], Bound: 0.38663676381111145, Entropy: 139.3279266357422, Temp: 2.045529842376709, KL: 62.296173095703125, Loss: 0.03288468345999718, Learning Rate: 0.007202999999999998\n",
      "Epoch [4278/20000], Bound: 0.4580734372138977, Entropy: 135.71873474121094, Temp: 2.0468199253082275, KL: 82.407470703125, Loss: 0.03189941868185997, Learning Rate: 0.007202999999999998\n",
      "Epoch [4279/20000], Bound: 0.42538002133369446, Entropy: 136.27989196777344, Temp: 2.0481629371643066, KL: 73.57756042480469, Loss: 0.030896052718162537, Learning Rate: 0.007202999999999998\n",
      "Epoch [4280/20000], Bound: 0.41069474816322327, Entropy: 138.19590759277344, Temp: 2.049499034881592, KL: 70.08807373046875, Loss: 0.029627269133925438, Learning Rate: 0.007202999999999998\n",
      "Epoch [4281/20000], Bound: 0.3983968496322632, Entropy: 139.93792724609375, Temp: 2.0508220195770264, KL: 66.26902770996094, Loss: 0.030902104452252388, Learning Rate: 0.007202999999999998\n",
      "Epoch [4282/20000], Bound: 0.43798744678497314, Entropy: 138.82894897460938, Temp: 2.052075147628784, KL: 75.11424255371094, Loss: 0.035843029618263245, Learning Rate: 0.007202999999999998\n",
      "Epoch [4283/20000], Bound: 0.414311945438385, Entropy: 138.35791015625, Temp: 2.05324649810791, KL: 70.9814453125, Loss: 0.029945624992251396, Learning Rate: 0.007202999999999998\n",
      "Epoch [4284/20000], Bound: 0.45772549510002136, Entropy: 138.9894256591797, Temp: 2.0544192790985107, KL: 82.95065307617188, Loss: 0.030576461926102638, Learning Rate: 0.007202999999999998\n",
      "Epoch [4285/20000], Bound: 0.46073514223098755, Entropy: 141.60780334472656, Temp: 2.0556788444519043, KL: 83.18603515625, Loss: 0.032174356281757355, Learning Rate: 0.007202999999999998\n",
      "Epoch [4286/20000], Bound: 0.4503491222858429, Entropy: 137.7351531982422, Temp: 2.0569870471954346, KL: 80.49685668945312, Loss: 0.0314464271068573, Learning Rate: 0.007202999999999998\n",
      "Epoch [4287/20000], Bound: 0.3921610414981842, Entropy: 138.79347229003906, Temp: 2.0583300590515137, KL: 66.00506591796875, Loss: 0.02768225595355034, Learning Rate: 0.007202999999999998\n",
      "Epoch [4288/20000], Bound: 0.41274261474609375, Entropy: 136.31703186035156, Temp: 2.059654474258423, KL: 69.9031982421875, Loss: 0.031688109040260315, Learning Rate: 0.007202999999999998\n",
      "Epoch [4289/20000], Bound: 0.4269314408302307, Entropy: 137.24017333984375, Temp: 2.060917615890503, KL: 74.53195190429688, Loss: 0.029971526935696602, Learning Rate: 0.007202999999999998\n",
      "Epoch [4290/20000], Bound: 0.4372393488883972, Entropy: 135.24607849121094, Temp: 2.0621957778930664, KL: 75.63914489746094, Loss: 0.0343266986310482, Learning Rate: 0.007202999999999998\n",
      "Epoch [4291/20000], Bound: 0.43789729475975037, Entropy: 135.51312255859375, Temp: 2.063413143157959, KL: 79.18946838378906, Loss: 0.026206783950328827, Learning Rate: 0.007202999999999998\n",
      "Epoch [4292/20000], Bound: 0.42437565326690674, Entropy: 136.10060119628906, Temp: 2.0647566318511963, KL: 74.17474365234375, Loss: 0.029221517965197563, Learning Rate: 0.007202999999999998\n",
      "Epoch [4293/20000], Bound: 0.40166646242141724, Entropy: 135.57606506347656, Temp: 2.066115140914917, KL: 66.33229064941406, Loss: 0.03322133049368858, Learning Rate: 0.007202999999999998\n",
      "Epoch [4294/20000], Bound: 0.40770381689071655, Entropy: 136.1174774169922, Temp: 2.0673434734344482, KL: 68.22079467773438, Loss: 0.032626230269670486, Learning Rate: 0.007202999999999998\n",
      "Epoch [4295/20000], Bound: 0.4112578332424164, Entropy: 138.5723114013672, Temp: 2.0684807300567627, KL: 70.89987182617188, Loss: 0.02851346880197525, Learning Rate: 0.007202999999999998\n",
      "Epoch [4296/20000], Bound: 0.44505393505096436, Entropy: 135.671142578125, Temp: 2.069636821746826, KL: 78.99678039550781, Loss: 0.03178244084119797, Learning Rate: 0.007202999999999998\n",
      "Epoch [4297/20000], Bound: 0.42265117168426514, Entropy: 139.947998046875, Temp: 2.070812702178955, KL: 73.73744201660156, Loss: 0.02928595244884491, Learning Rate: 0.007202999999999998\n",
      "Epoch [4298/20000], Bound: 0.44745582342147827, Entropy: 138.55233764648438, Temp: 2.0720102787017822, KL: 81.6788330078125, Loss: 0.02704031392931938, Learning Rate: 0.007202999999999998\n",
      "Epoch [4299/20000], Bound: 0.4128880202770233, Entropy: 136.81800842285156, Temp: 2.0733327865600586, KL: 68.18717956542969, Loss: 0.036252446472644806, Learning Rate: 0.007202999999999998\n",
      "Epoch [4300/20000], Bound: 0.4477064907550812, Entropy: 139.05406188964844, Temp: 2.074479818344116, KL: 80.11412048339844, Loss: 0.03106311894953251, Learning Rate: 0.007202999999999998\n",
      "Epoch [4301/20000], Bound: 0.4154001474380493, Entropy: 136.9523468017578, Temp: 2.0756657123565674, KL: 72.79734802246094, Loss: 0.026857055723667145, Learning Rate: 0.007202999999999998\n",
      "Epoch [4302/20000], Bound: 0.4375057518482208, Entropy: 138.0519256591797, Temp: 2.076907157897949, KL: 79.00248718261719, Loss: 0.02680092304944992, Learning Rate: 0.007202999999999998\n",
      "Epoch [4303/20000], Bound: 0.414655476808548, Entropy: 139.0856475830078, Temp: 2.0782487392425537, KL: 74.72752380371094, Loss: 0.021789364516735077, Learning Rate: 0.007202999999999998\n",
      "Epoch [4304/20000], Bound: 0.44663116335868835, Entropy: 138.92413330078125, Temp: 2.0797417163848877, KL: 79.96218872070312, Loss: 0.030839763581752777, Learning Rate: 0.007202999999999998\n",
      "Epoch [4305/20000], Bound: 0.4466874301433563, Entropy: 138.2822265625, Temp: 2.081237316131592, KL: 80.00296020507812, Loss: 0.03082432597875595, Learning Rate: 0.007202999999999998\n",
      "Epoch [4306/20000], Bound: 0.41857072710990906, Entropy: 137.56858825683594, Temp: 2.0827348232269287, KL: 70.9443359375, Loss: 0.0335901714861393, Learning Rate: 0.007202999999999998\n",
      "Epoch [4307/20000], Bound: 0.4367986023426056, Entropy: 141.68540954589844, Temp: 2.0841057300567627, KL: 76.50445556640625, Loss: 0.0325259193778038, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4308/20000], Bound: 0.46161073446273804, Entropy: 137.90164184570312, Temp: 2.085427761077881, KL: 85.1820068359375, Loss: 0.02893427573144436, Learning Rate: 0.007202999999999998\n",
      "Epoch [4309/20000], Bound: 0.4298700988292694, Entropy: 138.75218200683594, Temp: 2.086841344833374, KL: 75.34123229980469, Loss: 0.030698150396347046, Learning Rate: 0.007202999999999998\n",
      "Epoch [4310/20000], Bound: 0.42968103289604187, Entropy: 132.32264709472656, Temp: 2.0882248878479004, KL: 74.55062866210938, Loss: 0.0325002446770668, Learning Rate: 0.007202999999999998\n",
      "Epoch [4311/20000], Bound: 0.4067646861076355, Entropy: 135.9728240966797, Temp: 2.089539051055908, KL: 70.70509338378906, Loss: 0.02656504139304161, Learning Rate: 0.007202999999999998\n",
      "Epoch [4312/20000], Bound: 0.430910587310791, Entropy: 135.43150329589844, Temp: 2.0908732414245605, KL: 76.18867492675781, Loss: 0.02947746217250824, Learning Rate: 0.007202999999999998\n",
      "Epoch [4313/20000], Bound: 0.4340102970600128, Entropy: 133.37559509277344, Temp: 2.092212200164795, KL: 77.02398681640625, Loss: 0.02960868738591671, Learning Rate: 0.007202999999999998\n",
      "Epoch [4314/20000], Bound: 0.4418485462665558, Entropy: 134.3237762451172, Temp: 2.0935583114624023, KL: 81.32328796386719, Loss: 0.02470574714243412, Learning Rate: 0.007202999999999998\n",
      "Epoch [4315/20000], Bound: 0.43318629264831543, Entropy: 136.36050415039062, Temp: 2.0950379371643066, KL: 77.27058410644531, Loss: 0.028540143743157387, Learning Rate: 0.007202999999999998\n",
      "Epoch [4316/20000], Bound: 0.44771233201026917, Entropy: 137.29156494140625, Temp: 2.0965309143066406, KL: 78.25810241699219, Loss: 0.03612925484776497, Learning Rate: 0.007202999999999998\n",
      "Epoch [4317/20000], Bound: 0.43903452157974243, Entropy: 134.12359619140625, Temp: 2.0978965759277344, KL: 77.59811401367188, Loss: 0.03179871663451195, Learning Rate: 0.007202999999999998\n",
      "Epoch [4318/20000], Bound: 0.3973037302494049, Entropy: 135.11167907714844, Temp: 2.099224090576172, KL: 66.82041931152344, Loss: 0.029944323003292084, Learning Rate: 0.007202999999999998\n",
      "Epoch [4319/20000], Bound: 0.4412451386451721, Entropy: 135.84718322753906, Temp: 2.100464105606079, KL: 77.10694885253906, Loss: 0.034540917724370956, Learning Rate: 0.007202999999999998\n",
      "Epoch [4320/20000], Bound: 0.4288809299468994, Entropy: 135.4730987548828, Temp: 2.101619005203247, KL: 72.34721374511719, Loss: 0.03753998503088951, Learning Rate: 0.007202999999999998\n",
      "Epoch [4321/20000], Bound: 0.3965301215648651, Entropy: 135.12107849121094, Temp: 2.102599620819092, KL: 67.8502197265625, Loss: 0.027071485295891762, Learning Rate: 0.007202999999999998\n",
      "Epoch [4322/20000], Bound: 0.4057160019874573, Entropy: 135.2077178955078, Temp: 2.1035892963409424, KL: 69.53292846679688, Loss: 0.02901238575577736, Learning Rate: 0.007202999999999998\n",
      "Epoch [4323/20000], Bound: 0.42053163051605225, Entropy: 133.8433380126953, Temp: 2.104562282562256, KL: 71.57876586914062, Loss: 0.033875081688165665, Learning Rate: 0.007202999999999998\n",
      "Epoch [4324/20000], Bound: 0.418622225522995, Entropy: 135.51072692871094, Temp: 2.105440855026245, KL: 71.71604919433594, Loss: 0.03230768069624901, Learning Rate: 0.007202999999999998\n",
      "Epoch [4325/20000], Bound: 0.43707144260406494, Entropy: 135.5634002685547, Temp: 2.1062653064727783, KL: 77.22065734863281, Loss: 0.0315847173333168, Learning Rate: 0.007202999999999998\n",
      "Epoch [4326/20000], Bound: 0.43248873949050903, Entropy: 138.03282165527344, Temp: 2.1070992946624756, KL: 75.37513732910156, Loss: 0.03289473056793213, Learning Rate: 0.007202999999999998\n",
      "Epoch [4327/20000], Bound: 0.44972723722457886, Entropy: 141.2573699951172, Temp: 2.107900381088257, KL: 79.67367553710938, Loss: 0.03444886952638626, Learning Rate: 0.007202999999999998\n",
      "Epoch [4328/20000], Bound: 0.39894118905067444, Entropy: 139.71066284179688, Temp: 2.108675956726074, KL: 64.76254272460938, Loss: 0.03607254847884178, Learning Rate: 0.007202999999999998\n",
      "Epoch [4329/20000], Bound: 0.36382123827934265, Entropy: 142.48410034179688, Temp: 2.109272003173828, KL: 58.73548889160156, Loss: 0.028362683951854706, Learning Rate: 0.007202999999999998\n",
      "Epoch [4330/20000], Bound: 0.3546214997768402, Entropy: 144.64053344726562, Temp: 2.1098074913024902, KL: 56.26036071777344, Loss: 0.028638483956456184, Learning Rate: 0.007202999999999998\n",
      "Epoch [4331/20000], Bound: 0.4758184254169464, Entropy: 147.30770874023438, Temp: 2.1102612018585205, KL: 80.93482971191406, Loss: 0.04983823373913765, Learning Rate: 0.007202999999999998\n",
      "Epoch [4332/20000], Bound: 0.4443260729312897, Entropy: 142.61595153808594, Temp: 2.1104378700256348, KL: 77.78472900390625, Loss: 0.03528670594096184, Learning Rate: 0.007202999999999998\n",
      "Epoch [4333/20000], Bound: 0.40638217329978943, Entropy: 145.3400421142578, Temp: 2.1106173992156982, KL: 67.58091735839844, Loss: 0.03422657400369644, Learning Rate: 0.007202999999999998\n",
      "Epoch [4334/20000], Bound: 0.42491012811660767, Entropy: 141.92910766601562, Temp: 2.1107354164123535, KL: 73.74925231933594, Loss: 0.031772226095199585, Learning Rate: 0.007202999999999998\n",
      "Epoch [4335/20000], Bound: 0.43533217906951904, Entropy: 145.95957946777344, Temp: 2.1108975410461426, KL: 77.43638610839844, Loss: 0.030019110068678856, Learning Rate: 0.007202999999999998\n",
      "Epoch [4336/20000], Bound: 0.45228320360183716, Entropy: 142.7665557861328, Temp: 2.111163377761841, KL: 80.86257934570312, Loss: 0.0334821417927742, Learning Rate: 0.007202999999999998\n",
      "Epoch [4337/20000], Bound: 0.444796085357666, Entropy: 144.5143585205078, Temp: 2.1114823818206787, KL: 79.75140380859375, Loss: 0.030976969748735428, Learning Rate: 0.007202999999999998\n",
      "Epoch [4338/20000], Bound: 0.3999233543872833, Entropy: 144.87689208984375, Temp: 2.1118886470794678, KL: 62.601837158203125, Loss: 0.04187648370862007, Learning Rate: 0.007202999999999998\n",
      "Epoch [4339/20000], Bound: 0.387546181678772, Entropy: 145.05764770507812, Temp: 2.1120150089263916, KL: 61.620819091796875, Loss: 0.03632301092147827, Learning Rate: 0.007202999999999998\n",
      "Epoch [4340/20000], Bound: 0.4455009996891022, Entropy: 140.4672393798828, Temp: 2.1119906902313232, KL: 78.17501831054688, Loss: 0.03520477935671806, Learning Rate: 0.007202999999999998\n",
      "Epoch [4341/20000], Bound: 0.4136468470096588, Entropy: 141.30345153808594, Temp: 2.111992597579956, KL: 70.93162536621094, Loss: 0.031048830598592758, Learning Rate: 0.007202999999999998\n",
      "Epoch [4342/20000], Bound: 0.4235726594924927, Entropy: 139.03302001953125, Temp: 2.1120402812957764, KL: 72.38656616210938, Loss: 0.03414164111018181, Learning Rate: 0.007202999999999998\n",
      "Epoch [4343/20000], Bound: 0.4474833309650421, Entropy: 139.14254760742188, Temp: 2.1120800971984863, KL: 81.05073547363281, Loss: 0.029757240787148476, Learning Rate: 0.007202999999999998\n",
      "Epoch [4344/20000], Bound: 0.4219364821910858, Entropy: 135.4750518798828, Temp: 2.112269163131714, KL: 70.69509887695312, Loss: 0.03706709295511246, Learning Rate: 0.007202999999999998\n",
      "Epoch [4345/20000], Bound: 0.4151199162006378, Entropy: 135.36480712890625, Temp: 2.1123645305633545, KL: 69.27493286132812, Loss: 0.035942863672971725, Learning Rate: 0.007202999999999998\n",
      "Epoch [4346/20000], Bound: 0.41149038076400757, Entropy: 131.49012756347656, Temp: 2.1123852729797363, KL: 71.21867370605469, Loss: 0.028970208019018173, Learning Rate: 0.007202999999999998\n",
      "Epoch [4347/20000], Bound: 0.4408625364303589, Entropy: 130.254150390625, Temp: 2.1124937534332275, KL: 78.21334838867188, Loss: 0.031964633613824844, Learning Rate: 0.007202999999999998\n",
      "Epoch [4348/20000], Bound: 0.448643296957016, Entropy: 134.57485961914062, Temp: 2.112678289413452, KL: 78.00416564941406, Loss: 0.03778060898184776, Learning Rate: 0.007202999999999998\n",
      "Epoch [4349/20000], Bound: 0.44331997632980347, Entropy: 133.565673828125, Temp: 2.112816095352173, KL: 77.7735595703125, Loss: 0.03468635305762291, Learning Rate: 0.007202999999999998\n",
      "Epoch [4350/20000], Bound: 0.4312230348587036, Entropy: 133.0883331298828, Temp: 2.1129698753356934, KL: 76.44731140136719, Loss: 0.029652314260601997, Learning Rate: 0.007202999999999998\n",
      "Epoch [4351/20000], Bound: 0.4524410665035248, Entropy: 133.93260192871094, Temp: 2.1132266521453857, KL: 79.471923828125, Loss: 0.036937497556209564, Learning Rate: 0.007202999999999998\n",
      "Epoch [4352/20000], Bound: 0.4174362123012543, Entropy: 133.27125549316406, Temp: 2.113457441329956, KL: 70.97671508789062, Loss: 0.0334588922560215, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4353/20000], Bound: 0.41456276178359985, Entropy: 132.50790405273438, Temp: 2.1136624813079834, KL: 69.84675598144531, Loss: 0.03425180912017822, Learning Rate: 0.007202999999999998\n",
      "Epoch [4354/20000], Bound: 0.4390484690666199, Entropy: 135.57406616210938, Temp: 2.1138193607330322, KL: 77.29754638671875, Loss: 0.03293481096625328, Learning Rate: 0.007202999999999998\n",
      "Epoch [4355/20000], Bound: 0.43416088819503784, Entropy: 133.16729736328125, Temp: 2.1140198707580566, KL: 73.81828308105469, Loss: 0.037869542837142944, Learning Rate: 0.007202999999999998\n",
      "Epoch [4356/20000], Bound: 0.46090254187583923, Entropy: 135.5840301513672, Temp: 2.1141340732574463, KL: 84.21119689941406, Loss: 0.03163236379623413, Learning Rate: 0.007202999999999998\n",
      "Epoch [4357/20000], Bound: 0.40949395298957825, Entropy: 136.8816680908203, Temp: 2.114377021789551, KL: 69.74168395996094, Loss: 0.031209873035550117, Learning Rate: 0.007202999999999998\n",
      "Epoch [4358/20000], Bound: 0.44101738929748535, Entropy: 135.5540771484375, Temp: 2.114626884460449, KL: 77.20716857910156, Loss: 0.034504447132349014, Learning Rate: 0.007202999999999998\n",
      "Epoch [4359/20000], Bound: 0.4353647232055664, Entropy: 137.5720672607422, Temp: 2.1148793697357178, KL: 79.6185302734375, Loss: 0.024985993281006813, Learning Rate: 0.007202999999999998\n",
      "Epoch [4360/20000], Bound: 0.4264522194862366, Entropy: 135.27471923828125, Temp: 2.1153414249420166, KL: 75.4783935546875, Loss: 0.028819087892770767, Learning Rate: 0.007202999999999998\n",
      "Epoch [4361/20000], Bound: 0.43597984313964844, Entropy: 135.9904327392578, Temp: 2.115882635116577, KL: 76.62956237792969, Loss: 0.03249216452240944, Learning Rate: 0.007202999999999998\n",
      "Epoch [4362/20000], Bound: 0.44991543889045715, Entropy: 136.4588623046875, Temp: 2.116431474685669, KL: 79.40989685058594, Loss: 0.03542521968483925, Learning Rate: 0.007202999999999998\n",
      "Epoch [4363/20000], Bound: 0.4088900685310364, Entropy: 138.1562042236328, Temp: 2.116950750350952, KL: 71.0318603515625, Loss: 0.027826473116874695, Learning Rate: 0.007202999999999998\n",
      "Epoch [4364/20000], Bound: 0.4314427971839905, Entropy: 135.46267700195312, Temp: 2.1175265312194824, KL: 75.413818359375, Loss: 0.032356176525354385, Learning Rate: 0.007202999999999998\n",
      "Epoch [4365/20000], Bound: 0.3766247630119324, Entropy: 137.92173767089844, Temp: 2.1180973052978516, KL: 62.83580017089844, Loss: 0.02672119066119194, Learning Rate: 0.007202999999999998\n",
      "Epoch [4366/20000], Bound: 0.432731956243515, Entropy: 136.5502471923828, Temp: 2.1186726093292236, KL: 76.59954833984375, Loss: 0.03044935315847397, Learning Rate: 0.007202999999999998\n",
      "Epoch [4367/20000], Bound: 0.45495131611824036, Entropy: 137.3623046875, Temp: 2.1192893981933594, KL: 80.25, Loss: 0.036989834159612656, Learning Rate: 0.007202999999999998\n",
      "Epoch [4368/20000], Bound: 0.3845420777797699, Entropy: 140.0417022705078, Temp: 2.1198439598083496, KL: 64.68528747558594, Loss: 0.02733396552503109, Learning Rate: 0.007202999999999998\n",
      "Epoch [4369/20000], Bound: 0.4325971007347107, Entropy: 137.6396026611328, Temp: 2.120405912399292, KL: 76.37519836425781, Loss: 0.030931705608963966, Learning Rate: 0.007202999999999998\n",
      "Epoch [4370/20000], Bound: 0.4354642331600189, Entropy: 138.51158142089844, Temp: 2.1209983825683594, KL: 76.99726867675781, Loss: 0.0314050056040287, Learning Rate: 0.007202999999999998\n",
      "Epoch [4371/20000], Bound: 0.37284985184669495, Entropy: 140.3651123046875, Temp: 2.121613025665283, KL: 59.93840026855469, Loss: 0.031280506402254105, Learning Rate: 0.007202999999999998\n",
      "Epoch [4372/20000], Bound: 0.4317105710506439, Entropy: 141.0660400390625, Temp: 2.1221060752868652, KL: 74.39364624023438, Loss: 0.03504937142133713, Learning Rate: 0.007202999999999998\n",
      "Epoch [4373/20000], Bound: 0.44185635447502136, Entropy: 140.3970489501953, Temp: 2.1225366592407227, KL: 77.52653503417969, Loss: 0.03451535478234291, Learning Rate: 0.007202999999999998\n",
      "Epoch [4374/20000], Bound: 0.3996524512767792, Entropy: 142.37030029296875, Temp: 2.1229467391967773, KL: 63.92338562011719, Loss: 0.0387592613697052, Learning Rate: 0.007202999999999998\n",
      "Epoch [4375/20000], Bound: 0.4052252173423767, Entropy: 142.4731903076172, Temp: 2.1231372356414795, KL: 68.92959594726562, Loss: 0.030551284551620483, Learning Rate: 0.007202999999999998\n",
      "Epoch [4376/20000], Bound: 0.45862457156181335, Entropy: 138.0022735595703, Temp: 2.1233391761779785, KL: 83.46955871582031, Loss: 0.03205880522727966, Learning Rate: 0.007202999999999998\n",
      "Epoch [4377/20000], Bound: 0.4952680170536041, Entropy: 139.71568298339844, Temp: 2.123638868331909, KL: 92.17379760742188, Loss: 0.0377681665122509, Learning Rate: 0.007202999999999998\n",
      "Epoch [4378/20000], Bound: 0.46765947341918945, Entropy: 138.91757202148438, Temp: 2.1239821910858154, KL: 82.46449279785156, Loss: 0.040766943246126175, Learning Rate: 0.007202999999999998\n",
      "Epoch [4379/20000], Bound: 0.4310239255428314, Entropy: 138.55931091308594, Temp: 2.1242291927337646, KL: 73.29728698730469, Loss: 0.037219300866127014, Learning Rate: 0.007202999999999998\n",
      "Epoch [4380/20000], Bound: 0.4199630618095398, Entropy: 135.53721618652344, Temp: 2.1243834495544434, KL: 72.54594421386719, Loss: 0.03166000917553902, Learning Rate: 0.007202999999999998\n",
      "Epoch [4381/20000], Bound: 0.44527503848075867, Entropy: 136.98080444335938, Temp: 2.124559164047241, KL: 75.75001525878906, Loss: 0.04107065126299858, Learning Rate: 0.007202999999999998\n",
      "Epoch [4382/20000], Bound: 0.4246133267879486, Entropy: 136.0554656982422, Temp: 2.1245932579040527, KL: 74.87139892578125, Loss: 0.029260100796818733, Learning Rate: 0.007202999999999998\n",
      "Epoch [4383/20000], Bound: 0.4203798472881317, Entropy: 133.5305938720703, Temp: 2.124727964401245, KL: 72.89274597167969, Loss: 0.031125802546739578, Learning Rate: 0.007202999999999998\n",
      "Epoch [4384/20000], Bound: 0.4724692106246948, Entropy: 130.75460815429688, Temp: 2.1248998641967773, KL: 84.40313720703125, Loss: 0.03962947055697441, Learning Rate: 0.007202999999999998\n",
      "Epoch [4385/20000], Bound: 0.41993701457977295, Entropy: 129.11752319335938, Temp: 2.1250298023223877, KL: 73.01048278808594, Loss: 0.030564412474632263, Learning Rate: 0.007202999999999998\n",
      "Epoch [4386/20000], Bound: 0.41771945357322693, Entropy: 131.6312713623047, Temp: 2.125208854675293, KL: 70.9173583984375, Loss: 0.03403732553124428, Learning Rate: 0.007202999999999998\n",
      "Epoch [4387/20000], Bound: 0.4263644516468048, Entropy: 131.26097106933594, Temp: 2.12534499168396, KL: 72.99066162109375, Loss: 0.03486324101686478, Learning Rate: 0.007202999999999998\n",
      "Epoch [4388/20000], Bound: 0.4617568254470825, Entropy: 132.34620666503906, Temp: 2.1254427433013916, KL: 85.44187927246094, Loss: 0.029660366475582123, Learning Rate: 0.007202999999999998\n",
      "Epoch [4389/20000], Bound: 0.42656397819519043, Entropy: 132.5448455810547, Temp: 2.1257104873657227, KL: 73.869140625, Loss: 0.03293735533952713, Learning Rate: 0.007202999999999998\n",
      "Epoch [4390/20000], Bound: 0.44011199474334717, Entropy: 133.412841796875, Temp: 2.1259729862213135, KL: 77.59066772460938, Loss: 0.03326701745390892, Learning Rate: 0.007202999999999998\n",
      "Epoch [4391/20000], Bound: 0.44965043663978577, Entropy: 135.52310180664062, Temp: 2.126253843307495, KL: 81.7135009765625, Loss: 0.030075257644057274, Learning Rate: 0.007202999999999998\n",
      "Epoch [4392/20000], Bound: 0.4203513562679291, Entropy: 135.40110778808594, Temp: 2.126648426055908, KL: 71.43197631835938, Loss: 0.03458535671234131, Learning Rate: 0.007202999999999998\n",
      "Epoch [4393/20000], Bound: 0.43397772312164307, Entropy: 136.0034637451172, Temp: 2.1269705295562744, KL: 72.10054016113281, Loss: 0.04206741601228714, Learning Rate: 0.007202999999999998\n",
      "Epoch [4394/20000], Bound: 0.44459694623947144, Entropy: 132.81529235839844, Temp: 2.1270816326141357, KL: 79.02632141113281, Loss: 0.032962460070848465, Learning Rate: 0.007202999999999998\n",
      "Epoch [4395/20000], Bound: 0.4370001554489136, Entropy: 132.74534606933594, Temp: 2.1272435188293457, KL: 77.40577697753906, Loss: 0.031633343547582626, Learning Rate: 0.007202999999999998\n",
      "Epoch [4396/20000], Bound: 0.43501976132392883, Entropy: 134.9404296875, Temp: 2.1274642944335938, KL: 76.54521179199219, Loss: 0.032330069690942764, Learning Rate: 0.007202999999999998\n",
      "Epoch [4397/20000], Bound: 0.44062790274620056, Entropy: 133.57850646972656, Temp: 2.1277172565460205, KL: 77.3125, Loss: 0.03431268036365509, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4398/20000], Bound: 0.4291080832481384, Entropy: 132.87831115722656, Temp: 2.127964735031128, KL: 75.78453063964844, Loss: 0.030178433284163475, Learning Rate: 0.007202999999999998\n",
      "Epoch [4399/20000], Bound: 0.4174352288246155, Entropy: 133.94830322265625, Temp: 2.1282787322998047, KL: 72.96272277832031, Loss: 0.029110554605722427, Learning Rate: 0.007202999999999998\n",
      "Epoch [4400/20000], Bound: 0.42662790417671204, Entropy: 134.72903442382812, Temp: 2.128650665283203, KL: 73.0498046875, Loss: 0.03497127816081047, Learning Rate: 0.007202999999999998\n",
      "Epoch [4401/20000], Bound: 0.4093703627586365, Entropy: 134.04345703125, Temp: 2.1289560794830322, KL: 69.76397705078125, Loss: 0.031389184296131134, Learning Rate: 0.007202999999999998\n",
      "Epoch [4402/20000], Bound: 0.42286020517349243, Entropy: 137.4678497314453, Temp: 2.129246473312378, KL: 74.54487609863281, Loss: 0.02898203395307064, Learning Rate: 0.007202999999999998\n",
      "Epoch [4403/20000], Bound: 0.40287989377975464, Entropy: 137.58290100097656, Temp: 2.1296122074127197, KL: 68.02613830566406, Loss: 0.031301286071538925, Learning Rate: 0.007202999999999998\n",
      "Epoch [4404/20000], Bound: 0.4295031428337097, Entropy: 138.5312042236328, Temp: 2.129943609237671, KL: 76.46881103515625, Loss: 0.028883187100291252, Learning Rate: 0.007202999999999998\n",
      "Epoch [4405/20000], Bound: 0.39747780561447144, Entropy: 137.26422119140625, Temp: 2.1303634643554688, KL: 68.84724426269531, Loss: 0.025937432423233986, Learning Rate: 0.007202999999999998\n",
      "Epoch [4406/20000], Bound: 0.4449779689311981, Entropy: 139.5970458984375, Temp: 2.1308605670928955, KL: 79.12992858886719, Loss: 0.033074259757995605, Learning Rate: 0.007202999999999998\n",
      "Epoch [4407/20000], Bound: 0.4394318461418152, Entropy: 137.9695587158203, Temp: 2.1313652992248535, KL: 75.93971252441406, Loss: 0.03681304678320885, Learning Rate: 0.007202999999999998\n",
      "Epoch [4408/20000], Bound: 0.40427857637405396, Entropy: 140.69851684570312, Temp: 2.131774425506592, KL: 70.24472045898438, Loss: 0.02703869342803955, Learning Rate: 0.007202999999999998\n",
      "Epoch [4409/20000], Bound: 0.4147370755672455, Entropy: 140.0843505859375, Temp: 2.1322498321533203, KL: 70.64834594726562, Loss: 0.03286821395158768, Learning Rate: 0.007202999999999998\n",
      "Epoch [4410/20000], Bound: 0.43570494651794434, Entropy: 138.1648712158203, Temp: 2.1326677799224854, KL: 77.48910522460938, Loss: 0.0307023748755455, Learning Rate: 0.007202999999999998\n",
      "Epoch [4411/20000], Bound: 0.3998355269432068, Entropy: 140.80810546875, Temp: 2.133134365081787, KL: 68.60610961914062, Loss: 0.028065552935004234, Learning Rate: 0.007202999999999998\n",
      "Epoch [4412/20000], Bound: 0.4055219292640686, Entropy: 140.511474609375, Temp: 2.1336255073547363, KL: 67.553466796875, Loss: 0.03418544679880142, Learning Rate: 0.007202999999999998\n",
      "Epoch [4413/20000], Bound: 0.43582895398139954, Entropy: 140.5552520751953, Temp: 2.134002685546875, KL: 75.64741516113281, Loss: 0.03513389453291893, Learning Rate: 0.007202999999999998\n",
      "Epoch [4414/20000], Bound: 0.43857505917549133, Entropy: 138.5115509033203, Temp: 2.1343259811401367, KL: 79.08137512207031, Loss: 0.02894260361790657, Learning Rate: 0.007202999999999998\n",
      "Epoch [4415/20000], Bound: 0.43854567408561707, Entropy: 142.114013671875, Temp: 2.1347548961639404, KL: 77.96649169921875, Loss: 0.03154527768492699, Learning Rate: 0.007202999999999998\n",
      "Epoch [4416/20000], Bound: 0.42723348736763, Entropy: 139.71478271484375, Temp: 2.135215997695923, KL: 77.61868286132812, Loss: 0.02481526881456375, Learning Rate: 0.007202999999999998\n",
      "Epoch [4417/20000], Bound: 0.4110945165157318, Entropy: 143.6757049560547, Temp: 2.1358416080474854, KL: 71.52406311035156, Loss: 0.02852775901556015, Learning Rate: 0.007202999999999998\n",
      "Epoch [4418/20000], Bound: 0.403950572013855, Entropy: 144.26991271972656, Temp: 2.136488676071167, KL: 69.56568908691406, Loss: 0.028521841391921043, Learning Rate: 0.007202999999999998\n",
      "Epoch [4419/20000], Bound: 0.42863306403160095, Entropy: 142.4754638671875, Temp: 2.1371381282806396, KL: 73.12948608398438, Loss: 0.03629640117287636, Learning Rate: 0.007202999999999998\n",
      "Epoch [4420/20000], Bound: 0.4002316892147064, Entropy: 141.6670379638672, Temp: 2.1376583576202393, KL: 68.5672607421875, Loss: 0.02850414253771305, Learning Rate: 0.007202999999999998\n",
      "Epoch [4421/20000], Bound: 0.3805735111236572, Entropy: 140.04689025878906, Temp: 2.1381850242614746, KL: 62.36773681640625, Loss: 0.03064112365245819, Learning Rate: 0.007202999999999998\n",
      "Epoch [4422/20000], Bound: 0.46571823954582214, Entropy: 137.9345245361328, Temp: 2.1386187076568604, KL: 85.41749572753906, Loss: 0.032868996262550354, Learning Rate: 0.007202999999999998\n",
      "Epoch [4423/20000], Bound: 0.4440191984176636, Entropy: 137.27301025390625, Temp: 2.139113664627075, KL: 76.66929626464844, Loss: 0.03838168829679489, Learning Rate: 0.007202999999999998\n",
      "Epoch [4424/20000], Bound: 0.43345391750335693, Entropy: 142.44366455078125, Temp: 2.1394805908203125, KL: 77.23037719726562, Loss: 0.029969820752739906, Learning Rate: 0.007202999999999998\n",
      "Epoch [4425/20000], Bound: 0.4239378273487091, Entropy: 138.98513793945312, Temp: 2.1399080753326416, KL: 75.05728149414062, Loss: 0.028747044503688812, Learning Rate: 0.007202999999999998\n",
      "Epoch [4426/20000], Bound: 0.38454118371009827, Entropy: 137.50804138183594, Temp: 2.1403982639312744, KL: 64.52243041992188, Loss: 0.02811501920223236, Learning Rate: 0.007202999999999998\n",
      "Epoch [4427/20000], Bound: 0.4344404339790344, Entropy: 135.98130798339844, Temp: 2.140869140625, KL: 75.06155395507812, Loss: 0.035728197544813156, Learning Rate: 0.007202999999999998\n",
      "Epoch [4428/20000], Bound: 0.42358630895614624, Entropy: 137.85902404785156, Temp: 2.1412529945373535, KL: 73.78582763671875, Loss: 0.031516559422016144, Learning Rate: 0.007202999999999998\n",
      "Epoch [4429/20000], Bound: 0.4099592864513397, Entropy: 135.7413787841797, Temp: 2.141634464263916, KL: 71.48980712890625, Loss: 0.028002537786960602, Learning Rate: 0.007202999999999998\n",
      "Epoch [4430/20000], Bound: 0.4191747307777405, Entropy: 136.2183074951172, Temp: 2.1420676708221436, KL: 73.7108154296875, Loss: 0.028815878555178642, Learning Rate: 0.007202999999999998\n",
      "Epoch [4431/20000], Bound: 0.4115999937057495, Entropy: 136.86427307128906, Temp: 2.1425483226776123, KL: 72.57009887695312, Loss: 0.026562070474028587, Learning Rate: 0.007202999999999998\n",
      "Epoch [4432/20000], Bound: 0.4298597574234009, Entropy: 136.22103881835938, Temp: 2.143109083175659, KL: 76.86436462402344, Loss: 0.02852105163037777, Learning Rate: 0.007202999999999998\n",
      "Epoch [4433/20000], Bound: 0.41070252656936646, Entropy: 136.10598754882812, Temp: 2.143735647201538, KL: 71.80691528320312, Loss: 0.027789290994405746, Learning Rate: 0.007202999999999998\n",
      "Epoch [4434/20000], Bound: 0.4068017601966858, Entropy: 134.78939819335938, Temp: 2.144395112991333, KL: 68.06028747558594, Loss: 0.03402703255414963, Learning Rate: 0.007202999999999998\n",
      "Epoch [4435/20000], Bound: 0.4481542706489563, Entropy: 136.534912109375, Temp: 2.14492130279541, KL: 81.65199279785156, Loss: 0.02970295585691929, Learning Rate: 0.007202999999999998\n",
      "Epoch [4436/20000], Bound: 0.4250926077365875, Entropy: 137.51280212402344, Temp: 2.145528793334961, KL: 76.85420227050781, Loss: 0.025452598929405212, Learning Rate: 0.007202999999999998\n",
      "Epoch [4437/20000], Bound: 0.4534977674484253, Entropy: 133.24256896972656, Temp: 2.146259307861328, KL: 83.30281066894531, Loss: 0.029545597732067108, Learning Rate: 0.007202999999999998\n",
      "Epoch [4438/20000], Bound: 0.4311506748199463, Entropy: 135.66488647460938, Temp: 2.147066116333008, KL: 75.5245361328125, Loss: 0.032594822347164154, Learning Rate: 0.007202999999999998\n",
      "Epoch [4439/20000], Bound: 0.4103168547153473, Entropy: 135.26324462890625, Temp: 2.147815465927124, KL: 71.45881652832031, Loss: 0.028441183269023895, Learning Rate: 0.007202999999999998\n",
      "Epoch [4440/20000], Bound: 0.433928906917572, Entropy: 134.5049591064453, Temp: 2.1485650539398193, KL: 79.18026733398438, Loss: 0.025969935581088066, Learning Rate: 0.007202999999999998\n",
      "Epoch [4441/20000], Bound: 0.42248818278312683, Entropy: 137.13294982910156, Temp: 2.1494290828704834, KL: 74.34919738769531, Loss: 0.029666656628251076, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4442/20000], Bound: 0.38827794790267944, Entropy: 137.1608428955078, Temp: 2.1502788066864014, KL: 65.37721252441406, Loss: 0.028642753139138222, Learning Rate: 0.007202999999999998\n",
      "Epoch [4443/20000], Bound: 0.4224548935890198, Entropy: 137.57237243652344, Temp: 2.151061773300171, KL: 72.83775329589844, Loss: 0.03319517895579338, Learning Rate: 0.007202999999999998\n",
      "Epoch [4444/20000], Bound: 0.4675072133541107, Entropy: 139.02561950683594, Temp: 2.1517510414123535, KL: 85.82933044433594, Loss: 0.03352676331996918, Learning Rate: 0.007202999999999998\n",
      "Epoch [4445/20000], Bound: 0.47769513726234436, Entropy: 138.5753631591797, Temp: 2.152453660964966, KL: 89.26939392089844, Loss: 0.03273880481719971, Learning Rate: 0.007202999999999998\n",
      "Epoch [4446/20000], Bound: 0.4279457926750183, Entropy: 137.05177307128906, Temp: 2.1532108783721924, KL: 74.39872741699219, Loss: 0.033226680010557175, Learning Rate: 0.007202999999999998\n",
      "Epoch [4447/20000], Bound: 0.42057082056999207, Entropy: 138.41281127929688, Temp: 2.1538870334625244, KL: 72.19287109375, Loss: 0.033518947660923004, Learning Rate: 0.007202999999999998\n",
      "Epoch [4448/20000], Bound: 0.43113812804222107, Entropy: 138.33963012695312, Temp: 2.154465436935425, KL: 75.11918640136719, Loss: 0.033693693578243256, Learning Rate: 0.007202999999999998\n",
      "Epoch [4449/20000], Bound: 0.41735878586769104, Entropy: 137.04306030273438, Temp: 2.154975652694702, KL: 72.49267578125, Loss: 0.030752619728446007, Learning Rate: 0.007202999999999998\n",
      "Epoch [4450/20000], Bound: 0.4233049154281616, Entropy: 138.45408630371094, Temp: 2.1554644107818604, KL: 74.33076477050781, Loss: 0.030380822718143463, Learning Rate: 0.007202999999999998\n",
      "Epoch [4451/20000], Bound: 0.4255868196487427, Entropy: 137.35423278808594, Temp: 2.1559557914733887, KL: 72.64990234375, Loss: 0.035788025707006454, Learning Rate: 0.007202999999999998\n",
      "Epoch [4452/20000], Bound: 0.3986240029335022, Entropy: 137.16897583007812, Temp: 2.156322479248047, KL: 66.44157409667969, Loss: 0.03279312700033188, Learning Rate: 0.007202999999999998\n",
      "Epoch [4453/20000], Bound: 0.4135096073150635, Entropy: 137.26197814941406, Temp: 2.156585454940796, KL: 69.08375549316406, Loss: 0.036195553839206696, Learning Rate: 0.007202999999999998\n",
      "Epoch [4454/20000], Bound: 0.41357263922691345, Entropy: 137.40135192871094, Temp: 2.1567063331604004, KL: 70.85012817382812, Loss: 0.03214341402053833, Learning Rate: 0.007202999999999998\n",
      "Epoch [4455/20000], Bound: 0.4468623995780945, Entropy: 134.41470336914062, Temp: 2.156799793243408, KL: 78.53125, Loss: 0.036373041570186615, Learning Rate: 0.007202999999999998\n",
      "Epoch [4456/20000], Bound: 0.41320401430130005, Entropy: 138.71006774902344, Temp: 2.156843423843384, KL: 69.8648681640625, Loss: 0.03419197350740433, Learning Rate: 0.007202999999999998\n",
      "Epoch [4457/20000], Bound: 0.4367128908634186, Entropy: 139.01402282714844, Temp: 2.156815528869629, KL: 76.359130859375, Loss: 0.034579310566186905, Learning Rate: 0.007202999999999998\n",
      "Epoch [4458/20000], Bound: 0.40168002247810364, Entropy: 137.38194274902344, Temp: 2.156769275665283, KL: 69.37109375, Loss: 0.027948835864663124, Learning Rate: 0.007202999999999998\n",
      "Epoch [4459/20000], Bound: 0.4329560399055481, Entropy: 141.6314239501953, Temp: 2.1567888259887695, KL: 74.46945190429688, Loss: 0.036456771194934845, Learning Rate: 0.007202999999999998\n",
      "Epoch [4460/20000], Bound: 0.39784330129623413, Entropy: 142.403564453125, Temp: 2.1567304134368896, KL: 66.96006774902344, Loss: 0.03110428713262081, Learning Rate: 0.007202999999999998\n",
      "Epoch [4461/20000], Bound: 0.4068351686000824, Entropy: 140.31195068359375, Temp: 2.156651258468628, KL: 69.43930053710938, Loss: 0.031078103929758072, Learning Rate: 0.007202999999999998\n",
      "Epoch [4462/20000], Bound: 0.42013540863990784, Entropy: 137.94754028320312, Temp: 2.1565749645233154, KL: 72.68925476074219, Loss: 0.03213886171579361, Learning Rate: 0.007202999999999998\n",
      "Epoch [4463/20000], Bound: 0.39932581782341003, Entropy: 139.25619506835938, Temp: 2.156506061553955, KL: 68.10151672363281, Loss: 0.029392365366220474, Learning Rate: 0.007202999999999998\n",
      "Epoch [4464/20000], Bound: 0.3969686031341553, Entropy: 142.52279663085938, Temp: 2.156464099884033, KL: 66.51411437988281, Loss: 0.0315803624689579, Learning Rate: 0.007202999999999998\n",
      "Epoch [4465/20000], Bound: 0.38020211458206177, Entropy: 142.63818359375, Temp: 2.1563851833343506, KL: 62.01300048828125, Loss: 0.03154068812727928, Learning Rate: 0.007202999999999998\n",
      "Epoch [4466/20000], Bound: 0.41931048035621643, Entropy: 140.89529418945312, Temp: 2.156235456466675, KL: 72.87339782714844, Loss: 0.03116675280034542, Learning Rate: 0.007202999999999998\n",
      "Epoch [4467/20000], Bound: 0.39581772685050964, Entropy: 143.61727905273438, Temp: 2.156122922897339, KL: 68.92384338378906, Loss: 0.025259332731366158, Learning Rate: 0.007202999999999998\n",
      "Epoch [4468/20000], Bound: 0.4155810475349426, Entropy: 144.08494567871094, Temp: 2.156137704849243, KL: 71.20448303222656, Loss: 0.03261057659983635, Learning Rate: 0.007202999999999998\n",
      "Epoch [4469/20000], Bound: 0.4186759889125824, Entropy: 140.2466583251953, Temp: 2.1561288833618164, KL: 75.46310424804688, Loss: 0.024745745584368706, Learning Rate: 0.007202999999999998\n",
      "Epoch [4470/20000], Bound: 0.43542245030403137, Entropy: 140.68115234375, Temp: 2.156301736831665, KL: 76.95298767089844, Loss: 0.032329633831977844, Learning Rate: 0.007202999999999998\n",
      "Epoch [4471/20000], Bound: 0.43758973479270935, Entropy: 142.48329162597656, Temp: 2.156489372253418, KL: 77.09849548339844, Loss: 0.03344394639134407, Learning Rate: 0.007202999999999998\n",
      "Epoch [4472/20000], Bound: 0.42448171973228455, Entropy: 142.59983825683594, Temp: 2.156667470932007, KL: 76.33601379394531, Loss: 0.026530465111136436, Learning Rate: 0.007202999999999998\n",
      "Epoch [4473/20000], Bound: 0.4388800263404846, Entropy: 142.85891723632812, Temp: 2.1569771766662598, KL: 78.73678588867188, Loss: 0.030521154403686523, Learning Rate: 0.007202999999999998\n",
      "Epoch [4474/20000], Bound: 0.41056832671165466, Entropy: 139.4596710205078, Temp: 2.1573400497436523, KL: 69.51089477539062, Loss: 0.0333222821354866, Learning Rate: 0.007202999999999998\n",
      "Epoch [4475/20000], Bound: 0.4237743318080902, Entropy: 139.87872314453125, Temp: 2.15761399269104, KL: 75.20791625976562, Loss: 0.028703512623906136, Learning Rate: 0.007202999999999998\n",
      "Epoch [4476/20000], Bound: 0.41099387407302856, Entropy: 141.3351593017578, Temp: 2.1579537391662598, KL: 72.26141357421875, Loss: 0.02723497897386551, Learning Rate: 0.007202999999999998\n",
      "Epoch [4477/20000], Bound: 0.4403117895126343, Entropy: 140.60348510742188, Temp: 2.1583597660064697, KL: 79.691162109375, Loss: 0.0293037798255682, Learning Rate: 0.007202999999999998\n",
      "Epoch [4478/20000], Bound: 0.40455350279808044, Entropy: 140.4944305419922, Temp: 2.158841609954834, KL: 70.46829223632812, Loss: 0.02727976068854332, Learning Rate: 0.007202999999999998\n",
      "Epoch [4479/20000], Bound: 0.4268377423286438, Entropy: 141.84239196777344, Temp: 2.1593592166900635, KL: 74.23725891113281, Loss: 0.033003050833940506, Learning Rate: 0.007202999999999998\n",
      "Epoch [4480/20000], Bound: 0.4151898920536041, Entropy: 137.70977783203125, Temp: 2.1598172187805176, KL: 72.92764282226562, Loss: 0.028441952541470528, Learning Rate: 0.007202999999999998\n",
      "Epoch [4481/20000], Bound: 0.4335864782333374, Entropy: 139.06983947753906, Temp: 2.1603078842163086, KL: 77.67839050292969, Loss: 0.029521044343709946, Learning Rate: 0.007202999999999998\n",
      "Epoch [4482/20000], Bound: 0.40968436002731323, Entropy: 137.9918670654297, Temp: 2.1608433723449707, KL: 70.16537475585938, Loss: 0.031306445598602295, Learning Rate: 0.007202999999999998\n",
      "Epoch [4483/20000], Bound: 0.38157394528388977, Entropy: 139.67869567871094, Temp: 2.161318302154541, KL: 64.0950927734375, Loss: 0.02765008434653282, Learning Rate: 0.007202999999999998\n",
      "Epoch [4484/20000], Bound: 0.42975181341171265, Entropy: 136.5537872314453, Temp: 2.1617653369903564, KL: 76.83038330078125, Loss: 0.028977373614907265, Learning Rate: 0.007202999999999998\n",
      "Epoch [4485/20000], Bound: 0.48387736082077026, Entropy: 138.4569854736328, Temp: 2.1622653007507324, KL: 90.47711181640625, Loss: 0.03463546186685562, Learning Rate: 0.007202999999999998\n",
      "Epoch [4486/20000], Bound: 0.4039615988731384, Entropy: 137.5613250732422, Temp: 2.162801742553711, KL: 65.93972778320312, Loss: 0.03745487332344055, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4487/20000], Bound: 0.4239167273044586, Entropy: 135.07757568359375, Temp: 2.163106679916382, KL: 76.34465026855469, Loss: 0.026294780895113945, Learning Rate: 0.007202999999999998\n",
      "Epoch [4488/20000], Bound: 0.4302564859390259, Entropy: 136.26451110839844, Temp: 2.1635308265686035, KL: 76.64857482910156, Loss: 0.029772266745567322, Learning Rate: 0.007202999999999998\n",
      "Epoch [4489/20000], Bound: 0.451498419046402, Entropy: 133.6254425048828, Temp: 2.1639904975891113, KL: 82.68040466308594, Loss: 0.03009626269340515, Learning Rate: 0.007202999999999998\n",
      "Epoch [4490/20000], Bound: 0.41213229298591614, Entropy: 133.40536499023438, Temp: 2.164522647857666, KL: 70.82205200195312, Loss: 0.03143533319234848, Learning Rate: 0.007202999999999998\n",
      "Epoch [4491/20000], Bound: 0.45584243535995483, Entropy: 134.104248046875, Temp: 2.16499400138855, KL: 82.60385131835938, Loss: 0.033267512917518616, Learning Rate: 0.007202999999999998\n",
      "Epoch [4492/20000], Bound: 0.41154736280441284, Entropy: 132.7942657470703, Temp: 2.16546893119812, KL: 71.37287902832031, Loss: 0.029805967584252357, Learning Rate: 0.007202999999999998\n",
      "Epoch [4493/20000], Bound: 0.40561550855636597, Entropy: 132.96376037597656, Temp: 2.1659274101257324, KL: 71.58135986328125, Loss: 0.025535091757774353, Learning Rate: 0.007202999999999998\n",
      "Epoch [4494/20000], Bound: 0.4402535557746887, Entropy: 131.9081573486328, Temp: 2.1664652824401855, KL: 79.55714416503906, Loss: 0.029774999246001244, Learning Rate: 0.007202999999999998\n",
      "Epoch [4495/20000], Bound: 0.44122669100761414, Entropy: 132.44444274902344, Temp: 2.167048215866089, KL: 77.51876831054688, Loss: 0.035144466906785965, Learning Rate: 0.007202999999999998\n",
      "Epoch [4496/20000], Bound: 0.45343995094299316, Entropy: 132.1222686767578, Temp: 2.167539596557617, KL: 84.01113891601562, Loss: 0.02844204753637314, Learning Rate: 0.007202999999999998\n",
      "Epoch [4497/20000], Bound: 0.4386192262172699, Entropy: 134.36305236816406, Temp: 2.1681437492370605, KL: 76.73704528808594, Loss: 0.03522650897502899, Learning Rate: 0.007202999999999998\n",
      "Epoch [4498/20000], Bound: 0.4246213436126709, Entropy: 135.15899658203125, Temp: 2.168644905090332, KL: 73.10205078125, Loss: 0.034363411366939545, Learning Rate: 0.007202999999999998\n",
      "Epoch [4499/20000], Bound: 0.4320279657840729, Entropy: 134.481689453125, Temp: 2.1690406799316406, KL: 72.74752807617188, Loss: 0.04006180167198181, Learning Rate: 0.007202999999999998\n",
      "Epoch [4500/20000], Bound: 0.4347696602344513, Entropy: 136.0318603515625, Temp: 2.1692159175872803, KL: 78.47946166992188, Loss: 0.028670024126768112, Learning Rate: 0.007202999999999998\n",
      "Epoch [4501/20000], Bound: 0.4369698166847229, Entropy: 136.5233612060547, Temp: 2.169485092163086, KL: 79.00617980957031, Loss: 0.028925755992531776, Learning Rate: 0.007202999999999998\n",
      "Epoch [4502/20000], Bound: 0.43313106894493103, Entropy: 134.48126220703125, Temp: 2.16983699798584, KL: 77.19851684570312, Loss: 0.0305500365793705, Learning Rate: 0.007202999999999998\n",
      "Epoch [4503/20000], Bound: 0.4256778061389923, Entropy: 138.33477783203125, Temp: 2.1702136993408203, KL: 72.45402526855469, Loss: 0.03657952696084976, Learning Rate: 0.007202999999999998\n",
      "Epoch [4504/20000], Bound: 0.40931597352027893, Entropy: 136.5633087158203, Temp: 2.1704423427581787, KL: 70.99165344238281, Loss: 0.029354622587561607, Learning Rate: 0.007202999999999998\n",
      "Epoch [4505/20000], Bound: 0.4194468557834625, Entropy: 137.0639190673828, Temp: 2.170681953430176, KL: 74.15728759765625, Loss: 0.028600549325346947, Learning Rate: 0.007202999999999998\n",
      "Epoch [4506/20000], Bound: 0.43426650762557983, Entropy: 134.556884765625, Temp: 2.170973539352417, KL: 74.63165283203125, Loss: 0.037240248173475266, Learning Rate: 0.007202999999999998\n",
      "Epoch [4507/20000], Bound: 0.4083556532859802, Entropy: 136.4779510498047, Temp: 2.1711297035217285, KL: 71.08164978027344, Loss: 0.028546806424856186, Learning Rate: 0.007202999999999998\n",
      "Epoch [4508/20000], Bound: 0.403317928314209, Entropy: 138.16693115234375, Temp: 2.1713216304779053, KL: 69.43869018554688, Loss: 0.029124459251761436, Learning Rate: 0.007202999999999998\n",
      "Epoch [4509/20000], Bound: 0.46489447355270386, Entropy: 137.08352661132812, Temp: 2.1715188026428223, KL: 84.16229248046875, Loss: 0.03607948124408722, Learning Rate: 0.007202999999999998\n",
      "Epoch [4510/20000], Bound: 0.42711204290390015, Entropy: 136.2760772705078, Temp: 2.1716935634613037, KL: 72.22799682617188, Loss: 0.03806854784488678, Learning Rate: 0.007202999999999998\n",
      "Epoch [4511/20000], Bound: 0.4222347140312195, Entropy: 136.4980010986328, Temp: 2.1717047691345215, KL: 73.50352478027344, Loss: 0.031941793859004974, Learning Rate: 0.007202999999999998\n",
      "Epoch [4512/20000], Bound: 0.43075382709503174, Entropy: 137.705810546875, Temp: 2.1717119216918945, KL: 77.85050964355469, Loss: 0.027520572766661644, Learning Rate: 0.007202999999999998\n",
      "Epoch [4513/20000], Bound: 0.42681506276130676, Entropy: 138.23904418945312, Temp: 2.1718475818634033, KL: 74.66645812988281, Loss: 0.032262738794088364, Learning Rate: 0.007202999999999998\n",
      "Epoch [4514/20000], Bound: 0.42016345262527466, Entropy: 138.650634765625, Temp: 2.1719696521759033, KL: 75.7257080078125, Loss: 0.0254836343228817, Learning Rate: 0.007202999999999998\n",
      "Epoch [4515/20000], Bound: 0.47563236951828003, Entropy: 139.5756072998047, Temp: 2.172236204147339, KL: 90.11482238769531, Loss: 0.029905136674642563, Learning Rate: 0.007202999999999998\n",
      "Epoch [4516/20000], Bound: 0.4498085081577301, Entropy: 134.50607299804688, Temp: 2.1726505756378174, KL: 82.13357543945312, Loss: 0.030430292710661888, Learning Rate: 0.007202999999999998\n",
      "Epoch [4517/20000], Bound: 0.4324437379837036, Entropy: 138.8186798095703, Temp: 2.173123598098755, KL: 79.36320495605469, Loss: 0.025189779698848724, Learning Rate: 0.007202999999999998\n",
      "Epoch [4518/20000], Bound: 0.4078938961029053, Entropy: 136.1632537841797, Temp: 2.173741102218628, KL: 71.87825012207031, Loss: 0.02647281624376774, Learning Rate: 0.007202999999999998\n",
      "Epoch [4519/20000], Bound: 0.4199092388153076, Entropy: 138.66171264648438, Temp: 2.174398422241211, KL: 69.34645080566406, Loss: 0.04004418104887009, Learning Rate: 0.007202999999999998\n",
      "Epoch [4520/20000], Bound: 0.44749563932418823, Entropy: 135.62857055664062, Temp: 2.1747732162475586, KL: 80.59066772460938, Loss: 0.03246589004993439, Learning Rate: 0.007202999999999998\n",
      "Epoch [4521/20000], Bound: 0.3939802050590515, Entropy: 134.1340789794922, Temp: 2.175152540206909, KL: 67.29714965820312, Loss: 0.028236841782927513, Learning Rate: 0.007202999999999998\n",
      "Epoch [4522/20000], Bound: 0.40053847432136536, Entropy: 135.8588104248047, Temp: 2.1755166053771973, KL: 69.74462890625, Loss: 0.02674349769949913, Learning Rate: 0.007202999999999998\n",
      "Epoch [4523/20000], Bound: 0.44732990860939026, Entropy: 132.59814453125, Temp: 2.1759209632873535, KL: 79.30534362792969, Loss: 0.03533480316400528, Learning Rate: 0.007202999999999998\n",
      "Epoch [4524/20000], Bound: 0.4382690191268921, Entropy: 137.99354553222656, Temp: 2.176253318786621, KL: 81.03797912597656, Loss: 0.025284942239522934, Learning Rate: 0.007202999999999998\n",
      "Epoch [4525/20000], Bound: 0.4218027591705322, Entropy: 136.49667358398438, Temp: 2.176753044128418, KL: 74.12336730957031, Loss: 0.030340787023305893, Learning Rate: 0.007202999999999998\n",
      "Epoch [4526/20000], Bound: 0.4085860848426819, Entropy: 136.5595245361328, Temp: 2.1772353649139404, KL: 69.28305053710938, Loss: 0.03294840455055237, Learning Rate: 0.007202999999999998\n",
      "Epoch [4527/20000], Bound: 0.372226357460022, Entropy: 139.58079528808594, Temp: 2.1776039600372314, KL: 60.271575927734375, Loss: 0.030998138710856438, Learning Rate: 0.007202999999999998\n",
      "Epoch [4528/20000], Bound: 0.4314098656177521, Entropy: 143.02157592773438, Temp: 2.177832841873169, KL: 75.77915954589844, Loss: 0.032853685319423676, Learning Rate: 0.007202999999999998\n",
      "Epoch [4529/20000], Bound: 0.4256504476070404, Entropy: 140.3199920654297, Temp: 2.178030014038086, KL: 76.480712890625, Loss: 0.027465438470244408, Learning Rate: 0.007202999999999998\n",
      "Epoch [4530/20000], Bound: 0.4288727641105652, Entropy: 141.23455810546875, Temp: 2.1783218383789062, KL: 77.39031982421875, Loss: 0.02749621868133545, Learning Rate: 0.007202999999999998\n",
      "Epoch [4531/20000], Bound: 0.42182794213294983, Entropy: 143.13668823242188, Temp: 2.1787052154541016, KL: 75.27008056640625, Loss: 0.027766698971390724, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4532/20000], Bound: 0.41502633690834045, Entropy: 143.090087890625, Temp: 2.179147958755493, KL: 72.22349548339844, Loss: 0.030365440994501114, Learning Rate: 0.007202999999999998\n",
      "Epoch [4533/20000], Bound: 0.4104507565498352, Entropy: 141.0982666015625, Temp: 2.179560661315918, KL: 70.7283935546875, Loss: 0.03086652234196663, Learning Rate: 0.007202999999999998\n",
      "Epoch [4534/20000], Bound: 0.40821054577827454, Entropy: 141.221923828125, Temp: 2.179922580718994, KL: 72.33283996582031, Loss: 0.02576245740056038, Learning Rate: 0.007202999999999998\n",
      "Epoch [4535/20000], Bound: 0.3977559208869934, Entropy: 140.93016052246094, Temp: 2.1803646087646484, KL: 69.25956726074219, Loss: 0.026200370863080025, Learning Rate: 0.007202999999999998\n",
      "Epoch [4536/20000], Bound: 0.45387032628059387, Entropy: 142.61802673339844, Temp: 2.1808435916900635, KL: 85.72329711914062, Loss: 0.025161823257803917, Learning Rate: 0.007202999999999998\n",
      "Epoch [4537/20000], Bound: 0.39585182070732117, Entropy: 138.9442901611328, Temp: 2.1815109252929688, KL: 67.6356201171875, Loss: 0.028750166296958923, Learning Rate: 0.007202999999999998\n",
      "Epoch [4538/20000], Bound: 0.37457671761512756, Entropy: 143.9147186279297, Temp: 2.1821205615997314, KL: 59.87493896484375, Loss: 0.03340185433626175, Learning Rate: 0.007202999999999998\n",
      "Epoch [4539/20000], Bound: 0.40442657470703125, Entropy: 141.606689453125, Temp: 2.1825051307678223, KL: 71.47216796875, Loss: 0.025383787229657173, Learning Rate: 0.007202999999999998\n",
      "Epoch [4540/20000], Bound: 0.45406728982925415, Entropy: 141.0810089111328, Temp: 2.182967185974121, KL: 83.09580993652344, Loss: 0.03137301281094551, Learning Rate: 0.007202999999999998\n",
      "Epoch [4541/20000], Bound: 0.43692561984062195, Entropy: 139.9744873046875, Temp: 2.1834616661071777, KL: 79.70578002929688, Loss: 0.027627231553196907, Learning Rate: 0.007202999999999998\n",
      "Epoch [4542/20000], Bound: 0.40644001960754395, Entropy: 139.9615936279297, Temp: 2.184039354324341, KL: 70.44129943847656, Loss: 0.029053697362542152, Learning Rate: 0.007202999999999998\n",
      "Epoch [4543/20000], Bound: 0.438430517911911, Entropy: 136.47425842285156, Temp: 2.1845836639404297, KL: 81.00727844238281, Loss: 0.02567511796951294, Learning Rate: 0.007202999999999998\n",
      "Epoch [4544/20000], Bound: 0.43250173330307007, Entropy: 138.72377014160156, Temp: 2.185258626937866, KL: 77.14743041992188, Loss: 0.030598586425185204, Learning Rate: 0.007202999999999998\n",
      "Epoch [4545/20000], Bound: 0.4249401390552521, Entropy: 136.74755859375, Temp: 2.185911178588867, KL: 76.36552429199219, Loss: 0.02744472399353981, Learning Rate: 0.007202999999999998\n",
      "Epoch [4546/20000], Bound: 0.44633686542510986, Entropy: 136.92904663085938, Temp: 2.1866064071655273, KL: 80.97044372558594, Loss: 0.031095558777451515, Learning Rate: 0.007202999999999998\n",
      "Epoch [4547/20000], Bound: 0.44333332777023315, Entropy: 134.42481994628906, Temp: 2.1872963905334473, KL: 78.5440673828125, Loss: 0.03464379534125328, Learning Rate: 0.007202999999999998\n",
      "Epoch [4548/20000], Bound: 0.4330208897590637, Entropy: 135.28997802734375, Temp: 2.1878833770751953, KL: 77.4376220703125, Loss: 0.030335448682308197, Learning Rate: 0.007202999999999998\n",
      "Epoch [4549/20000], Bound: 0.40941277146339417, Entropy: 135.349609375, Temp: 2.188462257385254, KL: 71.339111328125, Loss: 0.028979158028960228, Learning Rate: 0.007202999999999998\n",
      "Epoch [4550/20000], Bound: 0.41187185049057007, Entropy: 136.4298095703125, Temp: 2.1890132427215576, KL: 70.79501342773438, Loss: 0.03180284425616264, Learning Rate: 0.007202999999999998\n",
      "Epoch [4551/20000], Bound: 0.4242991805076599, Entropy: 137.79054260253906, Temp: 2.1894702911376953, KL: 75.66889953613281, Loss: 0.028698090463876724, Learning Rate: 0.007202999999999998\n",
      "Epoch [4552/20000], Bound: 0.3977563977241516, Entropy: 136.406982421875, Temp: 2.189952850341797, KL: 70.39105224609375, Loss: 0.023805327713489532, Learning Rate: 0.007202999999999998\n",
      "Epoch [4553/20000], Bound: 0.42672255635261536, Entropy: 137.07772827148438, Temp: 2.190523862838745, KL: 75.72288513183594, Loss: 0.03017711639404297, Learning Rate: 0.007202999999999998\n",
      "Epoch [4554/20000], Bound: 0.416763037443161, Entropy: 137.3545379638672, Temp: 2.1910758018493652, KL: 74.00917053222656, Loss: 0.027643918991088867, Learning Rate: 0.007202999999999998\n",
      "Epoch [4555/20000], Bound: 0.42119234800338745, Entropy: 139.05648803710938, Temp: 2.1916518211364746, KL: 73.89280700683594, Loss: 0.03078092448413372, Learning Rate: 0.007202999999999998\n",
      "Epoch [4556/20000], Bound: 0.3939547836780548, Entropy: 138.74266052246094, Temp: 2.19217848777771, KL: 67.76002502441406, Loss: 0.027475828304886818, Learning Rate: 0.007202999999999998\n",
      "Epoch [4557/20000], Bound: 0.44987207651138306, Entropy: 139.01669311523438, Temp: 2.1926827430725098, KL: 82.77462768554688, Loss: 0.02950431779026985, Learning Rate: 0.007202999999999998\n",
      "Epoch [4558/20000], Bound: 0.4109373986721039, Entropy: 139.41294860839844, Temp: 2.193244695663452, KL: 71.71946716308594, Loss: 0.029177462682127953, Learning Rate: 0.007202999999999998\n",
      "Epoch [4559/20000], Bound: 0.4383945167064667, Entropy: 139.18276977539062, Temp: 2.193774700164795, KL: 81.71429443359375, Loss: 0.024269191548228264, Learning Rate: 0.007202999999999998\n",
      "Epoch [4560/20000], Bound: 0.4320777654647827, Entropy: 141.36732482910156, Temp: 2.1944665908813477, KL: 74.9705810546875, Loss: 0.035481106489896774, Learning Rate: 0.007202999999999998\n",
      "Epoch [4561/20000], Bound: 0.4098406136035919, Entropy: 139.64695739746094, Temp: 2.1949994564056396, KL: 69.51284790039062, Loss: 0.03353935107588768, Learning Rate: 0.007202999999999998\n",
      "Epoch [4562/20000], Bound: 0.4411071836948395, Entropy: 140.6737823486328, Temp: 2.1953847408294678, KL: 82.55790710449219, Loss: 0.024191144853830338, Learning Rate: 0.007202999999999998\n",
      "Epoch [4563/20000], Bound: 0.44458386301994324, Entropy: 138.63140869140625, Temp: 2.195953607559204, KL: 81.1053466796875, Loss: 0.029833437874913216, Learning Rate: 0.007202999999999998\n",
      "Epoch [4564/20000], Bound: 0.43943482637405396, Entropy: 137.67999267578125, Temp: 2.1965503692626953, KL: 81.12994384765625, Loss: 0.026359882205724716, Learning Rate: 0.007202999999999998\n",
      "Epoch [4565/20000], Bound: 0.4050532877445221, Entropy: 135.7161865234375, Temp: 2.197249174118042, KL: 69.8057861328125, Loss: 0.029875459149479866, Learning Rate: 0.007202999999999998\n",
      "Epoch [4566/20000], Bound: 0.39144274592399597, Entropy: 137.31541442871094, Temp: 2.197866678237915, KL: 64.75375366210938, Loss: 0.03285754472017288, Learning Rate: 0.007202999999999998\n",
      "Epoch [4567/20000], Bound: 0.4040183424949646, Entropy: 136.2451171875, Temp: 2.1982991695404053, KL: 69.82196044921875, Loss: 0.02920433133840561, Learning Rate: 0.007202999999999998\n",
      "Epoch [4568/20000], Bound: 0.4392089247703552, Entropy: 136.5248565673828, Temp: 2.1986913681030273, KL: 80.45213317871094, Loss: 0.027803706005215645, Learning Rate: 0.007202999999999998\n",
      "Epoch [4569/20000], Bound: 0.4302690327167511, Entropy: 137.2062530517578, Temp: 2.199166774749756, KL: 80.41334533691406, Loss: 0.02201147936284542, Learning Rate: 0.007202999999999998\n",
      "Epoch [4570/20000], Bound: 0.40252041816711426, Entropy: 136.99319458007812, Temp: 2.199846029281616, KL: 68.56668090820312, Loss: 0.03114211931824684, Learning Rate: 0.007202999999999998\n",
      "Epoch [4571/20000], Bound: 0.42444711923599243, Entropy: 135.7123565673828, Temp: 2.200404405593872, KL: 74.10307312011719, Loss: 0.03258642926812172, Learning Rate: 0.007202999999999998\n",
      "Epoch [4572/20000], Bound: 0.46852922439575195, Entropy: 135.6768035888672, Temp: 2.2008683681488037, KL: 88.58302307128906, Loss: 0.02925180457532406, Learning Rate: 0.007202999999999998\n",
      "Epoch [4573/20000], Bound: 0.4106358289718628, Entropy: 140.23777770996094, Temp: 2.2014379501342773, KL: 71.5384521484375, Loss: 0.029555724933743477, Learning Rate: 0.007202999999999998\n",
      "Epoch [4574/20000], Bound: 0.4210819900035858, Entropy: 136.47988891601562, Temp: 2.2019577026367188, KL: 75.63046264648438, Loss: 0.02697013132274151, Learning Rate: 0.007202999999999998\n",
      "Epoch [4575/20000], Bound: 0.43374955654144287, Entropy: 137.67762756347656, Temp: 2.202524423599243, KL: 74.91775512695312, Loss: 0.03685399144887924, Learning Rate: 0.007202999999999998\n",
      "Epoch [4576/20000], Bound: 0.41832050681114197, Entropy: 138.06716918945312, Temp: 2.2029054164886475, KL: 74.79032897949219, Loss: 0.02711803838610649, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4577/20000], Bound: 0.43323618173599243, Entropy: 137.0778350830078, Temp: 2.203336238861084, KL: 78.09634399414062, Loss: 0.02931866981089115, Learning Rate: 0.007202999999999998\n",
      "Epoch [4578/20000], Bound: 0.39327818155288696, Entropy: 137.24234008789062, Temp: 2.203788995742798, KL: 67.65132141113281, Loss: 0.027512039989233017, Learning Rate: 0.007202999999999998\n",
      "Epoch [4579/20000], Bound: 0.4430777430534363, Entropy: 137.65975952148438, Temp: 2.2042150497436523, KL: 78.84544372558594, Loss: 0.034148748964071274, Learning Rate: 0.007202999999999998\n",
      "Epoch [4580/20000], Bound: 0.40046635270118713, Entropy: 134.8636932373047, Temp: 2.2045609951019287, KL: 70.54505920410156, Loss: 0.025447139516472816, Learning Rate: 0.007202999999999998\n",
      "Epoch [4581/20000], Bound: 0.42899978160858154, Entropy: 136.97073364257812, Temp: 2.2049615383148193, KL: 78.796875, Loss: 0.024992622435092926, Learning Rate: 0.007202999999999998\n",
      "Epoch [4582/20000], Bound: 0.44169995188713074, Entropy: 137.06434631347656, Temp: 2.20548939704895, KL: 78.53889465332031, Loss: 0.03395382687449455, Learning Rate: 0.007202999999999998\n",
      "Epoch [4583/20000], Bound: 0.44669196009635925, Entropy: 137.1962890625, Temp: 2.2059273719787598, KL: 80.22210693359375, Loss: 0.033477798104286194, Learning Rate: 0.007202999999999998\n",
      "Epoch [4584/20000], Bound: 0.42212632298469543, Entropy: 136.80517578125, Temp: 2.206308603286743, KL: 76.86260986328125, Loss: 0.02494526281952858, Learning Rate: 0.007202999999999998\n",
      "Epoch [4585/20000], Bound: 0.4513694941997528, Entropy: 136.35691833496094, Temp: 2.206803560256958, KL: 83.95979309082031, Loss: 0.028170228004455566, Learning Rate: 0.007202999999999998\n",
      "Epoch [4586/20000], Bound: 0.4631957709789276, Entropy: 135.41485595703125, Temp: 2.2073841094970703, KL: 85.53121948242188, Loss: 0.032666273415088654, Learning Rate: 0.007202999999999998\n",
      "Epoch [4587/20000], Bound: 0.4420871436595917, Entropy: 133.3026580810547, Temp: 2.2079532146453857, KL: 81.59913635253906, Loss: 0.02733258716762066, Learning Rate: 0.007202999999999998\n",
      "Epoch [4588/20000], Bound: 0.4162759482860565, Entropy: 133.14126586914062, Temp: 2.2086000442504883, KL: 74.76179504394531, Loss: 0.025991002097725868, Learning Rate: 0.007202999999999998\n",
      "Epoch [4589/20000], Bound: 0.41521546244621277, Entropy: 133.62966918945312, Temp: 2.2092909812927246, KL: 72.160888671875, Loss: 0.03121342882514, Learning Rate: 0.007202999999999998\n",
      "Epoch [4590/20000], Bound: 0.4351760447025299, Entropy: 132.3115997314453, Temp: 2.2098805904388428, KL: 77.83436584472656, Loss: 0.03132842853665352, Learning Rate: 0.007202999999999998\n",
      "Epoch [4591/20000], Bound: 0.40940043330192566, Entropy: 131.16102600097656, Temp: 2.2104227542877197, KL: 72.65972900390625, Loss: 0.02640526555478573, Learning Rate: 0.007202999999999998\n",
      "Epoch [4592/20000], Bound: 0.42205315828323364, Entropy: 131.55821228027344, Temp: 2.210991382598877, KL: 74.13912963867188, Loss: 0.03116152249276638, Learning Rate: 0.007202999999999998\n",
      "Epoch [4593/20000], Bound: 0.45095929503440857, Entropy: 131.23988342285156, Temp: 2.211487054824829, KL: 83.65132141113281, Loss: 0.02870742790400982, Learning Rate: 0.007202999999999998\n",
      "Epoch [4594/20000], Bound: 0.43684059381484985, Entropy: 132.57888793945312, Temp: 2.212049961090088, KL: 76.573486328125, Loss: 0.035319529473781586, Learning Rate: 0.007202999999999998\n",
      "Epoch [4595/20000], Bound: 0.4212897717952728, Entropy: 133.71214294433594, Temp: 2.212465524673462, KL: 76.06590270996094, Loss: 0.026343664154410362, Learning Rate: 0.007202999999999998\n",
      "Epoch [4596/20000], Bound: 0.4146699011325836, Entropy: 135.38186645507812, Temp: 2.2129480838775635, KL: 71.84226989746094, Loss: 0.03165280818939209, Learning Rate: 0.007202999999999998\n",
      "Epoch [4597/20000], Bound: 0.43592867255210876, Entropy: 132.57069396972656, Temp: 2.2133333683013916, KL: 78.99050903320312, Loss: 0.029284019023180008, Learning Rate: 0.007202999999999998\n",
      "Epoch [4598/20000], Bound: 0.4098719358444214, Entropy: 132.98062133789062, Temp: 2.213744878768921, KL: 73.5487060546875, Loss: 0.024762315675616264, Learning Rate: 0.007202999999999998\n",
      "Epoch [4599/20000], Bound: 0.38008031249046326, Entropy: 137.18801879882812, Temp: 2.214238166809082, KL: 63.11213684082031, Loss: 0.029846634715795517, Learning Rate: 0.007202999999999998\n",
      "Epoch [4600/20000], Bound: 0.4453239440917969, Entropy: 135.02810668945312, Temp: 2.2145979404449463, KL: 80.11833190917969, Loss: 0.03298645839095116, Learning Rate: 0.007202999999999998\n",
      "Epoch [4601/20000], Bound: 0.4569893479347229, Entropy: 137.0004425048828, Temp: 2.2149107456207275, KL: 82.19244384765625, Loss: 0.03615586832165718, Learning Rate: 0.007202999999999998\n",
      "Epoch [4602/20000], Bound: 0.4308561086654663, Entropy: 138.23129272460938, Temp: 2.2151260375976562, KL: 76.14291381835938, Loss: 0.03242785483598709, Learning Rate: 0.007202999999999998\n",
      "Epoch [4603/20000], Bound: 0.4194566309452057, Entropy: 138.1752471923828, Temp: 2.215287685394287, KL: 73.34873962402344, Loss: 0.03135819360613823, Learning Rate: 0.007202999999999998\n",
      "Epoch [4604/20000], Bound: 0.43933364748954773, Entropy: 135.99729919433594, Temp: 2.2154016494750977, KL: 82.49937438964844, Loss: 0.023654311895370483, Learning Rate: 0.007202999999999998\n",
      "Epoch [4605/20000], Bound: 0.4076146185398102, Entropy: 140.70608520507812, Temp: 2.2157230377197266, KL: 70.63188171386719, Loss: 0.029957793653011322, Learning Rate: 0.007202999999999998\n",
      "Epoch [4606/20000], Bound: 0.41563060879707336, Entropy: 140.00584411621094, Temp: 2.215989589691162, KL: 68.77621459960938, Loss: 0.03923891857266426, Learning Rate: 0.007202999999999998\n",
      "Epoch [4607/20000], Bound: 0.41132989525794983, Entropy: 138.89015197753906, Temp: 2.215977191925049, KL: 70.01823425292969, Loss: 0.0336986668407917, Learning Rate: 0.007202999999999998\n",
      "Epoch [4608/20000], Bound: 0.4724035859107971, Entropy: 138.0255584716797, Temp: 2.215851306915283, KL: 84.33265686035156, Loss: 0.04192391410470009, Learning Rate: 0.007202999999999998\n",
      "Epoch [4609/20000], Bound: 0.43134260177612305, Entropy: 136.90098571777344, Temp: 2.215559720993042, KL: 76.72743225097656, Loss: 0.03143491968512535, Learning Rate: 0.007202999999999998\n",
      "Epoch [4610/20000], Bound: 0.4301789700984955, Entropy: 138.62210083007812, Temp: 2.215291976928711, KL: 76.76025390625, Loss: 0.030596069991588593, Learning Rate: 0.007202999999999998\n",
      "Epoch [4611/20000], Bound: 0.40330687165260315, Entropy: 136.99615478515625, Temp: 2.2150650024414062, KL: 70.83482360839844, Loss: 0.026775728911161423, Learning Rate: 0.007202999999999998\n",
      "Epoch [4612/20000], Bound: 0.39895114302635193, Entropy: 134.69273376464844, Temp: 2.214914083480835, KL: 67.25062561035156, Loss: 0.032137978821992874, Learning Rate: 0.007202999999999998\n",
      "Epoch [4613/20000], Bound: 0.4238283932209015, Entropy: 135.44546508789062, Temp: 2.2146759033203125, KL: 69.91206359863281, Loss: 0.041921719908714294, Learning Rate: 0.007202999999999998\n",
      "Epoch [4614/20000], Bound: 0.42355620861053467, Entropy: 137.22817993164062, Temp: 2.214158773422241, KL: 72.73976135253906, Loss: 0.035352807492017746, Learning Rate: 0.007202999999999998\n",
      "Epoch [4615/20000], Bound: 0.4024338126182556, Entropy: 137.02308654785156, Temp: 2.2135653495788574, KL: 72.04412841796875, Loss: 0.0234677717089653, Learning Rate: 0.007202999999999998\n",
      "Epoch [4616/20000], Bound: 0.4480818212032318, Entropy: 138.65919494628906, Temp: 2.2131729125976562, KL: 83.78974914550781, Loss: 0.026504479348659515, Learning Rate: 0.007202999999999998\n",
      "Epoch [4617/20000], Bound: 0.42240577936172485, Entropy: 138.4401092529297, Temp: 2.212986946105957, KL: 71.82366943359375, Loss: 0.03665915131568909, Learning Rate: 0.007202999999999998\n",
      "Epoch [4618/20000], Bound: 0.3943541347980499, Entropy: 137.9729766845703, Temp: 2.2126548290252686, KL: 67.20140075683594, Loss: 0.02935192920267582, Learning Rate: 0.007202999999999998\n",
      "Epoch [4619/20000], Bound: 0.47599390149116516, Entropy: 140.55662536621094, Temp: 2.2123196125030518, KL: 89.62065124511719, Loss: 0.03239909186959267, Learning Rate: 0.007202999999999998\n",
      "Epoch [4620/20000], Bound: 0.40207621455192566, Entropy: 138.5709991455078, Temp: 2.2120988368988037, KL: 66.0809326171875, Loss: 0.03669214993715286, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4621/20000], Bound: 0.43605145812034607, Entropy: 139.55816650390625, Temp: 2.211684226989746, KL: 76.4906005859375, Loss: 0.03498011454939842, Learning Rate: 0.007202999999999998\n",
      "Epoch [4622/20000], Bound: 0.43691176176071167, Entropy: 139.9210968017578, Temp: 2.211225986480713, KL: 77.88690185546875, Loss: 0.03238057717680931, Learning Rate: 0.007202999999999998\n",
      "Epoch [4623/20000], Bound: 0.454047828912735, Entropy: 139.31985473632812, Temp: 2.2108004093170166, KL: 81.19436645507812, Loss: 0.03633060306310654, Learning Rate: 0.007202999999999998\n",
      "Epoch [4624/20000], Bound: 0.47305408120155334, Entropy: 138.78668212890625, Temp: 2.2103424072265625, KL: 84.72116088867188, Loss: 0.041379738599061966, Learning Rate: 0.007202999999999998\n",
      "Epoch [4625/20000], Bound: 0.4195815324783325, Entropy: 137.37855529785156, Temp: 2.2097718715667725, KL: 72.14790344238281, Loss: 0.03405100107192993, Learning Rate: 0.007202999999999998\n",
      "Epoch [4626/20000], Bound: 0.4393390715122223, Entropy: 136.78512573242188, Temp: 2.2091591358184814, KL: 69.59947204589844, Loss: 0.05269794911146164, Learning Rate: 0.007202999999999998\n",
      "Epoch [4627/20000], Bound: 0.43395912647247314, Entropy: 135.00286865234375, Temp: 2.208063840866089, KL: 73.65541076660156, Loss: 0.039953019469976425, Learning Rate: 0.007202999999999998\n",
      "Epoch [4628/20000], Bound: 0.4378468990325928, Entropy: 132.88491821289062, Temp: 2.2068588733673096, KL: 74.39961242675781, Loss: 0.04080774262547493, Learning Rate: 0.007202999999999998\n",
      "Epoch [4629/20000], Bound: 0.4674956500530243, Entropy: 133.60987854003906, Temp: 2.2055435180664062, KL: 84.67823791503906, Loss: 0.037515513598918915, Learning Rate: 0.007202999999999998\n",
      "Epoch [4630/20000], Bound: 0.4140564203262329, Entropy: 132.73301696777344, Temp: 2.2042922973632812, KL: 68.95660400390625, Loss: 0.03764697164297104, Learning Rate: 0.007202999999999998\n",
      "Epoch [4631/20000], Bound: 0.4660162925720215, Entropy: 131.69021606445312, Temp: 2.202960968017578, KL: 83.67268371582031, Loss: 0.0387168750166893, Learning Rate: 0.007202999999999998\n",
      "Epoch [4632/20000], Bound: 0.3826204240322113, Entropy: 133.5797119140625, Temp: 2.201662540435791, KL: 59.23197937011719, Loss: 0.04002108424901962, Learning Rate: 0.007202999999999998\n",
      "Epoch [4633/20000], Bound: 0.43749502301216125, Entropy: 132.00421142578125, Temp: 2.200146198272705, KL: 77.18386840820312, Loss: 0.034130360931158066, Learning Rate: 0.007202999999999998\n",
      "Epoch [4634/20000], Bound: 0.41973230242729187, Entropy: 133.06280517578125, Temp: 2.198732614517212, KL: 72.25694274902344, Loss: 0.033700358122587204, Learning Rate: 0.007202999999999998\n",
      "Epoch [4635/20000], Bound: 0.42564842104911804, Entropy: 131.87234497070312, Temp: 2.1973800659179688, KL: 74.25569152832031, Loss: 0.03296102583408356, Learning Rate: 0.007202999999999998\n",
      "Epoch [4636/20000], Bound: 0.4200409948825836, Entropy: 131.3389129638672, Temp: 2.196117639541626, KL: 73.67549133300781, Loss: 0.030621571466326714, Learning Rate: 0.007202999999999998\n",
      "Epoch [4637/20000], Bound: 0.41355887055397034, Entropy: 132.731689453125, Temp: 2.1949868202209473, KL: 69.62911987304688, Loss: 0.035648126155138016, Learning Rate: 0.007202999999999998\n",
      "Epoch [4638/20000], Bound: 0.4163264334201813, Entropy: 134.1996612548828, Temp: 2.193824052810669, KL: 68.07719421386719, Loss: 0.04094087332487106, Learning Rate: 0.007202999999999998\n",
      "Epoch [4639/20000], Bound: 0.4300650656223297, Entropy: 135.65065002441406, Temp: 2.192497968673706, KL: 74.09712219238281, Loss: 0.036113061010837555, Learning Rate: 0.007202999999999998\n",
      "Epoch [4640/20000], Bound: 0.39637434482574463, Entropy: 137.22991943359375, Temp: 2.1911911964416504, KL: 68.14265441894531, Loss: 0.028095802292227745, Learning Rate: 0.007202999999999998\n",
      "Epoch [4641/20000], Bound: 0.4070682227611542, Entropy: 139.8029022216797, Temp: 2.190035104751587, KL: 67.05848693847656, Loss: 0.03729158639907837, Learning Rate: 0.007202999999999998\n",
      "Epoch [4642/20000], Bound: 0.41569381952285767, Entropy: 140.3284149169922, Temp: 2.1887922286987305, KL: 70.40487670898438, Loss: 0.0351409986615181, Learning Rate: 0.007202999999999998\n",
      "Epoch [4643/20000], Bound: 0.4332403540611267, Entropy: 140.1237030029297, Temp: 2.1875531673431396, KL: 74.44572448730469, Loss: 0.037311308085918427, Learning Rate: 0.007202999999999998\n",
      "Epoch [4644/20000], Bound: 0.4108664095401764, Entropy: 140.35018920898438, Temp: 2.186304807662964, KL: 70.03829956054688, Loss: 0.032839585095644, Learning Rate: 0.007202999999999998\n",
      "Epoch [4645/20000], Bound: 0.4433707296848297, Entropy: 140.65606689453125, Temp: 2.185112237930298, KL: 80.1763916015625, Loss: 0.030886245891451836, Learning Rate: 0.007202999999999998\n",
      "Epoch [4646/20000], Bound: 0.4182794988155365, Entropy: 139.24986267089844, Temp: 2.1841049194335938, KL: 71.06243896484375, Loss: 0.03522001579403877, Learning Rate: 0.007202999999999998\n",
      "Epoch [4647/20000], Bound: 0.4110783040523529, Entropy: 137.88851928710938, Temp: 2.1830854415893555, KL: 70.40521240234375, Loss: 0.032075896859169006, Learning Rate: 0.007202999999999998\n",
      "Epoch [4648/20000], Bound: 0.44085341691970825, Entropy: 140.85585021972656, Temp: 2.1821231842041016, KL: 77.16986083984375, Loss: 0.03602078557014465, Learning Rate: 0.007202999999999998\n",
      "Epoch [4649/20000], Bound: 0.4816780686378479, Entropy: 137.9989776611328, Temp: 2.181182861328125, KL: 89.63978576660156, Loss: 0.03552732244133949, Learning Rate: 0.007202999999999998\n",
      "Epoch [4650/20000], Bound: 0.43626224994659424, Entropy: 138.12974548339844, Temp: 2.1803810596466064, KL: 78.01081848144531, Loss: 0.030998876318335533, Learning Rate: 0.007202999999999998\n",
      "Epoch [4651/20000], Bound: 0.4272812604904175, Entropy: 135.42807006835938, Temp: 2.179708957672119, KL: 74.87907409667969, Loss: 0.03224509581923485, Learning Rate: 0.007202999999999998\n",
      "Epoch [4652/20000], Bound: 0.4340463876724243, Entropy: 137.08265686035156, Temp: 2.17909836769104, KL: 76.62814331054688, Loss: 0.03267379850149155, Learning Rate: 0.007202999999999998\n",
      "Epoch [4653/20000], Bound: 0.4144774079322815, Entropy: 135.07388305664062, Temp: 2.17854905128479, KL: 72.74795532226562, Loss: 0.028796250000596046, Learning Rate: 0.007202999999999998\n",
      "Epoch [4654/20000], Bound: 0.40053415298461914, Entropy: 132.74862670898438, Temp: 2.1781113147735596, KL: 68.74945068359375, Loss: 0.02907729148864746, Learning Rate: 0.007202999999999998\n",
      "Epoch [4655/20000], Bound: 0.42588356137275696, Entropy: 134.73069763183594, Temp: 2.177731990814209, KL: 70.6474609375, Loss: 0.0410040020942688, Learning Rate: 0.007202999999999998\n",
      "Epoch [4656/20000], Bound: 0.43712928891181946, Entropy: 133.61351013183594, Temp: 2.177147150039673, KL: 76.26902770996094, Loss: 0.035501718521118164, Learning Rate: 0.007202999999999998\n",
      "Epoch [4657/20000], Bound: 0.44188952445983887, Entropy: 132.71786499023438, Temp: 2.1765553951263428, KL: 76.90377807617188, Loss: 0.03720799461007118, Learning Rate: 0.007202999999999998\n",
      "Epoch [4658/20000], Bound: 0.46511510014533997, Entropy: 134.11036682128906, Temp: 2.17592453956604, KL: 85.85713195800781, Loss: 0.03244688734412193, Learning Rate: 0.007202999999999998\n",
      "Epoch [4659/20000], Bound: 0.4185875952243805, Entropy: 133.59124755859375, Temp: 2.175445079803467, KL: 73.55656433105469, Loss: 0.029528005048632622, Learning Rate: 0.007202999999999998\n",
      "Epoch [4660/20000], Bound: 0.4195825159549713, Entropy: 135.5244598388672, Temp: 2.175063371658325, KL: 75.13650512695312, Loss: 0.026533080264925957, Learning Rate: 0.007202999999999998\n",
      "Epoch [4661/20000], Bound: 0.4284781515598297, Entropy: 136.1822967529297, Temp: 2.1748533248901367, KL: 74.40182495117188, Loss: 0.034026116132736206, Learning Rate: 0.007202999999999998\n",
      "Epoch [4662/20000], Bound: 0.40662235021591187, Entropy: 135.14793395996094, Temp: 2.1746180057525635, KL: 68.38877868652344, Loss: 0.03370357304811478, Learning Rate: 0.007202999999999998\n",
      "Epoch [4663/20000], Bound: 0.4387024939060211, Entropy: 135.2315673828125, Temp: 2.1743128299713135, KL: 77.83842468261719, Loss: 0.0328814834356308, Learning Rate: 0.007202999999999998\n",
      "Epoch [4664/20000], Bound: 0.4761340022087097, Entropy: 133.8169708251953, Temp: 2.1740493774414062, KL: 87.46553039550781, Loss: 0.03640490770339966, Learning Rate: 0.007202999999999998\n",
      "Epoch [4665/20000], Bound: 0.42431890964508057, Entropy: 137.5369415283203, Temp: 2.1738271713256836, KL: 75.51789855957031, Loss: 0.028712209314107895, Learning Rate: 0.007202999999999998\n",
      "Epoch [4666/20000], Bound: 0.42164263129234314, Entropy: 136.9906463623047, Temp: 2.1737148761749268, KL: 73.72496032714844, Loss: 0.031088391318917274, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4667/20000], Bound: 0.43674445152282715, Entropy: 135.94851684570312, Temp: 2.173630714416504, KL: 79.08975219726562, Loss: 0.028683463111519814, Learning Rate: 0.007202999999999998\n",
      "Epoch [4668/20000], Bound: 0.44725826382637024, Entropy: 137.23812866210938, Temp: 2.1736745834350586, KL: 81.86503601074219, Loss: 0.02934790588915348, Learning Rate: 0.007202999999999998\n",
      "Epoch [4669/20000], Bound: 0.4272344708442688, Entropy: 138.37294006347656, Temp: 2.173842430114746, KL: 70.647216796875, Loss: 0.04182453081011772, Learning Rate: 0.007202999999999998\n",
      "Epoch [4670/20000], Bound: 0.4394574761390686, Entropy: 134.77426147460938, Temp: 2.173734426498413, KL: 79.18586730957031, Loss: 0.030273037031292915, Learning Rate: 0.007202999999999998\n",
      "Epoch [4671/20000], Bound: 0.43889471888542175, Entropy: 137.19248962402344, Temp: 2.1737208366394043, KL: 79.67581176757812, Loss: 0.02877010591328144, Learning Rate: 0.007202999999999998\n",
      "Epoch [4672/20000], Bound: 0.43573689460754395, Entropy: 136.59169006347656, Temp: 2.1738314628601074, KL: 75.4287109375, Loss: 0.03643929585814476, Learning Rate: 0.007202999999999998\n",
      "Epoch [4673/20000], Bound: 0.42906618118286133, Entropy: 138.19056701660156, Temp: 2.173839569091797, KL: 73.67778015136719, Loss: 0.036057353019714355, Learning Rate: 0.007202999999999998\n",
      "Epoch [4674/20000], Bound: 0.4862343966960907, Entropy: 137.0052032470703, Temp: 2.173748016357422, KL: 88.73336791992188, Loss: 0.04066065698862076, Learning Rate: 0.007202999999999998\n",
      "Epoch [4675/20000], Bound: 0.40423667430877686, Entropy: 136.2898406982422, Temp: 2.173596143722534, KL: 67.4140625, Loss: 0.03440941125154495, Learning Rate: 0.007202999999999998\n",
      "Epoch [4676/20000], Bound: 0.43823716044425964, Entropy: 132.42958068847656, Temp: 2.1733410358428955, KL: 78.85368347167969, Loss: 0.030213648453354836, Learning Rate: 0.007202999999999998\n",
      "Epoch [4677/20000], Bound: 0.449211984872818, Entropy: 132.76889038085938, Temp: 2.173194169998169, KL: 79.7115478515625, Loss: 0.035612139850854874, Learning Rate: 0.007202999999999998\n",
      "Epoch [4678/20000], Bound: 0.43799182772636414, Entropy: 131.9178466796875, Temp: 2.1730282306671143, KL: 73.21792602539062, Loss: 0.04301026836037636, Learning Rate: 0.007202999999999998\n",
      "Epoch [4679/20000], Bound: 0.42399924993515015, Entropy: 130.51904296875, Temp: 2.172616481781006, KL: 72.2484130859375, Loss: 0.03600047901272774, Learning Rate: 0.007202999999999998\n",
      "Epoch [4680/20000], Bound: 0.4338291883468628, Entropy: 131.83302307128906, Temp: 2.1721365451812744, KL: 76.02134704589844, Loss: 0.03377460315823555, Learning Rate: 0.007202999999999998\n",
      "Epoch [4681/20000], Bound: 0.41273799538612366, Entropy: 132.9635467529297, Temp: 2.1716809272766113, KL: 71.78323364257812, Loss: 0.02975291945040226, Learning Rate: 0.007202999999999998\n",
      "Epoch [4682/20000], Bound: 0.4136916697025299, Entropy: 131.85853576660156, Temp: 2.1713032722473145, KL: 70.03071594238281, Loss: 0.034394726157188416, Learning Rate: 0.007202999999999998\n",
      "Epoch [4683/20000], Bound: 0.3993260860443115, Entropy: 132.9830780029297, Temp: 2.1708712577819824, KL: 68.96517944335938, Loss: 0.02767845056951046, Learning Rate: 0.007202999999999998\n",
      "Epoch [4684/20000], Bound: 0.41735371947288513, Entropy: 133.65335083007812, Temp: 2.170539379119873, KL: 72.08547973632812, Loss: 0.032012566924095154, Learning Rate: 0.007202999999999998\n",
      "Epoch [4685/20000], Bound: 0.41453447937965393, Entropy: 134.81419372558594, Temp: 2.1702239513397217, KL: 71.56391906738281, Loss: 0.03138568252325058, Learning Rate: 0.007202999999999998\n",
      "Epoch [4686/20000], Bound: 0.4255278408527374, Entropy: 136.30963134765625, Temp: 2.169933795928955, KL: 71.89219665527344, Loss: 0.03777051717042923, Learning Rate: 0.007202999999999998\n",
      "Epoch [4687/20000], Bound: 0.38619861006736755, Entropy: 137.99478149414062, Temp: 2.169520854949951, KL: 62.20159912109375, Loss: 0.03502027317881584, Learning Rate: 0.007202999999999998\n",
      "Epoch [4688/20000], Bound: 0.4185818135738373, Entropy: 140.42356872558594, Temp: 2.1689696311950684, KL: 72.20845031738281, Loss: 0.032493818551301956, Learning Rate: 0.007202999999999998\n",
      "Epoch [4689/20000], Bound: 0.42994651198387146, Entropy: 137.79898071289062, Temp: 2.168447971343994, KL: 73.53695678710938, Loss: 0.03685566037893295, Learning Rate: 0.007202999999999998\n",
      "Epoch [4690/20000], Bound: 0.43276989459991455, Entropy: 140.5563507080078, Temp: 2.1678645610809326, KL: 76.11500549316406, Loss: 0.03276444971561432, Learning Rate: 0.007202999999999998\n",
      "Epoch [4691/20000], Bound: 0.4247162640094757, Entropy: 143.02732849121094, Temp: 2.167344570159912, KL: 70.54962158203125, Loss: 0.04028783366084099, Learning Rate: 0.007202999999999998\n",
      "Epoch [4692/20000], Bound: 0.4108560085296631, Entropy: 142.3265838623047, Temp: 2.166656017303467, KL: 71.04827880859375, Loss: 0.030135057866573334, Learning Rate: 0.007202999999999998\n",
      "Epoch [4693/20000], Bound: 0.43594813346862793, Entropy: 144.076171875, Temp: 2.16605806350708, KL: 77.36018371582031, Loss: 0.03196138143539429, Learning Rate: 0.007202999999999998\n",
      "Epoch [4694/20000], Bound: 0.4392842948436737, Entropy: 142.23516845703125, Temp: 2.1655566692352295, KL: 78.2935791015625, Loss: 0.03202132508158684, Learning Rate: 0.007202999999999998\n",
      "Epoch [4695/20000], Bound: 0.40039515495300293, Entropy: 143.8588104248047, Temp: 2.1651499271392822, KL: 69.01091003417969, Loss: 0.02813585288822651, Learning Rate: 0.007202999999999998\n",
      "Epoch [4696/20000], Bound: 0.45075467228889465, Entropy: 141.24276733398438, Temp: 2.1648354530334473, KL: 81.98812866210938, Loss: 0.031210817396640778, Learning Rate: 0.007202999999999998\n",
      "Epoch [4697/20000], Bound: 0.43095436692237854, Entropy: 139.9503631591797, Temp: 2.164649724960327, KL: 76.5372314453125, Loss: 0.03051650896668434, Learning Rate: 0.007202999999999998\n",
      "Epoch [4698/20000], Bound: 0.4214351177215576, Entropy: 138.6575927734375, Temp: 2.1645469665527344, KL: 73.58763122558594, Loss: 0.031076187267899513, Learning Rate: 0.007202999999999998\n",
      "Epoch [4699/20000], Bound: 0.4232233762741089, Entropy: 139.94094848632812, Temp: 2.164479970932007, KL: 74.33341979980469, Loss: 0.030519887804985046, Learning Rate: 0.007202999999999998\n",
      "Epoch [4700/20000], Bound: 0.45287567377090454, Entropy: 133.8492889404297, Temp: 2.1644647121429443, KL: 81.33534240722656, Loss: 0.034154925495386124, Learning Rate: 0.007202999999999998\n",
      "Epoch [4701/20000], Bound: 0.43646544218063354, Entropy: 134.93994140625, Temp: 2.1644747257232666, KL: 79.14334106445312, Loss: 0.028150631114840508, Learning Rate: 0.007202999999999998\n",
      "Epoch [4702/20000], Bound: 0.43333762884140015, Entropy: 132.3565673828125, Temp: 2.1646275520324707, KL: 77.84544372558594, Loss: 0.029072608798742294, Learning Rate: 0.007202999999999998\n",
      "Epoch [4703/20000], Bound: 0.37868016958236694, Entropy: 134.2828826904297, Temp: 2.1648757457733154, KL: 63.359710693359375, Loss: 0.027628449723124504, Learning Rate: 0.007202999999999998\n",
      "Epoch [4704/20000], Bound: 0.4327158033847809, Entropy: 131.60079956054688, Temp: 2.1651108264923096, KL: 79.40711975097656, Loss: 0.025065382942557335, Learning Rate: 0.007202999999999998\n",
      "Epoch [4705/20000], Bound: 0.3911362290382385, Entropy: 134.43809509277344, Temp: 2.165540933609009, KL: 67.04733276367188, Loss: 0.02685045637190342, Learning Rate: 0.007202999999999998\n",
      "Epoch [4706/20000], Bound: 0.42573752999305725, Entropy: 133.0556182861328, Temp: 2.165992259979248, KL: 74.64910888671875, Loss: 0.03147050365805626, Learning Rate: 0.007202999999999998\n",
      "Epoch [4707/20000], Bound: 0.4213823080062866, Entropy: 135.0448760986328, Temp: 2.1664230823516846, KL: 76.02145385742188, Loss: 0.025464478880167007, Learning Rate: 0.007202999999999998\n",
      "Epoch [4708/20000], Bound: 0.4380235970020294, Entropy: 135.71141052246094, Temp: 2.1669890880584717, KL: 77.21063232421875, Loss: 0.033711016178131104, Learning Rate: 0.007202999999999998\n",
      "Epoch [4709/20000], Bound: 0.4282068908214569, Entropy: 135.42640686035156, Temp: 2.1674931049346924, KL: 74.77752685546875, Loss: 0.032828930765390396, Learning Rate: 0.007202999999999998\n",
      "Epoch [4710/20000], Bound: 0.4720677435398102, Entropy: 135.6457977294922, Temp: 2.167938709259033, KL: 89.84754943847656, Loss: 0.027885813266038895, Learning Rate: 0.007202999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4711/20000], Bound: 0.44540753960609436, Entropy: 136.824951171875, Temp: 2.168578863143921, KL: 80.98652648925781, Loss: 0.02999393828213215, Learning Rate: 0.007202999999999998\n",
      "Epoch [4712/20000], Bound: 0.4436611831188202, Entropy: 136.71923828125, Temp: 2.1692686080932617, KL: 74.90985107421875, Loss: 0.042841289192438126, Learning Rate: 0.007202999999999998\n",
      "Epoch [4713/20000], Bound: 0.39312922954559326, Entropy: 139.10671997070312, Temp: 2.169647216796875, KL: 67.26118469238281, Loss: 0.02768312580883503, Learning Rate: 0.007202999999999998\n",
      "Epoch [4714/20000], Bound: 0.47312697768211365, Entropy: 136.74742126464844, Temp: 2.1700310707092285, KL: 86.94854736328125, Loss: 0.03537175431847572, Learning Rate: 0.007202999999999998\n",
      "Epoch [4715/20000], Bound: 0.4089934825897217, Entropy: 139.04335021972656, Temp: 2.1704156398773193, KL: 71.10755920410156, Loss: 0.02888057567179203, Learning Rate: 0.007202999999999998\n",
      "Epoch [4716/20000], Bound: 0.400631308555603, Entropy: 137.19837951660156, Temp: 2.170811176300049, KL: 68.299072265625, Loss: 0.03003651462495327, Learning Rate: 0.007202999999999998\n",
      "Epoch [4717/20000], Bound: 0.4491785764694214, Entropy: 136.0110321044922, Temp: 2.171161651611328, KL: 80.74993896484375, Loss: 0.03315189853310585, Learning Rate: 0.007202999999999998\n",
      "Epoch [4718/20000], Bound: 0.4498710632324219, Entropy: 135.6902618408203, Temp: 2.1715123653411865, KL: 80.96376037597656, Loss: 0.03313751518726349, Learning Rate: 0.007202999999999998\n",
      "Epoch [4719/20000], Bound: 0.4551315903663635, Entropy: 133.70077514648438, Temp: 2.1718647480010986, KL: 82.47265625, Loss: 0.033254653215408325, Learning Rate: 0.007202999999999998\n",
      "Epoch [4720/20000], Bound: 0.40459269285202026, Entropy: 139.18833923339844, Temp: 2.172229051589966, KL: 70.62544250488281, Loss: 0.027220355346798897, Learning Rate: 0.007202999999999998\n",
      "Epoch [4721/20000], Bound: 0.383546382188797, Entropy: 134.51356506347656, Temp: 2.172639846801758, KL: 64.7867431640625, Loss: 0.027475234121084213, Learning Rate: 0.007202999999999998\n",
      "Epoch [4722/20000], Bound: 0.41701647639274597, Entropy: 136.7375946044922, Temp: 2.1730315685272217, KL: 69.87428283691406, Loss: 0.03693205490708351, Learning Rate: 0.007202999999999998\n",
      "Epoch [4723/20000], Bound: 0.41629740595817566, Entropy: 133.36163330078125, Temp: 2.173227310180664, KL: 74.41055297851562, Loss: 0.026033949106931686, Learning Rate: 0.007202999999999998\n",
      "Epoch [4724/20000], Bound: 0.4306371212005615, Entropy: 136.66197204589844, Temp: 2.173548698425293, KL: 77.30380249023438, Loss: 0.028745051473379135, Learning Rate: 0.007202999999999998\n",
      "Epoch [4725/20000], Bound: 0.4169827699661255, Entropy: 139.04261779785156, Temp: 2.173943519592285, KL: 74.0330810546875, Loss: 0.027361243963241577, Learning Rate: 0.007202999999999998\n",
      "Epoch [4726/20000], Bound: 0.44330963492393494, Entropy: 138.66993713378906, Temp: 2.174407720565796, KL: 80.86607360839844, Loss: 0.029004612937569618, Learning Rate: 0.007202999999999998\n",
      "Epoch [4727/20000], Bound: 0.3980740010738373, Entropy: 136.94329833984375, Temp: 2.1749563217163086, KL: 66.09782409667969, Loss: 0.03356120362877846, Learning Rate: 0.007202999999999998\n",
      "Epoch [4728/20000], Bound: 0.4222326874732971, Entropy: 138.81185913085938, Temp: 2.175335168838501, KL: 74.81770324707031, Loss: 0.02899465523660183, Learning Rate: 0.007202999999999998\n",
      "Epoch [4729/20000], Bound: 0.4575575888156891, Entropy: 143.19422912597656, Temp: 2.1757519245147705, KL: 82.31365966796875, Loss: 0.03537556901574135, Learning Rate: 0.007202999999999998\n",
      "Epoch [4730/20000], Bound: 0.42765113711357117, Entropy: 138.7772674560547, Temp: 2.176119089126587, KL: 76.78079223632812, Loss: 0.028042875230312347, Learning Rate: 0.007202999999999998\n",
      "Epoch [4731/20000], Bound: 0.4088531732559204, Entropy: 140.59580993652344, Temp: 2.176564931869507, KL: 71.57907104492188, Loss: 0.02783236838877201, Learning Rate: 0.007202999999999998\n",
      "Epoch [4732/20000], Bound: 0.4460054636001587, Entropy: 135.6714324951172, Temp: 2.177039623260498, KL: 81.73634338378906, Loss: 0.02888353355228901, Learning Rate: 0.007202999999999998\n",
      "Epoch [4733/20000], Bound: 0.4371594190597534, Entropy: 137.5543670654297, Temp: 2.177605390548706, KL: 76.09640502929688, Loss: 0.035927701741456985, Learning Rate: 0.007202999999999998\n",
      "Epoch [4734/20000], Bound: 0.4382619559764862, Entropy: 137.6806640625, Temp: 2.178034782409668, KL: 77.8719482421875, Loss: 0.032594092190265656, Learning Rate: 0.007202999999999998\n",
      "Epoch [4735/20000], Bound: 0.430550217628479, Entropy: 138.0504608154297, Temp: 2.178436517715454, KL: 77.05215454101562, Loss: 0.0293782539665699, Learning Rate: 0.007202999999999998\n",
      "Epoch [4736/20000], Bound: 0.43157532811164856, Entropy: 137.2800750732422, Temp: 2.178882122039795, KL: 77.64788818359375, Loss: 0.028696788474917412, Learning Rate: 0.007202999999999998\n",
      "Epoch [4737/20000], Bound: 0.42917630076408386, Entropy: 137.7173309326172, Temp: 2.1793882846832275, KL: 77.10719299316406, Loss: 0.02837003394961357, Learning Rate: 0.007202999999999998\n",
      "Epoch [4738/20000], Bound: 0.42655229568481445, Entropy: 136.24053955078125, Temp: 2.1799516677856445, KL: 76.67486572265625, Loss: 0.027653945609927177, Learning Rate: 0.007202999999999998\n",
      "Epoch [4739/20000], Bound: 0.43585267663002014, Entropy: 139.68994140625, Temp: 2.18057918548584, KL: 77.50791931152344, Loss: 0.03188486024737358, Learning Rate: 0.007202999999999998\n",
      "Epoch [4740/20000], Bound: 0.42160332202911377, Entropy: 135.918701171875, Temp: 2.1811704635620117, KL: 73.3956298828125, Loss: 0.03197237104177475, Learning Rate: 0.007202999999999998\n",
      "Epoch [4741/20000], Bound: 0.41177406907081604, Entropy: 137.8999786376953, Temp: 2.1816883087158203, KL: 71.94010925292969, Loss: 0.028977548703551292, Learning Rate: 0.007202999999999998\n",
      "Epoch [4742/20000], Bound: 0.42576614022254944, Entropy: 136.1559600830078, Temp: 2.1821987628936768, KL: 78.29611206054688, Loss: 0.02347663789987564, Learning Rate: 0.007202999999999998\n",
      "Epoch [4743/20000], Bound: 0.4369681775569916, Entropy: 137.92977905273438, Temp: 2.182891368865967, KL: 76.16572570800781, Loss: 0.03575029596686363, Learning Rate: 0.007202999999999998\n",
      "Epoch [4744/20000], Bound: 0.42893967032432556, Entropy: 134.23851013183594, Temp: 2.18343448638916, KL: 75.16679382324219, Loss: 0.03275061398744583, Learning Rate: 0.007202999999999998\n",
      "Epoch [4745/20000], Bound: 0.4474059045314789, Entropy: 135.54376220703125, Temp: 2.1839046478271484, KL: 81.76786804199219, Loss: 0.029925381764769554, Learning Rate: 0.007202999999999998\n",
      "Epoch [4746/20000], Bound: 0.4917491674423218, Entropy: 137.53839111328125, Temp: 2.1844353675842285, KL: 92.27522277832031, Loss: 0.03678607940673828, Learning Rate: 0.007202999999999998\n",
      "Epoch [4747/20000], Bound: 0.402588814496994, Entropy: 135.98191833496094, Temp: 2.1849496364593506, KL: 70.24647521972656, Loss: 0.027076708152890205, Learning Rate: 0.007202999999999998\n",
      "Epoch [4748/20000], Bound: 0.40309932827949524, Entropy: 136.59593200683594, Temp: 2.1854844093322754, KL: 68.27742004394531, Loss: 0.03191492334008217, Learning Rate: 0.005042099999999998\n",
      "Epoch [4749/20000], Bound: 0.4683252274990082, Entropy: 131.98049926757812, Temp: 2.1859006881713867, KL: 86.55929565429688, Loss: 0.03332631289958954, Learning Rate: 0.005042099999999998\n",
      "Epoch [4750/20000], Bound: 0.414898544549942, Entropy: 135.65780639648438, Temp: 2.186342239379883, KL: 70.81794738769531, Loss: 0.03364147990942001, Learning Rate: 0.005042099999999998\n",
      "Epoch [4751/20000], Bound: 0.42332109808921814, Entropy: 132.75851440429688, Temp: 2.1866555213928223, KL: 76.00418090820312, Loss: 0.027233509346842766, Learning Rate: 0.005042099999999998\n",
      "Epoch [4752/20000], Bound: 0.3982158601284027, Entropy: 133.4846954345703, Temp: 2.187056541442871, KL: 69.78950500488281, Loss: 0.025409208610653877, Learning Rate: 0.005042099999999998\n",
      "Epoch [4753/20000], Bound: 0.4133525490760803, Entropy: 132.67771911621094, Temp: 2.18752384185791, KL: 74.63070678710938, Loss: 0.023956069722771645, Learning Rate: 0.005042099999999998\n",
      "Epoch [4754/20000], Bound: 0.4332706332206726, Entropy: 135.2042236328125, Temp: 2.1881299018859863, KL: 78.47096252441406, Loss: 0.028144339099526405, Learning Rate: 0.005042099999999998\n",
      "Epoch [4755/20000], Bound: 0.4283468425273895, Entropy: 135.41876220703125, Temp: 2.1887929439544678, KL: 79.29910278320312, Loss: 0.023032987490296364, Learning Rate: 0.005042099999999998\n",
      "Epoch [4756/20000], Bound: 0.45295313000679016, Entropy: 135.07972717285156, Temp: 2.1896378993988037, KL: 82.83297729492188, Loss: 0.03138130530714989, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4757/20000], Bound: 0.3964644968509674, Entropy: 137.9638671875, Temp: 2.19047474861145, KL: 68.68977355957031, Loss: 0.026890186592936516, Learning Rate: 0.005042099999999998\n",
      "Epoch [4758/20000], Bound: 0.39262598752975464, Entropy: 139.24224853515625, Temp: 2.1912853717803955, KL: 68.21145629882812, Loss: 0.025601578876376152, Learning Rate: 0.005042099999999998\n",
      "Epoch [4759/20000], Bound: 0.43141409754753113, Entropy: 138.67030334472656, Temp: 2.192099094390869, KL: 78.3931884765625, Loss: 0.027192039415240288, Learning Rate: 0.005042099999999998\n",
      "Epoch [4760/20000], Bound: 0.39663180708885193, Entropy: 137.00054931640625, Temp: 2.192967414855957, KL: 70.18389892578125, Loss: 0.023635363206267357, Learning Rate: 0.005042099999999998\n",
      "Epoch [4761/20000], Bound: 0.42880189418792725, Entropy: 139.0346221923828, Temp: 2.1938982009887695, KL: 78.30400085449219, Loss: 0.02572472207248211, Learning Rate: 0.005042099999999998\n",
      "Epoch [4762/20000], Bound: 0.3918568193912506, Entropy: 139.7875213623047, Temp: 2.1949052810668945, KL: 69.07196044921875, Loss: 0.02323201857507229, Learning Rate: 0.005042099999999998\n",
      "Epoch [4763/20000], Bound: 0.4237784743309021, Entropy: 140.40087890625, Temp: 2.1959590911865234, KL: 76.3011474609375, Loss: 0.027060100808739662, Learning Rate: 0.005042099999999998\n",
      "Epoch [4764/20000], Bound: 0.41167810559272766, Entropy: 139.7128448486328, Temp: 2.1970250606536865, KL: 72.44013977050781, Loss: 0.028083590790629387, Learning Rate: 0.005042099999999998\n",
      "Epoch [4765/20000], Bound: 0.42017319798469543, Entropy: 141.13966369628906, Temp: 2.1980409622192383, KL: 74.44041442871094, Loss: 0.02900526486337185, Learning Rate: 0.005042099999999998\n",
      "Epoch [4766/20000], Bound: 0.40408626198768616, Entropy: 141.4542694091797, Temp: 2.1990063190460205, KL: 71.35966491699219, Loss: 0.025763925164937973, Learning Rate: 0.005042099999999998\n",
      "Epoch [4767/20000], Bound: 0.3867431879043579, Entropy: 142.12515258789062, Temp: 2.199976921081543, KL: 67.32908630371094, Loss: 0.02413247339427471, Learning Rate: 0.005042099999999998\n",
      "Epoch [4768/20000], Bound: 0.37136927247047424, Entropy: 141.0048065185547, Temp: 2.200955629348755, KL: 60.646881103515625, Loss: 0.029960015788674355, Learning Rate: 0.005042099999999998\n",
      "Epoch [4769/20000], Bound: 0.40431636571884155, Entropy: 140.04461669921875, Temp: 2.2017316818237305, KL: 72.74893188476562, Loss: 0.022808723151683807, Learning Rate: 0.005042099999999998\n",
      "Epoch [4770/20000], Bound: 0.42840948700904846, Entropy: 140.6495819091797, Temp: 2.202615261077881, KL: 76.16481018066406, Loss: 0.030527237802743912, Learning Rate: 0.005042099999999998\n",
      "Epoch [4771/20000], Bound: 0.45978426933288574, Entropy: 140.27593994140625, Temp: 2.2034354209899902, KL: 85.39407348632812, Loss: 0.03054390661418438, Learning Rate: 0.005042099999999998\n",
      "Epoch [4772/20000], Bound: 0.4243379235267639, Entropy: 142.5114288330078, Temp: 2.2042784690856934, KL: 77.56907653808594, Loss: 0.02472888119518757, Learning Rate: 0.005042099999999998\n",
      "Epoch [4773/20000], Bound: 0.41430556774139404, Entropy: 141.2399139404297, Temp: 2.2052149772644043, KL: 74.87208557128906, Loss: 0.024408141151070595, Learning Rate: 0.005042099999999998\n",
      "Epoch [4774/20000], Bound: 0.39974433183670044, Entropy: 142.14537048339844, Temp: 2.206219434738159, KL: 67.01161193847656, Loss: 0.03303524851799011, Learning Rate: 0.005042099999999998\n",
      "Epoch [4775/20000], Bound: 0.4093111455440521, Entropy: 141.6591339111328, Temp: 2.206998109817505, KL: 72.27926635742188, Loss: 0.0271417535841465, Learning Rate: 0.005042099999999998\n",
      "Epoch [4776/20000], Bound: 0.43967145681381226, Entropy: 141.20736694335938, Temp: 2.2077677249908447, KL: 81.138427734375, Loss: 0.026768701151013374, Learning Rate: 0.005042099999999998\n",
      "Epoch [4777/20000], Bound: 0.4097629487514496, Entropy: 140.36009216308594, Temp: 2.208616018295288, KL: 71.85360717773438, Loss: 0.028423909097909927, Learning Rate: 0.005042099999999998\n",
      "Epoch [4778/20000], Bound: 0.4023856222629547, Entropy: 138.73768615722656, Temp: 2.209411144256592, KL: 69.42410278320312, Loss: 0.02928173914551735, Learning Rate: 0.005042099999999998\n",
      "Epoch [4779/20000], Bound: 0.40899980068206787, Entropy: 138.68377685546875, Temp: 2.210114002227783, KL: 72.61141967773438, Loss: 0.026254495605826378, Learning Rate: 0.005042099999999998\n",
      "Epoch [4780/20000], Bound: 0.40053999423980713, Entropy: 138.10984802246094, Temp: 2.2108378410339355, KL: 69.3997802734375, Loss: 0.028205394744873047, Learning Rate: 0.005042099999999998\n",
      "Epoch [4781/20000], Bound: 0.41200506687164307, Entropy: 137.11386108398438, Temp: 2.2115015983581543, KL: 72.99441528320312, Loss: 0.027323603630065918, Learning Rate: 0.005042099999999998\n",
      "Epoch [4782/20000], Bound: 0.40102851390838623, Entropy: 138.62547302246094, Temp: 2.2121658325195312, KL: 72.13014221191406, Loss: 0.022363947704434395, Learning Rate: 0.005042099999999998\n",
      "Epoch [4783/20000], Bound: 0.44297194480895996, Entropy: 139.49819946289062, Temp: 2.2129456996917725, KL: 82.59718322753906, Loss: 0.025783998891711235, Learning Rate: 0.005042099999999998\n",
      "Epoch [4784/20000], Bound: 0.4055621325969696, Entropy: 137.65858459472656, Temp: 2.2138350009918213, KL: 72.09393310546875, Loss: 0.025326959788799286, Learning Rate: 0.005042099999999998\n",
      "Epoch [4785/20000], Bound: 0.42337164282798767, Entropy: 138.79953002929688, Temp: 2.2147417068481445, KL: 76.52897644042969, Loss: 0.0266893170773983, Learning Rate: 0.005042099999999998\n",
      "Epoch [4786/20000], Bound: 0.3781699240207672, Entropy: 137.46392822265625, Temp: 2.2156686782836914, KL: 64.36660766601562, Loss: 0.025876978412270546, Learning Rate: 0.005042099999999998\n",
      "Epoch [4787/20000], Bound: 0.4419700801372528, Entropy: 138.360107421875, Temp: 2.2165231704711914, KL: 81.18283081054688, Loss: 0.028396964073181152, Learning Rate: 0.005042099999999998\n",
      "Epoch [4788/20000], Bound: 0.3979402184486389, Entropy: 138.19622802734375, Temp: 2.217399835586548, KL: 68.1844482421875, Loss: 0.029441965743899345, Learning Rate: 0.005042099999999998\n",
      "Epoch [4789/20000], Bound: 0.44116896390914917, Entropy: 137.11041259765625, Temp: 2.218153238296509, KL: 83.16841125488281, Loss: 0.02342795766890049, Learning Rate: 0.005042099999999998\n",
      "Epoch [4790/20000], Bound: 0.4115431606769562, Entropy: 139.90972900390625, Temp: 2.2190771102905273, KL: 73.09898376464844, Loss: 0.02694409340620041, Learning Rate: 0.005042099999999998\n",
      "Epoch [4791/20000], Bound: 0.41354626417160034, Entropy: 141.31202697753906, Temp: 2.2199783325195312, KL: 72.66580200195312, Loss: 0.029209565371274948, Learning Rate: 0.005042099999999998\n",
      "Epoch [4792/20000], Bound: 0.41287761926651, Entropy: 138.84642028808594, Temp: 2.2207984924316406, KL: 75.03080749511719, Loss: 0.02347545325756073, Learning Rate: 0.005042099999999998\n",
      "Epoch [4793/20000], Bound: 0.426734983921051, Entropy: 139.89569091796875, Temp: 2.2217092514038086, KL: 76.08843994140625, Loss: 0.030002426356077194, Learning Rate: 0.005042099999999998\n",
      "Epoch [4794/20000], Bound: 0.36504030227661133, Entropy: 140.5030975341797, Temp: 2.2225470542907715, KL: 61.867156982421875, Loss: 0.02373381331562996, Learning Rate: 0.005042099999999998\n",
      "Epoch [4795/20000], Bound: 0.3777279555797577, Entropy: 141.70480346679688, Temp: 2.2233469486236572, KL: 62.28968811035156, Loss: 0.030404996126890182, Learning Rate: 0.005042099999999998\n",
      "Epoch [4796/20000], Bound: 0.4156225621700287, Entropy: 141.4786834716797, Temp: 2.223945140838623, KL: 75.59651184082031, Loss: 0.02401617541909218, Learning Rate: 0.005042099999999998\n",
      "Epoch [4797/20000], Bound: 0.4092983901500702, Entropy: 142.58338928222656, Temp: 2.224644660949707, KL: 71.850830078125, Loss: 0.028438689187169075, Learning Rate: 0.005042099999999998\n",
      "Epoch [4798/20000], Bound: 0.4530619978904724, Entropy: 139.88685607910156, Temp: 2.225290298461914, KL: 82.90657043457031, Loss: 0.032123684883117676, Learning Rate: 0.005042099999999998\n",
      "Epoch [4799/20000], Bound: 0.3972035348415375, Entropy: 141.2741241455078, Temp: 2.225893974304199, KL: 70.36582946777344, Loss: 0.024227356538176537, Learning Rate: 0.005042099999999998\n",
      "Epoch [4800/20000], Bound: 0.42602378129959106, Entropy: 140.65272521972656, Temp: 2.2265448570251465, KL: 76.95590209960938, Loss: 0.02769123762845993, Learning Rate: 0.005042099999999998\n",
      "Epoch [4801/20000], Bound: 0.42746078968048096, Entropy: 139.98934936523438, Temp: 2.2272095680236816, KL: 77.84257507324219, Loss: 0.026644038036465645, Learning Rate: 0.005042099999999998\n",
      "Epoch [4802/20000], Bound: 0.42910391092300415, Entropy: 139.9012451171875, Temp: 2.227919816970825, KL: 78.38029479980469, Loss: 0.026517581194639206, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4803/20000], Bound: 0.4097682237625122, Entropy: 138.73123168945312, Temp: 2.2286782264709473, KL: 73.23329162597656, Loss: 0.025708064436912537, Learning Rate: 0.005042099999999998\n",
      "Epoch [4804/20000], Bound: 0.41116437315940857, Entropy: 139.23367309570312, Temp: 2.229454517364502, KL: 72.28640747070312, Loss: 0.028728794306516647, Learning Rate: 0.005042099999999998\n",
      "Epoch [4805/20000], Bound: 0.42392557859420776, Entropy: 138.5486297607422, Temp: 2.230161666870117, KL: 75.24647521972656, Loss: 0.03024609014391899, Learning Rate: 0.005042099999999998\n",
      "Epoch [4806/20000], Bound: 0.4184165596961975, Entropy: 135.7174835205078, Temp: 2.2307941913604736, KL: 75.88951110839844, Loss: 0.025285083800554276, Learning Rate: 0.005042099999999998\n",
      "Epoch [4807/20000], Bound: 0.4310101270675659, Entropy: 135.28662109375, Temp: 2.2314891815185547, KL: 78.37527465820312, Loss: 0.02784365601837635, Learning Rate: 0.005042099999999998\n",
      "Epoch [4808/20000], Bound: 0.3904983699321747, Entropy: 138.64349365234375, Temp: 2.2321972846984863, KL: 69.49032592773438, Loss: 0.02217140793800354, Learning Rate: 0.005042099999999998\n",
      "Epoch [4809/20000], Bound: 0.4676196277141571, Entropy: 133.70822143554688, Temp: 2.2329823970794678, KL: 87.57614135742188, Loss: 0.031721148639917374, Learning Rate: 0.005042099999999998\n",
      "Epoch [4810/20000], Bound: 0.40526145696640015, Entropy: 135.03421020507812, Temp: 2.2337534427642822, KL: 71.73629760742188, Loss: 0.02632630057632923, Learning Rate: 0.005042099999999998\n",
      "Epoch [4811/20000], Bound: 0.3973018229007721, Entropy: 136.5448760986328, Temp: 2.2345073223114014, KL: 69.19776916503906, Loss: 0.027063515037298203, Learning Rate: 0.005042099999999998\n",
      "Epoch [4812/20000], Bound: 0.44214969873428345, Entropy: 134.96653747558594, Temp: 2.235203742980957, KL: 83.61935424804688, Loss: 0.023486264050006866, Learning Rate: 0.005042099999999998\n",
      "Epoch [4813/20000], Bound: 0.44866135716438293, Entropy: 136.44227600097656, Temp: 2.2360637187957764, KL: 84.17207336425781, Loss: 0.02659033052623272, Learning Rate: 0.005042099999999998\n",
      "Epoch [4814/20000], Bound: 0.3863818347454071, Entropy: 136.42584228515625, Temp: 2.236997127532959, KL: 65.19856262207031, Loss: 0.029337136074900627, Learning Rate: 0.005042099999999998\n",
      "Epoch [4815/20000], Bound: 0.40673208236694336, Entropy: 135.70306396484375, Temp: 2.237757682800293, KL: 71.8101806640625, Loss: 0.027156729251146317, Learning Rate: 0.005042099999999998\n",
      "Epoch [4816/20000], Bound: 0.3985495865345001, Entropy: 135.87351989746094, Temp: 2.238478183746338, KL: 70.35359191894531, Loss: 0.025323137640953064, Learning Rate: 0.005042099999999998\n",
      "Epoch [4817/20000], Bound: 0.41678714752197266, Entropy: 139.12103271484375, Temp: 2.2391958236694336, KL: 73.93763732910156, Loss: 0.02877974882721901, Learning Rate: 0.005042099999999998\n",
      "Epoch [4818/20000], Bound: 0.3879735469818115, Entropy: 138.17843627929688, Temp: 2.239854097366333, KL: 65.40855407714844, Loss: 0.029882030561566353, Learning Rate: 0.005042099999999998\n",
      "Epoch [4819/20000], Bound: 0.40597599744796753, Entropy: 139.71287536621094, Temp: 2.240352153778076, KL: 72.1231689453125, Loss: 0.02603212371468544, Learning Rate: 0.005042099999999998\n",
      "Epoch [4820/20000], Bound: 0.4007049798965454, Entropy: 140.1366729736328, Temp: 2.2408652305603027, KL: 71.53121948242188, Loss: 0.02407580055296421, Learning Rate: 0.005042099999999998\n",
      "Epoch [4821/20000], Bound: 0.4202050566673279, Entropy: 139.7703857421875, Temp: 2.241436719894409, KL: 74.37724304199219, Loss: 0.030017971992492676, Learning Rate: 0.005042099999999998\n",
      "Epoch [4822/20000], Bound: 0.4276525378227234, Entropy: 139.59378051757812, Temp: 2.241933822631836, KL: 76.27128601074219, Loss: 0.030583109706640244, Learning Rate: 0.005042099999999998\n",
      "Epoch [4823/20000], Bound: 0.40398937463760376, Entropy: 140.904052734375, Temp: 2.242366313934326, KL: 71.92059326171875, Loss: 0.025280222296714783, Learning Rate: 0.005042099999999998\n",
      "Epoch [4824/20000], Bound: 0.4329359531402588, Entropy: 141.13934326171875, Temp: 2.2428359985351562, KL: 78.45671081542969, Loss: 0.029150018468499184, Learning Rate: 0.005042099999999998\n",
      "Epoch [4825/20000], Bound: 0.4224222004413605, Entropy: 140.99063110351562, Temp: 2.2432985305786133, KL: 78.09176635742188, Loss: 0.02319042570888996, Learning Rate: 0.005042099999999998\n",
      "Epoch [4826/20000], Bound: 0.3889845311641693, Entropy: 136.8612823486328, Temp: 2.2439022064208984, KL: 69.41203308105469, Loss: 0.021638307720422745, Learning Rate: 0.005042099999999998\n",
      "Epoch [4827/20000], Bound: 0.42467910051345825, Entropy: 139.82144165039062, Temp: 2.2445967197418213, KL: 76.5260009765625, Loss: 0.028153279796242714, Learning Rate: 0.005042099999999998\n",
      "Epoch [4828/20000], Bound: 0.38753360509872437, Entropy: 142.4278106689453, Temp: 2.2452683448791504, KL: 67.6922607421875, Loss: 0.024608314037322998, Learning Rate: 0.005042099999999998\n",
      "Epoch [4829/20000], Bound: 0.4482356905937195, Entropy: 139.8419647216797, Temp: 2.2459301948547363, KL: 83.97322082519531, Loss: 0.02698323130607605, Learning Rate: 0.005042099999999998\n",
      "Epoch [4830/20000], Bound: 0.41972076892852783, Entropy: 141.32156372070312, Temp: 2.2466647624969482, KL: 76.27603149414062, Loss: 0.025579681620001793, Learning Rate: 0.005042099999999998\n",
      "Epoch [4831/20000], Bound: 0.4194226861000061, Entropy: 139.9521484375, Temp: 2.24743390083313, KL: 75.77200317382812, Loss: 0.02652670256793499, Learning Rate: 0.005042099999999998\n",
      "Epoch [4832/20000], Bound: 0.42134708166122437, Entropy: 140.8393096923828, Temp: 2.2479734420776367, KL: 76.81695556640625, Loss: 0.02544010616838932, Learning Rate: 0.005042099999999998\n",
      "Epoch [4833/20000], Bound: 0.4417228400707245, Entropy: 141.0607147216797, Temp: 2.248539447784424, KL: 79.57565307617188, Loss: 0.03251869976520538, Learning Rate: 0.005042099999999998\n",
      "Epoch [4834/20000], Bound: 0.4203251004219055, Entropy: 139.6745147705078, Temp: 2.2490200996398926, KL: 76.72605895996094, Loss: 0.025011735036969185, Learning Rate: 0.005042099999999998\n",
      "Epoch [4835/20000], Bound: 0.4391688108444214, Entropy: 137.18072509765625, Temp: 2.249539852142334, KL: 80.47129821777344, Loss: 0.028871828690171242, Learning Rate: 0.005042099999999998\n",
      "Epoch [4836/20000], Bound: 0.4510810077190399, Entropy: 135.64353942871094, Temp: 2.2500481605529785, KL: 82.15531921386719, Loss: 0.03301074728369713, Learning Rate: 0.005042099999999998\n",
      "Epoch [4837/20000], Bound: 0.4177909791469574, Entropy: 135.227783203125, Temp: 2.2504827976226807, KL: 76.50296020507812, Loss: 0.023925701156258583, Learning Rate: 0.005042099999999998\n",
      "Epoch [4838/20000], Bound: 0.40766704082489014, Entropy: 136.79241943359375, Temp: 2.2509782314300537, KL: 74.08735656738281, Loss: 0.02292156219482422, Learning Rate: 0.005042099999999998\n",
      "Epoch [4839/20000], Bound: 0.4418065547943115, Entropy: 135.14044189453125, Temp: 2.2515316009521484, KL: 78.97654724121094, Loss: 0.03396323695778847, Learning Rate: 0.005042099999999998\n",
      "Epoch [4840/20000], Bound: 0.4168355166912079, Entropy: 131.74249267578125, Temp: 2.251969337463379, KL: 73.62661743164062, Loss: 0.029736649245023727, Learning Rate: 0.005042099999999998\n",
      "Epoch [4841/20000], Bound: 0.44403553009033203, Entropy: 130.95663452148438, Temp: 2.2523443698883057, KL: 83.55769348144531, Loss: 0.025276008993387222, Learning Rate: 0.005042099999999998\n",
      "Epoch [4842/20000], Bound: 0.4038888216018677, Entropy: 132.30453491210938, Temp: 2.252802848815918, KL: 71.19309997558594, Loss: 0.027024749666452408, Learning Rate: 0.005042099999999998\n",
      "Epoch [4843/20000], Bound: 0.41354694962501526, Entropy: 131.30503845214844, Temp: 2.2532296180725098, KL: 70.91496276855469, Loss: 0.0336979441344738, Learning Rate: 0.005042099999999998\n",
      "Epoch [4844/20000], Bound: 0.3982994854450226, Entropy: 132.37255859375, Temp: 2.253504991531372, KL: 70.24374389648438, Loss: 0.0256806630641222, Learning Rate: 0.005042099999999998\n",
      "Epoch [4845/20000], Bound: 0.43665674328804016, Entropy: 133.1381378173828, Temp: 2.2537851333618164, KL: 81.96792602539062, Loss: 0.024000024423003197, Learning Rate: 0.005042099999999998\n",
      "Epoch [4846/20000], Bound: 0.41296887397766113, Entropy: 134.949462890625, Temp: 2.2541706562042236, KL: 73.27670288085938, Loss: 0.028109783306717873, Learning Rate: 0.005042099999999998\n",
      "Epoch [4847/20000], Bound: 0.4401276707649231, Entropy: 134.93214416503906, Temp: 2.2545242309570312, KL: 79.2237548828125, Loss: 0.03237070143222809, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4848/20000], Bound: 0.4070321023464203, Entropy: 134.86380004882812, Temp: 2.254809617996216, KL: 73.95979309082031, Loss: 0.022883407771587372, Learning Rate: 0.005042099999999998\n",
      "Epoch [4849/20000], Bound: 0.40493014454841614, Entropy: 136.56390380859375, Temp: 2.255171775817871, KL: 71.47752380371094, Loss: 0.027083389461040497, Learning Rate: 0.005042099999999998\n",
      "Epoch [4850/20000], Bound: 0.3983902633190155, Entropy: 138.73565673828125, Temp: 2.2555108070373535, KL: 66.10371398925781, Loss: 0.034949079155921936, Learning Rate: 0.005042099999999998\n",
      "Epoch [4851/20000], Bound: 0.39795199036598206, Entropy: 137.4285888671875, Temp: 2.2556509971618652, KL: 69.63179016113281, Loss: 0.026860155165195465, Learning Rate: 0.005042099999999998\n",
      "Epoch [4852/20000], Bound: 0.39161232113838196, Entropy: 139.63999938964844, Temp: 2.2557826042175293, KL: 67.37535095214844, Loss: 0.0279727503657341, Learning Rate: 0.005042099999999998\n",
      "Epoch [4853/20000], Bound: 0.41546007990837097, Entropy: 139.90512084960938, Temp: 2.255871057510376, KL: 74.13925170898438, Loss: 0.02779986709356308, Learning Rate: 0.005042099999999998\n",
      "Epoch [4854/20000], Bound: 0.3862861394882202, Entropy: 141.93609619140625, Temp: 2.255967140197754, KL: 66.65885925292969, Loss: 0.026320964097976685, Learning Rate: 0.005042099999999998\n",
      "Epoch [4855/20000], Bound: 0.4070456027984619, Entropy: 142.39537048339844, Temp: 2.256049156188965, KL: 70.19021606445312, Loss: 0.03127051889896393, Learning Rate: 0.005042099999999998\n",
      "Epoch [4856/20000], Bound: 0.4137010872364044, Entropy: 143.875, Temp: 2.256051540374756, KL: 74.4666748046875, Loss: 0.02596755139529705, Learning Rate: 0.005042099999999998\n",
      "Epoch [4857/20000], Bound: 0.40556588768959045, Entropy: 140.9183349609375, Temp: 2.2561049461364746, KL: 72.32147216796875, Loss: 0.025625258684158325, Learning Rate: 0.005042099999999998\n",
      "Epoch [4858/20000], Bound: 0.4038289785385132, Entropy: 143.07728576660156, Temp: 2.256197452545166, KL: 71.67170715332031, Loss: 0.025986112654209137, Learning Rate: 0.005042099999999998\n",
      "Epoch [4859/20000], Bound: 0.41466233134269714, Entropy: 141.4051971435547, Temp: 2.256314277648926, KL: 75.50433349609375, Loss: 0.02427920699119568, Learning Rate: 0.005042099999999998\n",
      "Epoch [4860/20000], Bound: 0.43915680050849915, Entropy: 142.10987854003906, Temp: 2.2565083503723145, KL: 82.38511657714844, Loss: 0.024768956005573273, Learning Rate: 0.005042099999999998\n",
      "Epoch [4861/20000], Bound: 0.38283100724220276, Entropy: 142.79052734375, Temp: 2.2568042278289795, KL: 67.40083312988281, Loss: 0.02259981259703636, Learning Rate: 0.005042099999999998\n",
      "Epoch [4862/20000], Bound: 0.39590543508529663, Entropy: 143.9667205810547, Temp: 2.2571396827697754, KL: 67.10874938964844, Loss: 0.031214820221066475, Learning Rate: 0.005042099999999998\n",
      "Epoch [4863/20000], Bound: 0.4138338565826416, Entropy: 144.2548370361328, Temp: 2.2573494911193848, KL: 73.52183532714844, Loss: 0.02816859632730484, Learning Rate: 0.005042099999999998\n",
      "Epoch [4864/20000], Bound: 0.37326744198799133, Entropy: 145.33181762695312, Temp: 2.2575430870056152, KL: 65.23298645019531, Loss: 0.021679462864995003, Learning Rate: 0.005042099999999998\n",
      "Epoch [4865/20000], Bound: 0.4282587468624115, Entropy: 145.56124877929688, Temp: 2.257789373397827, KL: 77.70877075195312, Loss: 0.028087444603443146, Learning Rate: 0.005042099999999998\n",
      "Epoch [4866/20000], Bound: 0.4101814329624176, Entropy: 144.2530975341797, Temp: 2.2580432891845703, KL: 72.30923461914062, Loss: 0.02857101522386074, Learning Rate: 0.005042099999999998\n",
      "Epoch [4867/20000], Bound: 0.4043857157230377, Entropy: 140.4418487548828, Temp: 2.258261203765869, KL: 73.13871765136719, Loss: 0.023120708763599396, Learning Rate: 0.005042099999999998\n",
      "Epoch [4868/20000], Bound: 0.4145711660385132, Entropy: 143.22779846191406, Temp: 2.258551836013794, KL: 75.58668518066406, Loss: 0.024083778262138367, Learning Rate: 0.005042099999999998\n",
      "Epoch [4869/20000], Bound: 0.4127339720726013, Entropy: 143.76351928710938, Temp: 2.2589051723480225, KL: 73.734619140625, Loss: 0.02703331969678402, Learning Rate: 0.005042099999999998\n",
      "Epoch [4870/20000], Bound: 0.3724477291107178, Entropy: 143.36444091796875, Temp: 2.259249210357666, KL: 63.82090759277344, Loss: 0.024344639852643013, Learning Rate: 0.005042099999999998\n",
      "Epoch [4871/20000], Bound: 0.4305710792541504, Entropy: 140.85482788085938, Temp: 2.259571075439453, KL: 79.03840637207031, Loss: 0.0266695786267519, Learning Rate: 0.005042099999999998\n",
      "Epoch [4872/20000], Bound: 0.4102322459220886, Entropy: 141.82717895507812, Temp: 2.2599258422851562, KL: 74.09317016601562, Loss: 0.024688558652997017, Learning Rate: 0.005042099999999998\n",
      "Epoch [4873/20000], Bound: 0.42649757862091064, Entropy: 139.68899536132812, Temp: 2.2603156566619873, KL: 77.22021484375, Loss: 0.028087200596928596, Learning Rate: 0.005042099999999998\n",
      "Epoch [4874/20000], Bound: 0.40113383531570435, Entropy: 138.77235412597656, Temp: 2.2606942653656006, KL: 70.28143310546875, Loss: 0.027470974251627922, Learning Rate: 0.005042099999999998\n",
      "Epoch [4875/20000], Bound: 0.3967092037200928, Entropy: 138.2081298828125, Temp: 2.2610299587249756, KL: 70.27484130859375, Loss: 0.024763576686382294, Learning Rate: 0.005042099999999998\n",
      "Epoch [4876/20000], Bound: 0.43975046277046204, Entropy: 138.9886932373047, Temp: 2.2613768577575684, KL: 81.73292541503906, Loss: 0.026708249002695084, Learning Rate: 0.005042099999999998\n",
      "Epoch [4877/20000], Bound: 0.40054601430892944, Entropy: 137.77700805664062, Temp: 2.2617685794830322, KL: 70.75967407226562, Loss: 0.0260684322565794, Learning Rate: 0.005042099999999998\n",
      "Epoch [4878/20000], Bound: 0.4026581346988678, Entropy: 137.01356506347656, Temp: 2.2621445655822754, KL: 71.64935302734375, Loss: 0.025415005162358284, Learning Rate: 0.005042099999999998\n",
      "Epoch [4879/20000], Bound: 0.4416789412498474, Entropy: 136.6273651123047, Temp: 2.2625234127044678, KL: 81.10665893554688, Loss: 0.02937891334295273, Learning Rate: 0.005042099999999998\n",
      "Epoch [4880/20000], Bound: 0.41389796137809753, Entropy: 135.3778076171875, Temp: 2.2628910541534424, KL: 74.58619689941406, Loss: 0.02595623955130577, Learning Rate: 0.005042099999999998\n",
      "Epoch [4881/20000], Bound: 0.414188414812088, Entropy: 138.53846740722656, Temp: 2.263270616531372, KL: 73.65792846679688, Loss: 0.028196856379508972, Learning Rate: 0.005042099999999998\n",
      "Epoch [4882/20000], Bound: 0.3932707607746124, Entropy: 137.35952758789062, Temp: 2.2636139392852783, KL: 67.36972045898438, Loss: 0.02911878004670143, Learning Rate: 0.005042099999999998\n",
      "Epoch [4883/20000], Bound: 0.3904065787792206, Entropy: 138.06407165527344, Temp: 2.2638661861419678, KL: 68.75701904296875, Loss: 0.024311494082212448, Learning Rate: 0.005042099999999998\n",
      "Epoch [4884/20000], Bound: 0.4263513386249542, Entropy: 138.51918029785156, Temp: 2.2641351222991943, KL: 75.69577026367188, Loss: 0.03143388777971268, Learning Rate: 0.005042099999999998\n",
      "Epoch [4885/20000], Bound: 0.4214934706687927, Entropy: 133.79034423828125, Temp: 2.264331340789795, KL: 75.06564331054688, Loss: 0.02972855418920517, Learning Rate: 0.005042099999999998\n",
      "Epoch [4886/20000], Bound: 0.43383100628852844, Entropy: 136.43618774414062, Temp: 2.26448917388916, KL: 79.9254150390625, Loss: 0.026917222887277603, Learning Rate: 0.005042099999999998\n",
      "Epoch [4887/20000], Bound: 0.40522539615631104, Entropy: 138.2921905517578, Temp: 2.2646939754486084, KL: 73.15950012207031, Loss: 0.023718221113085747, Learning Rate: 0.005042099999999998\n",
      "Epoch [4888/20000], Bound: 0.42717528343200684, Entropy: 136.4245147705078, Temp: 2.264958620071411, KL: 76.36302185058594, Loss: 0.030503394082188606, Learning Rate: 0.005042099999999998\n",
      "Epoch [4889/20000], Bound: 0.4178079068660736, Entropy: 136.89752197265625, Temp: 2.265171766281128, KL: 73.83595275878906, Loss: 0.03012092225253582, Learning Rate: 0.005042099999999998\n",
      "Epoch [4890/20000], Bound: 0.41843080520629883, Entropy: 138.36624145507812, Temp: 2.265328884124756, KL: 75.71803283691406, Loss: 0.02636362425982952, Learning Rate: 0.005042099999999998\n",
      "Epoch [4891/20000], Bound: 0.43294841051101685, Entropy: 135.04832458496094, Temp: 2.265517234802246, KL: 79.24723815917969, Loss: 0.02786428853869438, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4892/20000], Bound: 0.43497705459594727, Entropy: 138.64060974121094, Temp: 2.2657275199890137, KL: 80.49736022949219, Loss: 0.026422971859574318, Learning Rate: 0.005042099999999998\n",
      "Epoch [4893/20000], Bound: 0.42640191316604614, Entropy: 137.12327575683594, Temp: 2.265991687774658, KL: 78.30531311035156, Loss: 0.025741133838891983, Learning Rate: 0.005042099999999998\n",
      "Epoch [4894/20000], Bound: 0.39504435658454895, Entropy: 138.66014099121094, Temp: 2.266303300857544, KL: 66.8353271484375, Loss: 0.031422678381204605, Learning Rate: 0.005042099999999998\n",
      "Epoch [4895/20000], Bound: 0.43057310581207275, Entropy: 138.56886291503906, Temp: 2.2664785385131836, KL: 79.57951354980469, Loss: 0.02561819925904274, Learning Rate: 0.005042099999999998\n",
      "Epoch [4896/20000], Bound: 0.39573827385902405, Entropy: 139.6183319091797, Temp: 2.2667195796966553, KL: 70.31962585449219, Loss: 0.024167802184820175, Learning Rate: 0.005042099999999998\n",
      "Epoch [4897/20000], Bound: 0.416139155626297, Entropy: 138.3319549560547, Temp: 2.2669899463653564, KL: 77.26806640625, Loss: 0.021528929471969604, Learning Rate: 0.005042099999999998\n",
      "Epoch [4898/20000], Bound: 0.4124203324317932, Entropy: 139.278076171875, Temp: 2.267378091812134, KL: 71.789794921875, Loss: 0.03127880394458771, Learning Rate: 0.005042099999999998\n",
      "Epoch [4899/20000], Bound: 0.3909037113189697, Entropy: 141.29417419433594, Temp: 2.2676570415496826, KL: 69.70236206054688, Loss: 0.022593030706048012, Learning Rate: 0.005042099999999998\n",
      "Epoch [4900/20000], Bound: 0.4169468283653259, Entropy: 141.35862731933594, Temp: 2.267986297607422, KL: 74.19308471679688, Loss: 0.0288382675498724, Learning Rate: 0.005042099999999998\n",
      "Epoch [4901/20000], Bound: 0.44658029079437256, Entropy: 140.05117797851562, Temp: 2.2682721614837646, KL: 82.31053161621094, Loss: 0.03006189502775669, Learning Rate: 0.005042099999999998\n",
      "Epoch [4902/20000], Bound: 0.4096175730228424, Entropy: 141.21267700195312, Temp: 2.2685470581054688, KL: 73.82345581054688, Loss: 0.025061335414648056, Learning Rate: 0.005042099999999998\n",
      "Epoch [4903/20000], Bound: 0.41171249747276306, Entropy: 140.90731811523438, Temp: 2.2688517570495605, KL: 74.27098083496094, Loss: 0.02539098635315895, Learning Rate: 0.005042099999999998\n",
      "Epoch [4904/20000], Bound: 0.43025845289230347, Entropy: 140.61753845214844, Temp: 2.269179582595825, KL: 77.08792114257812, Loss: 0.03096170909702778, Learning Rate: 0.005042099999999998\n",
      "Epoch [4905/20000], Bound: 0.38591936230659485, Entropy: 141.51763916015625, Temp: 2.269442558288574, KL: 65.97897338867188, Loss: 0.02780599147081375, Learning Rate: 0.005042099999999998\n",
      "Epoch [4906/20000], Bound: 0.40418750047683716, Entropy: 140.89219665527344, Temp: 2.2696337699890137, KL: 72.79769897460938, Loss: 0.023963598534464836, Learning Rate: 0.005042099999999998\n",
      "Epoch [4907/20000], Bound: 0.40566861629486084, Entropy: 138.84368896484375, Temp: 2.269876718521118, KL: 73.56675720214844, Loss: 0.023192644119262695, Learning Rate: 0.005042099999999998\n",
      "Epoch [4908/20000], Bound: 0.4046194553375244, Entropy: 141.41661071777344, Temp: 2.2701854705810547, KL: 73.26277160644531, Loss: 0.023217061534523964, Learning Rate: 0.005042099999999998\n",
      "Epoch [4909/20000], Bound: 0.36228904128074646, Entropy: 140.0530242919922, Temp: 2.270550489425659, KL: 60.566314697265625, Loss: 0.025696098804473877, Learning Rate: 0.005042099999999998\n",
      "Epoch [4910/20000], Bound: 0.4404066801071167, Entropy: 138.128662109375, Temp: 2.2708373069763184, KL: 82.55926513671875, Loss: 0.025519225746393204, Learning Rate: 0.005042099999999998\n",
      "Epoch [4911/20000], Bound: 0.42989158630371094, Entropy: 137.9909210205078, Temp: 2.2711966037750244, KL: 79.98321533203125, Loss: 0.024388737976551056, Learning Rate: 0.005042099999999998\n",
      "Epoch [4912/20000], Bound: 0.4416305124759674, Entropy: 137.27088928222656, Temp: 2.2716259956359863, KL: 82.37098693847656, Loss: 0.026750165969133377, Learning Rate: 0.005042099999999998\n",
      "Epoch [4913/20000], Bound: 0.40355026721954346, Entropy: 139.16201782226562, Temp: 2.2720894813537598, KL: 71.90280151367188, Loss: 0.02558331936597824, Learning Rate: 0.005042099999999998\n",
      "Epoch [4914/20000], Bound: 0.42661499977111816, Entropy: 137.3363800048828, Temp: 2.2725398540496826, KL: 75.06016540527344, Loss: 0.03314870223402977, Learning Rate: 0.005042099999999998\n",
      "Epoch [4915/20000], Bound: 0.3897117078304291, Entropy: 139.93374633789062, Temp: 2.2728569507598877, KL: 66.8311767578125, Loss: 0.028275970369577408, Learning Rate: 0.005042099999999998\n",
      "Epoch [4916/20000], Bound: 0.446210652589798, Entropy: 137.75840759277344, Temp: 2.2730913162231445, KL: 82.32801818847656, Loss: 0.029879407957196236, Learning Rate: 0.005042099999999998\n",
      "Epoch [4917/20000], Bound: 0.4028904139995575, Entropy: 139.401123046875, Temp: 2.273319959640503, KL: 72.56404113769531, Loss: 0.023742493242025375, Learning Rate: 0.005042099999999998\n",
      "Epoch [4918/20000], Bound: 0.3940461277961731, Entropy: 136.97894287109375, Temp: 2.273597240447998, KL: 68.16900634765625, Loss: 0.02798202633857727, Learning Rate: 0.005042099999999998\n",
      "Epoch [4919/20000], Bound: 0.41770321130752563, Entropy: 139.3146514892578, Temp: 2.2738094329833984, KL: 75.17594909667969, Loss: 0.0272561926394701, Learning Rate: 0.005042099999999998\n",
      "Epoch [4920/20000], Bound: 0.41868820786476135, Entropy: 138.1368865966797, Temp: 2.274021625518799, KL: 73.9287109375, Loss: 0.030624283477663994, Learning Rate: 0.005042099999999998\n",
      "Epoch [4921/20000], Bound: 0.39732322096824646, Entropy: 139.705078125, Temp: 2.274162769317627, KL: 69.09800720214844, Loss: 0.02795235626399517, Learning Rate: 0.005042099999999998\n",
      "Epoch [4922/20000], Bound: 0.4180097281932831, Entropy: 137.52615356445312, Temp: 2.274258852005005, KL: 76.59677124023438, Loss: 0.024334073066711426, Learning Rate: 0.005042099999999998\n",
      "Epoch [4923/20000], Bound: 0.4291140139102936, Entropy: 136.8256378173828, Temp: 2.274430274963379, KL: 78.42752075195312, Loss: 0.027377519756555557, Learning Rate: 0.005042099999999998\n",
      "Epoch [4924/20000], Bound: 0.42934221029281616, Entropy: 140.81272888183594, Temp: 2.2746236324310303, KL: 79.29559326171875, Loss: 0.02561948634684086, Learning Rate: 0.005042099999999998\n",
      "Epoch [4925/20000], Bound: 0.4221304655075073, Entropy: 138.58358764648438, Temp: 2.274874687194824, KL: 77.40190124511719, Loss: 0.025184644386172295, Learning Rate: 0.005042099999999998\n",
      "Epoch [4926/20000], Bound: 0.4377190172672272, Entropy: 138.00149536132812, Temp: 2.275174379348755, KL: 81.55183410644531, Loss: 0.02607869729399681, Learning Rate: 0.005042099999999998\n",
      "Epoch [4927/20000], Bound: 0.43372780084609985, Entropy: 139.23072814941406, Temp: 2.275526285171509, KL: 78.3831787109375, Loss: 0.030464595183730125, Learning Rate: 0.005042099999999998\n",
      "Epoch [4928/20000], Bound: 0.443136066198349, Entropy: 137.69497680664062, Temp: 2.2758235931396484, KL: 82.91439819335938, Loss: 0.02663019858300686, Learning Rate: 0.005042099999999998\n",
      "Epoch [4929/20000], Bound: 0.4113554060459137, Entropy: 137.20199584960938, Temp: 2.2761707305908203, KL: 75.08717346191406, Loss: 0.023509519174695015, Learning Rate: 0.005042099999999998\n",
      "Epoch [4930/20000], Bound: 0.4083259701728821, Entropy: 136.2042999267578, Temp: 2.2765731811523438, KL: 74.47651672363281, Loss: 0.022969570010900497, Learning Rate: 0.005042099999999998\n",
      "Epoch [4931/20000], Bound: 0.44348686933517456, Entropy: 134.8423614501953, Temp: 2.2770321369171143, KL: 82.57498168945312, Loss: 0.0276310034096241, Learning Rate: 0.005042099999999998\n",
      "Epoch [4932/20000], Bound: 0.4215071201324463, Entropy: 135.57000732421875, Temp: 2.277503490447998, KL: 75.08346557617188, Loss: 0.02993066795170307, Learning Rate: 0.005042099999999998\n",
      "Epoch [4933/20000], Bound: 0.4505002498626709, Entropy: 134.855224609375, Temp: 2.277895927429199, KL: 86.62632751464844, Loss: 0.023371024057269096, Learning Rate: 0.005042099999999998\n",
      "Epoch [4934/20000], Bound: 0.38048696517944336, Entropy: 135.44293212890625, Temp: 2.2784101963043213, KL: 67.39224243164062, Loss: 0.02157220058143139, Learning Rate: 0.005042099999999998\n",
      "Epoch [4935/20000], Bound: 0.3986332416534424, Entropy: 134.581787109375, Temp: 2.2789509296417236, KL: 71.82513427734375, Loss: 0.022846216335892677, Learning Rate: 0.005042099999999998\n",
      "Epoch [4936/20000], Bound: 0.4133088290691376, Entropy: 136.13064575195312, Temp: 2.2795186042785645, KL: 74.99520874023438, Loss: 0.024996181949973106, Learning Rate: 0.005042099999999998\n",
      "Epoch [4937/20000], Bound: 0.3844194710254669, Entropy: 135.56504821777344, Temp: 2.2800889015197754, KL: 66.83367919921875, Loss: 0.025182638317346573, Learning Rate: 0.005042099999999998\n",
      "Epoch [4938/20000], Bound: 0.42311790585517883, Entropy: 136.92137145996094, Temp: 2.2806055545806885, KL: 73.933349609375, Loss: 0.03352726250886917, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4939/20000], Bound: 0.4247635304927826, Entropy: 140.62673950195312, Temp: 2.280961513519287, KL: 79.52888488769531, Loss: 0.022312991321086884, Learning Rate: 0.005042099999999998\n",
      "Epoch [4940/20000], Bound: 0.38507527112960815, Entropy: 141.7865753173828, Temp: 2.281418800354004, KL: 66.45376586914062, Loss: 0.02642986737191677, Learning Rate: 0.005042099999999998\n",
      "Epoch [4941/20000], Bound: 0.3971189558506012, Entropy: 140.15562438964844, Temp: 2.281806468963623, KL: 70.498779296875, Loss: 0.024876372888684273, Learning Rate: 0.005042099999999998\n",
      "Epoch [4942/20000], Bound: 0.39946985244750977, Entropy: 142.32086181640625, Temp: 2.2821874618530273, KL: 70.90626525878906, Loss: 0.025429939851164818, Learning Rate: 0.005042099999999998\n",
      "Epoch [4943/20000], Bound: 0.4519847631454468, Entropy: 139.9808807373047, Temp: 2.2825536727905273, KL: 85.35675048828125, Loss: 0.02724413387477398, Learning Rate: 0.005042099999999998\n",
      "Epoch [4944/20000], Bound: 0.4320582151412964, Entropy: 143.36508178710938, Temp: 2.282961845397949, KL: 79.95986938476562, Loss: 0.026074599474668503, Learning Rate: 0.005042099999999998\n",
      "Epoch [4945/20000], Bound: 0.37911051511764526, Entropy: 140.87692260742188, Temp: 2.2833971977233887, KL: 65.39399719238281, Loss: 0.02520810253918171, Learning Rate: 0.005042099999999998\n",
      "Epoch [4946/20000], Bound: 0.3734409511089325, Entropy: 144.2952880859375, Temp: 2.2837798595428467, KL: 60.60728454589844, Loss: 0.032325878739356995, Learning Rate: 0.005042099999999998\n",
      "Epoch [4947/20000], Bound: 0.4380347728729248, Entropy: 142.39418029785156, Temp: 2.283945322036743, KL: 77.92680358886719, Loss: 0.03440243750810623, Learning Rate: 0.005042099999999998\n",
      "Epoch [4948/20000], Bound: 0.40696626901626587, Entropy: 141.39418029785156, Temp: 2.2839913368225098, KL: 74.43247985839844, Loss: 0.022362036630511284, Learning Rate: 0.005042099999999998\n",
      "Epoch [4949/20000], Bound: 0.4118000268936157, Entropy: 142.07211303710938, Temp: 2.2841362953186035, KL: 74.14826965332031, Loss: 0.025992754846811295, Learning Rate: 0.005042099999999998\n",
      "Epoch [4950/20000], Bound: 0.41353699564933777, Entropy: 139.26681518554688, Temp: 2.2842986583709717, KL: 75.46701049804688, Loss: 0.024194274097681046, Learning Rate: 0.005042099999999998\n",
      "Epoch [4951/20000], Bound: 0.4074043929576874, Entropy: 142.0789031982422, Temp: 2.284519672393799, KL: 71.22529602050781, Loss: 0.02966289222240448, Learning Rate: 0.005042099999999998\n",
      "Epoch [4952/20000], Bound: 0.41750118136405945, Entropy: 141.21853637695312, Temp: 2.2846615314483643, KL: 75.71449279785156, Loss: 0.026145918294787407, Learning Rate: 0.005042099999999998\n",
      "Epoch [4953/20000], Bound: 0.411161333322525, Entropy: 138.05328369140625, Temp: 2.284827947616577, KL: 75.1109619140625, Loss: 0.023500069975852966, Learning Rate: 0.005042099999999998\n",
      "Epoch [4954/20000], Bound: 0.42456698417663574, Entropy: 138.09579467773438, Temp: 2.2850632667541504, KL: 77.52757263183594, Loss: 0.026652835309505463, Learning Rate: 0.005042099999999998\n",
      "Epoch [4955/20000], Bound: 0.41570916771888733, Entropy: 136.19607543945312, Temp: 2.2853152751922607, KL: 73.31007385253906, Loss: 0.030292915180325508, Learning Rate: 0.005042099999999998\n",
      "Epoch [4956/20000], Bound: 0.41442421078681946, Entropy: 135.32127380371094, Temp: 2.2854859828948975, KL: 75.45669555664062, Loss: 0.024794230237603188, Learning Rate: 0.005042099999999998\n",
      "Epoch [4957/20000], Bound: 0.42863866686820984, Entropy: 136.93197631835938, Temp: 2.2857022285461426, KL: 79.82647705078125, Loss: 0.02423044852912426, Learning Rate: 0.005042099999999998\n",
      "Epoch [4958/20000], Bound: 0.46308407187461853, Entropy: 138.75279235839844, Temp: 2.285996913909912, KL: 90.67161560058594, Loss: 0.023100590333342552, Learning Rate: 0.005042099999999998\n",
      "Epoch [4959/20000], Bound: 0.4097638428211212, Entropy: 135.51666259765625, Temp: 2.2864480018615723, KL: 75.31666564941406, Loss: 0.022210709750652313, Learning Rate: 0.005042099999999998\n",
      "Epoch [4960/20000], Bound: 0.40864303708076477, Entropy: 135.4101104736328, Temp: 2.2869648933410645, KL: 72.94180297851562, Loss: 0.026716483756899834, Learning Rate: 0.005042099999999998\n",
      "Epoch [4961/20000], Bound: 0.43005701899528503, Entropy: 133.69189453125, Temp: 2.2874391078948975, KL: 78.67341613769531, Loss: 0.027693230658769608, Learning Rate: 0.005042099999999998\n",
      "Epoch [4962/20000], Bound: 0.3990582227706909, Entropy: 136.1610870361328, Temp: 2.2878921031951904, KL: 68.87492370605469, Loss: 0.029711855575442314, Learning Rate: 0.005042099999999998\n",
      "Epoch [4963/20000], Bound: 0.4415430426597595, Entropy: 134.63536071777344, Temp: 2.2882235050201416, KL: 81.59576416015625, Loss: 0.028736107051372528, Learning Rate: 0.005042099999999998\n",
      "Epoch [4964/20000], Bound: 0.4338315725326538, Entropy: 131.57662963867188, Temp: 2.288546085357666, KL: 80.21195983886719, Loss: 0.0267750583589077, Learning Rate: 0.005042099999999998\n",
      "Epoch [4965/20000], Bound: 0.4216049909591675, Entropy: 132.88088989257812, Temp: 2.288888692855835, KL: 77.40890502929688, Loss: 0.0251078512519598, Learning Rate: 0.005042099999999998\n",
      "Epoch [4966/20000], Bound: 0.4120079576969147, Entropy: 132.45211791992188, Temp: 2.2892634868621826, KL: 76.41078186035156, Loss: 0.02127130888402462, Learning Rate: 0.005042099999999998\n",
      "Epoch [4967/20000], Bound: 0.4070712625980377, Entropy: 133.72801208496094, Temp: 2.2897355556488037, KL: 74.46083068847656, Loss: 0.022472448647022247, Learning Rate: 0.005042099999999998\n",
      "Epoch [4968/20000], Bound: 0.38647571206092834, Entropy: 134.14588928222656, Temp: 2.2902588844299316, KL: 68.56843566894531, Loss: 0.022783488035202026, Learning Rate: 0.005042099999999998\n",
      "Epoch [4969/20000], Bound: 0.4304339289665222, Entropy: 133.5152130126953, Temp: 2.290785074234009, KL: 77.92588806152344, Loss: 0.029629329219460487, Learning Rate: 0.005042099999999998\n",
      "Epoch [4970/20000], Bound: 0.3999072015285492, Entropy: 134.11587524414062, Temp: 2.291240930557251, KL: 72.35208129882812, Loss: 0.022692259401082993, Learning Rate: 0.005042099999999998\n",
      "Epoch [4971/20000], Bound: 0.41171079874038696, Entropy: 135.06185913085938, Temp: 2.291731834411621, KL: 72.29832458496094, Loss: 0.03010701574385166, Learning Rate: 0.005042099999999998\n",
      "Epoch [4972/20000], Bound: 0.40007978677749634, Entropy: 136.76666259765625, Temp: 2.292109489440918, KL: 70.58413696289062, Loss: 0.02666984498500824, Learning Rate: 0.005042099999999998\n",
      "Epoch [4973/20000], Bound: 0.4268680214881897, Entropy: 137.34559631347656, Temp: 2.29244065284729, KL: 79.80392456054688, Loss: 0.023287948220968246, Learning Rate: 0.005042099999999998\n",
      "Epoch [4974/20000], Bound: 0.3908471465110779, Entropy: 141.25662231445312, Temp: 2.2928526401519775, KL: 65.95112609863281, Loss: 0.031167592853307724, Learning Rate: 0.005042099999999998\n",
      "Epoch [4975/20000], Bound: 0.40229105949401855, Entropy: 139.67581176757812, Temp: 2.2930960655212402, KL: 68.14324951171875, Loss: 0.03336390480399132, Learning Rate: 0.005042099999999998\n",
      "Epoch [4976/20000], Bound: 0.4014089107513428, Entropy: 142.2180938720703, Temp: 2.293159246444702, KL: 73.15077209472656, Loss: 0.02190486527979374, Learning Rate: 0.005042099999999998\n",
      "Epoch [4977/20000], Bound: 0.41245168447494507, Entropy: 140.8412322998047, Temp: 2.293315887451172, KL: 75.55668640136719, Loss: 0.023489177227020264, Learning Rate: 0.005042099999999998\n",
      "Epoch [4978/20000], Bound: 0.4076898396015167, Entropy: 140.39979553222656, Temp: 2.2935404777526855, KL: 72.73213195800781, Loss: 0.026694118976593018, Learning Rate: 0.005042099999999998\n",
      "Epoch [4979/20000], Bound: 0.4096584916114807, Entropy: 141.8766326904297, Temp: 2.293745994567871, KL: 74.54330444335938, Loss: 0.023969419300556183, Learning Rate: 0.005042099999999998\n",
      "Epoch [4980/20000], Bound: 0.4177837371826172, Entropy: 142.21188354492188, Temp: 2.293998956680298, KL: 78.08158874511719, Loss: 0.02133313938975334, Learning Rate: 0.005042099999999998\n",
      "Epoch [4981/20000], Bound: 0.4140563905239105, Entropy: 141.53038024902344, Temp: 2.294367551803589, KL: 75.07292175292969, Loss: 0.025563769042491913, Learning Rate: 0.005042099999999998\n",
      "Epoch [4982/20000], Bound: 0.41487616300582886, Entropy: 143.59327697753906, Temp: 2.294739246368408, KL: 76.99394226074219, Loss: 0.021896738559007645, Learning Rate: 0.005042099999999998\n",
      "Epoch [4983/20000], Bound: 0.4135655462741852, Entropy: 143.16888427734375, Temp: 2.295196533203125, KL: 74.81216430664062, Loss: 0.025840267539024353, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4984/20000], Bound: 0.40795615315437317, Entropy: 144.12921142578125, Temp: 2.295640707015991, KL: 73.67332458496094, Loss: 0.02484384924173355, Learning Rate: 0.005042099999999998\n",
      "Epoch [4985/20000], Bound: 0.4106423556804657, Entropy: 142.76953125, Temp: 2.2960846424102783, KL: 73.17204284667969, Loss: 0.027608288452029228, Learning Rate: 0.005042099999999998\n",
      "Epoch [4986/20000], Bound: 0.42378586530685425, Entropy: 144.33567810058594, Temp: 2.296470880508423, KL: 75.40800476074219, Loss: 0.030985765159130096, Learning Rate: 0.005042099999999998\n",
      "Epoch [4987/20000], Bound: 0.4202940762042999, Entropy: 142.32855224609375, Temp: 2.2967543601989746, KL: 76.18905639648438, Loss: 0.027086080983281136, Learning Rate: 0.005042099999999998\n",
      "Epoch [4988/20000], Bound: 0.40375086665153503, Entropy: 143.1009979248047, Temp: 2.297025203704834, KL: 73.72792053222656, Loss: 0.02215578593313694, Learning Rate: 0.005042099999999998\n",
      "Epoch [4989/20000], Bound: 0.44347167015075684, Entropy: 140.44265747070312, Temp: 2.297365188598633, KL: 83.05316162109375, Loss: 0.02699844352900982, Learning Rate: 0.005042099999999998\n",
      "Epoch [4990/20000], Bound: 0.42479822039604187, Entropy: 140.62307739257812, Temp: 2.297731399536133, KL: 75.3028564453125, Loss: 0.03187629580497742, Learning Rate: 0.005042099999999998\n",
      "Epoch [4991/20000], Bound: 0.42405927181243896, Entropy: 140.27317810058594, Temp: 2.2979774475097656, KL: 79.05101013183594, Loss: 0.023257030174136162, Learning Rate: 0.005042099999999998\n",
      "Epoch [4992/20000], Bound: 0.4249773621559143, Entropy: 139.6593475341797, Temp: 2.2983062267303467, KL: 77.79002380371094, Loss: 0.02658817730844021, Learning Rate: 0.005042099999999998\n",
      "Epoch [4993/20000], Bound: 0.4075731933116913, Entropy: 139.7430419921875, Temp: 2.2986366748809814, KL: 73.94566345214844, Loss: 0.024066852405667305, Learning Rate: 0.005042099999999998\n",
      "Epoch [4994/20000], Bound: 0.41779810190200806, Entropy: 136.7224884033203, Temp: 2.2989933490753174, KL: 73.63134765625, Loss: 0.03112073615193367, Learning Rate: 0.005042099999999998\n",
      "Epoch [4995/20000], Bound: 0.4153200685977936, Entropy: 139.0621795654297, Temp: 2.299233913421631, KL: 76.30213928222656, Loss: 0.02376571297645569, Learning Rate: 0.005042099999999998\n",
      "Epoch [4996/20000], Bound: 0.43775951862335205, Entropy: 135.21922302246094, Temp: 2.299529790878296, KL: 79.09750366210938, Loss: 0.03194107860326767, Learning Rate: 0.005042099999999998\n",
      "Epoch [4997/20000], Bound: 0.43192318081855774, Entropy: 137.33668518066406, Temp: 2.299734592437744, KL: 78.76373291015625, Loss: 0.02891959622502327, Learning Rate: 0.005042099999999998\n",
      "Epoch [4998/20000], Bound: 0.416363000869751, Entropy: 134.58387756347656, Temp: 2.2999136447906494, KL: 77.66049194335938, Loss: 0.02147717960178852, Learning Rate: 0.005042099999999998\n",
      "Epoch [4999/20000], Bound: 0.42674723267555237, Entropy: 131.25914001464844, Temp: 2.3002071380615234, KL: 79.10682678222656, Loss: 0.02488330751657486, Learning Rate: 0.005042099999999998\n",
      "Epoch [5000/20000], Bound: 0.4331969916820526, Entropy: 131.92138671875, Temp: 2.300546169281006, KL: 80.48223876953125, Loss: 0.02601507119834423, Learning Rate: 0.005042099999999998\n",
      "Epoch [5001/20000], Bound: 0.42133790254592896, Entropy: 133.72183227539062, Temp: 2.300912380218506, KL: 75.91062927246094, Loss: 0.0284217968583107, Learning Rate: 0.005042099999999998\n",
      "Epoch [5002/20000], Bound: 0.4359135925769806, Entropy: 131.63211059570312, Temp: 2.30122709274292, KL: 81.10379028320312, Loss: 0.02642235718667507, Learning Rate: 0.005042099999999998\n",
      "Epoch [5003/20000], Bound: 0.39345088601112366, Entropy: 129.685791015625, Temp: 2.3015668392181396, KL: 69.28611755371094, Loss: 0.025607237592339516, Learning Rate: 0.005042099999999998\n",
      "Epoch [5004/20000], Bound: 0.4259393811225891, Entropy: 131.23941040039062, Temp: 2.3018698692321777, KL: 80.63917541503906, Loss: 0.021074514836072922, Learning Rate: 0.005042099999999998\n",
      "Epoch [5005/20000], Bound: 0.42383497953414917, Entropy: 132.568115234375, Temp: 2.302300214767456, KL: 77.01071166992188, Loss: 0.027631668373942375, Learning Rate: 0.005042099999999998\n",
      "Epoch [5006/20000], Bound: 0.3966331481933594, Entropy: 132.50238037109375, Temp: 2.302694320678711, KL: 70.41925048828125, Loss: 0.025094054639339447, Learning Rate: 0.005042099999999998\n",
      "Epoch [5007/20000], Bound: 0.4002103805541992, Entropy: 132.06532287597656, Temp: 2.303062677383423, KL: 71.85990905761719, Loss: 0.024151785299181938, Learning Rate: 0.005042099999999998\n",
      "Epoch [5008/20000], Bound: 0.4313565790653229, Entropy: 132.73204040527344, Temp: 2.303436517715454, KL: 80.29920959472656, Loss: 0.02529171109199524, Learning Rate: 0.005042099999999998\n",
      "Epoch [5009/20000], Bound: 0.3830900192260742, Entropy: 134.30380249023438, Temp: 2.303844928741455, KL: 66.37818908691406, Loss: 0.025730149820446968, Learning Rate: 0.005042099999999998\n",
      "Epoch [5010/20000], Bound: 0.41033127903938293, Entropy: 136.6295623779297, Temp: 2.3041868209838867, KL: 74.59564208984375, Loss: 0.02445886842906475, Learning Rate: 0.005042099999999998\n",
      "Epoch [5011/20000], Bound: 0.4003370702266693, Entropy: 136.40098571777344, Temp: 2.304547071456909, KL: 72.05213928222656, Loss: 0.02383679337799549, Learning Rate: 0.005042099999999998\n",
      "Epoch [5012/20000], Bound: 0.40457844734191895, Entropy: 139.1588897705078, Temp: 2.304919719696045, KL: 74.730224609375, Loss: 0.020631277933716774, Learning Rate: 0.005042099999999998\n",
      "Epoch [5013/20000], Bound: 0.39234599471092224, Entropy: 139.7334747314453, Temp: 2.3053839206695557, KL: 68.71762084960938, Loss: 0.026230940595269203, Learning Rate: 0.005042099999999998\n",
      "Epoch [5014/20000], Bound: 0.413006991147995, Entropy: 139.07899475097656, Temp: 2.3057799339294434, KL: 73.41851806640625, Loss: 0.02869941107928753, Learning Rate: 0.005042099999999998\n",
      "Epoch [5015/20000], Bound: 0.41202595829963684, Entropy: 139.80569458007812, Temp: 2.3060970306396484, KL: 76.2999267578125, Loss: 0.02184772863984108, Learning Rate: 0.005042099999999998\n",
      "Epoch [5016/20000], Bound: 0.39627206325531006, Entropy: 141.28553771972656, Temp: 2.3064959049224854, KL: 70.1573486328125, Loss: 0.025502724573016167, Learning Rate: 0.005042099999999998\n",
      "Epoch [5017/20000], Bound: 0.42009344696998596, Entropy: 142.20875549316406, Temp: 2.306856632232666, KL: 77.25431823730469, Loss: 0.024828867986798286, Learning Rate: 0.005042099999999998\n",
      "Epoch [5018/20000], Bound: 0.4193517565727234, Entropy: 143.35498046875, Temp: 2.307241678237915, KL: 77.13890075683594, Loss: 0.024620672687888145, Learning Rate: 0.005042099999999998\n",
      "Epoch [5019/20000], Bound: 0.3880954682826996, Entropy: 143.31362915039062, Temp: 2.3076515197753906, KL: 68.60121154785156, Loss: 0.023959467187523842, Learning Rate: 0.005042099999999998\n",
      "Epoch [5020/20000], Bound: 0.4098394215106964, Entropy: 143.83428955078125, Temp: 2.308042049407959, KL: 74.35935974121094, Loss: 0.024733908474445343, Learning Rate: 0.005042099999999998\n",
      "Epoch [5021/20000], Bound: 0.41855430603027344, Entropy: 142.99072265625, Temp: 2.3084371089935303, KL: 77.15251159667969, Loss: 0.02411358803510666, Learning Rate: 0.005042099999999998\n",
      "Epoch [5022/20000], Bound: 0.42473962903022766, Entropy: 145.1211395263672, Temp: 2.3088650703430176, KL: 78.51498413085938, Loss: 0.02506091818213463, Learning Rate: 0.005042099999999998\n",
      "Epoch [5023/20000], Bound: 0.43565982580184937, Entropy: 142.67416381835938, Temp: 2.3093128204345703, KL: 81.34934997558594, Loss: 0.025885790586471558, Learning Rate: 0.005042099999999998\n",
      "Epoch [5024/20000], Bound: 0.43004822731018066, Entropy: 141.63233947753906, Temp: 2.309779167175293, KL: 80.7767333984375, Loss: 0.02354784868657589, Learning Rate: 0.005042099999999998\n",
      "Epoch [5025/20000], Bound: 0.40343552827835083, Entropy: 137.52618408203125, Temp: 2.3103044033050537, KL: 70.01239013671875, Loss: 0.030240599066019058, Learning Rate: 0.005042099999999998\n",
      "Epoch [5026/20000], Bound: 0.43151581287384033, Entropy: 140.15565490722656, Temp: 2.310680866241455, KL: 78.86061096191406, Loss: 0.028647011145949364, Learning Rate: 0.005042099999999998\n",
      "Epoch [5027/20000], Bound: 0.3966403007507324, Entropy: 140.1251678466797, Temp: 2.3110122680664062, KL: 70.24610900878906, Loss: 0.02560378424823284, Learning Rate: 0.005042099999999998\n",
      "Epoch [5028/20000], Bound: 0.41417890787124634, Entropy: 139.3583984375, Temp: 2.311307668685913, KL: 75.6697998046875, Loss: 0.024644972756505013, Learning Rate: 0.005042099999999998\n",
      "Epoch [5029/20000], Bound: 0.4761716425418854, Entropy: 137.92947387695312, Temp: 2.311624765396118, KL: 94.38978576660156, Loss: 0.024526989087462425, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5030/20000], Bound: 0.3825407028198242, Entropy: 137.00804138183594, Temp: 2.3120763301849365, KL: 67.03831481933594, Loss: 0.024091340601444244, Learning Rate: 0.005042099999999998\n",
      "Epoch [5031/20000], Bound: 0.4223426282405853, Entropy: 138.52113342285156, Temp: 2.3124887943267822, KL: 79.10527038574219, Loss: 0.022342033684253693, Learning Rate: 0.005042099999999998\n",
      "Epoch [5032/20000], Bound: 0.3811536133289337, Entropy: 134.70169067382812, Temp: 2.3129775524139404, KL: 69.20588684082031, Loss: 0.018595030531287193, Learning Rate: 0.005042099999999998\n",
      "Epoch [5033/20000], Bound: 0.42859533429145813, Entropy: 137.2138671875, Temp: 2.313549518585205, KL: 79.45404052734375, Loss: 0.025558508932590485, Learning Rate: 0.005042099999999998\n",
      "Epoch [5034/20000], Bound: 0.4121243953704834, Entropy: 134.65663146972656, Temp: 2.3141191005706787, KL: 75.39788818359375, Loss: 0.024007409811019897, Learning Rate: 0.005042099999999998\n",
      "Epoch [5035/20000], Bound: 0.42594313621520996, Entropy: 136.30496215820312, Temp: 2.314692497253418, KL: 80.33670043945312, Loss: 0.02199413813650608, Learning Rate: 0.005042099999999998\n",
      "Epoch [5036/20000], Bound: 0.3970158100128174, Entropy: 134.974609375, Temp: 2.3153393268585205, KL: 71.78550720214844, Loss: 0.02257329411804676, Learning Rate: 0.005042099999999998\n",
      "Epoch [5037/20000], Bound: 0.3984920382499695, Entropy: 133.44302368164062, Temp: 2.3159873485565186, KL: 72.42459106445312, Loss: 0.022099914029240608, Learning Rate: 0.005042099999999998\n",
      "Epoch [5038/20000], Bound: 0.38459256291389465, Entropy: 135.94407653808594, Temp: 2.316649913787842, KL: 68.04660034179688, Loss: 0.023201774805784225, Learning Rate: 0.005042099999999998\n",
      "Epoch [5039/20000], Bound: 0.40223023295402527, Entropy: 136.584716796875, Temp: 2.3172738552093506, KL: 71.60914611816406, Loss: 0.026156632229685783, Learning Rate: 0.005042099999999998\n",
      "Epoch [5040/20000], Bound: 0.3895227611064911, Entropy: 136.88623046875, Temp: 2.317826747894287, KL: 68.00164794921875, Loss: 0.026261769235134125, Learning Rate: 0.005042099999999998\n",
      "Epoch [5041/20000], Bound: 0.4135708808898926, Entropy: 135.90809631347656, Temp: 2.318289041519165, KL: 75.4200439453125, Loss: 0.024928783997893333, Learning Rate: 0.005042099999999998\n",
      "Epoch [5042/20000], Bound: 0.4097014367580414, Entropy: 140.30279541015625, Temp: 2.318744659423828, KL: 73.51579284667969, Loss: 0.026650361716747284, Learning Rate: 0.005042099999999998\n",
      "Epoch [5043/20000], Bound: 0.40570151805877686, Entropy: 139.52456665039062, Temp: 2.3191471099853516, KL: 73.72268676757812, Loss: 0.02375173009932041, Learning Rate: 0.005042099999999998\n",
      "Epoch [5044/20000], Bound: 0.3996247351169586, Entropy: 139.9576873779297, Temp: 2.319561004638672, KL: 72.03773498535156, Loss: 0.023681964725255966, Learning Rate: 0.005042099999999998\n",
      "Epoch [5045/20000], Bound: 0.4175637662410736, Entropy: 140.10256958007812, Temp: 2.3199760913848877, KL: 75.35958862304688, Loss: 0.027570201084017754, Learning Rate: 0.005042099999999998\n",
      "Epoch [5046/20000], Bound: 0.43606898188591003, Entropy: 141.21331787109375, Temp: 2.3203341960906982, KL: 80.42579650878906, Loss: 0.02835152857005596, Learning Rate: 0.005042099999999998\n",
      "Epoch [5047/20000], Bound: 0.4091152250766754, Entropy: 143.12689208984375, Temp: 2.3206582069396973, KL: 72.91987609863281, Loss: 0.02760384790599346, Learning Rate: 0.005042099999999998\n",
      "Epoch [5048/20000], Bound: 0.42379143834114075, Entropy: 139.93191528320312, Temp: 2.3209173679351807, KL: 77.97367858886719, Loss: 0.025853117927908897, Learning Rate: 0.005042099999999998\n",
      "Epoch [5049/20000], Bound: 0.4247685670852661, Entropy: 140.829833984375, Temp: 2.321185827255249, KL: 78.89137268066406, Loss: 0.02449609711766243, Learning Rate: 0.005042099999999998\n",
      "Epoch [5050/20000], Bound: 0.3916473686695099, Entropy: 140.55357360839844, Temp: 2.321495532989502, KL: 70.657470703125, Loss: 0.02186770923435688, Learning Rate: 0.005042099999999998\n",
      "Epoch [5051/20000], Bound: 0.367664635181427, Entropy: 141.30160522460938, Temp: 2.3218436241149902, KL: 64.22817993164062, Loss: 0.0215503741055727, Learning Rate: 0.005042099999999998\n",
      "Epoch [5052/20000], Bound: 0.38825225830078125, Entropy: 140.13600158691406, Temp: 2.322190523147583, KL: 69.26126098632812, Loss: 0.02285107597708702, Learning Rate: 0.005042099999999998\n",
      "Epoch [5053/20000], Bound: 0.40289777517318726, Entropy: 139.3370819091797, Temp: 2.322542428970337, KL: 70.74931335449219, Loss: 0.028496140614151955, Learning Rate: 0.005042099999999998\n",
      "Epoch [5054/20000], Bound: 0.4187931716442108, Entropy: 140.74497985839844, Temp: 2.32279372215271, KL: 76.3968505859375, Loss: 0.026150260120630264, Learning Rate: 0.005042099999999998\n",
      "Epoch [5055/20000], Bound: 0.4290538430213928, Entropy: 141.29124450683594, Temp: 2.32303786277771, KL: 79.96333312988281, Loss: 0.024929145351052284, Learning Rate: 0.005042099999999998\n",
      "Epoch [5056/20000], Bound: 0.3998281955718994, Entropy: 139.35491943359375, Temp: 2.323322296142578, KL: 71.5408935546875, Loss: 0.02493567019701004, Learning Rate: 0.005042099999999998\n",
      "Epoch [5057/20000], Bound: 0.41859865188598633, Entropy: 140.35850524902344, Temp: 2.323589563369751, KL: 77.98005676269531, Loss: 0.02263564057648182, Learning Rate: 0.005042099999999998\n",
      "Epoch [5058/20000], Bound: 0.3927053213119507, Entropy: 139.9223175048828, Temp: 2.3239283561706543, KL: 69.00660705566406, Loss: 0.026094388216733932, Learning Rate: 0.005042099999999998\n",
      "Epoch [5059/20000], Bound: 0.4082721769809723, Entropy: 139.65097045898438, Temp: 2.324204206466675, KL: 72.82955932617188, Loss: 0.027334680780768394, Learning Rate: 0.005042099999999998\n",
      "Epoch [5060/20000], Bound: 0.44229015707969666, Entropy: 140.93724060058594, Temp: 2.3244223594665527, KL: 83.64585876464844, Loss: 0.025498051196336746, Learning Rate: 0.005042099999999998\n",
      "Epoch [5061/20000], Bound: 0.4068622887134552, Entropy: 139.5213165283203, Temp: 2.3246941566467285, KL: 72.2783203125, Loss: 0.027662545442581177, Learning Rate: 0.005042099999999998\n",
      "Epoch [5062/20000], Bound: 0.3913399577140808, Entropy: 138.31207275390625, Temp: 2.3248982429504395, KL: 68.064697265625, Loss: 0.027314187958836555, Learning Rate: 0.005042099999999998\n",
      "Epoch [5063/20000], Bound: 0.41499364376068115, Entropy: 139.42745971679688, Temp: 2.3250205516815186, KL: 73.30772399902344, Loss: 0.030468201264739037, Learning Rate: 0.005042099999999998\n",
      "Epoch [5064/20000], Bound: 0.42245933413505554, Entropy: 137.12728881835938, Temp: 2.3250396251678467, KL: 75.66822814941406, Loss: 0.030048267915844917, Learning Rate: 0.005042099999999998\n",
      "Epoch [5065/20000], Bound: 0.41488906741142273, Entropy: 135.89306640625, Temp: 2.3249897956848145, KL: 73.69462585449219, Loss: 0.029570868238806725, Learning Rate: 0.005042099999999998\n",
      "Epoch [5066/20000], Bound: 0.4272032082080841, Entropy: 137.70223999023438, Temp: 2.3248746395111084, KL: 77.19288635253906, Loss: 0.029751675203442574, Learning Rate: 0.005042099999999998\n",
      "Epoch [5067/20000], Bound: 0.4510951340198517, Entropy: 137.00424194335938, Temp: 2.3247199058532715, KL: 85.909423828125, Loss: 0.026359418407082558, Learning Rate: 0.005042099999999998\n",
      "Epoch [5068/20000], Bound: 0.41905200481414795, Entropy: 139.1435546875, Temp: 2.3246524333953857, KL: 77.35311889648438, Loss: 0.024286698549985886, Learning Rate: 0.005042099999999998\n",
      "Epoch [5069/20000], Bound: 0.37772136926651, Entropy: 136.3158721923828, Temp: 2.3246524333953857, KL: 66.26040649414062, Loss: 0.023098554462194443, Learning Rate: 0.005042099999999998\n",
      "Epoch [5070/20000], Bound: 0.3743380606174469, Entropy: 139.5160369873047, Temp: 2.3246657848358154, KL: 65.29341125488281, Loss: 0.02319139614701271, Learning Rate: 0.005042099999999998\n",
      "Epoch [5071/20000], Bound: 0.4047517478466034, Entropy: 141.3172607421875, Temp: 2.3246829509735107, KL: 73.21539306640625, Loss: 0.02435467205941677, Learning Rate: 0.005042099999999998\n",
      "Epoch [5072/20000], Bound: 0.40992972254753113, Entropy: 137.90191650390625, Temp: 2.3247313499450684, KL: 75.074951171875, Loss: 0.02353302389383316, Learning Rate: 0.005042099999999998\n",
      "Epoch [5073/20000], Bound: 0.38534286618232727, Entropy: 141.21522521972656, Temp: 2.3248367309570312, KL: 65.93293762207031, Loss: 0.02831561490893364, Learning Rate: 0.005042099999999998\n",
      "Epoch [5074/20000], Bound: 0.410793274641037, Entropy: 138.8000946044922, Temp: 2.3248345851898193, KL: 74.2589111328125, Loss: 0.025822000578045845, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5075/20000], Bound: 0.42766204476356506, Entropy: 137.9755096435547, Temp: 2.3248424530029297, KL: 78.46694946289062, Loss: 0.027300912886857986, Learning Rate: 0.005042099999999998\n",
      "Epoch [5076/20000], Bound: 0.39016205072402954, Entropy: 141.68809509277344, Temp: 2.3248558044433594, KL: 69.38661193847656, Loss: 0.023764636367559433, Learning Rate: 0.005042099999999998\n",
      "Epoch [5077/20000], Bound: 0.37989458441734314, Entropy: 139.44358825683594, Temp: 2.324888229370117, KL: 65.52546691894531, Loss: 0.025963710620999336, Learning Rate: 0.005042099999999998\n",
      "Epoch [5078/20000], Bound: 0.4220418930053711, Entropy: 142.30484008789062, Temp: 2.3248660564422607, KL: 76.41140747070312, Loss: 0.028185490518808365, Learning Rate: 0.005042099999999998\n",
      "Epoch [5079/20000], Bound: 0.4072577655315399, Entropy: 139.48736572265625, Temp: 2.3248214721679688, KL: 72.72352600097656, Loss: 0.026949482038617134, Learning Rate: 0.005042099999999998\n",
      "Epoch [5080/20000], Bound: 0.36814746260643005, Entropy: 143.5113067626953, Temp: 2.3247578144073486, KL: 62.65864562988281, Loss: 0.025247054174542427, Learning Rate: 0.005042099999999998\n",
      "Epoch [5081/20000], Bound: 0.4207127094268799, Entropy: 144.53648376464844, Temp: 2.3246448040008545, KL: 79.45858764648438, Loss: 0.020795542746782303, Learning Rate: 0.005042099999999998\n",
      "Epoch [5082/20000], Bound: 0.4281219244003296, Entropy: 145.76736450195312, Temp: 2.3246889114379883, KL: 77.25997924804688, Loss: 0.030184898525476456, Learning Rate: 0.005042099999999998\n",
      "Epoch [5083/20000], Bound: 0.3882961869239807, Entropy: 141.7066192626953, Temp: 2.3246688842773438, KL: 66.33251953125, Loss: 0.0292146485298872, Learning Rate: 0.005042099999999998\n",
      "Epoch [5084/20000], Bound: 0.4131462275981903, Entropy: 143.4462890625, Temp: 2.324537754058838, KL: 74.18365478515625, Loss: 0.027432378381490707, Learning Rate: 0.005042099999999998\n",
      "Epoch [5085/20000], Bound: 0.41493263840675354, Entropy: 141.55267333984375, Temp: 2.3243961334228516, KL: 76.28720092773438, Loss: 0.02401196025311947, Learning Rate: 0.005042099999999998\n",
      "Epoch [5086/20000], Bound: 0.3853874206542969, Entropy: 141.21665954589844, Temp: 2.324328660964966, KL: 63.42716979980469, Loss: 0.033725984394550323, Learning Rate: 0.005042099999999998\n",
      "Epoch [5087/20000], Bound: 0.3921445310115814, Entropy: 141.05177307128906, Temp: 2.3240416049957275, KL: 68.39146423339844, Loss: 0.02708246558904648, Learning Rate: 0.005042099999999998\n",
      "Epoch [5088/20000], Bound: 0.43726804852485657, Entropy: 144.65533447265625, Temp: 2.323728322982788, KL: 78.64520263671875, Loss: 0.03301247954368591, Learning Rate: 0.005042099999999998\n",
      "Epoch [5089/20000], Bound: 0.44479605555534363, Entropy: 140.27615356445312, Temp: 2.3233394622802734, KL: 85.19010925292969, Loss: 0.02377491444349289, Learning Rate: 0.005042099999999998\n",
      "Epoch [5090/20000], Bound: 0.39846208691596985, Entropy: 140.53941345214844, Temp: 2.323110580444336, KL: 73.18827819824219, Loss: 0.020557880401611328, Learning Rate: 0.005042099999999998\n",
      "Epoch [5091/20000], Bound: 0.4263341426849365, Entropy: 140.927734375, Temp: 2.3230173587799072, KL: 80.44664001464844, Loss: 0.022169137373566628, Learning Rate: 0.005042099999999998\n",
      "Epoch [5092/20000], Bound: 0.4311133921146393, Entropy: 138.6759796142578, Temp: 2.323058843612671, KL: 79.18389892578125, Loss: 0.027913762256503105, Learning Rate: 0.005042099999999998\n",
      "Epoch [5093/20000], Bound: 0.41482728719711304, Entropy: 139.6970672607422, Temp: 2.3230960369110107, KL: 76.81669616699219, Loss: 0.022784098982810974, Learning Rate: 0.005042099999999998\n",
      "Epoch [5094/20000], Bound: 0.44101959466934204, Entropy: 138.82107543945312, Temp: 2.3232195377349854, KL: 83.24844360351562, Loss: 0.025509314611554146, Learning Rate: 0.005042099999999998\n",
      "Epoch [5095/20000], Bound: 0.4209502339363098, Entropy: 134.47470092773438, Temp: 2.3234052658081055, KL: 78.00604248046875, Loss: 0.024045785889029503, Learning Rate: 0.005042099999999998\n",
      "Epoch [5096/20000], Bound: 0.38790836930274963, Entropy: 135.04222106933594, Temp: 2.323643922805786, KL: 67.51785278320312, Loss: 0.02641945704817772, Learning Rate: 0.005042099999999998\n",
      "Epoch [5097/20000], Bound: 0.40835458040237427, Entropy: 136.392822265625, Temp: 2.3238120079040527, KL: 75.06170654296875, Loss: 0.02257651276886463, Learning Rate: 0.005042099999999998\n",
      "Epoch [5098/20000], Bound: 0.3749379813671112, Entropy: 136.0011444091797, Temp: 2.3240461349487305, KL: 63.852996826171875, Loss: 0.026633530855178833, Learning Rate: 0.005042099999999998\n",
      "Epoch [5099/20000], Bound: 0.4178325831890106, Entropy: 132.9228057861328, Temp: 2.3241801261901855, KL: 76.94252014160156, Loss: 0.024401243776082993, Learning Rate: 0.005042099999999998\n",
      "Epoch [5100/20000], Bound: 0.4329116642475128, Entropy: 136.2697296142578, Temp: 2.3243579864501953, KL: 82.3084716796875, Loss: 0.0223590936511755, Learning Rate: 0.005042099999999998\n",
      "Epoch [5101/20000], Bound: 0.3856675326824188, Entropy: 135.677978515625, Temp: 2.324650526046753, KL: 67.83517456054688, Loss: 0.02441505901515484, Learning Rate: 0.005042099999999998\n",
      "Epoch [5102/20000], Bound: 0.43417736887931824, Entropy: 135.02943420410156, Temp: 2.3249101638793945, KL: 82.43124389648438, Loss: 0.02291298657655716, Learning Rate: 0.005042099999999998\n",
      "Epoch [5103/20000], Bound: 0.39652687311172485, Entropy: 138.1903533935547, Temp: 2.325265884399414, KL: 70.55276489257812, Loss: 0.025091538205742836, Learning Rate: 0.005042099999999998\n",
      "Epoch [5104/20000], Bound: 0.37186601758003235, Entropy: 139.04037475585938, Temp: 2.3255860805511475, KL: 63.759429931640625, Loss: 0.02505630999803543, Learning Rate: 0.005042099999999998\n",
      "Epoch [5105/20000], Bound: 0.38147997856140137, Entropy: 140.56063842773438, Temp: 2.32582950592041, KL: 65.64849853515625, Loss: 0.026648398488759995, Learning Rate: 0.005042099999999998\n",
      "Epoch [5106/20000], Bound: 0.40333715081214905, Entropy: 138.86322021484375, Temp: 2.3259825706481934, KL: 73.26924133300781, Loss: 0.02339654415845871, Learning Rate: 0.005042099999999998\n",
      "Epoch [5107/20000], Bound: 0.41702234745025635, Entropy: 141.64312744140625, Temp: 2.3261735439300537, KL: 76.28146362304688, Loss: 0.025353172793984413, Learning Rate: 0.005042099999999998\n",
      "Epoch [5108/20000], Bound: 0.4339101016521454, Entropy: 141.76791381835938, Temp: 2.3263769149780273, KL: 81.54200744628906, Loss: 0.02468331716954708, Learning Rate: 0.005042099999999998\n",
      "Epoch [5109/20000], Bound: 0.39803361892700195, Entropy: 143.45216369628906, Temp: 2.326639175415039, KL: 70.60540771484375, Loss: 0.02591017447412014, Learning Rate: 0.005042099999999998\n",
      "Epoch [5110/20000], Bound: 0.37207356095314026, Entropy: 144.235595703125, Temp: 2.326857566833496, KL: 65.65924072265625, Loss: 0.02111130580306053, Learning Rate: 0.005042099999999998\n",
      "Epoch [5111/20000], Bound: 0.4393172264099121, Entropy: 142.13861083984375, Temp: 2.3271048069000244, KL: 83.28530883789062, Loss: 0.024411911144852638, Learning Rate: 0.005042099999999998\n",
      "Epoch [5112/20000], Bound: 0.4652298092842102, Entropy: 143.61753845214844, Temp: 2.3274223804473877, KL: 90.68977355957031, Loss: 0.02549012564122677, Learning Rate: 0.005042099999999998\n",
      "Epoch [5113/20000], Bound: 0.40616950392723083, Entropy: 143.50204467773438, Temp: 2.327826499938965, KL: 71.9188232421875, Loss: 0.02805771306157112, Learning Rate: 0.005042099999999998\n",
      "Epoch [5114/20000], Bound: 0.4063156545162201, Entropy: 141.3335723876953, Temp: 2.3281359672546387, KL: 72.36894226074219, Loss: 0.027185095474123955, Learning Rate: 0.005042099999999998\n",
      "Epoch [5115/20000], Bound: 0.41333019733428955, Entropy: 139.6925506591797, Temp: 2.328381299972534, KL: 74.50276184082031, Loss: 0.026921775192022324, Learning Rate: 0.005042099999999998\n",
      "Epoch [5116/20000], Bound: 0.4076259732246399, Entropy: 141.1946563720703, Temp: 2.3285880088806152, KL: 70.82640075683594, Loss: 0.031306859105825424, Learning Rate: 0.005042099999999998\n",
      "Epoch [5117/20000], Bound: 0.4567558467388153, Entropy: 137.9322052001953, Temp: 2.328644275665283, KL: 87.21253967285156, Loss: 0.027359021827578545, Learning Rate: 0.005042099999999998\n",
      "Epoch [5118/20000], Bound: 0.3971274793148041, Entropy: 137.27325439453125, Temp: 2.3287532329559326, KL: 72.07931518554688, Loss: 0.022229265421628952, Learning Rate: 0.005042099999999998\n",
      "Epoch [5119/20000], Bound: 0.3995392322540283, Entropy: 133.94265747070312, Temp: 2.3289194107055664, KL: 72.81887817382812, Loss: 0.022103631868958473, Learning Rate: 0.005042099999999998\n",
      "Epoch [5120/20000], Bound: 0.4204446077346802, Entropy: 136.04620361328125, Temp: 2.3291444778442383, KL: 77.37287902832031, Loss: 0.025193197652697563, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5121/20000], Bound: 0.4420408308506012, Entropy: 133.8498992919922, Temp: 2.3293874263763428, KL: 83.05699157714844, Loss: 0.02670009806752205, Learning Rate: 0.005042099999999998\n",
      "Epoch [5122/20000], Bound: 0.4061233103275299, Entropy: 134.2868194580078, Temp: 2.329651117324829, KL: 71.40704345703125, Loss: 0.029154814779758453, Learning Rate: 0.005042099999999998\n",
      "Epoch [5123/20000], Bound: 0.38610851764678955, Entropy: 136.54795837402344, Temp: 2.329806327819824, KL: 68.11834716796875, Loss: 0.024143187329173088, Learning Rate: 0.005042099999999998\n",
      "Epoch [5124/20000], Bound: 0.412248820066452, Entropy: 136.74090576171875, Temp: 2.3299472332000732, KL: 76.84788513183594, Loss: 0.021246470510959625, Learning Rate: 0.005042099999999998\n",
      "Epoch [5125/20000], Bound: 0.4154815971851349, Entropy: 135.40420532226562, Temp: 2.330192804336548, KL: 75.7337646484375, Loss: 0.02564091794192791, Learning Rate: 0.005042099999999998\n",
      "Epoch [5126/20000], Bound: 0.40207046270370483, Entropy: 135.93063354492188, Temp: 2.3304336071014404, KL: 71.19338989257812, Loss: 0.02715279720723629, Learning Rate: 0.005042099999999998\n",
      "Epoch [5127/20000], Bound: 0.42997273802757263, Entropy: 138.82908630371094, Temp: 2.3306076526641846, KL: 78.192626953125, Loss: 0.029450297355651855, Learning Rate: 0.005042099999999998\n",
      "Epoch [5128/20000], Bound: 0.4238424599170685, Entropy: 135.65550231933594, Temp: 2.3307206630706787, KL: 78.81648254394531, Loss: 0.024250337854027748, Learning Rate: 0.005042099999999998\n",
      "Epoch [5129/20000], Bound: 0.39103934168815613, Entropy: 138.19540405273438, Temp: 2.330890417098999, KL: 69.00294494628906, Loss: 0.025203002616763115, Learning Rate: 0.005042099999999998\n",
      "Epoch [5130/20000], Bound: 0.4019339680671692, Entropy: 138.1517333984375, Temp: 2.331027030944824, KL: 72.19125366210938, Loss: 0.024938110262155533, Learning Rate: 0.005042099999999998\n",
      "Epoch [5131/20000], Bound: 0.39254340529441833, Entropy: 137.22079467773438, Temp: 2.331160545349121, KL: 69.25689697265625, Loss: 0.025563517585396767, Learning Rate: 0.005042099999999998\n",
      "Epoch [5132/20000], Bound: 0.3987405002117157, Entropy: 138.70265197753906, Temp: 2.3312580585479736, KL: 69.72135925292969, Loss: 0.028302302584052086, Learning Rate: 0.005042099999999998\n",
      "Epoch [5133/20000], Bound: 0.4133782982826233, Entropy: 137.96682739257812, Temp: 2.331268548965454, KL: 74.44711303710938, Loss: 0.02711700089275837, Learning Rate: 0.005042099999999998\n",
      "Epoch [5134/20000], Bound: 0.42802223563194275, Entropy: 138.81259155273438, Temp: 2.3312575817108154, KL: 79.88052368164062, Loss: 0.024608125910162926, Learning Rate: 0.005042099999999998\n",
      "Epoch [5135/20000], Bound: 0.40080690383911133, Entropy: 139.32298278808594, Temp: 2.3313148021698, KL: 73.26312255859375, Loss: 0.02195926196873188, Learning Rate: 0.005042099999999998\n",
      "Epoch [5136/20000], Bound: 0.4063582122325897, Entropy: 142.74903869628906, Temp: 2.331446886062622, KL: 73.87374877929688, Loss: 0.02403397485613823, Learning Rate: 0.005042099999999998\n",
      "Epoch [5137/20000], Bound: 0.41564109921455383, Entropy: 144.486083984375, Temp: 2.331606149673462, KL: 72.35267639160156, Loss: 0.03301399201154709, Learning Rate: 0.005042099999999998\n",
      "Epoch [5138/20000], Bound: 0.40836629271507263, Entropy: 142.66375732421875, Temp: 2.3315911293029785, KL: 74.58439636230469, Loss: 0.023742321878671646, Learning Rate: 0.005042099999999998\n",
      "Epoch [5139/20000], Bound: 0.3874147832393646, Entropy: 140.00502014160156, Temp: 2.3316283226013184, KL: 68.74552917480469, Loss: 0.023601602762937546, Learning Rate: 0.005042099999999998\n",
      "Epoch [5140/20000], Bound: 0.4023014008998871, Entropy: 142.5465850830078, Temp: 2.3316779136657715, KL: 73.48727416992188, Loss: 0.022392481565475464, Learning Rate: 0.005042099999999998\n",
      "Epoch [5141/20000], Bound: 0.41697612404823303, Entropy: 141.49575805664062, Temp: 2.3317947387695312, KL: 76.34877014160156, Loss: 0.02527601458132267, Learning Rate: 0.005042099999999998\n",
      "Epoch [5142/20000], Bound: 0.37775319814682007, Entropy: 141.65927124023438, Temp: 2.331930160522461, KL: 66.55630493164062, Loss: 0.022585038095712662, Learning Rate: 0.005042099999999998\n",
      "Epoch [5143/20000], Bound: 0.42962372303009033, Entropy: 145.8663787841797, Temp: 2.3320741653442383, KL: 79.16091918945312, Loss: 0.027177860960364342, Learning Rate: 0.005042099999999998\n",
      "Epoch [5144/20000], Bound: 0.4417959153652191, Entropy: 142.29649353027344, Temp: 2.3322126865386963, KL: 83.42333984375, Loss: 0.025811178609728813, Learning Rate: 0.005042099999999998\n",
      "Epoch [5145/20000], Bound: 0.38180696964263916, Entropy: 140.4663848876953, Temp: 2.332401752471924, KL: 64.77897644042969, Loss: 0.028790144249796867, Learning Rate: 0.005042099999999998\n",
      "Epoch [5146/20000], Bound: 0.3951095640659332, Entropy: 143.47071838378906, Temp: 2.3324484825134277, KL: 72.31745910644531, Loss: 0.020562762394547462, Learning Rate: 0.005042099999999998\n",
      "Epoch [5147/20000], Bound: 0.37794506549835205, Entropy: 141.74684143066406, Temp: 2.332594156265259, KL: 65.37635803222656, Loss: 0.025236429646611214, Learning Rate: 0.005042099999999998\n",
      "Epoch [5148/20000], Bound: 0.40501895546913147, Entropy: 141.28775024414062, Temp: 2.3326821327209473, KL: 74.1572265625, Loss: 0.02262849360704422, Learning Rate: 0.005042099999999998\n",
      "Epoch [5149/20000], Bound: 0.3967393636703491, Entropy: 141.54112243652344, Temp: 2.3328328132629395, KL: 68.96745300292969, Loss: 0.028731221333146095, Learning Rate: 0.005042099999999998\n",
      "Epoch [5150/20000], Bound: 0.41160744428634644, Entropy: 140.0216522216797, Temp: 2.3328752517700195, KL: 75.32122802734375, Loss: 0.02417663484811783, Learning Rate: 0.005042099999999998\n",
      "Epoch [5151/20000], Bound: 0.44579142332077026, Entropy: 140.76248168945312, Temp: 2.3329596519470215, KL: 85.66763305664062, Loss: 0.0235965047031641, Learning Rate: 0.005042099999999998\n",
      "Epoch [5152/20000], Bound: 0.39815935492515564, Entropy: 141.14730834960938, Temp: 2.333159923553467, KL: 69.70051574707031, Loss: 0.02802186831831932, Learning Rate: 0.005042099999999998\n",
      "Epoch [5153/20000], Bound: 0.41539260745048523, Entropy: 140.122314453125, Temp: 2.3332667350769043, KL: 76.7296142578125, Loss: 0.023503107950091362, Learning Rate: 0.005042099999999998\n",
      "Epoch [5154/20000], Bound: 0.4178353250026703, Entropy: 141.1707763671875, Temp: 2.333432197570801, KL: 75.9423828125, Loss: 0.026708314195275307, Learning Rate: 0.005042099999999998\n",
      "Epoch [5155/20000], Bound: 0.4502395689487457, Entropy: 139.21469116210938, Temp: 2.3335776329040527, KL: 83.71681213378906, Loss: 0.030680285766720772, Learning Rate: 0.005042099999999998\n",
      "Epoch [5156/20000], Bound: 0.40536510944366455, Entropy: 139.88290405273438, Temp: 2.333672523498535, KL: 72.96427917480469, Loss: 0.025412393733859062, Learning Rate: 0.005042099999999998\n",
      "Epoch [5157/20000], Bound: 0.4019522964954376, Entropy: 138.17648315429688, Temp: 2.3337619304656982, KL: 71.39964294433594, Loss: 0.02668777108192444, Learning Rate: 0.005042099999999998\n",
      "Epoch [5158/20000], Bound: 0.43322473764419556, Entropy: 137.26248168945312, Temp: 2.333808183670044, KL: 80.80953979492188, Loss: 0.025959070771932602, Learning Rate: 0.005042099999999998\n",
      "Epoch [5159/20000], Bound: 0.4129764437675476, Entropy: 136.14418029785156, Temp: 2.3338937759399414, KL: 74.75115966796875, Loss: 0.02625906839966774, Learning Rate: 0.005042099999999998\n",
      "Epoch [5160/20000], Bound: 0.4157004952430725, Entropy: 137.11048889160156, Temp: 2.333968162536621, KL: 77.18284606933594, Loss: 0.022735146805644035, Learning Rate: 0.005042099999999998\n",
      "Epoch [5161/20000], Bound: 0.4253058135509491, Entropy: 134.01951599121094, Temp: 2.3341236114501953, KL: 80.65692138671875, Loss: 0.021288014948368073, Learning Rate: 0.005042099999999998\n",
      "Epoch [5162/20000], Bound: 0.4078925549983978, Entropy: 135.32530212402344, Temp: 2.334404945373535, KL: 74.00743103027344, Loss: 0.024734875187277794, Learning Rate: 0.005042099999999998\n",
      "Epoch [5163/20000], Bound: 0.39370113611221313, Entropy: 136.352294921875, Temp: 2.3346829414367676, KL: 69.58808898925781, Loss: 0.025599045678973198, Learning Rate: 0.005042099999999998\n",
      "Epoch [5164/20000], Bound: 0.3892340660095215, Entropy: 136.99766540527344, Temp: 2.334909439086914, KL: 67.714599609375, Loss: 0.0269412063062191, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5165/20000], Bound: 0.41357430815696716, Entropy: 135.17503356933594, Temp: 2.335047721862793, KL: 73.56977844238281, Loss: 0.029176240786910057, Learning Rate: 0.005042099999999998\n",
      "Epoch [5166/20000], Bound: 0.3741820156574249, Entropy: 137.57664489746094, Temp: 2.3350989818573, KL: 66.15409851074219, Loss: 0.02139945514500141, Learning Rate: 0.005042099999999998\n",
      "Epoch [5167/20000], Bound: 0.4105941355228424, Entropy: 137.61810302734375, Temp: 2.335188388824463, KL: 72.14630126953125, Loss: 0.03039020299911499, Learning Rate: 0.005042099999999998\n",
      "Epoch [5168/20000], Bound: 0.45654189586639404, Entropy: 135.89828491210938, Temp: 2.335160255432129, KL: 90.36213684082031, Loss: 0.020608633756637573, Learning Rate: 0.005042099999999998\n",
      "Epoch [5169/20000], Bound: 0.48127007484436035, Entropy: 137.66232299804688, Temp: 2.3353495597839355, KL: 93.28189086914062, Loss: 0.030954264104366302, Learning Rate: 0.005042099999999998\n",
      "Epoch [5170/20000], Bound: 0.3845823109149933, Entropy: 138.3173370361328, Temp: 2.335538387298584, KL: 68.3956298828125, Loss: 0.02272612228989601, Learning Rate: 0.005042099999999998\n",
      "Epoch [5171/20000], Bound: 0.4112565815448761, Entropy: 138.839599609375, Temp: 2.335737705230713, KL: 75.67735290527344, Loss: 0.02324649505317211, Learning Rate: 0.005042099999999998\n",
      "Epoch [5172/20000], Bound: 0.43069225549697876, Entropy: 135.7621307373047, Temp: 2.335984230041504, KL: 80.06591796875, Loss: 0.02598489634692669, Learning Rate: 0.005042099999999998\n",
      "Epoch [5173/20000], Bound: 0.38713523745536804, Entropy: 137.72378540039062, Temp: 2.336243152618408, KL: 68.0479736328125, Loss: 0.024995727464556694, Learning Rate: 0.005042099999999998\n",
      "Epoch [5174/20000], Bound: 0.42954960465431213, Entropy: 136.93048095703125, Temp: 2.336453676223755, KL: 81.05853271484375, Loss: 0.023146705701947212, Learning Rate: 0.005042099999999998\n",
      "Epoch [5175/20000], Bound: 0.42068132758140564, Entropy: 134.97418212890625, Temp: 2.3367466926574707, KL: 78.34921264648438, Loss: 0.02338358201086521, Learning Rate: 0.005042099999999998\n",
      "Epoch [5176/20000], Bound: 0.39153242111206055, Entropy: 137.20664978027344, Temp: 2.3370909690856934, KL: 68.18377685546875, Loss: 0.027338897809386253, Learning Rate: 0.005042099999999998\n",
      "Epoch [5177/20000], Bound: 0.40111103653907776, Entropy: 137.2702178955078, Temp: 2.3373281955718994, KL: 73.11775207519531, Loss: 0.022554585710167885, Learning Rate: 0.005042099999999998\n",
      "Epoch [5178/20000], Bound: 0.40075334906578064, Entropy: 139.04794311523438, Temp: 2.3376059532165527, KL: 74.94204711914062, Loss: 0.01844034716486931, Learning Rate: 0.005042099999999998\n",
      "Epoch [5179/20000], Bound: 0.4042395353317261, Entropy: 137.42398071289062, Temp: 2.3380205631256104, KL: 74.15895080566406, Loss: 0.02223949134349823, Learning Rate: 0.005042099999999998\n",
      "Epoch [5180/20000], Bound: 0.4047664701938629, Entropy: 137.9076690673828, Temp: 2.3384711742401123, KL: 72.06021118164062, Loss: 0.027055377140641212, Learning Rate: 0.005042099999999998\n",
      "Epoch [5181/20000], Bound: 0.39737430214881897, Entropy: 136.7270050048828, Temp: 2.3388359546661377, KL: 70.75056457519531, Loss: 0.025381548330187798, Learning Rate: 0.005042099999999998\n",
      "Epoch [5182/20000], Bound: 0.41011282801628113, Entropy: 137.7032012939453, Temp: 2.3391504287719727, KL: 74.26983642578125, Loss: 0.02561088092625141, Learning Rate: 0.005042099999999998\n",
      "Epoch [5183/20000], Bound: 0.40165767073631287, Entropy: 136.2067413330078, Temp: 2.33943772315979, KL: 73.78610229492188, Loss: 0.02149187959730625, Learning Rate: 0.005042099999999998\n",
      "Epoch [5184/20000], Bound: 0.3856043815612793, Entropy: 138.7935333251953, Temp: 2.3397865295410156, KL: 68.07583618164062, Loss: 0.02407696843147278, Learning Rate: 0.005042099999999998\n",
      "Epoch [5185/20000], Bound: 0.40773922204971313, Entropy: 141.09983825683594, Temp: 2.3400962352752686, KL: 74.1854248046875, Loss: 0.02435237355530262, Learning Rate: 0.005042099999999998\n",
      "Epoch [5186/20000], Bound: 0.39088907837867737, Entropy: 140.28834533691406, Temp: 2.3404054641723633, KL: 70.05067443847656, Loss: 0.023010244593024254, Learning Rate: 0.005042099999999998\n",
      "Epoch [5187/20000], Bound: 0.41547197103500366, Entropy: 140.02536010742188, Temp: 2.3407154083251953, KL: 76.58793640136719, Loss: 0.023984458297491074, Learning Rate: 0.005042099999999998\n",
      "Epoch [5188/20000], Bound: 0.40936991572380066, Entropy: 138.45938110351562, Temp: 2.341048240661621, KL: 71.71257019042969, Loss: 0.03064732253551483, Learning Rate: 0.005042099999999998\n",
      "Epoch [5189/20000], Bound: 0.4209517538547516, Entropy: 141.29913330078125, Temp: 2.3412251472473145, KL: 76.96371459960938, Loss: 0.026591438800096512, Learning Rate: 0.005042099999999998\n",
      "Epoch [5190/20000], Bound: 0.41745612025260925, Entropy: 142.20120239257812, Temp: 2.3413844108581543, KL: 78.92875671386719, Loss: 0.02022526226937771, Learning Rate: 0.005042099999999998\n",
      "Epoch [5191/20000], Bound: 0.4196401834487915, Entropy: 140.934326171875, Temp: 2.34167742729187, KL: 78.22297668457031, Loss: 0.02309376932680607, Learning Rate: 0.005042099999999998\n",
      "Epoch [5192/20000], Bound: 0.4488677680492401, Entropy: 139.66275024414062, Temp: 2.342024326324463, KL: 84.64199829101562, Loss: 0.02796691656112671, Learning Rate: 0.005042099999999998\n",
      "Epoch [5193/20000], Bound: 0.44141459465026855, Entropy: 139.146484375, Temp: 2.3423571586608887, KL: 79.85235595703125, Loss: 0.03338468447327614, Learning Rate: 0.005042099999999998\n",
      "Epoch [5194/20000], Bound: 0.4228823184967041, Entropy: 140.64439392089844, Temp: 2.3425304889678955, KL: 75.76850891113281, Loss: 0.03036830760538578, Learning Rate: 0.005042099999999998\n",
      "Epoch [5195/20000], Bound: 0.44060656428337097, Entropy: 138.57061767578125, Temp: 2.3425960540771484, KL: 81.21128845214844, Loss: 0.02996952459216118, Learning Rate: 0.005042099999999998\n",
      "Epoch [5196/20000], Bound: 0.44200441241264343, Entropy: 133.87799072265625, Temp: 2.3426101207733154, KL: 84.03846740722656, Loss: 0.024832846596837044, Learning Rate: 0.005042099999999998\n",
      "Epoch [5197/20000], Bound: 0.38417428731918335, Entropy: 133.33645629882812, Temp: 2.342705488204956, KL: 68.39707946777344, Loss: 0.022586101666092873, Learning Rate: 0.005042099999999998\n",
      "Epoch [5198/20000], Bound: 0.36395734548568726, Entropy: 133.6845245361328, Temp: 2.3428194522857666, KL: 60.259735107421875, Loss: 0.02816803567111492, Learning Rate: 0.005042099999999998\n",
      "Epoch [5199/20000], Bound: 0.44498392939567566, Entropy: 130.73670959472656, Temp: 2.3427700996398926, KL: 84.14334106445312, Loss: 0.026531053707003593, Learning Rate: 0.005042099999999998\n",
      "Epoch [5200/20000], Bound: 0.39550915360450745, Entropy: 132.56227111816406, Temp: 2.3427724838256836, KL: 69.51417541503906, Loss: 0.02695632167160511, Learning Rate: 0.005042099999999998\n",
      "Epoch [5201/20000], Bound: 0.41690799593925476, Entropy: 133.53367614746094, Temp: 2.342714786529541, KL: 75.74307250976562, Loss: 0.026709984987974167, Learning Rate: 0.005042099999999998\n",
      "Epoch [5202/20000], Bound: 0.3866089880466461, Entropy: 134.5233154296875, Temp: 2.342650890350342, KL: 67.3985595703125, Loss: 0.026158113032579422, Learning Rate: 0.005042099999999998\n",
      "Epoch [5203/20000], Bound: 0.4210479259490967, Entropy: 134.48240661621094, Temp: 2.342536211013794, KL: 77.65461730957031, Loss: 0.0251983180642128, Learning Rate: 0.005042099999999998\n",
      "Epoch [5204/20000], Bound: 0.4039859175682068, Entropy: 135.15731811523438, Temp: 2.3424665927886963, KL: 71.73121643066406, Loss: 0.027341555804014206, Learning Rate: 0.005042099999999998\n",
      "Epoch [5205/20000], Bound: 0.41910016536712646, Entropy: 136.63221740722656, Temp: 2.342351198196411, KL: 76.98698425292969, Loss: 0.02540856972336769, Learning Rate: 0.005042099999999998\n",
      "Epoch [5206/20000], Bound: 0.39505743980407715, Entropy: 140.5472869873047, Temp: 2.3422720432281494, KL: 68.09518432617188, Loss: 0.029707325622439384, Learning Rate: 0.005042099999999998\n",
      "Epoch [5207/20000], Bound: 0.42570754885673523, Entropy: 138.2366943359375, Temp: 2.342071056365967, KL: 75.87088012695312, Loss: 0.03191147744655609, Learning Rate: 0.005042099999999998\n",
      "Epoch [5208/20000], Bound: 0.39992231130599976, Entropy: 139.18397521972656, Temp: 2.3417670726776123, KL: 71.23965454101562, Loss: 0.025917652994394302, Learning Rate: 0.005042099999999998\n",
      "Epoch [5209/20000], Bound: 0.4437834918498993, Entropy: 139.49606323242188, Temp: 2.3414688110351562, KL: 82.66796875, Loss: 0.028882142156362534, Learning Rate: 0.005042099999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5210/20000], Bound: 0.4227505922317505, Entropy: 138.11087036132812, Temp: 2.341188669204712, KL: 77.4052734375, Loss: 0.026770101860165596, Learning Rate: 0.005042099999999998\n",
      "Epoch [5211/20000], Bound: 0.4657025933265686, Entropy: 141.21096801757812, Temp: 2.340935230255127, KL: 90.72718811035156, Loss: 0.026026839390397072, Learning Rate: 0.005042099999999998\n",
      "Epoch [5212/20000], Bound: 0.411285400390625, Entropy: 139.83126831054688, Temp: 2.340808629989624, KL: 74.16716003417969, Loss: 0.02657642774283886, Learning Rate: 0.005042099999999998\n",
      "Epoch [5213/20000], Bound: 0.402558296918869, Entropy: 139.68482971191406, Temp: 2.3406760692596436, KL: 69.05659484863281, Loss: 0.03216170892119408, Learning Rate: 0.005042099999999998\n",
      "Epoch [5214/20000], Bound: 0.3814389407634735, Entropy: 140.6647491455078, Temp: 2.340381145477295, KL: 67.1563720703125, Loss: 0.023588070645928383, Learning Rate: 0.005042099999999998\n",
      "Epoch [5215/20000], Bound: 0.39997759461402893, Entropy: 138.78787231445312, Temp: 2.3401148319244385, KL: 71.32298278808594, Loss: 0.025748757645487785, Learning Rate: 0.005042099999999998\n",
      "Epoch [5216/20000], Bound: 0.38138172030448914, Entropy: 138.80564880371094, Temp: 2.339855670928955, KL: 67.1622314453125, Loss: 0.023534506559371948, Learning Rate: 0.005042099999999998\n",
      "Epoch [5217/20000], Bound: 0.42817461490631104, Entropy: 141.49252319335938, Temp: 2.339623212814331, KL: 79.09895324707031, Loss: 0.026527829468250275, Learning Rate: 0.005042099999999998\n",
      "Epoch [5218/20000], Bound: 0.45913076400756836, Entropy: 137.40103149414062, Temp: 2.339430570602417, KL: 86.13560485839844, Loss: 0.03144686296582222, Learning Rate: 0.005042099999999998\n",
      "Epoch [5219/20000], Bound: 0.4318554103374481, Entropy: 137.15548706054688, Temp: 2.3392152786254883, KL: 80.61244201660156, Loss: 0.025611072778701782, Learning Rate: 0.005042099999999998\n",
      "Epoch [5220/20000], Bound: 0.4242686629295349, Entropy: 137.91004943847656, Temp: 2.3390684127807617, KL: 76.60169982910156, Loss: 0.029402321204543114, Learning Rate: 0.005042099999999998\n",
      "Epoch [5221/20000], Bound: 0.41129904985427856, Entropy: 137.18101501464844, Temp: 2.338874101638794, KL: 74.87942504882812, Loss: 0.025032013654708862, Learning Rate: 0.005042099999999998\n",
      "Epoch [5222/20000], Bound: 0.4447513818740845, Entropy: 137.32720947265625, Temp: 2.3387207984924316, KL: 83.77281188964844, Loss: 0.02709485962986946, Learning Rate: 0.005042099999999998\n",
      "Epoch [5223/20000], Bound: 0.43599382042884827, Entropy: 136.24688720703125, Temp: 2.338618755340576, KL: 77.67192077636719, Loss: 0.03451715037226677, Learning Rate: 0.005042099999999998\n",
      "Epoch [5224/20000], Bound: 0.41245177388191223, Entropy: 133.2950897216797, Temp: 2.338362455368042, KL: 73.56201171875, Loss: 0.028550025075674057, Learning Rate: 0.005042099999999998\n",
      "Epoch [5225/20000], Bound: 0.3929804563522339, Entropy: 137.40536499023438, Temp: 2.3380675315856934, KL: 66.64173889160156, Loss: 0.03151629492640495, Learning Rate: 0.005042099999999998\n",
      "Epoch [5226/20000], Bound: 0.4278251528739929, Entropy: 134.8300323486328, Temp: 2.3376240730285645, KL: 79.47265625, Loss: 0.02547336556017399, Learning Rate: 0.005042099999999998\n",
      "Epoch [5227/20000], Bound: 0.43693792819976807, Entropy: 135.8212127685547, Temp: 2.337268352508545, KL: 79.08149719238281, Loss: 0.03208396956324577, Learning Rate: 0.005042099999999998\n",
      "Epoch [5228/20000], Bound: 0.43495893478393555, Entropy: 134.92323303222656, Temp: 2.3368468284606934, KL: 81.53019714355469, Loss: 0.025575852021574974, Learning Rate: 0.005042099999999998\n",
      "Epoch [5229/20000], Bound: 0.42665228247642517, Entropy: 134.4347686767578, Temp: 2.3365228176116943, KL: 77.06495666503906, Loss: 0.029867516830563545, Learning Rate: 0.005042099999999998\n",
      "Epoch [5230/20000], Bound: 0.4193597435951233, Entropy: 131.71194458007812, Temp: 2.3361639976501465, KL: 76.79617309570312, Loss: 0.025873562321066856, Learning Rate: 0.005042099999999998\n",
      "Epoch [5231/20000], Bound: 0.42647284269332886, Entropy: 135.56602478027344, Temp: 2.3358590602874756, KL: 77.26785278320312, Loss: 0.029309703037142754, Learning Rate: 0.005042099999999998\n",
      "Epoch [5232/20000], Bound: 0.41246116161346436, Entropy: 134.25563049316406, Temp: 2.335531234741211, KL: 73.73602294921875, Loss: 0.028140876442193985, Learning Rate: 0.005042099999999998\n",
      "Epoch [5233/20000], Bound: 0.3924839496612549, Entropy: 136.65907287597656, Temp: 2.335184097290039, KL: 70.06976318359375, Loss: 0.023844975978136063, Learning Rate: 0.005042099999999998\n",
      "Epoch [5234/20000], Bound: 0.460043728351593, Entropy: 136.42828369140625, Temp: 2.3348889350891113, KL: 88.37138366699219, Loss: 0.02717655524611473, Learning Rate: 0.005042099999999998\n",
      "Epoch [5235/20000], Bound: 0.4672599732875824, Entropy: 133.51226806640625, Temp: 2.334690809249878, KL: 90.13087463378906, Loss: 0.02820703014731407, Learning Rate: 0.005042099999999998\n",
      "Epoch [5236/20000], Bound: 0.42468756437301636, Entropy: 136.0562286376953, Temp: 2.3345694541931152, KL: 78.02349853515625, Loss: 0.02654857747256756, Learning Rate: 0.005042099999999998\n",
      "Epoch [5237/20000], Bound: 0.422507643699646, Entropy: 135.5087890625, Temp: 2.334473133087158, KL: 78.16371154785156, Loss: 0.02488071843981743, Learning Rate: 0.005042099999999998\n",
      "Epoch [5238/20000], Bound: 0.43657004833221436, Entropy: 136.12863159179688, Temp: 2.3344368934631348, KL: 82.738037109375, Loss: 0.023971471935510635, Learning Rate: 0.005042099999999998\n",
      "Epoch [5239/20000], Bound: 0.4366033971309662, Entropy: 136.91510009765625, Temp: 2.334505319595337, KL: 78.82817077636719, Loss: 0.03236817941069603, Learning Rate: 0.005042099999999998\n",
      "Epoch [5240/20000], Bound: 0.389201819896698, Entropy: 133.6785430908203, Temp: 2.334458827972412, KL: 69.15574645996094, Loss: 0.023829201236367226, Learning Rate: 0.005042099999999998\n",
      "Epoch [5241/20000], Bound: 0.4341680407524109, Entropy: 136.131103515625, Temp: 2.3344287872314453, KL: 82.03651428222656, Loss: 0.02394241839647293, Learning Rate: 0.005042099999999998\n",
      "Epoch [5242/20000], Bound: 0.4187723398208618, Entropy: 136.94070434570312, Temp: 2.334498882293701, KL: 76.96647644042969, Loss: 0.02511502616107464, Learning Rate: 0.005042099999999998\n",
      "Epoch [5243/20000], Bound: 0.44049835205078125, Entropy: 136.51522827148438, Temp: 2.334599018096924, KL: 79.38224792480469, Loss: 0.03367801010608673, Learning Rate: 0.005042099999999998\n",
      "Epoch [5244/20000], Bound: 0.41145145893096924, Entropy: 134.1217041015625, Temp: 2.334556818008423, KL: 74.32144165039062, Loss: 0.026250185444951057, Learning Rate: 0.005042099999999998\n",
      "Epoch [5245/20000], Bound: 0.4184163510799408, Entropy: 135.0963134765625, Temp: 2.334512948989868, KL: 76.70524597167969, Loss: 0.025453323498368263, Learning Rate: 0.005042099999999998\n",
      "Epoch [5246/20000], Bound: 0.40830478072166443, Entropy: 134.2221221923828, Temp: 2.3345017433166504, KL: 70.61143493652344, Loss: 0.032262492924928665, Learning Rate: 0.005042099999999998\n",
      "Epoch [5247/20000], Bound: 0.42742031812667847, Entropy: 137.3957977294922, Temp: 2.33432674407959, KL: 80.384765625, Loss: 0.023205239325761795, Learning Rate: 0.005042099999999998\n",
      "Epoch [5248/20000], Bound: 0.4287894666194916, Entropy: 136.44398498535156, Temp: 2.3342721462249756, KL: 80.91534423828125, Loss: 0.02293132245540619, Learning Rate: 0.005042099999999998\n",
      "Epoch [5249/20000], Bound: 0.431495726108551, Entropy: 134.9897918701172, Temp: 2.3343348503112793, KL: 80.48171997070312, Loss: 0.02557346597313881, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5250/20000], Bound: 0.40071243047714233, Entropy: 137.42950439453125, Temp: 2.3344428539276123, KL: 71.23222351074219, Loss: 0.02630404382944107, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5251/20000], Bound: 0.41015470027923584, Entropy: 136.9337615966797, Temp: 2.3345112800598145, KL: 75.49562072753906, Loss: 0.02293703891336918, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5252/20000], Bound: 0.42642512917518616, Entropy: 136.47349548339844, Temp: 2.334648609161377, KL: 79.34677124023438, Loss: 0.024807777255773544, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5253/20000], Bound: 0.37119030952453613, Entropy: 135.00738525390625, Temp: 2.3348329067230225, KL: 66.90635681152344, Loss: 0.01804026961326599, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5254/20000], Bound: 0.40278497338294983, Entropy: 136.47792053222656, Temp: 2.33512544631958, KL: 73.10914611816406, Loss: 0.023553380742669106, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5255/20000], Bound: 0.3531039357185364, Entropy: 139.86647033691406, Temp: 2.3354341983795166, KL: 59.59722900390625, Loss: 0.02330215461552143, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5256/20000], Bound: 0.4161681532859802, Entropy: 138.70811462402344, Temp: 2.3356666564941406, KL: 76.89585876464844, Loss: 0.023669518530368805, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5257/20000], Bound: 0.4042639136314392, Entropy: 139.00164794921875, Temp: 2.335944175720215, KL: 74.51310729980469, Loss: 0.02146141789853573, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5258/20000], Bound: 0.4195154011249542, Entropy: 139.2954864501953, Temp: 2.3362951278686523, KL: 76.10118103027344, Loss: 0.02746005542576313, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5259/20000], Bound: 0.4286595582962036, Entropy: 139.39468383789062, Temp: 2.336589813232422, KL: 80.69281005859375, Loss: 0.02336999773979187, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5260/20000], Bound: 0.410830020904541, Entropy: 138.00279235839844, Temp: 2.3369548320770264, KL: 74.61790466308594, Loss: 0.025271782651543617, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5261/20000], Bound: 0.4152136445045471, Entropy: 138.61163330078125, Temp: 2.3372998237609863, KL: 76.66818237304688, Loss: 0.023594295606017113, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5262/20000], Bound: 0.4145813286304474, Entropy: 140.4810791015625, Temp: 2.337677478790283, KL: 76.10934448242188, Loss: 0.024405188858509064, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5263/20000], Bound: 0.43965378403663635, Entropy: 139.8711395263672, Temp: 2.3380627632141113, KL: 84.1339111328125, Loss: 0.023029590025544167, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5264/20000], Bound: 0.4127900302410126, Entropy: 140.58595275878906, Temp: 2.338538885116577, KL: 73.37162780761719, Loss: 0.029168089851737022, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5265/20000], Bound: 0.4102814793586731, Entropy: 139.12156677246094, Temp: 2.3388872146606445, KL: 74.83845520019531, Loss: 0.02449464239180088, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5266/20000], Bound: 0.38484257459640503, Entropy: 140.27569580078125, Temp: 2.3392345905303955, KL: 66.99797058105469, Loss: 0.02592168003320694, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5267/20000], Bound: 0.39193347096443176, Entropy: 139.5664825439453, Temp: 2.3394932746887207, KL: 70.35406494140625, Loss: 0.02297228015959263, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5268/20000], Bound: 0.4089457094669342, Entropy: 139.974609375, Temp: 2.3397624492645264, KL: 74.48307800292969, Loss: 0.02444932609796524, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5269/20000], Bound: 0.4328233003616333, Entropy: 139.66873168945312, Temp: 2.3400368690490723, KL: 82.17829895019531, Loss: 0.022893691435456276, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5270/20000], Bound: 0.4270683526992798, Entropy: 139.98231506347656, Temp: 2.3404018878936768, KL: 80.81097412109375, Loss: 0.022187644615769386, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5271/20000], Bound: 0.4031812250614166, Entropy: 137.94932556152344, Temp: 2.3408546447753906, KL: 72.5068359375, Loss: 0.025172490626573563, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5272/20000], Bound: 0.4170737862586975, Entropy: 141.20172119140625, Temp: 2.34126353263855, KL: 76.75917053222656, Loss: 0.024619419127702713, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5273/20000], Bound: 0.3802741467952728, Entropy: 141.80331420898438, Temp: 2.3416740894317627, KL: 66.95735168457031, Loss: 0.023345336318016052, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5274/20000], Bound: 0.366641104221344, Entropy: 140.6924591064453, Temp: 2.3420462608337402, KL: 63.60008239746094, Loss: 0.02257520891726017, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5275/20000], Bound: 0.43168535828590393, Entropy: 138.09580993652344, Temp: 2.342376947402954, KL: 83.21063232421875, Loss: 0.020014867186546326, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5276/20000], Bound: 0.39044785499572754, Entropy: 140.0325164794922, Temp: 2.342862367630005, KL: 69.25257873535156, Loss: 0.024487141519784927, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5277/20000], Bound: 0.38813722133636475, Entropy: 140.3606719970703, Temp: 2.3432912826538086, KL: 65.47869873046875, Loss: 0.03117036260664463, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5278/20000], Bound: 0.40565571188926697, Entropy: 141.975830078125, Temp: 2.3434906005859375, KL: 75.77435302734375, Loss: 0.019745921716094017, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5279/20000], Bound: 0.43636032938957214, Entropy: 144.50885009765625, Temp: 2.3438141345977783, KL: 82.16136169433594, Loss: 0.025250252336263657, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5280/20000], Bound: 0.4119620621204376, Entropy: 142.38331604003906, Temp: 2.344168186187744, KL: 75.01986694335938, Loss: 0.025225380435585976, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5281/20000], Bound: 0.4033178687095642, Entropy: 141.16221618652344, Temp: 2.3445022106170654, KL: 73.44851684570312, Loss: 0.023302802816033363, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5282/20000], Bound: 0.42149195075035095, Entropy: 140.60409545898438, Temp: 2.3448498249053955, KL: 79.38887023925781, Loss: 0.021816400811076164, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5283/20000], Bound: 0.4409596920013428, Entropy: 142.32476806640625, Temp: 2.345283269882202, KL: 84.1229248046875, Loss: 0.024034619331359863, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5284/20000], Bound: 0.4191897511482239, Entropy: 140.22279357910156, Temp: 2.345775604248047, KL: 77.58903503417969, Loss: 0.024238338693976402, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5285/20000], Bound: 0.4133903682231903, Entropy: 139.1061248779297, Temp: 2.3462724685668945, KL: 74.69424438476562, Loss: 0.02683132141828537, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5286/20000], Bound: 0.40828150510787964, Entropy: 139.62939453125, Temp: 2.3466949462890625, KL: 74.02989196777344, Loss: 0.025120798498392105, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5287/20000], Bound: 0.4407667815685272, Entropy: 137.28857421875, Temp: 2.347083806991577, KL: 83.09001159667969, Loss: 0.026146812364459038, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5288/20000], Bound: 0.409077525138855, Entropy: 138.783935546875, Temp: 2.3474807739257812, KL: 75.07633972167969, Loss: 0.023390697315335274, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5289/20000], Bound: 0.4162423014640808, Entropy: 137.19107055664062, Temp: 2.3478925228118896, KL: 78.08489990234375, Loss: 0.02139393799006939, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5290/20000], Bound: 0.4285632371902466, Entropy: 138.05838012695312, Temp: 2.3483829498291016, KL: 79.75520324707031, Loss: 0.025526050478219986, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5291/20000], Bound: 0.41703569889068604, Entropy: 136.32373046875, Temp: 2.3488619327545166, KL: 76.00384521484375, Loss: 0.02633161097764969, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5292/20000], Bound: 0.4312191307544708, Entropy: 137.35118103027344, Temp: 2.3492865562438965, KL: 80.80157470703125, Loss: 0.0249882023781538, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5293/20000], Bound: 0.4063933789730072, Entropy: 138.60655212402344, Temp: 2.349724769592285, KL: 72.69371032714844, Loss: 0.026860341429710388, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5294/20000], Bound: 0.40158697962760925, Entropy: 136.4364776611328, Temp: 2.350076675415039, KL: 70.23158264160156, Loss: 0.029188113287091255, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5295/20000], Bound: 0.39364588260650635, Entropy: 135.0909881591797, Temp: 2.350280523300171, KL: 68.65713500976562, Loss: 0.027766425162553787, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5296/20000], Bound: 0.42219042778015137, Entropy: 136.1527862548828, Temp: 2.3503715991973877, KL: 78.71224975585938, Loss: 0.02379227802157402, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5297/20000], Bound: 0.3822017312049866, Entropy: 137.09315490722656, Temp: 2.3505215644836426, KL: 67.85661315917969, Loss: 0.022685937583446503, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5298/20000], Bound: 0.408253014087677, Entropy: 135.1937255859375, Temp: 2.350674867630005, KL: 74.88407897949219, Loss: 0.023348607122898102, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5299/20000], Bound: 0.37988170981407166, Entropy: 137.54818725585938, Temp: 2.3508646488189697, KL: 67.68601989746094, Loss: 0.02169102430343628, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5300/20000], Bound: 0.4078167974948883, Entropy: 137.66018676757812, Temp: 2.3510751724243164, KL: 75.56257629394531, Loss: 0.021646084263920784, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5301/20000], Bound: 0.3862403631210327, Entropy: 136.24851989746094, Temp: 2.3513598442077637, KL: 70.11569213867188, Loss: 0.02027599699795246, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5302/20000], Bound: 0.39420977234840393, Entropy: 139.03097534179688, Temp: 2.3517050743103027, KL: 70.28990173339844, Loss: 0.02465052157640457, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5303/20000], Bound: 0.4251144230365753, Entropy: 137.3019561767578, Temp: 2.3520054817199707, KL: 81.392822265625, Loss: 0.019946543499827385, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5304/20000], Bound: 0.4151969254016876, Entropy: 139.09779357910156, Temp: 2.352447509765625, KL: 76.24711608886719, Loss: 0.024737300351262093, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5305/20000], Bound: 0.40074974298477173, Entropy: 139.90565490722656, Temp: 2.352874279022217, KL: 73.04118347167969, Loss: 0.022748736664652824, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5306/20000], Bound: 0.3990325927734375, Entropy: 140.72393798828125, Temp: 2.3533101081848145, KL: 73.59762573242188, Loss: 0.020538564771413803, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5307/20000], Bound: 0.40920591354370117, Entropy: 140.0714874267578, Temp: 2.3538079261779785, KL: 76.69584655761719, Loss: 0.020132388919591904, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5308/20000], Bound: 0.38667038083076477, Entropy: 139.25875854492188, Temp: 2.3543920516967773, KL: 70.24005126953125, Loss: 0.020312685519456863, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5309/20000], Bound: 0.41622886061668396, Entropy: 141.75588989257812, Temp: 2.3550050258636475, KL: 76.56402587890625, Loss: 0.024742094799876213, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5310/20000], Bound: 0.4131963551044464, Entropy: 140.19859313964844, Temp: 2.355585813522339, KL: 75.78323364257812, Loss: 0.024543380364775658, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5311/20000], Bound: 0.43637946248054504, Entropy: 141.02679443359375, Temp: 2.3561360836029053, KL: 82.17012023925781, Loss: 0.025471337139606476, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5312/20000], Bound: 0.45264220237731934, Entropy: 139.81890869140625, Temp: 2.3566811084747314, KL: 87.90870666503906, Loss: 0.023760080337524414, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5313/20000], Bound: 0.40449151396751404, Entropy: 136.62783813476562, Temp: 2.357296943664551, KL: 74.74908447265625, Loss: 0.021456751972436905, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5314/20000], Bound: 0.41688889265060425, Entropy: 139.57725524902344, Temp: 2.3579413890838623, KL: 78.05258178710938, Loss: 0.02204051986336708, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5315/20000], Bound: 0.3664475083351135, Entropy: 141.08236694335938, Temp: 2.3586201667785645, KL: 64.96537780761719, Loss: 0.019778618589043617, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5316/20000], Bound: 0.43086203932762146, Entropy: 139.92922973632812, Temp: 2.3592913150787354, KL: 80.487548828125, Loss: 0.025607747957110405, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5317/20000], Bound: 0.41900715231895447, Entropy: 136.69017028808594, Temp: 2.3599283695220947, KL: 78.50025939941406, Loss: 0.022434618324041367, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5318/20000], Bound: 0.38986045122146606, Entropy: 139.12506103515625, Temp: 2.36059308052063, KL: 69.3675537109375, Loss: 0.024142956361174583, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5319/20000], Bound: 0.4232657253742218, Entropy: 136.57937622070312, Temp: 2.3611795902252197, KL: 80.70091247558594, Loss: 0.020437069237232208, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5320/20000], Bound: 0.4046397805213928, Entropy: 137.37196350097656, Temp: 2.361858367919922, KL: 74.58619689941406, Loss: 0.02196631208062172, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5321/20000], Bound: 0.4119071364402771, Entropy: 137.16688537597656, Temp: 2.3625435829162598, KL: 76.31240844726562, Loss: 0.02274569496512413, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5322/20000], Bound: 0.414344847202301, Entropy: 137.64532470703125, Temp: 2.3632278442382812, KL: 77.64471435546875, Loss: 0.021431488916277885, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5323/20000], Bound: 0.4016590714454651, Entropy: 136.4054412841797, Temp: 2.363950252532959, KL: 73.61976623535156, Loss: 0.022244585677981377, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5324/20000], Bound: 0.3700238764286041, Entropy: 136.48077392578125, Temp: 2.3646602630615234, KL: 64.41461181640625, Loss: 0.023081084713339806, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5325/20000], Bound: 0.38316211104393005, Entropy: 137.68185424804688, Temp: 2.365273952484131, KL: 64.43257141113281, Loss: 0.030690670013427734, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5326/20000], Bound: 0.389507532119751, Entropy: 139.1180419921875, Temp: 2.3656234741210938, KL: 69.33308410644531, Loss: 0.024076156318187714, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5327/20000], Bound: 0.4019927978515625, Entropy: 137.60040283203125, Temp: 2.3659238815307617, KL: 72.41703796386719, Loss: 0.025018267333507538, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5328/20000], Bound: 0.43510234355926514, Entropy: 140.23355102539062, Temp: 2.3661797046661377, KL: 84.42422485351562, Loss: 0.020080694928765297, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5329/20000], Bound: 0.3736957311630249, Entropy: 137.88412475585938, Temp: 2.3665900230407715, KL: 65.34748840332031, Loss: 0.02325456030666828, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5330/20000], Bound: 0.3938645124435425, Entropy: 141.58518981933594, Temp: 2.3669352531433105, KL: 70.5472412109375, Loss: 0.024115396663546562, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5331/20000], Bound: 0.4187963306903839, Entropy: 140.13870239257812, Temp: 2.367238998413086, KL: 77.05067443847656, Loss: 0.025493232533335686, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5332/20000], Bound: 0.3827974796295166, Entropy: 141.2028045654297, Temp: 2.3675179481506348, KL: 68.76719665527344, Loss: 0.021345598623156548, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5333/20000], Bound: 0.4324825406074524, Entropy: 140.41957092285156, Temp: 2.3678138256073, KL: 82.19120788574219, Loss: 0.023176249116659164, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5334/20000], Bound: 0.41719457507133484, Entropy: 140.23184204101562, Temp: 2.368173599243164, KL: 76.58230590820312, Loss: 0.02551007643342018, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5335/20000], Bound: 0.37832772731781006, Entropy: 144.64376831054688, Temp: 2.3684990406036377, KL: 67.21611022949219, Loss: 0.022022733464837074, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5336/20000], Bound: 0.36775824427604675, Entropy: 143.06802368164062, Temp: 2.368809223175049, KL: 63.5355224609375, Loss: 0.02368522435426712, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5337/20000], Bound: 0.3913944959640503, Entropy: 143.53421020507812, Temp: 2.369039297103882, KL: 68.726806640625, Loss: 0.026519829407334328, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5338/20000], Bound: 0.42192894220352173, Entropy: 141.333984375, Temp: 2.369168519973755, KL: 79.04457092285156, Loss: 0.023252232000231743, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5339/20000], Bound: 0.3854602873325348, Entropy: 140.92190551757812, Temp: 2.369354724884033, KL: 67.2520751953125, Loss: 0.02613186277449131, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5340/20000], Bound: 0.42218485474586487, Entropy: 140.61097717285156, Temp: 2.3694422245025635, KL: 78.79061889648438, Loss: 0.023951411247253418, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5341/20000], Bound: 0.39492014050483704, Entropy: 141.46893310546875, Temp: 2.369572639465332, KL: 71.55989074707031, Loss: 0.022644180804491043, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5342/20000], Bound: 0.4262343645095825, Entropy: 140.94241333007812, Temp: 2.369722366333008, KL: 79.69186401367188, Loss: 0.02457151561975479, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5343/20000], Bound: 0.4163905680179596, Entropy: 140.80776977539062, Temp: 2.369900703430176, KL: 78.33418273925781, Loss: 0.02134682610630989, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5344/20000], Bound: 0.39298951625823975, Entropy: 140.70132446289062, Temp: 2.3701701164245605, KL: 70.30148315429688, Loss: 0.024158978834748268, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5345/20000], Bound: 0.4211951792240143, Entropy: 140.547119140625, Temp: 2.3704001903533936, KL: 77.52737426757812, Loss: 0.026019487529993057, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5346/20000], Bound: 0.43924450874328613, Entropy: 137.0944061279297, Temp: 2.3706016540527344, KL: 85.63201904296875, Loss: 0.02024536021053791, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5347/20000], Bound: 0.3937702476978302, Entropy: 139.78250122070312, Temp: 2.370965003967285, KL: 71.77081298828125, Loss: 0.021535467356443405, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5348/20000], Bound: 0.3985752463340759, Entropy: 139.6324920654297, Temp: 2.3713510036468506, KL: 73.59988403320312, Loss: 0.020551536232233047, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5349/20000], Bound: 0.4096274673938751, Entropy: 140.0899200439453, Temp: 2.371793508529663, KL: 77.77105712890625, Loss: 0.018433626741170883, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5350/20000], Bound: 0.43330633640289307, Entropy: 139.450927734375, Temp: 2.372363567352295, KL: 83.572509765625, Loss: 0.02086641825735569, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5351/20000], Bound: 0.42174187302589417, Entropy: 137.83477783203125, Temp: 2.3730297088623047, KL: 78.13365173339844, Loss: 0.02512180060148239, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5352/20000], Bound: 0.418501079082489, Entropy: 138.29859924316406, Temp: 2.373647451400757, KL: 78.25358581542969, Loss: 0.022878961637616158, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5353/20000], Bound: 0.3987477123737335, Entropy: 138.34255981445312, Temp: 2.3742737770080566, KL: 72.82136535644531, Loss: 0.022340502589941025, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5354/20000], Bound: 0.4259995222091675, Entropy: 138.34022521972656, Temp: 2.374882459640503, KL: 79.63937377929688, Loss: 0.02462320774793625, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5355/20000], Bound: 0.39647603034973145, Entropy: 137.6537322998047, Temp: 2.3754689693450928, KL: 72.62646484375, Loss: 0.021412067115306854, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5356/20000], Bound: 0.38114240765571594, Entropy: 139.43714904785156, Temp: 2.3760616779327393, KL: 66.01043701171875, Loss: 0.026299895718693733, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5357/20000], Bound: 0.39977094531059265, Entropy: 137.19927978515625, Temp: 2.376497507095337, KL: 72.02839660644531, Loss: 0.024654697626829147, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5358/20000], Bound: 0.3957291543483734, Entropy: 136.58984375, Temp: 2.376873016357422, KL: 71.494140625, Loss: 0.02337043732404709, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5359/20000], Bound: 0.38552165031433105, Entropy: 140.13975524902344, Temp: 2.37722110748291, KL: 69.67410278320312, Loss: 0.021169893443584442, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5360/20000], Bound: 0.4127267897129059, Entropy: 138.99720764160156, Temp: 2.3775835037231445, KL: 76.43986511230469, Loss: 0.023222118616104126, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5361/20000], Bound: 0.398807555437088, Entropy: 138.85888671875, Temp: 2.3779566287994385, KL: 72.62493896484375, Loss: 0.022844599559903145, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5362/20000], Bound: 0.3857954740524292, Entropy: 140.79222106933594, Temp: 2.378322124481201, KL: 68.06465148925781, Loss: 0.02472982555627823, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5363/20000], Bound: 0.429190993309021, Entropy: 137.7636260986328, Temp: 2.378603219985962, KL: 79.10328674316406, Loss: 0.027801547199487686, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5364/20000], Bound: 0.4184221625328064, Entropy: 140.12384033203125, Temp: 2.378814220428467, KL: 77.69364929199219, Loss: 0.02409379370510578, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5365/20000], Bound: 0.4152640700340271, Entropy: 135.931396484375, Temp: 2.3790383338928223, KL: 78.50653076171875, Loss: 0.02045178785920143, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5366/20000], Bound: 0.38721540570259094, Entropy: 137.73513793945312, Temp: 2.3793654441833496, KL: 70.58644104003906, Loss: 0.020277835428714752, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5367/20000], Bound: 0.4005092978477478, Entropy: 140.791748046875, Temp: 2.3797354698181152, KL: 72.62956237792969, Loss: 0.023879215121269226, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5368/20000], Bound: 0.4033571481704712, Entropy: 137.64712524414062, Temp: 2.3800721168518066, KL: 75.20278930664062, Loss: 0.020187539979815483, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5369/20000], Bound: 0.38147658109664917, Entropy: 139.2785186767578, Temp: 2.380483865737915, KL: 68.10562133789062, Loss: 0.022145409137010574, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5370/20000], Bound: 0.3943263292312622, Entropy: 139.23074340820312, Temp: 2.3808672428131104, KL: 69.53013610839844, Loss: 0.026718195527791977, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5371/20000], Bound: 0.4390890300273895, Entropy: 138.82965087890625, Temp: 2.3811259269714355, KL: 84.0946044921875, Loss: 0.023585578426718712, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5372/20000], Bound: 0.38417667150497437, Entropy: 142.341796875, Temp: 2.381446361541748, KL: 69.30203247070312, Loss: 0.02122323215007782, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5373/20000], Bound: 0.3518355190753937, Entropy: 142.6073455810547, Temp: 2.381777286529541, KL: 59.87455749511719, Loss: 0.02251080982387066, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5374/20000], Bound: 0.4263002276420593, Entropy: 140.80136108398438, Temp: 2.382018566131592, KL: 79.90888977050781, Loss: 0.024363411590456963, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5375/20000], Bound: 0.4161778688430786, Entropy: 140.4122772216797, Temp: 2.3822765350341797, KL: 76.356689453125, Loss: 0.025579063221812248, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5376/20000], Bound: 0.3966447114944458, Entropy: 143.86900329589844, Temp: 2.3824963569641113, KL: 71.0072021484375, Loss: 0.02501680515706539, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5377/20000], Bound: 0.4137875735759735, Entropy: 139.19480895996094, Temp: 2.382657289505005, KL: 74.0577392578125, Loss: 0.028948301449418068, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5378/20000], Bound: 0.40628767013549805, Entropy: 140.62644958496094, Temp: 2.3826942443847656, KL: 74.08151245117188, Loss: 0.02434791438281536, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5379/20000], Bound: 0.3532821834087372, Entropy: 142.20628356933594, Temp: 2.382727861404419, KL: 61.587615966796875, Loss: 0.01973663829267025, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5380/20000], Bound: 0.40735411643981934, Entropy: 142.44969177246094, Temp: 2.3827807903289795, KL: 72.34684753417969, Loss: 0.02863328903913498, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5381/20000], Bound: 0.402804434299469, Entropy: 143.35304260253906, Temp: 2.3827152252197266, KL: 71.84129333496094, Loss: 0.026952235028147697, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5382/20000], Bound: 0.4329257905483246, Entropy: 142.00682067871094, Temp: 2.3825788497924805, KL: 81.32688903808594, Loss: 0.025534668937325478, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5383/20000], Bound: 0.42894065380096436, Entropy: 142.11453247070312, Temp: 2.3824784755706787, KL: 77.98872375488281, Loss: 0.03004496544599533, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5384/20000], Bound: 0.4040989577770233, Entropy: 140.16754150390625, Temp: 2.382282257080078, KL: 70.35710144042969, Loss: 0.03083972819149494, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5385/20000], Bound: 0.43287914991378784, Entropy: 143.30691528320312, Temp: 2.381925582885742, KL: 78.28300476074219, Loss: 0.03188387677073479, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5386/20000], Bound: 0.3932514786720276, Entropy: 141.07298278808594, Temp: 2.381458044052124, KL: 70.12556457519531, Loss: 0.024838080629706383, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5387/20000], Bound: 0.40417981147766113, Entropy: 140.1805877685547, Temp: 2.3809986114501953, KL: 72.89569091796875, Loss: 0.025542354211211205, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5388/20000], Bound: 0.4127123951911926, Entropy: 141.7042999267578, Temp: 2.380549907684326, KL: 75.05361938476562, Loss: 0.026172228157520294, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5389/20000], Bound: 0.41462430357933044, Entropy: 137.35862731933594, Temp: 2.3801116943359375, KL: 74.91070556640625, Loss: 0.02763303741812706, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5390/20000], Bound: 0.40969225764274597, Entropy: 140.228759765625, Temp: 2.3796472549438477, KL: 73.99037170410156, Loss: 0.026555856689810753, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5391/20000], Bound: 0.40384769439697266, Entropy: 140.97618103027344, Temp: 2.379179000854492, KL: 73.42762756347656, Loss: 0.024198833853006363, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5392/20000], Bound: 0.39809533953666687, Entropy: 139.3556671142578, Temp: 2.3787591457366943, KL: 71.43162536621094, Loss: 0.024939192458987236, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5393/20000], Bound: 0.41441622376441956, Entropy: 140.11265563964844, Temp: 2.378351926803589, KL: 76.65180969238281, Loss: 0.02382040023803711, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5394/20000], Bound: 0.3683772385120392, Entropy: 139.6110076904297, Temp: 2.378019332885742, KL: 61.50807189941406, Loss: 0.028410056605935097, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5395/20000], Bound: 0.3523694574832916, Entropy: 138.303955078125, Temp: 2.3775339126586914, KL: 60.496490478515625, Loss: 0.021457035094499588, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5396/20000], Bound: 0.4085216820240021, Entropy: 139.14938354492188, Temp: 2.377072334289551, KL: 74.94102478027344, Loss: 0.023809131234884262, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5397/20000], Bound: 0.3642641007900238, Entropy: 139.10531616210938, Temp: 2.376680374145508, KL: 61.38166809082031, Loss: 0.026311755180358887, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5398/20000], Bound: 0.3852284848690033, Entropy: 141.57179260253906, Temp: 2.3761916160583496, KL: 66.35148620605469, Loss: 0.02797461301088333, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5399/20000], Bound: 0.4032108187675476, Entropy: 142.3932342529297, Temp: 2.3756139278411865, KL: 73.11183166503906, Loss: 0.02442747727036476, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5400/20000], Bound: 0.37427058815956116, Entropy: 141.4969024658203, Temp: 2.3750905990600586, KL: 64.237548828125, Loss: 0.026029061526060104, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5401/20000], Bound: 0.4044927954673767, Entropy: 140.4082489013672, Temp: 2.3745131492614746, KL: 74.59315490722656, Loss: 0.02206485904753208, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5402/20000], Bound: 0.4108887016773224, Entropy: 142.14686584472656, Temp: 2.374058246612549, KL: 76.80230712890625, Loss: 0.021281560882925987, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5403/20000], Bound: 0.4161626398563385, Entropy: 142.73216247558594, Temp: 2.373748302459717, KL: 79.71067810058594, Loss: 0.01837405376136303, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5404/20000], Bound: 0.39796969294548035, Entropy: 141.46153259277344, Temp: 2.3736581802368164, KL: 71.06065368652344, Loss: 0.025574740022420883, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5405/20000], Bound: 0.4216478765010834, Entropy: 143.37855529785156, Temp: 2.373533248901367, KL: 78.58831787109375, Loss: 0.024114051833748817, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5406/20000], Bound: 0.4200393557548523, Entropy: 143.45616149902344, Temp: 2.3734655380249023, KL: 77.37918090820312, Loss: 0.02566619962453842, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5407/20000], Bound: 0.4048474133014679, Entropy: 144.2670440673828, Temp: 2.373404026031494, KL: 73.416015625, Loss: 0.02474125288426876, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5408/20000], Bound: 0.4208346903324127, Entropy: 141.54159545898438, Temp: 2.3733417987823486, KL: 77.94589233398438, Loss: 0.024961501359939575, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5409/20000], Bound: 0.42691826820373535, Entropy: 143.5004425048828, Temp: 2.3733060359954834, KL: 79.37957763671875, Loss: 0.025716403499245644, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5410/20000], Bound: 0.42092201113700867, Entropy: 140.56552124023438, Temp: 2.373286247253418, KL: 75.57545471191406, Loss: 0.03000856749713421, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5411/20000], Bound: 0.40601789951324463, Entropy: 142.21966552734375, Temp: 2.3731517791748047, KL: 72.98670959472656, Loss: 0.026349328458309174, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5412/20000], Bound: 0.42520150542259216, Entropy: 141.40330505371094, Temp: 2.3729825019836426, KL: 78.28492736816406, Loss: 0.0269484743475914, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5413/20000], Bound: 0.42086687684059143, Entropy: 139.85675048828125, Temp: 2.372805595397949, KL: 78.525634765625, Loss: 0.023751065135002136, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5414/20000], Bound: 0.3970288634300232, Entropy: 138.59349060058594, Temp: 2.3726999759674072, KL: 70.01298522949219, Loss: 0.027207206934690475, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5415/20000], Bound: 0.36660704016685486, Entropy: 139.94369506835938, Temp: 2.3725147247314453, KL: 62.571807861328125, Loss: 0.025100145488977432, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5416/20000], Bound: 0.3805731236934662, Entropy: 138.4305877685547, Temp: 2.3722524642944336, KL: 66.90521240234375, Loss: 0.02403673343360424, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5417/20000], Bound: 0.40229570865631104, Entropy: 140.08364868164062, Temp: 2.3719797134399414, KL: 74.38241577148438, Loss: 0.021144866943359375, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5418/20000], Bound: 0.3897688090801239, Entropy: 138.8834991455078, Temp: 2.3718223571777344, KL: 70.47813415527344, Loss: 0.02190111018717289, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5419/20000], Bound: 0.41214439272880554, Entropy: 136.9975128173828, Temp: 2.37172269821167, KL: 77.30447387695312, Loss: 0.020949464291334152, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5420/20000], Bound: 0.3913160264492035, Entropy: 137.6640625, Temp: 2.37174654006958, KL: 72.68084716796875, Loss: 0.01817205734550953, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5421/20000], Bound: 0.3896130323410034, Entropy: 138.4381561279297, Temp: 2.3719167709350586, KL: 69.57374572753906, Loss: 0.02371678315103054, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5422/20000], Bound: 0.3882526755332947, Entropy: 141.02914428710938, Temp: 2.372061014175415, KL: 69.01210021972656, Loss: 0.02409938909113407, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5423/20000], Bound: 0.38922667503356934, Entropy: 139.59841918945312, Temp: 2.372168779373169, KL: 69.77432250976562, Loss: 0.023069199174642563, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5424/20000], Bound: 0.43266868591308594, Entropy: 137.3094940185547, Temp: 2.3722739219665527, KL: 81.58749389648438, Loss: 0.024647168815135956, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5425/20000], Bound: 0.42320674657821655, Entropy: 137.98074340820312, Temp: 2.3724231719970703, KL: 80.81533813476562, Loss: 0.02036772295832634, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5426/20000], Bound: 0.4140113592147827, Entropy: 140.7643280029297, Temp: 2.372708797454834, KL: 77.8389892578125, Loss: 0.020980658009648323, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5427/20000], Bound: 0.37657561898231506, Entropy: 138.89901733398438, Temp: 2.373081922531128, KL: 67.8809814453125, Loss: 0.019665317609906197, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5428/20000], Bound: 0.4126304090023041, Entropy: 141.126708984375, Temp: 2.37349534034729, KL: 76.322998046875, Loss: 0.023343829438090324, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5429/20000], Bound: 0.38969987630844116, Entropy: 142.00657653808594, Temp: 2.373915433883667, KL: 70.51629638671875, Loss: 0.02181021124124527, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5430/20000], Bound: 0.4253544509410858, Entropy: 139.53990173339844, Temp: 2.3743367195129395, KL: 79.87870788574219, Loss: 0.023708710446953773, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5431/20000], Bound: 0.40903979539871216, Entropy: 141.02027893066406, Temp: 2.37477970123291, KL: 75.31607055664062, Loss: 0.023298731073737144, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5432/20000], Bound: 0.4367051422595978, Entropy: 140.53961181640625, Temp: 2.3752193450927734, KL: 81.64167785644531, Loss: 0.027129458263516426, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5433/20000], Bound: 0.4602760076522827, Entropy: 142.21444702148438, Temp: 2.375608205795288, KL: 91.98069763183594, Loss: 0.020548297092318535, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5434/20000], Bound: 0.4097661077976227, Entropy: 143.2752685546875, Temp: 2.3761775493621826, KL: 73.85469055175781, Loss: 0.026836667209863663, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5435/20000], Bound: 0.4377252459526062, Entropy: 138.274658203125, Temp: 2.376633644104004, KL: 83.61065673828125, Loss: 0.023656215518712997, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5436/20000], Bound: 0.3930388391017914, Entropy: 140.20533752441406, Temp: 2.3771328926086426, KL: 69.60821533203125, Loss: 0.025742268189787865, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5437/20000], Bound: 0.3913562297821045, Entropy: 139.08164978027344, Temp: 2.377520799636841, KL: 71.29812622070312, Loss: 0.021196726709604263, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5438/20000], Bound: 0.3938376307487488, Entropy: 139.62255859375, Temp: 2.377931594848633, KL: 72.16366577148438, Loss: 0.020853344351053238, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5439/20000], Bound: 0.3786064684391022, Entropy: 139.57765197753906, Temp: 2.3783771991729736, KL: 66.73054504394531, Loss: 0.023336851969361305, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5440/20000], Bound: 0.40949228405952454, Entropy: 138.07606506347656, Temp: 2.378753185272217, KL: 74.68513488769531, Loss: 0.024961404502391815, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5441/20000], Bound: 0.41840195655822754, Entropy: 139.93771362304688, Temp: 2.379084825515747, KL: 77.17095947265625, Loss: 0.025184206664562225, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5442/20000], Bound: 0.4142901301383972, Entropy: 141.19091796875, Temp: 2.3793885707855225, KL: 76.76348876953125, Loss: 0.023525061085820198, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5443/20000], Bound: 0.38771459460258484, Entropy: 138.37896728515625, Temp: 2.379704475402832, KL: 68.36357116699219, Loss: 0.025246819481253624, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5444/20000], Bound: 0.4117797911167145, Entropy: 142.11732482910156, Temp: 2.379927635192871, KL: 76.57310485839844, Loss: 0.022402677685022354, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5445/20000], Bound: 0.38525956869125366, Entropy: 138.53353881835938, Temp: 2.3801965713500977, KL: 69.35491943359375, Loss: 0.02172895334661007, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5446/20000], Bound: 0.3985555171966553, Entropy: 140.7248992919922, Temp: 2.3804712295532227, KL: 74.8394775390625, Loss: 0.018079563975334167, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5447/20000], Bound: 0.41208553314208984, Entropy: 139.958740234375, Temp: 2.3808798789978027, KL: 75.56852722167969, Loss: 0.024713844060897827, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5448/20000], Bound: 0.3917280435562134, Entropy: 137.5397491455078, Temp: 2.381251811981201, KL: 71.16864013671875, Loss: 0.02174326218664646, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5449/20000], Bound: 0.3723558783531189, Entropy: 141.20652770996094, Temp: 2.381631374359131, KL: 64.86555480957031, Loss: 0.02367984689772129, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5450/20000], Bound: 0.401449590921402, Entropy: 138.39599609375, Temp: 2.381922721862793, KL: 72.85723876953125, Loss: 0.02399611473083496, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5451/20000], Bound: 0.4222222566604614, Entropy: 142.73797607421875, Temp: 2.3821864128112793, KL: 79.10263061523438, Loss: 0.023531679064035416, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5452/20000], Bound: 0.38872167468070984, Entropy: 140.47337341308594, Temp: 2.3824803829193115, KL: 70.44587707519531, Loss: 0.021504273638129234, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5453/20000], Bound: 0.427295982837677, Entropy: 140.0907440185547, Temp: 2.382789373397827, KL: 80.31901550292969, Loss: 0.0241349209100008, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5454/20000], Bound: 0.3864193558692932, Entropy: 142.67498779296875, Temp: 2.383117914199829, KL: 70.22477722167969, Loss: 0.020624270662665367, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5455/20000], Bound: 0.40792006254196167, Entropy: 139.980712890625, Temp: 2.3834781646728516, KL: 74.45553588867188, Loss: 0.024560965597629547, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5456/20000], Bound: 0.4022842049598694, Entropy: 140.573974609375, Temp: 2.3838000297546387, KL: 72.25257873535156, Loss: 0.02579176053404808, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5457/20000], Bound: 0.3794333040714264, Entropy: 139.77207946777344, Temp: 2.3840408325195312, KL: 67.62045288085938, Loss: 0.022021720185875893, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5458/20000], Bound: 0.4289383888244629, Entropy: 141.34918212890625, Temp: 2.3842673301696777, KL: 81.85697937011719, Loss: 0.02195758931338787, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5459/20000], Bound: 0.40056777000427246, Entropy: 142.34178161621094, Temp: 2.3845841884613037, KL: 73.5050048828125, Loss: 0.022148199379444122, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5460/20000], Bound: 0.38925665616989136, Entropy: 141.9523162841797, Temp: 2.384918689727783, KL: 71.63337707519531, Loss: 0.01936418004333973, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5461/20000], Bound: 0.39733144640922546, Entropy: 141.08499145507812, Temp: 2.385324716567993, KL: 72.83050537109375, Loss: 0.021641654893755913, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5462/20000], Bound: 0.37502655386924744, Entropy: 141.91835021972656, Temp: 2.385746717453003, KL: 65.01763916015625, Loss: 0.024948174133896828, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5463/20000], Bound: 0.44537580013275146, Entropy: 139.40992736816406, Temp: 2.386042833328247, KL: 85.20916748046875, Loss: 0.025337878614664078, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5464/20000], Bound: 0.3817099928855896, Entropy: 141.21408081054688, Temp: 2.386362075805664, KL: 68.98271179199219, Loss: 0.02052164450287819, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5465/20000], Bound: 0.38222536444664, Entropy: 140.8711700439453, Temp: 2.3867053985595703, KL: 69.01763916015625, Loss: 0.02075345441699028, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5466/20000], Bound: 0.361582487821579, Entropy: 141.9585418701172, Temp: 2.3870646953582764, KL: 64.81961059570312, Loss: 0.017693139612674713, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5467/20000], Bound: 0.40639469027519226, Entropy: 139.5906219482422, Temp: 2.387484550476074, KL: 73.53190612792969, Loss: 0.025633549317717552, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5468/20000], Bound: 0.39963817596435547, Entropy: 142.0116729736328, Temp: 2.387824296951294, KL: 72.82008361816406, Loss: 0.023075556382536888, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5469/20000], Bound: 0.3972986340522766, Entropy: 141.51681518554688, Temp: 2.3881490230560303, KL: 72.27896118164062, Loss: 0.022818880155682564, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5470/20000], Bound: 0.38690775632858276, Entropy: 140.9060516357422, Temp: 2.388462781906128, KL: 69.53434753417969, Loss: 0.02243228070437908, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5471/20000], Bound: 0.41197896003723145, Entropy: 138.0663299560547, Temp: 2.388756036758423, KL: 76.27311706542969, Loss: 0.023292744532227516, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5472/20000], Bound: 0.40774503350257874, Entropy: 138.958740234375, Temp: 2.3890578746795654, KL: 77.36380004882812, Loss: 0.01845044642686844, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5473/20000], Bound: 0.3704502284526825, Entropy: 141.9464111328125, Temp: 2.3894948959350586, KL: 63.44694519042969, Loss: 0.025647534057497978, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5474/20000], Bound: 0.43053576350212097, Entropy: 142.93421936035156, Temp: 2.3897721767425537, KL: 81.621826171875, Loss: 0.023543672636151314, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5475/20000], Bound: 0.41692399978637695, Entropy: 141.8472442626953, Temp: 2.390089988708496, KL: 78.44218444824219, Loss: 0.021789994090795517, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5476/20000], Bound: 0.41263362765312195, Entropy: 141.9934844970703, Temp: 2.3904659748077393, KL: 74.939208984375, Loss: 0.02650703303515911, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5477/20000], Bound: 0.3802296221256256, Entropy: 139.78738403320312, Temp: 2.390751838684082, KL: 69.08039855957031, Loss: 0.01951787807047367, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5478/20000], Bound: 0.4050588607788086, Entropy: 141.48265075683594, Temp: 2.3910884857177734, KL: 74.88902282714844, Loss: 0.022042276337742805, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5479/20000], Bound: 0.3872070908546448, Entropy: 141.6468505859375, Temp: 2.391448974609375, KL: 71.27049255371094, Loss: 0.019018029794096947, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5480/20000], Bound: 0.40989384055137634, Entropy: 141.83775329589844, Temp: 2.391880512237549, KL: 75.46226501464844, Loss: 0.02377219684422016, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5481/20000], Bound: 0.4129995107650757, Entropy: 142.00900268554688, Temp: 2.392287254333496, KL: 76.77583312988281, Loss: 0.022916588932275772, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5482/20000], Bound: 0.3934760093688965, Entropy: 140.65383911132812, Temp: 2.3927016258239746, KL: 72.24319458007812, Loss: 0.020693011581897736, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5483/20000], Bound: 0.390170693397522, Entropy: 142.18212890625, Temp: 2.393145799636841, KL: 71.61868286132812, Loss: 0.020055484026670456, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5484/20000], Bound: 0.4070172607898712, Entropy: 140.52198791503906, Temp: 2.3936283588409424, KL: 75.76124572753906, Loss: 0.0214372631162405, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5485/20000], Bound: 0.3927234411239624, Entropy: 137.0977783203125, Temp: 2.394139528274536, KL: 71.51043701171875, Loss: 0.02180006541311741, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5486/20000], Bound: 0.4059808552265167, Entropy: 142.73912048339844, Temp: 2.394637107849121, KL: 71.65104675292969, Loss: 0.02941136248409748, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5487/20000], Bound: 0.42350447177886963, Entropy: 141.79490661621094, Temp: 2.394932270050049, KL: 82.85594177246094, Loss: 0.016695251688361168, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5488/20000], Bound: 0.4195617437362671, Entropy: 137.41903686523438, Temp: 2.3954415321350098, KL: 80.27523803710938, Loss: 0.019666727632284164, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5489/20000], Bound: 0.4065985381603241, Entropy: 142.4806365966797, Temp: 2.3960509300231934, KL: 74.76304626464844, Loss: 0.02330613136291504, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5490/20000], Bound: 0.44615042209625244, Entropy: 140.02603149414062, Temp: 2.3966212272644043, KL: 84.27903747558594, Loss: 0.027962608262896538, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5491/20000], Bound: 0.3810904920101166, Entropy: 141.06019592285156, Temp: 2.3971080780029297, KL: 68.87495422363281, Loss: 0.020534424111247063, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5492/20000], Bound: 0.41027072072029114, Entropy: 137.7485809326172, Temp: 2.397594451904297, KL: 76.06706237792969, Loss: 0.02282409556210041, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5493/20000], Bound: 0.39090707898139954, Entropy: 137.43038940429688, Temp: 2.398074150085449, KL: 70.86976623535156, Loss: 0.022121824324131012, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5494/20000], Bound: 0.4176817238330841, Entropy: 138.50787353515625, Temp: 2.3985273838043213, KL: 77.265380859375, Loss: 0.02484475076198578, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5495/20000], Bound: 0.41350096464157104, Entropy: 138.84324645996094, Temp: 2.3989346027374268, KL: 77.36866760253906, Loss: 0.022088846191763878, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5496/20000], Bound: 0.4014405906200409, Entropy: 138.75820922851562, Temp: 2.399369716644287, KL: 73.90446472167969, Loss: 0.022054709494113922, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5497/20000], Bound: 0.38357946276664734, Entropy: 138.79981994628906, Temp: 2.3998055458068848, KL: 70.16636657714844, Loss: 0.019325846806168556, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5498/20000], Bound: 0.38635432720184326, Entropy: 138.8454132080078, Temp: 2.4002842903137207, KL: 69.98873901367188, Loss: 0.021320022642612457, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5499/20000], Bound: 0.4287693202495575, Entropy: 138.70339965820312, Temp: 2.400749444961548, KL: 79.82760620117188, Loss: 0.026369454339146614, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5500/20000], Bound: 0.41235682368278503, Entropy: 137.8446502685547, Temp: 2.4011456966400146, KL: 76.28829956054688, Loss: 0.023679673671722412, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5501/20000], Bound: 0.3911827504634857, Entropy: 136.97303771972656, Temp: 2.401521921157837, KL: 71.36222839355469, Loss: 0.02130575105547905, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5502/20000], Bound: 0.4119367003440857, Entropy: 139.25328063964844, Temp: 2.4019038677215576, KL: 76.46589660644531, Loss: 0.02306690625846386, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5503/20000], Bound: 0.3744523525238037, Entropy: 139.65440368652344, Temp: 2.402283191680908, KL: 68.052001953125, Loss: 0.018488474190235138, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5504/20000], Bound: 0.39956483244895935, Entropy: 139.03111267089844, Temp: 2.4027156829833984, KL: 72.52189636230469, Loss: 0.02386394701898098, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5505/20000], Bound: 0.4092971384525299, Entropy: 140.77378845214844, Temp: 2.4030911922454834, KL: 77.95964050292969, Loss: 0.018382396548986435, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5506/20000], Bound: 0.41416552662849426, Entropy: 140.911865234375, Temp: 2.403592348098755, KL: 77.64619445800781, Loss: 0.021988382562994957, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5507/20000], Bound: 0.37622886896133423, Entropy: 140.12435913085938, Temp: 2.4041130542755127, KL: 66.57374572753906, Loss: 0.022608591243624687, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5508/20000], Bound: 0.4099179208278656, Entropy: 139.03115844726562, Temp: 2.404555082321167, KL: 76.67668151855469, Loss: 0.021449165418744087, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5509/20000], Bound: 0.4092191457748413, Entropy: 138.8152618408203, Temp: 2.405028820037842, KL: 76.36488342285156, Loss: 0.021683482453227043, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5510/20000], Bound: 0.41190019249916077, Entropy: 138.4935760498047, Temp: 2.405522346496582, KL: 76.65496826171875, Loss: 0.022706713527441025, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5511/20000], Bound: 0.37666308879852295, Entropy: 141.131591796875, Temp: 2.406010150909424, KL: 65.96000671386719, Loss: 0.024156637489795685, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5512/20000], Bound: 0.417306512594223, Entropy: 139.7197265625, Temp: 2.406376361846924, KL: 76.67469787597656, Loss: 0.025961093604564667, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5513/20000], Bound: 0.4029233157634735, Entropy: 140.71006774902344, Temp: 2.4066667556762695, KL: 71.29579162597656, Loss: 0.028465034440159798, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5514/20000], Bound: 0.41845476627349854, Entropy: 139.26919555664062, Temp: 2.4067859649658203, KL: 77.25599670410156, Loss: 0.02545979432761669, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5515/20000], Bound: 0.4552399516105652, Entropy: 138.15785217285156, Temp: 2.406870126724243, KL: 88.05101013183594, Loss: 0.026122331619262695, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5516/20000], Bound: 0.3986518383026123, Entropy: 138.5557861328125, Temp: 2.406982421875, KL: 72.59585571289062, Loss: 0.023226536810398102, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5517/20000], Bound: 0.3846823573112488, Entropy: 137.57664489746094, Temp: 2.407083034515381, KL: 68.2376708984375, Loss: 0.024075662717223167, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5518/20000], Bound: 0.4001700282096863, Entropy: 140.583740234375, Temp: 2.4071192741394043, KL: 69.4537353515625, Loss: 0.030656663700938225, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5519/20000], Bound: 0.37589704990386963, Entropy: 138.73744201660156, Temp: 2.4069390296936035, KL: 66.00791931152344, Loss: 0.02362733706831932, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5520/20000], Bound: 0.4140736758708954, Entropy: 137.39002990722656, Temp: 2.406717538833618, KL: 77.34602355957031, Loss: 0.02260560542345047, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5521/20000], Bound: 0.43032142519950867, Entropy: 136.9400177001953, Temp: 2.4065678119659424, KL: 80.48345947265625, Loss: 0.026060013100504875, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5522/20000], Bound: 0.4069685637950897, Entropy: 136.8909912109375, Temp: 2.4064180850982666, KL: 76.02510070800781, Loss: 0.02105739526450634, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5523/20000], Bound: 0.3882904052734375, Entropy: 138.82928466796875, Temp: 2.4063634872436523, KL: 71.41835021972656, Loss: 0.01956220716238022, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5524/20000], Bound: 0.4054265022277832, Entropy: 139.2612762451172, Temp: 2.406399726867676, KL: 76.62478637695312, Loss: 0.018886204808950424, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5525/20000], Bound: 0.42136773467063904, Entropy: 137.12484741210938, Temp: 2.406572103500366, KL: 79.1112060546875, Loss: 0.023384427651762962, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5526/20000], Bound: 0.4189912974834442, Entropy: 139.0537109375, Temp: 2.4067702293395996, KL: 78.61398315429688, Loss: 0.02296607382595539, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5527/20000], Bound: 0.4117119312286377, Entropy: 138.1810302734375, Temp: 2.4069983959198, KL: 75.55860900878906, Loss: 0.024892792105674744, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5528/20000], Bound: 0.42753511667251587, Entropy: 138.79351806640625, Temp: 2.407182455062866, KL: 81.4344482421875, Loss: 0.022366831079125404, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5529/20000], Bound: 0.4053320288658142, Entropy: 138.03982543945312, Temp: 2.407433032989502, KL: 73.5042724609375, Loss: 0.025327332317829132, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5530/20000], Bound: 0.40039291977882385, Entropy: 136.80087280273438, Temp: 2.4076106548309326, KL: 73.84017944335938, Loss: 0.021684937179088593, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5531/20000], Bound: 0.3819045424461365, Entropy: 138.69735717773438, Temp: 2.40781831741333, KL: 70.34696960449219, Loss: 0.01809239760041237, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5532/20000], Bound: 0.39459067583084106, Entropy: 138.65774536132812, Temp: 2.4081203937530518, KL: 73.96160888671875, Loss: 0.01800498552620411, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5533/20000], Bound: 0.41673216223716736, Entropy: 137.84942626953125, Temp: 2.4085354804992676, KL: 77.675537109375, Loss: 0.023564696311950684, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5534/20000], Bound: 0.41437599062919617, Entropy: 137.59921264648438, Temp: 2.408935785293579, KL: 77.41867065429688, Loss: 0.022672409191727638, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5535/20000], Bound: 0.40142905712127686, Entropy: 137.9286651611328, Temp: 2.4093434810638428, KL: 70.9727783203125, Loss: 0.028276991099119186, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5536/20000], Bound: 0.3973627984523773, Entropy: 138.7598876953125, Temp: 2.40956711769104, KL: 73.15287780761719, Loss: 0.0213426873087883, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5537/20000], Bound: 0.37914663553237915, Entropy: 139.24520874023438, Temp: 2.409818410873413, KL: 68.50668334960938, Loss: 0.02034580335021019, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5538/20000], Bound: 0.4129408001899719, Entropy: 139.5585479736328, Temp: 2.4100866317749023, KL: 75.08058166503906, Loss: 0.026671411469578743, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5539/20000], Bound: 0.417865514755249, Entropy: 137.11846923828125, Temp: 2.4102556705474854, KL: 77.666015625, Loss: 0.024300789460539818, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5540/20000], Bound: 0.41708824038505554, Entropy: 137.5023956298828, Temp: 2.410414218902588, KL: 76.68014526367188, Loss: 0.025874843820929527, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5541/20000], Bound: 0.42272695899009705, Entropy: 140.85935974121094, Temp: 2.4105162620544434, KL: 79.3265380859375, Loss: 0.023833569139242172, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5542/20000], Bound: 0.39139047265052795, Entropy: 139.69915771484375, Temp: 2.4106383323669434, KL: 69.5482177734375, Loss: 0.02531658113002777, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5543/20000], Bound: 0.41015955805778503, Entropy: 137.99913024902344, Temp: 2.410668134689331, KL: 74.73002624511719, Loss: 0.0257271658629179, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5544/20000], Bound: 0.39868485927581787, Entropy: 139.13807678222656, Temp: 2.4106435775756836, KL: 73.59262084960938, Loss: 0.02122882381081581, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5545/20000], Bound: 0.4140179753303528, Entropy: 137.84080505371094, Temp: 2.410676956176758, KL: 78.32070922851562, Loss: 0.020611237734556198, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5546/20000], Bound: 0.42047634720802307, Entropy: 138.24082946777344, Temp: 2.4108126163482666, KL: 78.77326965332031, Loss: 0.023606685921549797, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5547/20000], Bound: 0.4202382266521454, Entropy: 137.4934539794922, Temp: 2.410966634750366, KL: 78.3978271484375, Loss: 0.02424212358891964, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5548/20000], Bound: 0.4277860224246979, Entropy: 137.20555114746094, Temp: 2.4111177921295166, KL: 79.456787109375, Loss: 0.02668924257159233, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5549/20000], Bound: 0.3853994607925415, Entropy: 138.48016357421875, Temp: 2.4112119674682617, KL: 70.31532287597656, Loss: 0.020234059542417526, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5550/20000], Bound: 0.41584742069244385, Entropy: 137.8712921142578, Temp: 2.4113540649414062, KL: 76.69204711914062, Loss: 0.02510911412537098, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5551/20000], Bound: 0.37196084856987, Entropy: 138.5373077392578, Temp: 2.4114601612091064, KL: 63.85969543457031, Loss: 0.025880813598632812, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5552/20000], Bound: 0.4145946502685547, Entropy: 138.1885223388672, Temp: 2.4114162921905518, KL: 77.08326721191406, Loss: 0.023538531735539436, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5553/20000], Bound: 0.4067103862762451, Entropy: 136.70587158203125, Temp: 2.411397695541382, KL: 76.28959655761719, Loss: 0.020430678501725197, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5554/20000], Bound: 0.42944958806037903, Entropy: 136.5541534423828, Temp: 2.4114766120910645, KL: 80.6763916015625, Loss: 0.025195637717843056, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5555/20000], Bound: 0.38995635509490967, Entropy: 137.3786163330078, Temp: 2.4115519523620605, KL: 71.00357055664062, Loss: 0.021470090374350548, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5556/20000], Bound: 0.40666353702545166, Entropy: 137.02206420898438, Temp: 2.411649703979492, KL: 75.18830871582031, Loss: 0.0226898230612278, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5557/20000], Bound: 0.40450531244277954, Entropy: 137.79598999023438, Temp: 2.41176700592041, KL: 75.7452392578125, Loss: 0.020245417952537537, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5558/20000], Bound: 0.4017844498157501, Entropy: 136.99212646484375, Temp: 2.41196870803833, KL: 72.74067687988281, Loss: 0.024854624643921852, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5559/20000], Bound: 0.408740371465683, Entropy: 136.79861450195312, Temp: 2.412105083465576, KL: 74.802978515625, Loss: 0.02474169246852398, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5560/20000], Bound: 0.38650617003440857, Entropy: 137.00057983398438, Temp: 2.412200689315796, KL: 70.19461059570312, Loss: 0.021141771227121353, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5561/20000], Bound: 0.4037196636199951, Entropy: 137.3653106689453, Temp: 2.4123189449310303, KL: 71.90237426757812, Loss: 0.027749957516789436, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5562/20000], Bound: 0.4337213635444641, Entropy: 138.80303955078125, Temp: 2.4122984409332275, KL: 83.19790649414062, Loss: 0.022637778893113136, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5563/20000], Bound: 0.4131177067756653, Entropy: 137.80276489257812, Temp: 2.412367343902588, KL: 76.32899475097656, Loss: 0.02422178164124489, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5564/20000], Bound: 0.38910308480262756, Entropy: 138.29075622558594, Temp: 2.412426471710205, KL: 71.18936157226562, Loss: 0.020597901195287704, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5565/20000], Bound: 0.408963143825531, Entropy: 137.43321228027344, Temp: 2.4125332832336426, KL: 75.21687316894531, Loss: 0.0240237507969141, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5566/20000], Bound: 0.4322139024734497, Entropy: 140.14093017578125, Temp: 2.412623643875122, KL: 82.73843383789062, Loss: 0.02265664003789425, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5567/20000], Bound: 0.40912050008773804, Entropy: 140.65635681152344, Temp: 2.4127883911132812, KL: 76.13606262207031, Loss: 0.022217191755771637, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5568/20000], Bound: 0.4414962828159332, Entropy: 138.10231018066406, Temp: 2.4129843711853027, KL: 81.77787780761719, Loss: 0.03046857751905918, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5569/20000], Bound: 0.4001971185207367, Entropy: 138.00352478027344, Temp: 2.4130373001098633, KL: 73.58955383300781, Loss: 0.02216647006571293, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5570/20000], Bound: 0.3789246678352356, Entropy: 138.18592834472656, Temp: 2.4131150245666504, KL: 68.50775146484375, Loss: 0.020258845761418343, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5571/20000], Bound: 0.419916033744812, Entropy: 136.192138671875, Temp: 2.4132273197174072, KL: 78.47927856445312, Loss: 0.023910941556096077, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5572/20000], Bound: 0.4023103713989258, Entropy: 139.19216918945312, Temp: 2.413348913192749, KL: 72.63166809082031, Loss: 0.025411661714315414, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5573/20000], Bound: 0.4185875952243805, Entropy: 141.00379943847656, Temp: 2.413396120071411, KL: 78.09182739257812, Loss: 0.023905638605356216, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5574/20000], Bound: 0.42773526906967163, Entropy: 137.6909942626953, Temp: 2.413456439971924, KL: 79.37582397460938, Loss: 0.026860587298870087, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5575/20000], Bound: 0.3964608907699585, Entropy: 138.74029541015625, Temp: 2.4134609699249268, KL: 73.59613037109375, Loss: 0.019947290420532227, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5576/20000], Bound: 0.4357573688030243, Entropy: 138.80572509765625, Temp: 2.413553237915039, KL: 82.24969482421875, Loss: 0.025895018130540848, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5577/20000], Bound: 0.4136160612106323, Entropy: 136.94497680664062, Temp: 2.413632392883301, KL: 76.95516967773438, Loss: 0.023244451731443405, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5578/20000], Bound: 0.3720811903476715, Entropy: 136.96176147460938, Temp: 2.4137299060821533, KL: 67.29034423828125, Loss: 0.018865952268242836, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5579/20000], Bound: 0.42578884959220886, Entropy: 135.36631774902344, Temp: 2.4138879776000977, KL: 78.20635986328125, Loss: 0.028088554739952087, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5580/20000], Bound: 0.39861056208610535, Entropy: 135.09803771972656, Temp: 2.413939952850342, KL: 72.8876953125, Loss: 0.022692561149597168, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5581/20000], Bound: 0.40513795614242554, Entropy: 137.40423583984375, Temp: 2.4139974117279053, KL: 73.40994262695312, Loss: 0.025494631379842758, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5582/20000], Bound: 0.39610716700553894, Entropy: 138.76829528808594, Temp: 2.413990020751953, KL: 71.7974853515625, Loss: 0.023471767082810402, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5583/20000], Bound: 0.410759836435318, Entropy: 139.06581115722656, Temp: 2.4139649868011475, KL: 77.07063293457031, Loss: 0.021285662427544594, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5584/20000], Bound: 0.4253385663032532, Entropy: 136.79396057128906, Temp: 2.414020538330078, KL: 81.58711242675781, Loss: 0.020810836926102638, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5585/20000], Bound: 0.40008559823036194, Entropy: 137.8428497314453, Temp: 2.4141931533813477, KL: 73.59516906738281, Loss: 0.02210502326488495, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5586/20000], Bound: 0.3782713711261749, Entropy: 135.8735809326172, Temp: 2.414379596710205, KL: 66.27890014648438, Loss: 0.024515118449926376, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5587/20000], Bound: 0.4118194878101349, Entropy: 136.53958129882812, Temp: 2.414459228515625, KL: 76.81265258789062, Loss: 0.02246645838022232, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5588/20000], Bound: 0.4163638651371002, Entropy: 136.94680786132812, Temp: 2.4145758152008057, KL: 76.41525268554688, Loss: 0.02604236826300621, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5589/20000], Bound: 0.3874780535697937, Entropy: 138.1045684814453, Temp: 2.4146294593811035, KL: 69.66938781738281, Loss: 0.022827720269560814, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5590/20000], Bound: 0.4427897334098816, Entropy: 136.43695068359375, Temp: 2.4146599769592285, KL: 86.6649169921875, Loss: 0.021190332248806953, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5591/20000], Bound: 0.41886258125305176, Entropy: 139.22023010253906, Temp: 2.414835214614868, KL: 79.45603942871094, Loss: 0.021270358934998512, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5592/20000], Bound: 0.3619300425052643, Entropy: 140.620361328125, Temp: 2.415088415145874, KL: 60.0723876953125, Loss: 0.02807806432247162, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5593/20000], Bound: 0.43109366297721863, Entropy: 140.35308837890625, Temp: 2.4150829315185547, KL: 80.99359130859375, Loss: 0.025614900514483452, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5594/20000], Bound: 0.4188302457332611, Entropy: 138.43746948242188, Temp: 2.4150707721710205, KL: 79.36033630371094, Loss: 0.0214526504278183, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5595/20000], Bound: 0.4244697391986847, Entropy: 138.87428283691406, Temp: 2.415149211883545, KL: 79.89582824707031, Loss: 0.023797105997800827, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5596/20000], Bound: 0.4290357232093811, Entropy: 138.94711303710938, Temp: 2.415252447128296, KL: 79.78324890136719, Loss: 0.026847729459404945, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5597/20000], Bound: 0.3996731638908386, Entropy: 139.51736450195312, Temp: 2.415297269821167, KL: 70.67764282226562, Loss: 0.027915775775909424, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5598/20000], Bound: 0.42671990394592285, Entropy: 136.65365600585938, Temp: 2.415192127227783, KL: 79.0968017578125, Loss: 0.026837309822440147, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5599/20000], Bound: 0.41392752528190613, Entropy: 136.79086303710938, Temp: 2.4150450229644775, KL: 77.42155456542969, Loss: 0.022488422691822052, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5600/20000], Bound: 0.404817134141922, Entropy: 138.82257080078125, Temp: 2.414961099624634, KL: 73.52302551269531, Loss: 0.0250815711915493, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5601/20000], Bound: 0.3929504454135895, Entropy: 138.93466186523438, Temp: 2.414836883544922, KL: 69.7054443359375, Loss: 0.025956742465496063, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5602/20000], Bound: 0.38308900594711304, Entropy: 138.49557495117188, Temp: 2.4146242141723633, KL: 69.338134765625, Loss: 0.020963825285434723, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5603/20000], Bound: 0.41008469462394714, Entropy: 137.75071716308594, Temp: 2.414462089538574, KL: 75.83738708496094, Loss: 0.0234406515955925, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5604/20000], Bound: 0.3882698714733124, Entropy: 138.77227783203125, Temp: 2.4143285751342773, KL: 70.85433959960938, Loss: 0.020831609144806862, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5605/20000], Bound: 0.3597322404384613, Entropy: 140.4425506591797, Temp: 2.414252758026123, KL: 62.6826171875, Loss: 0.02143065445125103, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5606/20000], Bound: 0.4007467031478882, Entropy: 141.3492889404297, Temp: 2.4141504764556885, KL: 73.30412292480469, Loss: 0.023099664598703384, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5607/20000], Bound: 0.3790818452835083, Entropy: 140.15313720703125, Temp: 2.4140608310699463, KL: 68.00021362304688, Loss: 0.02141294628381729, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5608/20000], Bound: 0.4173567593097687, Entropy: 141.41946411132812, Temp: 2.413987874984741, KL: 76.2486572265625, Loss: 0.02698279358446598, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5609/20000], Bound: 0.39029616117477417, Entropy: 143.3810272216797, Temp: 2.4138448238372803, KL: 71.17379760742188, Loss: 0.021347302943468094, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5610/20000], Bound: 0.4260753393173218, Entropy: 139.1591339111328, Temp: 2.4137496948242188, KL: 80.93499755859375, Loss: 0.022610854357481003, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5611/20000], Bound: 0.40018603205680847, Entropy: 139.9316864013672, Temp: 2.413736343383789, KL: 75.38084411621094, Loss: 0.018459178507328033, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5612/20000], Bound: 0.3987789452075958, Entropy: 141.8432159423828, Temp: 2.4138662815093994, KL: 73.88720703125, Loss: 0.02072092890739441, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5613/20000], Bound: 0.38555818796157837, Entropy: 140.2513885498047, Temp: 2.4140541553497314, KL: 67.82293701171875, Loss: 0.025527331978082657, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5614/20000], Bound: 0.39744699001312256, Entropy: 139.44708251953125, Temp: 2.414119243621826, KL: 74.03807067871094, Loss: 0.019624076783657074, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5615/20000], Bound: 0.4042520225048065, Entropy: 142.46002197265625, Temp: 2.414278507232666, KL: 74.62825012207031, Loss: 0.022446103394031525, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5616/20000], Bound: 0.3783740699291229, Entropy: 140.94920349121094, Temp: 2.414452075958252, KL: 66.83456420898438, Loss: 0.023424314334988594, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5617/20000], Bound: 0.4022131562232971, Entropy: 142.7738494873047, Temp: 2.4145524501800537, KL: 72.08349609375, Loss: 0.02650463953614235, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5618/20000], Bound: 0.42646777629852295, Entropy: 141.9822998046875, Temp: 2.4145455360412598, KL: 78.38003540039062, Loss: 0.02815655805170536, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5619/20000], Bound: 0.45680615305900574, Entropy: 141.1674041748047, Temp: 2.4144465923309326, KL: 91.25314331054688, Loss: 0.020638197660446167, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5620/20000], Bound: 0.392609566450119, Entropy: 143.78201293945312, Temp: 2.414553165435791, KL: 72.09138488769531, Loss: 0.020812511444091797, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5621/20000], Bound: 0.3951822519302368, Entropy: 141.36581420898438, Temp: 2.4147040843963623, KL: 70.66201782226562, Loss: 0.025287127122282982, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5622/20000], Bound: 0.42340853810310364, Entropy: 142.40196228027344, Temp: 2.4147634506225586, KL: 80.64848327636719, Loss: 0.02158081904053688, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5623/20000], Bound: 0.39856427907943726, Entropy: 140.30450439453125, Temp: 2.4149138927459717, KL: 73.74017333984375, Loss: 0.020913533866405487, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5624/20000], Bound: 0.39938387274742126, Entropy: 139.585693359375, Temp: 2.4151132106781006, KL: 74.42828369140625, Loss: 0.019977228716015816, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5625/20000], Bound: 0.439922034740448, Entropy: 137.63633728027344, Temp: 2.41538667678833, KL: 83.57830810546875, Loss: 0.025785552337765694, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5626/20000], Bound: 0.38715600967407227, Entropy: 138.2267303466797, Temp: 2.415640354156494, KL: 71.42465209960938, Loss: 0.019019840285182, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5627/20000], Bound: 0.4017980396747589, Entropy: 137.91957092285156, Temp: 2.4159657955169678, KL: 73.02499389648438, Loss: 0.024326911196112633, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5628/20000], Bound: 0.392656147480011, Entropy: 137.22210693359375, Temp: 2.4162251949310303, KL: 70.04270935058594, Loss: 0.02510264329612255, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5629/20000], Bound: 0.4158630669116974, Entropy: 140.1854705810547, Temp: 2.4163811206817627, KL: 78.47659301757812, Loss: 0.02149835042655468, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5630/20000], Bound: 0.4293816089630127, Entropy: 137.61708068847656, Temp: 2.4166038036346436, KL: 79.99183654785156, Loss: 0.026650305837392807, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5631/20000], Bound: 0.39479345083236694, Entropy: 138.6945343017578, Temp: 2.4167611598968506, KL: 71.96153259277344, Loss: 0.02239515259861946, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5632/20000], Bound: 0.39742282032966614, Entropy: 137.66282653808594, Temp: 2.416912317276001, KL: 71.43771362304688, Loss: 0.02503093145787716, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5633/20000], Bound: 0.4021151065826416, Entropy: 140.2486114501953, Temp: 2.416983127593994, KL: 74.33418273925781, Loss: 0.021820668131113052, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5634/20000], Bound: 0.40410488843917847, Entropy: 138.44293212890625, Temp: 2.4170892238616943, KL: 75.53196716308594, Loss: 0.02052927203476429, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5635/20000], Bound: 0.41804516315460205, Entropy: 139.3328399658203, Temp: 2.417271137237549, KL: 79.17344665527344, Loss: 0.021396690979599953, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5636/20000], Bound: 0.37432801723480225, Entropy: 140.2469024658203, Temp: 2.417524576187134, KL: 64.34136962890625, Loss: 0.026296943426132202, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5637/20000], Bound: 0.44353485107421875, Entropy: 137.8246612548828, Temp: 2.4175963401794434, KL: 85.65306091308594, Loss: 0.023808473721146584, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5638/20000], Bound: 0.41640952229499817, Entropy: 134.81732177734375, Temp: 2.4177331924438477, KL: 77.36503601074219, Loss: 0.02415003813803196, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5639/20000], Bound: 0.41694876551628113, Entropy: 139.04916381835938, Temp: 2.4178590774536133, KL: 75.60398864746094, Loss: 0.02812105417251587, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5640/20000], Bound: 0.3904392123222351, Entropy: 136.80841064453125, Temp: 2.4178555011749268, KL: 70.98246765136719, Loss: 0.021880855783820152, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5641/20000], Bound: 0.4389013648033142, Entropy: 140.47073364257812, Temp: 2.417867422103882, KL: 84.97146606445312, Loss: 0.02230420522391796, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5642/20000], Bound: 0.4294734001159668, Entropy: 138.77749633789062, Temp: 2.417985439300537, KL: 82.78474426269531, Loss: 0.020952515304088593, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5643/20000], Bound: 0.397255539894104, Entropy: 137.74623107910156, Temp: 2.4182188510894775, KL: 72.163330078125, Loss: 0.023448355495929718, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5644/20000], Bound: 0.4172426760196686, Entropy: 138.73304748535156, Temp: 2.418410539627075, KL: 77.8905029296875, Loss: 0.023579517379403114, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5645/20000], Bound: 0.38930627703666687, Entropy: 138.3439178466797, Temp: 2.4186041355133057, KL: 71.70681762695312, Loss: 0.019731644541025162, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5646/20000], Bound: 0.43240395188331604, Entropy: 136.0730438232422, Temp: 2.4188570976257324, KL: 82.41888427734375, Loss: 0.023540878668427467, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5647/20000], Bound: 0.403306782245636, Entropy: 136.9366455078125, Temp: 2.419139862060547, KL: 73.70436096191406, Loss: 0.02386217564344406, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5648/20000], Bound: 0.4448910653591156, Entropy: 138.50193786621094, Temp: 2.4193758964538574, KL: 86.86497497558594, Loss: 0.02219298854470253, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5649/20000], Bound: 0.37936267256736755, Entropy: 136.57005310058594, Temp: 2.419711112976074, KL: 68.20558166503906, Loss: 0.021220730617642403, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5650/20000], Bound: 0.42644843459129333, Entropy: 135.6847381591797, Temp: 2.420023202896118, KL: 78.15254211425781, Loss: 0.0286917295306921, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5651/20000], Bound: 0.37825262546539307, Entropy: 136.96742248535156, Temp: 2.420189619064331, KL: 67.67724609375, Loss: 0.021680103614926338, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5652/20000], Bound: 0.405471533536911, Entropy: 136.5124969482422, Temp: 2.420332670211792, KL: 74.68757629394531, Loss: 0.023137474432587624, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5653/20000], Bound: 0.409205824136734, Entropy: 135.9665985107422, Temp: 2.420469045639038, KL: 76.15452575683594, Loss: 0.022344358265399933, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5654/20000], Bound: 0.37527045607566833, Entropy: 134.75152587890625, Temp: 2.4206318855285645, KL: 66.94671630859375, Loss: 0.021485256031155586, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5655/20000], Bound: 0.4149315059185028, Entropy: 137.98220825195312, Temp: 2.4207708835601807, KL: 77.06846618652344, Loss: 0.023911748081445694, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5656/20000], Bound: 0.40240350365638733, Entropy: 137.85276794433594, Temp: 2.420900344848633, KL: 70.49760437011719, Loss: 0.02997199445962906, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5657/20000], Bound: 0.3769446611404419, Entropy: 139.1030731201172, Temp: 2.420806646347046, KL: 67.89852905273438, Loss: 0.020480098202824593, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5658/20000], Bound: 0.435779869556427, Entropy: 138.89599609375, Temp: 2.4207499027252197, KL: 82.48817443847656, Loss: 0.02553103305399418, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5659/20000], Bound: 0.3978487253189087, Entropy: 139.7214813232422, Temp: 2.4207003116607666, KL: 72.392578125, Loss: 0.023357663303613663, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5660/20000], Bound: 0.37450432777404785, Entropy: 139.5684356689453, Temp: 2.4206390380859375, KL: 65.32710266113281, Loss: 0.02439279668033123, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5661/20000], Bound: 0.4433078467845917, Entropy: 140.2223663330078, Temp: 2.4204835891723633, KL: 85.42085266113281, Loss: 0.024195382371544838, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5662/20000], Bound: 0.41303956508636475, Entropy: 140.33311462402344, Temp: 2.4204022884368896, KL: 78.12544250488281, Loss: 0.020579390227794647, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5663/20000], Bound: 0.3997081518173218, Entropy: 142.4291534423828, Temp: 2.4204318523406982, KL: 73.76231384277344, Loss: 0.021623920649290085, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5664/20000], Bound: 0.4039952754974365, Entropy: 140.7421875, Temp: 2.420499801635742, KL: 72.69279479980469, Loss: 0.02638009935617447, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5665/20000], Bound: 0.4210812449455261, Entropy: 141.37826538085938, Temp: 2.4204649925231934, KL: 79.6571044921875, Loss: 0.022298790514469147, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5666/20000], Bound: 0.42135727405548096, Entropy: 139.96243286132812, Temp: 2.4205009937286377, KL: 77.267333984375, Loss: 0.02740447036921978, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5667/20000], Bound: 0.3890790343284607, Entropy: 140.66973876953125, Temp: 2.420445203781128, KL: 71.54670715332031, Loss: 0.01995566301047802, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5668/20000], Bound: 0.3934956192970276, Entropy: 140.76243591308594, Temp: 2.4204657077789307, KL: 72.2496337890625, Loss: 0.021087436005473137, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5669/20000], Bound: 0.40993595123291016, Entropy: 139.99105834960938, Temp: 2.4205286502838135, KL: 75.02342224121094, Loss: 0.025120146572589874, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5670/20000], Bound: 0.4061352014541626, Entropy: 141.13214111328125, Temp: 2.420541524887085, KL: 74.0816650390625, Loss: 0.02478836476802826, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5671/20000], Bound: 0.4025936424732208, Entropy: 141.2791748046875, Temp: 2.420510768890381, KL: 74.25363159179688, Loss: 0.02232200838625431, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5672/20000], Bound: 0.43553945422172546, Entropy: 139.70069885253906, Temp: 2.4205095767974854, KL: 83.14599609375, Loss: 0.024018296971917152, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5673/20000], Bound: 0.4066600203514099, Entropy: 138.96763610839844, Temp: 2.4205551147460938, KL: 75.38818359375, Loss: 0.022403497248888016, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5674/20000], Bound: 0.3855874240398407, Entropy: 138.04660034179688, Temp: 2.4206290245056152, KL: 69.44198608398438, Loss: 0.02227460965514183, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5675/20000], Bound: 0.39221689105033875, Entropy: 139.54969787597656, Temp: 2.4206857681274414, KL: 71.39373779296875, Loss: 0.022108759731054306, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5676/20000], Bound: 0.4197264611721039, Entropy: 136.68655395507812, Temp: 2.4207472801208496, KL: 77.232421875, Loss: 0.026484809815883636, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5677/20000], Bound: 0.4288634657859802, Entropy: 140.3690643310547, Temp: 2.4207379817962646, KL: 80.68499755859375, Loss: 0.024960096925497055, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5678/20000], Bound: 0.4005052447319031, Entropy: 138.01210021972656, Temp: 2.4207329750061035, KL: 73.47984313964844, Loss: 0.022683776915073395, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5679/20000], Bound: 0.39597752690315247, Entropy: 138.85647583007812, Temp: 2.420738935470581, KL: 72.30181884765625, Loss: 0.02244233340024948, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5680/20000], Bound: 0.43291398882865906, Entropy: 136.48704528808594, Temp: 2.4207520484924316, KL: 82.87669372558594, Loss: 0.02294340543448925, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5681/20000], Bound: 0.4295448660850525, Entropy: 138.05169677734375, Temp: 2.420837879180908, KL: 80.62451171875, Loss: 0.025507735088467598, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5682/20000], Bound: 0.414598286151886, Entropy: 136.48416137695312, Temp: 2.420903205871582, KL: 75.33120727539062, Loss: 0.027300113812088966, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5683/20000], Bound: 0.3760419189929962, Entropy: 136.1865234375, Temp: 2.4208602905273438, KL: 67.34349060058594, Loss: 0.021109962835907936, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5684/20000], Bound: 0.41625240445137024, Entropy: 137.79193115234375, Temp: 2.420827865600586, KL: 76.40093994140625, Loss: 0.026091434061527252, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5685/20000], Bound: 0.37646180391311646, Entropy: 136.29730224609375, Temp: 2.4207379817962646, KL: 66.33714294433594, Loss: 0.023427538573741913, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5686/20000], Bound: 0.4028436541557312, Entropy: 136.87840270996094, Temp: 2.420590877532959, KL: 72.55618286132812, Loss: 0.025978045538067818, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5687/20000], Bound: 0.41199252009391785, Entropy: 136.3158721923828, Temp: 2.4203712940216064, KL: 76.17588806152344, Loss: 0.02397482842206955, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5688/20000], Bound: 0.4064526855945587, Entropy: 139.25038146972656, Temp: 2.4201695919036865, KL: 73.59234619140625, Loss: 0.02598406746983528, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5689/20000], Bound: 0.4141102135181427, Entropy: 137.49510192871094, Temp: 2.4199090003967285, KL: 73.52372741699219, Loss: 0.03072638437151909, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5690/20000], Bound: 0.3924805819988251, Entropy: 139.31459045410156, Temp: 2.4194657802581787, KL: 71.62016296386719, Loss: 0.021779131144285202, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5691/20000], Bound: 0.392721951007843, Entropy: 137.03643798828125, Temp: 2.419088840484619, KL: 70.83395385742188, Loss: 0.023540575057268143, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5692/20000], Bound: 0.41163206100463867, Entropy: 137.03634643554688, Temp: 2.4187169075012207, KL: 77.06193542480469, Loss: 0.021902134642004967, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5693/20000], Bound: 0.41993507742881775, Entropy: 137.6659393310547, Temp: 2.418442964553833, KL: 78.26737976074219, Loss: 0.024440007284283638, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5694/20000], Bound: 0.3692323565483093, Entropy: 140.67723083496094, Temp: 2.4181971549987793, KL: 66.16131591796875, Loss: 0.01963820680975914, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5695/20000], Bound: 0.4047893285751343, Entropy: 141.263427734375, Temp: 2.418015956878662, KL: 73.37298583984375, Loss: 0.025415873154997826, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5696/20000], Bound: 0.3988315761089325, Entropy: 140.29818725585938, Temp: 2.4177889823913574, KL: 70.78273010253906, Loss: 0.027229225262999535, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5697/20000], Bound: 0.38859477639198303, Entropy: 142.1201171875, Temp: 2.4174509048461914, KL: 69.67683410644531, Loss: 0.023498818278312683, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5698/20000], Bound: 0.4097682535648346, Entropy: 141.86473083496094, Temp: 2.417107105255127, KL: 74.61506652832031, Loss: 0.025817174464464188, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5699/20000], Bound: 0.4151016175746918, Entropy: 139.65797424316406, Temp: 2.416733503341675, KL: 77.40289306640625, Loss: 0.023263640701770782, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5700/20000], Bound: 0.41193053126335144, Entropy: 141.10609436035156, Temp: 2.4164247512817383, KL: 76.46965026855469, Loss: 0.023272806778550148, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5701/20000], Bound: 0.40130603313446045, Entropy: 142.7646026611328, Temp: 2.416167974472046, KL: 73.58946228027344, Loss: 0.022869113832712173, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5702/20000], Bound: 0.43462759256362915, Entropy: 139.39353942871094, Temp: 2.4159464836120605, KL: 81.97567749023438, Loss: 0.02579493075609207, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5703/20000], Bound: 0.3797633945941925, Entropy: 138.87367248535156, Temp: 2.4157416820526123, KL: 66.14373779296875, Loss: 0.02566905878484249, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5704/20000], Bound: 0.38123613595962524, Entropy: 142.08334350585938, Temp: 2.4154295921325684, KL: 68.78457641601562, Loss: 0.0210487749427557, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5705/20000], Bound: 0.39112964272499084, Entropy: 140.3771514892578, Temp: 2.4151721000671387, KL: 71.17475891113281, Loss: 0.02185114286839962, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5706/20000], Bound: 0.40739336609840393, Entropy: 140.07443237304688, Temp: 2.4149601459503174, KL: 73.62690734863281, Loss: 0.026407979428768158, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5707/20000], Bound: 0.39284610748291016, Entropy: 140.44435119628906, Temp: 2.414682626724243, KL: 70.82328796386719, Loss: 0.023578979074954987, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5708/20000], Bound: 0.4050340950489044, Entropy: 140.21971130371094, Temp: 2.414402484893799, KL: 75.20928955078125, Loss: 0.02171165868639946, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5709/20000], Bound: 0.38195571303367615, Entropy: 138.94464111328125, Temp: 2.414206027984619, KL: 68.11424255371094, Loss: 0.022837232798337936, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5710/20000], Bound: 0.4379601776599884, Entropy: 140.09002685546875, Temp: 2.4139981269836426, KL: 81.22271728515625, Loss: 0.029408996924757957, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5711/20000], Bound: 0.42810553312301636, Entropy: 138.13075256347656, Temp: 2.4137027263641357, KL: 80.38372802734375, Loss: 0.02500518225133419, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5712/20000], Bound: 0.3992367684841156, Entropy: 139.40167236328125, Temp: 2.413442611694336, KL: 71.37794494628906, Loss: 0.026184462010860443, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5713/20000], Bound: 0.40399178862571716, Entropy: 139.7288818359375, Temp: 2.4131109714508057, KL: 73.52537536621094, Loss: 0.024559125304222107, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5714/20000], Bound: 0.4270564615726471, Entropy: 139.1923065185547, Temp: 2.4127774238586426, KL: 81.57745361328125, Loss: 0.0218687504529953, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5715/20000], Bound: 0.4255742132663727, Entropy: 139.6822052001953, Temp: 2.412578582763672, KL: 79.69735717773438, Loss: 0.0248477254062891, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5716/20000], Bound: 0.3700716495513916, Entropy: 137.74452209472656, Temp: 2.4124057292938232, KL: 65.35356140136719, Loss: 0.021717604249715805, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5717/20000], Bound: 0.40743041038513184, Entropy: 138.86907958984375, Temp: 2.412229061126709, KL: 73.91865539550781, Loss: 0.025789791718125343, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5718/20000], Bound: 0.43166691064834595, Entropy: 136.99758911132812, Temp: 2.4120044708251953, KL: 81.73883056640625, Loss: 0.02437795139849186, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5719/20000], Bound: 0.4122612178325653, Entropy: 140.01869201660156, Temp: 2.411837577819824, KL: 76.70442199707031, Loss: 0.02291795238852501, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5720/20000], Bound: 0.4075270891189575, Entropy: 141.38485717773438, Temp: 2.4117233753204346, KL: 76.09281921386719, Loss: 0.021333536133170128, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5721/20000], Bound: 0.41241124272346497, Entropy: 137.28195190429688, Temp: 2.4116969108581543, KL: 75.14260864257812, Loss: 0.02624449133872986, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5722/20000], Bound: 0.42054393887519836, Entropy: 139.09872436523438, Temp: 2.411604881286621, KL: 78.32200622558594, Loss: 0.024596013128757477, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5723/20000], Bound: 0.4527870714664459, Entropy: 138.4733123779297, Temp: 2.411525011062622, KL: 86.10418701171875, Loss: 0.028665056452155113, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5724/20000], Bound: 0.40387263894081116, Entropy: 140.9444122314453, Temp: 2.411405086517334, KL: 76.06968688964844, Loss: 0.019189221784472466, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5725/20000], Bound: 0.37110286951065063, Entropy: 137.89585876464844, Temp: 2.411432981491089, KL: 65.22491455078125, Loss: 0.022560404613614082, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5726/20000], Bound: 0.40103641152381897, Entropy: 139.6332244873047, Temp: 2.4114129543304443, KL: 71.87654113769531, Loss: 0.0261940099298954, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5727/20000], Bound: 0.41016194224357605, Entropy: 140.05325317382812, Temp: 2.411302328109741, KL: 77.79930114746094, Loss: 0.01937294751405716, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5728/20000], Bound: 0.4089611768722534, Entropy: 138.77642822265625, Temp: 2.411346912384033, KL: 74.97824096679688, Loss: 0.02450047992169857, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5729/20000], Bound: 0.4111545979976654, Entropy: 137.91586303710938, Temp: 2.4113667011260986, KL: 74.44087219238281, Loss: 0.026936164125800133, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5730/20000], Bound: 0.35906413197517395, Entropy: 136.8952178955078, Temp: 2.4112918376922607, KL: 62.52372741699219, Loss: 0.021352749317884445, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5731/20000], Bound: 0.41019102931022644, Entropy: 140.32603454589844, Temp: 2.411191463470459, KL: 77.01118469238281, Loss: 0.02102297730743885, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5732/20000], Bound: 0.41361573338508606, Entropy: 138.8074951171875, Temp: 2.411193609237671, KL: 75.27383422851562, Loss: 0.026694197207689285, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5733/20000], Bound: 0.40727630257606506, Entropy: 139.555908203125, Temp: 2.411116361618042, KL: 75.09953308105469, Loss: 0.023233603686094284, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5734/20000], Bound: 0.4168689250946045, Entropy: 137.698486328125, Temp: 2.411062479019165, KL: 77.51860046386719, Loss: 0.02401186339557171, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5735/20000], Bound: 0.376132994890213, Entropy: 136.48794555664062, Temp: 2.411027193069458, KL: 67.71896362304688, Loss: 0.020261449739336967, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5736/20000], Bound: 0.3976808190345764, Entropy: 140.1758575439453, Temp: 2.411036491394043, KL: 72.70210266113281, Loss: 0.02248673141002655, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5737/20000], Bound: 0.42960917949676514, Entropy: 138.74441528320312, Temp: 2.4110629558563232, KL: 82.09500122070312, Loss: 0.022346198558807373, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5738/20000], Bound: 0.39737024903297424, Entropy: 137.2613983154297, Temp: 2.411181688308716, KL: 72.01071166992188, Loss: 0.023738712072372437, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5739/20000], Bound: 0.3881404399871826, Entropy: 141.5424041748047, Temp: 2.4112653732299805, KL: 69.19087219238281, Loss: 0.024163536727428436, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5740/20000], Bound: 0.43624719977378845, Entropy: 138.62142944335938, Temp: 2.411282777786255, KL: 82.85075378417969, Loss: 0.024918708950281143, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5741/20000], Bound: 0.4255235493183136, Entropy: 140.0290069580078, Temp: 2.411327838897705, KL: 81.45442199707031, Loss: 0.021153707057237625, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5742/20000], Bound: 0.41586029529571533, Entropy: 139.85784912109375, Temp: 2.411491870880127, KL: 77.85017395019531, Loss: 0.02271767146885395, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5743/20000], Bound: 0.36986085772514343, Entropy: 139.77261352539062, Temp: 2.4116909503936768, KL: 65.99636840820312, Loss: 0.02025655284523964, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5744/20000], Bound: 0.4221118688583374, Entropy: 136.79403686523438, Temp: 2.4118971824645996, KL: 79.15238952636719, Loss: 0.023838987573981285, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5745/20000], Bound: 0.41530415415763855, Entropy: 139.55633544921875, Temp: 2.4121131896972656, KL: 79.00839233398438, Loss: 0.01998879201710224, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5746/20000], Bound: 0.3861885666847229, Entropy: 136.63658142089844, Temp: 2.4124443531036377, KL: 68.94007873535156, Loss: 0.023560253903269768, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5747/20000], Bound: 0.42678311467170715, Entropy: 136.61358642578125, Temp: 2.41269850730896, KL: 80.93809509277344, Loss: 0.023023691028356552, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5748/20000], Bound: 0.4161694645881653, Entropy: 137.87088012695312, Temp: 2.4129934310913086, KL: 79.69552612304688, Loss: 0.01910482347011566, Learning Rate: 0.0035294699999999985\n",
      "Epoch [5749/20000], Bound: 0.39383751153945923, Entropy: 136.1183319091797, Temp: 2.4134254455566406, KL: 72.06509399414062, Loss: 0.0215727798640728, Learning Rate: 0.0035294699999999985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5750/20000], Bound: 0.3860071301460266, Entropy: 137.7560577392578, Temp: 2.413851261138916, KL: 70.17050170898438, Loss: 0.020923389121890068, Learning Rate: 0.002470628999999999\n",
      "Epoch [5751/20000], Bound: 0.4095022678375244, Entropy: 139.1927032470703, Temp: 2.414274215698242, KL: 76.2550048828125, Loss: 0.022222591564059258, Learning Rate: 0.002470628999999999\n",
      "Epoch [5752/20000], Bound: 0.40985357761383057, Entropy: 137.36822509765625, Temp: 2.4147064685821533, KL: 77.06399536132812, Loss: 0.020765280351042747, Learning Rate: 0.002470628999999999\n",
      "Epoch [5753/20000], Bound: 0.42092165350914, Entropy: 138.16688537597656, Temp: 2.415193557739258, KL: 79.04299926757812, Loss: 0.0233883298933506, Learning Rate: 0.002470628999999999\n",
      "Epoch [5754/20000], Bound: 0.39098745584487915, Entropy: 138.769287109375, Temp: 2.4156713485717773, KL: 72.45504760742188, Loss: 0.019124651327729225, Learning Rate: 0.002470628999999999\n",
      "Epoch [5755/20000], Bound: 0.4080962538719177, Entropy: 139.6899871826172, Temp: 2.4162099361419678, KL: 78.27703857421875, Loss: 0.017222927883267403, Learning Rate: 0.002470628999999999\n",
      "Epoch [5756/20000], Bound: 0.4272167384624481, Entropy: 134.89418029785156, Temp: 2.4169013500213623, KL: 80.60289001464844, Loss: 0.024053005501627922, Learning Rate: 0.002470628999999999\n",
      "Epoch [5757/20000], Bound: 0.3883785903453827, Entropy: 137.7613983154297, Temp: 2.417555332183838, KL: 70.58355712890625, Loss: 0.021498817950487137, Learning Rate: 0.002470628999999999\n",
      "Epoch [5758/20000], Bound: 0.38902661204338074, Entropy: 138.48004150390625, Temp: 2.41816782951355, KL: 69.19874572753906, Loss: 0.0247481856495142, Learning Rate: 0.002470628999999999\n",
      "Epoch [5759/20000], Bound: 0.41275981068611145, Entropy: 139.83653259277344, Temp: 2.4186389446258545, KL: 77.17330932617188, Loss: 0.02235088311135769, Learning Rate: 0.002470628999999999\n",
      "Epoch [5760/20000], Bound: 0.41763901710510254, Entropy: 138.41587829589844, Temp: 2.419114112854004, KL: 79.213623046875, Loss: 0.021096259355545044, Learning Rate: 0.002470628999999999\n",
      "Epoch [5761/20000], Bound: 0.40693649649620056, Entropy: 138.84466552734375, Temp: 2.4196438789367676, KL: 75.79452514648438, Loss: 0.02171599306166172, Learning Rate: 0.002470628999999999\n",
      "Epoch [5762/20000], Bound: 0.39393797516822815, Entropy: 138.59671020507812, Temp: 2.4201784133911133, KL: 70.38400268554688, Loss: 0.025197500362992287, Learning Rate: 0.002470628999999999\n",
      "Epoch [5763/20000], Bound: 0.41111379861831665, Entropy: 137.8549041748047, Temp: 2.420574188232422, KL: 76.90043640136719, Loss: 0.021951863542199135, Learning Rate: 0.002470628999999999\n",
      "Epoch [5764/20000], Bound: 0.38740015029907227, Entropy: 137.05413818359375, Temp: 2.420989513397217, KL: 71.04208374023438, Loss: 0.02002762258052826, Learning Rate: 0.002470628999999999\n",
      "Epoch [5765/20000], Bound: 0.390596479177475, Entropy: 138.19461059570312, Temp: 2.4214303493499756, KL: 71.74763488769531, Loss: 0.020440036430954933, Learning Rate: 0.002470628999999999\n",
      "Epoch [5766/20000], Bound: 0.39527228474617004, Entropy: 137.59329223632812, Temp: 2.4218878746032715, KL: 72.23184204101562, Loss: 0.022187303751707077, Learning Rate: 0.002470628999999999\n",
      "Epoch [5767/20000], Bound: 0.394930362701416, Entropy: 138.36026000976562, Temp: 2.422313928604126, KL: 73.70864868164062, Loss: 0.018943684175610542, Learning Rate: 0.002470628999999999\n",
      "Epoch [5768/20000], Bound: 0.4109220504760742, Entropy: 140.83631896972656, Temp: 2.422816038131714, KL: 75.68582153320312, Loss: 0.024376843124628067, Learning Rate: 0.002470628999999999\n",
      "Epoch [5769/20000], Bound: 0.4050028920173645, Entropy: 139.1298370361328, Temp: 2.4232466220855713, KL: 75.71804809570312, Loss: 0.020772628486156464, Learning Rate: 0.002470628999999999\n",
      "Epoch [5770/20000], Bound: 0.4212948977947235, Entropy: 138.3563995361328, Temp: 2.423715591430664, KL: 80.42512512207031, Loss: 0.02089618146419525, Learning Rate: 0.002470628999999999\n",
      "Epoch [5771/20000], Bound: 0.37104395031929016, Entropy: 141.28932189941406, Temp: 2.424250841140747, KL: 65.83171081542969, Loss: 0.021420292556285858, Learning Rate: 0.002470628999999999\n",
      "Epoch [5772/20000], Bound: 0.39125585556030273, Entropy: 140.5771026611328, Temp: 2.424715280532837, KL: 70.7501220703125, Loss: 0.022927237674593925, Learning Rate: 0.002470628999999999\n",
      "Epoch [5773/20000], Bound: 0.4124027490615845, Entropy: 140.67996215820312, Temp: 2.425112009048462, KL: 77.95494079589844, Loss: 0.020621294155716896, Learning Rate: 0.002470628999999999\n",
      "Epoch [5774/20000], Bound: 0.4089846909046173, Entropy: 137.65089416503906, Temp: 2.4255707263946533, KL: 77.07968139648438, Loss: 0.020379433408379555, Learning Rate: 0.002470628999999999\n",
      "Epoch [5775/20000], Bound: 0.3987208604812622, Entropy: 139.1579132080078, Temp: 2.4260847568511963, KL: 73.37892150878906, Loss: 0.021910300478339195, Learning Rate: 0.002470628999999999\n",
      "Epoch [5776/20000], Bound: 0.38121503591537476, Entropy: 139.63742065429688, Temp: 2.426575183868408, KL: 68.63728332519531, Loss: 0.021481776610016823, Learning Rate: 0.002470628999999999\n",
      "Epoch [5777/20000], Bound: 0.4206055998802185, Entropy: 141.58631896972656, Temp: 2.427018642425537, KL: 79.30891418457031, Loss: 0.022829392924904823, Learning Rate: 0.002470628999999999\n",
      "Epoch [5778/20000], Bound: 0.4152889549732208, Entropy: 138.42660522460938, Temp: 2.4274654388427734, KL: 77.96556091308594, Loss: 0.022376984357833862, Learning Rate: 0.002470628999999999\n",
      "Epoch [5779/20000], Bound: 0.3989986181259155, Entropy: 140.77638244628906, Temp: 2.427917003631592, KL: 74.28907775878906, Loss: 0.02022506110370159, Learning Rate: 0.002470628999999999\n",
      "Epoch [5780/20000], Bound: 0.39911192655563354, Entropy: 140.44874572753906, Temp: 2.42840576171875, KL: 73.56831359863281, Loss: 0.021783053874969482, Learning Rate: 0.002470628999999999\n",
      "Epoch [5781/20000], Bound: 0.3987797796726227, Entropy: 138.77719116210938, Temp: 2.428877115249634, KL: 73.50428771972656, Loss: 0.021725358441472054, Learning Rate: 0.002470628999999999\n",
      "Epoch [5782/20000], Bound: 0.41773808002471924, Entropy: 138.42442321777344, Temp: 2.429333209991455, KL: 78.10720825195312, Loss: 0.023595914244651794, Learning Rate: 0.002470628999999999\n",
      "Epoch [5783/20000], Bound: 0.3864070177078247, Entropy: 136.3648681640625, Temp: 2.429758310317993, KL: 70.0782470703125, Loss: 0.02155395783483982, Learning Rate: 0.002470628999999999\n",
      "Epoch [5784/20000], Bound: 0.4080949127674103, Entropy: 140.82785034179688, Temp: 2.430149793624878, KL: 76.19546508789062, Loss: 0.021736521273851395, Learning Rate: 0.002470628999999999\n",
      "Epoch [5785/20000], Bound: 0.4100700616836548, Entropy: 139.65655517578125, Temp: 2.4305543899536133, KL: 78.12637329101562, Loss: 0.018952470272779465, Learning Rate: 0.002470628999999999\n",
      "Epoch [5786/20000], Bound: 0.3795332908630371, Entropy: 141.58261108398438, Temp: 2.4310648441314697, KL: 68.22621154785156, Loss: 0.02141657844185829, Learning Rate: 0.002470628999999999\n",
      "Epoch [5787/20000], Bound: 0.41675683856010437, Entropy: 140.9338836669922, Temp: 2.43152117729187, KL: 80.41775512695312, Loss: 0.0182826928794384, Learning Rate: 0.002470628999999999\n",
      "Epoch [5788/20000], Bound: 0.37432947754859924, Entropy: 139.86036682128906, Temp: 2.432114601135254, KL: 67.57290649414062, Loss: 0.01979818567633629, Learning Rate: 0.002470628999999999\n",
      "Epoch [5789/20000], Bound: 0.4120647609233856, Entropy: 139.09713745117188, Temp: 2.432687282562256, KL: 78.65231323242188, Loss: 0.019102031365036964, Learning Rate: 0.002470628999999999\n",
      "Epoch [5790/20000], Bound: 0.4191485643386841, Entropy: 137.54261779785156, Temp: 2.433347225189209, KL: 80.13882446289062, Loss: 0.020335618406534195, Learning Rate: 0.002470628999999999\n",
      "Epoch [5791/20000], Bound: 0.3916948735713959, Entropy: 140.37940979003906, Temp: 2.4340615272521973, KL: 72.28077697753906, Loss: 0.020156674087047577, Learning Rate: 0.002470628999999999\n",
      "Epoch [5792/20000], Bound: 0.398017555475235, Entropy: 140.59671020507812, Temp: 2.4347684383392334, KL: 72.92156982421875, Loss: 0.022554000839591026, Learning Rate: 0.002470628999999999\n",
      "Epoch [5793/20000], Bound: 0.413520485162735, Entropy: 139.849853515625, Temp: 2.43540358543396, KL: 76.60992431640625, Loss: 0.024213341996073723, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5794/20000], Bound: 0.38022908568382263, Entropy: 139.22100830078125, Temp: 2.435955286026001, KL: 68.45144653320312, Loss: 0.021412478759884834, Learning Rate: 0.002470628999999999\n",
      "Epoch [5795/20000], Bound: 0.3667506277561188, Entropy: 139.407958984375, Temp: 2.4364466667175293, KL: 63.883087158203125, Loss: 0.023135041818022728, Learning Rate: 0.002470628999999999\n",
      "Epoch [5796/20000], Bound: 0.4126395285129547, Entropy: 139.55245971679688, Temp: 2.436795234680176, KL: 77.37831115722656, Loss: 0.022126562893390656, Learning Rate: 0.002470628999999999\n",
      "Epoch [5797/20000], Bound: 0.39235031604766846, Entropy: 141.5095672607422, Temp: 2.4371535778045654, KL: 72.46388244628906, Loss: 0.02020527794957161, Learning Rate: 0.002470628999999999\n",
      "Epoch [5798/20000], Bound: 0.4377999007701874, Entropy: 140.66952514648438, Temp: 2.437537431716919, KL: 85.13571166992188, Loss: 0.021620910614728928, Learning Rate: 0.002470628999999999\n",
      "Epoch [5799/20000], Bound: 0.4147161543369293, Entropy: 142.6725616455078, Temp: 2.4380011558532715, KL: 78.08169555664062, Loss: 0.021950606256723404, Learning Rate: 0.002470628999999999\n",
      "Epoch [5800/20000], Bound: 0.38514629006385803, Entropy: 140.28257751464844, Temp: 2.4384727478027344, KL: 71.85380554199219, Loss: 0.01729484274983406, Learning Rate: 0.002470628999999999\n",
      "Epoch [5801/20000], Bound: 0.3999214172363281, Entropy: 142.6248016357422, Temp: 2.4390382766723633, KL: 75.22801208496094, Loss: 0.019002526998519897, Learning Rate: 0.002470628999999999\n",
      "Epoch [5802/20000], Bound: 0.38434848189353943, Entropy: 142.58045959472656, Temp: 2.439664363861084, KL: 69.16033935546875, Loss: 0.022371806204319, Learning Rate: 0.002470628999999999\n",
      "Epoch [5803/20000], Bound: 0.4020354449748993, Entropy: 140.44271850585938, Temp: 2.440197706222534, KL: 77.59132385253906, Loss: 0.015424609184265137, Learning Rate: 0.002470628999999999\n",
      "Epoch [5804/20000], Bound: 0.38942158222198486, Entropy: 140.64500427246094, Temp: 2.440916061401367, KL: 70.2120361328125, Loss: 0.023165937513113022, Learning Rate: 0.002470628999999999\n",
      "Epoch [5805/20000], Bound: 0.41897574067115784, Entropy: 139.440185546875, Temp: 2.441516876220703, KL: 79.13270568847656, Loss: 0.02242182195186615, Learning Rate: 0.002470628999999999\n",
      "Epoch [5806/20000], Bound: 0.4010337293148041, Entropy: 139.01959228515625, Temp: 2.442103624343872, KL: 74.29524230957031, Loss: 0.02161291614174843, Learning Rate: 0.002470628999999999\n",
      "Epoch [5807/20000], Bound: 0.4163576662540436, Entropy: 141.32687377929688, Temp: 2.4426627159118652, KL: 78.7548828125, Loss: 0.021630482748150826, Learning Rate: 0.002470628999999999\n",
      "Epoch [5808/20000], Bound: 0.4052308201789856, Entropy: 139.87701416015625, Temp: 2.4432311058044434, KL: 75.95909118652344, Loss: 0.02070566453039646, Learning Rate: 0.002470628999999999\n",
      "Epoch [5809/20000], Bound: 0.39577704668045044, Entropy: 141.31594848632812, Temp: 2.443812608718872, KL: 69.35501098632812, Loss: 0.028656305745244026, Learning Rate: 0.002470628999999999\n",
      "Epoch [5810/20000], Bound: 0.43836626410484314, Entropy: 138.45101928710938, Temp: 2.444119930267334, KL: 84.76889038085938, Loss: 0.022836918011307716, Learning Rate: 0.002470628999999999\n",
      "Epoch [5811/20000], Bound: 0.37826216220855713, Entropy: 140.54112243652344, Temp: 2.444471836090088, KL: 66.04522705078125, Loss: 0.025312237441539764, Learning Rate: 0.002470628999999999\n",
      "Epoch [5812/20000], Bound: 0.4260396361351013, Entropy: 137.19796752929688, Temp: 2.444641590118408, KL: 83.18890380859375, Loss: 0.018469221889972687, Learning Rate: 0.002470628999999999\n",
      "Epoch [5813/20000], Bound: 0.41551777720451355, Entropy: 139.326416015625, Temp: 2.4449830055236816, KL: 78.86988830566406, Loss: 0.02092418633401394, Learning Rate: 0.002470628999999999\n",
      "Epoch [5814/20000], Bound: 0.40419426560401917, Entropy: 137.3642120361328, Temp: 2.4453747272491455, KL: 74.56526184082031, Loss: 0.022972209379076958, Learning Rate: 0.002470628999999999\n",
      "Epoch [5815/20000], Bound: 0.4014133810997009, Entropy: 139.50892639160156, Temp: 2.445718288421631, KL: 74.24703979492188, Loss: 0.021984482184052467, Learning Rate: 0.002470628999999999\n",
      "Epoch [5816/20000], Bound: 0.39945268630981445, Entropy: 136.45860290527344, Temp: 2.446044445037842, KL: 75.63566589355469, Loss: 0.01799599640071392, Learning Rate: 0.002470628999999999\n",
      "Epoch [5817/20000], Bound: 0.4115806818008423, Entropy: 135.98695373535156, Temp: 2.4464826583862305, KL: 78.51480102539062, Loss: 0.01931041106581688, Learning Rate: 0.002470628999999999\n",
      "Epoch [5818/20000], Bound: 0.4278596043586731, Entropy: 135.42848205566406, Temp: 2.447005033493042, KL: 79.63111877441406, Loss: 0.026893142610788345, Learning Rate: 0.002470628999999999\n",
      "Epoch [5819/20000], Bound: 0.4394405484199524, Entropy: 136.9248809814453, Temp: 2.4473910331726074, KL: 85.37582397460938, Loss: 0.02232040837407112, Learning Rate: 0.002470628999999999\n",
      "Epoch [5820/20000], Bound: 0.4168885350227356, Entropy: 136.878662109375, Temp: 2.4478302001953125, KL: 78.42301940917969, Loss: 0.02270548790693283, Learning Rate: 0.002470628999999999\n",
      "Epoch [5821/20000], Bound: 0.40032759308815, Entropy: 135.34349060058594, Temp: 2.4482529163360596, KL: 75.98338317871094, Loss: 0.017833150923252106, Learning Rate: 0.002470628999999999\n",
      "Epoch [5822/20000], Bound: 0.41189518570899963, Entropy: 137.3516845703125, Temp: 2.4487836360931396, KL: 77.02885437011719, Loss: 0.022567996755242348, Learning Rate: 0.002470628999999999\n",
      "Epoch [5823/20000], Bound: 0.39701125025749207, Entropy: 135.85989379882812, Temp: 2.4492809772491455, KL: 72.22694396972656, Loss: 0.023571185767650604, Learning Rate: 0.002470628999999999\n",
      "Epoch [5824/20000], Bound: 0.41640809178352356, Entropy: 135.11749267578125, Temp: 2.4496798515319824, KL: 76.05218505859375, Loss: 0.02728200890123844, Learning Rate: 0.002470628999999999\n",
      "Epoch [5825/20000], Bound: 0.4040839374065399, Entropy: 135.36224365234375, Temp: 2.4499123096466064, KL: 75.53271484375, Loss: 0.02099265530705452, Learning Rate: 0.002470628999999999\n",
      "Epoch [5826/20000], Bound: 0.3870101273059845, Entropy: 137.1536407470703, Temp: 2.450174331665039, KL: 73.18878173828125, Loss: 0.01580815203487873, Learning Rate: 0.002470628999999999\n",
      "Epoch [5827/20000], Bound: 0.3822150230407715, Entropy: 138.4949951171875, Temp: 2.4505980014801025, KL: 66.90029907226562, Loss: 0.02588806301355362, Learning Rate: 0.002470628999999999\n",
      "Epoch [5828/20000], Bound: 0.41313600540161133, Entropy: 137.13951110839844, Temp: 2.450817108154297, KL: 79.21931457519531, Loss: 0.018870264291763306, Learning Rate: 0.002470628999999999\n",
      "Epoch [5829/20000], Bound: 0.386671781539917, Entropy: 138.39523315429688, Temp: 2.451157331466675, KL: 73.10464477539062, Loss: 0.01579907536506653, Learning Rate: 0.002470628999999999\n",
      "Epoch [5830/20000], Bound: 0.39951378107070923, Entropy: 139.6926727294922, Temp: 2.451650381088257, KL: 74.20559692382812, Loss: 0.021031519398093224, Learning Rate: 0.002470628999999999\n",
      "Epoch [5831/20000], Bound: 0.3921181559562683, Entropy: 137.564697265625, Temp: 2.452134370803833, KL: 71.27088928222656, Loss: 0.022703656926751137, Learning Rate: 0.002470628999999999\n",
      "Epoch [5832/20000], Bound: 0.43026524782180786, Entropy: 138.56605529785156, Temp: 2.4525365829467773, KL: 80.84397888183594, Loss: 0.025971338152885437, Learning Rate: 0.002470628999999999\n",
      "Epoch [5833/20000], Bound: 0.4100625216960907, Entropy: 139.8585662841797, Temp: 2.452845573425293, KL: 75.82237243652344, Loss: 0.023990655317902565, Learning Rate: 0.002470628999999999\n",
      "Epoch [5834/20000], Bound: 0.38911980390548706, Entropy: 138.6713104248047, Temp: 2.4530887603759766, KL: 71.16015625, Loss: 0.02120358496904373, Learning Rate: 0.002470628999999999\n",
      "Epoch [5835/20000], Bound: 0.38530853390693665, Entropy: 139.37405395507812, Temp: 2.4533166885375977, KL: 70.00791931152344, Loss: 0.021357044577598572, Learning Rate: 0.002470628999999999\n",
      "Epoch [5836/20000], Bound: 0.4279164671897888, Entropy: 140.93743896484375, Temp: 2.453517436981201, KL: 81.581298828125, Loss: 0.023043189197778702, Learning Rate: 0.002470628999999999\n",
      "Epoch [5837/20000], Bound: 0.39816105365753174, Entropy: 139.98182678222656, Temp: 2.453734874725342, KL: 75.11466979980469, Loss: 0.018414245918393135, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5838/20000], Bound: 0.4267866313457489, Entropy: 139.6969757080078, Temp: 2.4540538787841797, KL: 79.89059448242188, Loss: 0.0258055180311203, Learning Rate: 0.002470628999999999\n",
      "Epoch [5839/20000], Bound: 0.39974743127822876, Entropy: 139.80502319335938, Temp: 2.4542837142944336, KL: 73.55540466308594, Loss: 0.02252875827252865, Learning Rate: 0.002470628999999999\n",
      "Epoch [5840/20000], Bound: 0.3932720124721527, Entropy: 140.63265991210938, Temp: 2.4544789791107178, KL: 71.55404663085938, Loss: 0.022825703024864197, Learning Rate: 0.002470628999999999\n",
      "Epoch [5841/20000], Bound: 0.3893658220767975, Entropy: 139.70108032226562, Temp: 2.454617977142334, KL: 71.3668212890625, Loss: 0.02094395086169243, Learning Rate: 0.002470628999999999\n",
      "Epoch [5842/20000], Bound: 0.4292394518852234, Entropy: 139.85971069335938, Temp: 2.454760789871216, KL: 84.09669494628906, Loss: 0.018748579546809196, Learning Rate: 0.002470628999999999\n",
      "Epoch [5843/20000], Bound: 0.3886357545852661, Entropy: 138.7122039794922, Temp: 2.4550700187683105, KL: 69.99946594238281, Loss: 0.023312581703066826, Learning Rate: 0.002470628999999999\n",
      "Epoch [5844/20000], Bound: 0.38340407609939575, Entropy: 141.63963317871094, Temp: 2.4552838802337646, KL: 69.74568176269531, Loss: 0.02082175761461258, Learning Rate: 0.002470628999999999\n",
      "Epoch [5845/20000], Bound: 0.41191911697387695, Entropy: 138.2680206298828, Temp: 2.45548415184021, KL: 78.19096374511719, Loss: 0.020309578627347946, Learning Rate: 0.002470628999999999\n",
      "Epoch [5846/20000], Bound: 0.42361393570899963, Entropy: 138.13827514648438, Temp: 2.455754041671753, KL: 81.28024291992188, Loss: 0.02106768637895584, Learning Rate: 0.002470628999999999\n",
      "Epoch [5847/20000], Bound: 0.38193216919898987, Entropy: 140.5436248779297, Temp: 2.456087589263916, KL: 68.96762084960938, Loss: 0.021572710946202278, Learning Rate: 0.002470628999999999\n",
      "Epoch [5848/20000], Bound: 0.40172719955444336, Entropy: 138.81358337402344, Temp: 2.4563663005828857, KL: 74.20274353027344, Loss: 0.02240079827606678, Learning Rate: 0.002470628999999999\n",
      "Epoch [5849/20000], Bound: 0.4077185094356537, Entropy: 139.14193725585938, Temp: 2.4566125869750977, KL: 76.725341796875, Loss: 0.020809216424822807, Learning Rate: 0.002470628999999999\n",
      "Epoch [5850/20000], Bound: 0.40228769183158875, Entropy: 138.92117309570312, Temp: 2.456897020339966, KL: 74.54585266113281, Loss: 0.0220392607152462, Learning Rate: 0.002470628999999999\n",
      "Epoch [5851/20000], Bound: 0.4140303432941437, Entropy: 140.34976196289062, Temp: 2.4571616649627686, KL: 78.02679443359375, Loss: 0.021930627524852753, Learning Rate: 0.002470628999999999\n",
      "Epoch [5852/20000], Bound: 0.4124465882778168, Entropy: 140.25787353515625, Temp: 2.4574389457702637, KL: 77.65782165527344, Loss: 0.02173818275332451, Learning Rate: 0.002470628999999999\n",
      "Epoch [5853/20000], Bound: 0.40318629145622253, Entropy: 138.3512725830078, Temp: 2.457730293273926, KL: 74.47894287109375, Loss: 0.022715572267770767, Learning Rate: 0.002470628999999999\n",
      "Epoch [5854/20000], Bound: 0.3834189176559448, Entropy: 140.2924041748047, Temp: 2.457979917526245, KL: 70.8515625, Loss: 0.018613306805491447, Learning Rate: 0.002470628999999999\n",
      "Epoch [5855/20000], Bound: 0.4113122522830963, Entropy: 140.90684509277344, Temp: 2.4582855701446533, KL: 77.00416564941406, Loss: 0.022402867674827576, Learning Rate: 0.002470628999999999\n",
      "Epoch [5856/20000], Bound: 0.4205428957939148, Entropy: 138.36146545410156, Temp: 2.4585769176483154, KL: 78.89643859863281, Loss: 0.02409922145307064, Learning Rate: 0.002470628999999999\n",
      "Epoch [5857/20000], Bound: 0.4169461727142334, Entropy: 140.2677764892578, Temp: 2.458820104598999, KL: 79.61247253417969, Loss: 0.02047882042825222, Learning Rate: 0.002470628999999999\n",
      "Epoch [5858/20000], Bound: 0.4277104437351227, Entropy: 138.44297790527344, Temp: 2.459131956100464, KL: 83.57475280761719, Loss: 0.01894921250641346, Learning Rate: 0.002470628999999999\n",
      "Epoch [5859/20000], Bound: 0.4182124435901642, Entropy: 138.4315948486328, Temp: 2.4595813751220703, KL: 80.70292663574219, Loss: 0.019035261124372482, Learning Rate: 0.002470628999999999\n",
      "Epoch [5860/20000], Bound: 0.38264527916908264, Entropy: 138.0659637451172, Temp: 2.460129499435425, KL: 70.34381103515625, Loss: 0.019230451434850693, Learning Rate: 0.002470628999999999\n",
      "Epoch [5861/20000], Bound: 0.3996126055717468, Entropy: 140.25181579589844, Temp: 2.460679769515991, KL: 72.62812805175781, Loss: 0.024415649473667145, Learning Rate: 0.002470628999999999\n",
      "Epoch [5862/20000], Bound: 0.38813233375549316, Entropy: 140.4221649169922, Temp: 2.461094379425049, KL: 69.84928894042969, Loss: 0.023396749049425125, Learning Rate: 0.002470628999999999\n",
      "Epoch [5863/20000], Bound: 0.4164569675922394, Entropy: 138.64871215820312, Temp: 2.4613940715789795, KL: 79.60983276367188, Loss: 0.020229212939739227, Learning Rate: 0.002470628999999999\n",
      "Epoch [5864/20000], Bound: 0.3922298550605774, Entropy: 138.13919067382812, Temp: 2.4617624282836914, KL: 72.24142456054688, Loss: 0.020913224667310715, Learning Rate: 0.002470628999999999\n",
      "Epoch [5865/20000], Bound: 0.405932754278183, Entropy: 138.60635375976562, Temp: 2.462114095687866, KL: 75.14776611328125, Loss: 0.023033874109387398, Learning Rate: 0.002470628999999999\n",
      "Epoch [5866/20000], Bound: 0.3834230601787567, Entropy: 138.59812927246094, Temp: 2.4624099731445312, KL: 68.47503662109375, Loss: 0.023498548194766045, Learning Rate: 0.002470628999999999\n",
      "Epoch [5867/20000], Bound: 0.39140620827674866, Entropy: 138.30604553222656, Temp: 2.462587356567383, KL: 73.96864318847656, Loss: 0.01693984866142273, Learning Rate: 0.002470628999999999\n",
      "Epoch [5868/20000], Bound: 0.4037216901779175, Entropy: 140.60105895996094, Temp: 2.462900161743164, KL: 76.10443115234375, Loss: 0.019797664135694504, Learning Rate: 0.002470628999999999\n",
      "Epoch [5869/20000], Bound: 0.3913760185241699, Entropy: 140.46669006347656, Temp: 2.4632649421691895, KL: 71.06437683105469, Loss: 0.022827202454209328, Learning Rate: 0.002470628999999999\n",
      "Epoch [5870/20000], Bound: 0.3834845721721649, Entropy: 141.00125122070312, Temp: 2.463545083999634, KL: 67.35331726074219, Loss: 0.025822734460234642, Learning Rate: 0.002470628999999999\n",
      "Epoch [5871/20000], Bound: 0.3873644769191742, Entropy: 137.24868774414062, Temp: 2.4636270999908447, KL: 70.69352722167969, Loss: 0.021270137280225754, Learning Rate: 0.002470628999999999\n",
      "Epoch [5872/20000], Bound: 0.4289001524448395, Entropy: 139.64735412597656, Temp: 2.463696241378784, KL: 82.98385620117188, Loss: 0.02095075510442257, Learning Rate: 0.002470628999999999\n",
      "Epoch [5873/20000], Bound: 0.3927267789840698, Entropy: 138.25888061523438, Temp: 2.463860034942627, KL: 72.12222290039062, Loss: 0.021469611674547195, Learning Rate: 0.002470628999999999\n",
      "Epoch [5874/20000], Bound: 0.40014809370040894, Entropy: 140.80227661132812, Temp: 2.46400785446167, KL: 74.39080810546875, Loss: 0.021191896870732307, Learning Rate: 0.002470628999999999\n",
      "Epoch [5875/20000], Bound: 0.4086530804634094, Entropy: 138.3129425048828, Temp: 2.4641199111938477, KL: 74.56074523925781, Loss: 0.025861026719212532, Learning Rate: 0.002470628999999999\n",
      "Epoch [5876/20000], Bound: 0.3803960382938385, Entropy: 142.7179412841797, Temp: 2.4641425609588623, KL: 69.30581665039062, Loss: 0.020104138180613518, Learning Rate: 0.002470628999999999\n",
      "Epoch [5877/20000], Bound: 0.39659783244132996, Entropy: 140.22482299804688, Temp: 2.4641761779785156, KL: 73.86756896972656, Loss: 0.0201816838234663, Learning Rate: 0.002470628999999999\n",
      "Epoch [5878/20000], Bound: 0.3950400948524475, Entropy: 141.29104614257812, Temp: 2.4642438888549805, KL: 74.12420654296875, Loss: 0.018755104392766953, Learning Rate: 0.002470628999999999\n",
      "Epoch [5879/20000], Bound: 0.4323795437812805, Entropy: 139.78817749023438, Temp: 2.4643735885620117, KL: 83.12236022949219, Loss: 0.022812198847532272, Learning Rate: 0.002470628999999999\n",
      "Epoch [5880/20000], Bound: 0.39788690209388733, Entropy: 141.1589813232422, Temp: 2.4645233154296875, KL: 72.91812133789062, Loss: 0.02286430262029171, Learning Rate: 0.002470628999999999\n",
      "Epoch [5881/20000], Bound: 0.41516804695129395, Entropy: 140.99021911621094, Temp: 2.464632987976074, KL: 78.39204406738281, Loss: 0.021976107731461525, Learning Rate: 0.002470628999999999\n",
      "Epoch [5882/20000], Bound: 0.38988175988197327, Entropy: 140.91835021972656, Temp: 2.4647560119628906, KL: 70.81806945800781, Loss: 0.022481190040707588, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5883/20000], Bound: 0.41199710965156555, Entropy: 142.47784423828125, Temp: 2.4648377895355225, KL: 77.41363525390625, Loss: 0.02207006886601448, Learning Rate: 0.002470628999999999\n",
      "Epoch [5884/20000], Bound: 0.38360509276390076, Entropy: 141.7535858154297, Temp: 2.464928150177002, KL: 70.556396484375, Loss: 0.01940806396305561, Learning Rate: 0.002470628999999999\n",
      "Epoch [5885/20000], Bound: 0.40240606665611267, Entropy: 141.78707885742188, Temp: 2.4650444984436035, KL: 76.32093811035156, Loss: 0.018614985048770905, Learning Rate: 0.002470628999999999\n",
      "Epoch [5886/20000], Bound: 0.42548367381095886, Entropy: 140.6609344482422, Temp: 2.4652328491210938, KL: 83.28668212890625, Loss: 0.01827910542488098, Learning Rate: 0.002470628999999999\n",
      "Epoch [5887/20000], Bound: 0.41280803084373474, Entropy: 141.8242950439453, Temp: 2.4655306339263916, KL: 76.81597900390625, Loss: 0.023775020614266396, Learning Rate: 0.002470628999999999\n",
      "Epoch [5888/20000], Bound: 0.385519415140152, Entropy: 142.74127197265625, Temp: 2.465775966644287, KL: 69.12442016601562, Loss: 0.02341853268444538, Learning Rate: 0.002470628999999999\n",
      "Epoch [5889/20000], Bound: 0.4209141731262207, Entropy: 138.9456787109375, Temp: 2.465937376022339, KL: 81.02011108398438, Loss: 0.02011922374367714, Learning Rate: 0.002470628999999999\n",
      "Epoch [5890/20000], Bound: 0.4083065092563629, Entropy: 140.53639221191406, Temp: 2.4661598205566406, KL: 76.79454040527344, Loss: 0.02115129865705967, Learning Rate: 0.002470628999999999\n",
      "Epoch [5891/20000], Bound: 0.3907303214073181, Entropy: 140.58529663085938, Temp: 2.4663918018341064, KL: 67.95657348632812, Loss: 0.028791148215532303, Learning Rate: 0.002470628999999999\n",
      "Epoch [5892/20000], Bound: 0.40972307324409485, Entropy: 140.37948608398438, Temp: 2.4664196968078613, KL: 78.29560852050781, Loss: 0.018951868638396263, Learning Rate: 0.002470628999999999\n",
      "Epoch [5893/20000], Bound: 0.4025047719478607, Entropy: 140.2559814453125, Temp: 2.466531753540039, KL: 75.56999206542969, Loss: 0.02021661028265953, Learning Rate: 0.002470628999999999\n",
      "Epoch [5894/20000], Bound: 0.4005815088748932, Entropy: 140.30335998535156, Temp: 2.46667742729187, KL: 73.70565795898438, Loss: 0.022869624197483063, Learning Rate: 0.002470628999999999\n",
      "Epoch [5895/20000], Bound: 0.42701876163482666, Entropy: 140.430419921875, Temp: 2.4667863845825195, KL: 81.8653564453125, Loss: 0.02211950160562992, Learning Rate: 0.002470628999999999\n",
      "Epoch [5896/20000], Bound: 0.41136404871940613, Entropy: 140.65774536132812, Temp: 2.466923952102661, KL: 77.76170349121094, Loss: 0.02101658284664154, Learning Rate: 0.002470628999999999\n",
      "Epoch [5897/20000], Bound: 0.3806994557380676, Entropy: 140.6843719482422, Temp: 2.467087507247925, KL: 69.56776428222656, Loss: 0.019781094044446945, Learning Rate: 0.002470628999999999\n",
      "Epoch [5898/20000], Bound: 0.3992111086845398, Entropy: 137.49378967285156, Temp: 2.467255115509033, KL: 74.48304748535156, Loss: 0.020499657839536667, Learning Rate: 0.002470628999999999\n",
      "Epoch [5899/20000], Bound: 0.4244389832019806, Entropy: 137.28207397460938, Temp: 2.4674384593963623, KL: 80.27011108398438, Loss: 0.02379428595304489, Learning Rate: 0.002470628999999999\n",
      "Epoch [5900/20000], Bound: 0.4042659103870392, Entropy: 139.71038818359375, Temp: 2.4675984382629395, KL: 76.18116760253906, Loss: 0.02002827636897564, Learning Rate: 0.002470628999999999\n",
      "Epoch [5901/20000], Bound: 0.3963093161582947, Entropy: 137.42820739746094, Temp: 2.467794179916382, KL: 73.79399108886719, Loss: 0.020210670307278633, Learning Rate: 0.002470628999999999\n",
      "Epoch [5902/20000], Bound: 0.38575348258018494, Entropy: 140.59405517578125, Temp: 2.4680047035217285, KL: 70.65972900390625, Loss: 0.020466739311814308, Learning Rate: 0.002470628999999999\n",
      "Epoch [5903/20000], Bound: 0.42078521847724915, Entropy: 137.45301818847656, Temp: 2.4682061672210693, KL: 79.75373840332031, Loss: 0.02264166995882988, Learning Rate: 0.002470628999999999\n",
      "Epoch [5904/20000], Bound: 0.39043286442756653, Entropy: 138.81082153320312, Temp: 2.468402862548828, KL: 70.21354675292969, Loss: 0.02406632900238037, Learning Rate: 0.002470628999999999\n",
      "Epoch [5905/20000], Bound: 0.42583152651786804, Entropy: 136.23672485351562, Temp: 2.4685113430023193, KL: 82.10166931152344, Loss: 0.020944884046912193, Learning Rate: 0.002470628999999999\n",
      "Epoch [5906/20000], Bound: 0.3851689100265503, Entropy: 138.64930725097656, Temp: 2.4686732292175293, KL: 70.46330261230469, Loss: 0.020537860691547394, Learning Rate: 0.002470628999999999\n",
      "Epoch [5907/20000], Bound: 0.3823833763599396, Entropy: 139.96800231933594, Temp: 2.468827247619629, KL: 70.41868591308594, Loss: 0.019038058817386627, Learning Rate: 0.002470628999999999\n",
      "Epoch [5908/20000], Bound: 0.37634241580963135, Entropy: 136.87359619140625, Temp: 2.469006299972534, KL: 69.54396057128906, Loss: 0.017380766570568085, Learning Rate: 0.002470628999999999\n",
      "Epoch [5909/20000], Bound: 0.3577687442302704, Entropy: 138.5814666748047, Temp: 2.469238758087158, KL: 64.08937072753906, Loss: 0.01805908977985382, Learning Rate: 0.002470628999999999\n",
      "Epoch [5910/20000], Bound: 0.37954390048980713, Entropy: 140.18136596679688, Temp: 2.469473361968994, KL: 70.25347900390625, Loss: 0.017764508724212646, Learning Rate: 0.002470628999999999\n",
      "Epoch [5911/20000], Bound: 0.40685659646987915, Entropy: 139.88294982910156, Temp: 2.4697513580322266, KL: 74.57975769042969, Loss: 0.02482748031616211, Learning Rate: 0.002470628999999999\n",
      "Epoch [5912/20000], Bound: 0.43865421414375305, Entropy: 138.32675170898438, Temp: 2.469940662384033, KL: 86.65374755859375, Loss: 0.019616205245256424, Learning Rate: 0.002470628999999999\n",
      "Epoch [5913/20000], Bound: 0.3738715648651123, Entropy: 138.91844177246094, Temp: 2.4702274799346924, KL: 67.04292297363281, Loss: 0.02106402814388275, Learning Rate: 0.002470628999999999\n",
      "Epoch [5914/20000], Bound: 0.3803257942199707, Entropy: 141.16738891601562, Temp: 2.47046160697937, KL: 70.55612182617188, Loss: 0.017608821392059326, Learning Rate: 0.002470628999999999\n",
      "Epoch [5915/20000], Bound: 0.40162697434425354, Entropy: 140.53753662109375, Temp: 2.4707436561584473, KL: 74.439208984375, Loss: 0.02204781025648117, Learning Rate: 0.002470628999999999\n",
      "Epoch [5916/20000], Bound: 0.3805248737335205, Entropy: 140.41798400878906, Temp: 2.4709951877593994, KL: 67.7864990234375, Loss: 0.02333315834403038, Learning Rate: 0.002470628999999999\n",
      "Epoch [5917/20000], Bound: 0.3980267345905304, Entropy: 139.1144561767578, Temp: 2.4711523056030273, KL: 74.19554138183594, Loss: 0.020442122593522072, Learning Rate: 0.002470628999999999\n",
      "Epoch [5918/20000], Bound: 0.3961957097053528, Entropy: 140.21456909179688, Temp: 2.4713239669799805, KL: 74.08558654785156, Loss: 0.0196010060608387, Learning Rate: 0.002470628999999999\n",
      "Epoch [5919/20000], Bound: 0.42261970043182373, Entropy: 139.05056762695312, Temp: 2.4715261459350586, KL: 79.94717407226562, Loss: 0.02340465411543846, Learning Rate: 0.002470628999999999\n",
      "Epoch [5920/20000], Bound: 0.38421136140823364, Entropy: 139.6735076904297, Temp: 2.4717068672180176, KL: 71.34243774414062, Loss: 0.01824839971959591, Learning Rate: 0.002470628999999999\n",
      "Epoch [5921/20000], Bound: 0.4138782024383545, Entropy: 140.96923828125, Temp: 2.471930980682373, KL: 76.799072265625, Loss: 0.024530857801437378, Learning Rate: 0.002470628999999999\n",
      "Epoch [5922/20000], Bound: 0.39242392778396606, Entropy: 142.913818359375, Temp: 2.4720892906188965, KL: 72.92987060546875, Loss: 0.019762450829148293, Learning Rate: 0.002470628999999999\n",
      "Epoch [5923/20000], Bound: 0.4135868549346924, Entropy: 144.77322387695312, Temp: 2.472269296646118, KL: 78.22122192382812, Loss: 0.021485324949026108, Learning Rate: 0.002470628999999999\n",
      "Epoch [5924/20000], Bound: 0.4078952968120575, Entropy: 143.12185668945312, Temp: 2.472460985183716, KL: 77.44206237792969, Loss: 0.01968541368842125, Learning Rate: 0.002470628999999999\n",
      "Epoch [5925/20000], Bound: 0.4033389091491699, Entropy: 142.33013916015625, Temp: 2.4726974964141846, KL: 75.46296691894531, Loss: 0.021006526425480843, Learning Rate: 0.002470628999999999\n",
      "Epoch [5926/20000], Bound: 0.4085688889026642, Entropy: 142.89169311523438, Temp: 2.4729347229003906, KL: 76.38517761230469, Loss: 0.022227367386221886, Learning Rate: 0.002470628999999999\n",
      "Epoch [5927/20000], Bound: 0.38406988978385925, Entropy: 141.98638916015625, Temp: 2.473151206970215, KL: 69.83042907714844, Loss: 0.021242983639240265, Learning Rate: 0.002470628999999999\n",
      "Epoch [5928/20000], Bound: 0.3936704993247986, Entropy: 141.5170440673828, Temp: 2.4733331203460693, KL: 68.78193664550781, Loss: 0.028884850442409515, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5929/20000], Bound: 0.40408939123153687, Entropy: 144.27288818359375, Temp: 2.47331166267395, KL: 75.72294616699219, Loss: 0.02092994935810566, Learning Rate: 0.002470628999999999\n",
      "Epoch [5930/20000], Bound: 0.4013098478317261, Entropy: 143.5557403564453, Temp: 2.4733195304870605, KL: 73.34370422363281, Loss: 0.02410966530442238, Learning Rate: 0.002470628999999999\n",
      "Epoch [5931/20000], Bound: 0.40719056129455566, Entropy: 140.74172973632812, Temp: 2.473271608352661, KL: 76.63874816894531, Loss: 0.020904673263430595, Learning Rate: 0.002470628999999999\n",
      "Epoch [5932/20000], Bound: 0.38666608929634094, Entropy: 142.99203491210938, Temp: 2.4732611179351807, KL: 70.75050354003906, Loss: 0.02086963877081871, Learning Rate: 0.002470628999999999\n",
      "Epoch [5933/20000], Bound: 0.42011842131614685, Entropy: 140.70765686035156, Temp: 2.4732518196105957, KL: 81.11824035644531, Loss: 0.019553545862436295, Learning Rate: 0.002470628999999999\n",
      "Epoch [5934/20000], Bound: 0.38418638706207275, Entropy: 141.2160186767578, Temp: 2.473329782485962, KL: 71.03659057617188, Loss: 0.018873300403356552, Learning Rate: 0.002470628999999999\n",
      "Epoch [5935/20000], Bound: 0.4031991958618164, Entropy: 137.5380859375, Temp: 2.4734458923339844, KL: 74.13925170898438, Loss: 0.02361036278307438, Learning Rate: 0.002470628999999999\n",
      "Epoch [5936/20000], Bound: 0.4139583110809326, Entropy: 137.73367309570312, Temp: 2.473510265350342, KL: 79.55828857421875, Loss: 0.019021354615688324, Learning Rate: 0.002470628999999999\n",
      "Epoch [5937/20000], Bound: 0.41857266426086426, Entropy: 140.01927185058594, Temp: 2.4736576080322266, KL: 81.13162231445312, Loss: 0.018603811040520668, Learning Rate: 0.002470628999999999\n",
      "Epoch [5938/20000], Bound: 0.3784392774105072, Entropy: 139.55856323242188, Temp: 2.473897695541382, KL: 68.256103515625, Loss: 0.021231286227703094, Learning Rate: 0.002470628999999999\n",
      "Epoch [5939/20000], Bound: 0.37224864959716797, Entropy: 140.879638671875, Temp: 2.474091053009033, KL: 66.64715576171875, Loss: 0.020993471145629883, Learning Rate: 0.002470628999999999\n",
      "Epoch [5940/20000], Bound: 0.409273624420166, Entropy: 138.84780883789062, Temp: 2.474238395690918, KL: 77.45925903320312, Loss: 0.02049114927649498, Learning Rate: 0.002470628999999999\n",
      "Epoch [5941/20000], Bound: 0.4022831618785858, Entropy: 135.8368682861328, Temp: 2.474416732788086, KL: 75.28297424316406, Loss: 0.020773999392986298, Learning Rate: 0.002470628999999999\n",
      "Epoch [5942/20000], Bound: 0.4178071618080139, Entropy: 138.02890014648438, Temp: 2.474605083465576, KL: 79.2069091796875, Loss: 0.02204839698970318, Learning Rate: 0.002470628999999999\n",
      "Epoch [5943/20000], Bound: 0.39494138956069946, Entropy: 138.7556610107422, Temp: 2.4747962951660156, KL: 70.75611877441406, Loss: 0.025645965710282326, Learning Rate: 0.002470628999999999\n",
      "Epoch [5944/20000], Bound: 0.39203941822052, Entropy: 137.26187133789062, Temp: 2.4748635292053223, KL: 71.9599609375, Loss: 0.02153564803302288, Learning Rate: 0.002470628999999999\n",
      "Epoch [5945/20000], Bound: 0.38978663086891174, Entropy: 138.72515869140625, Temp: 2.4749157428741455, KL: 71.1239013671875, Loss: 0.021926797926425934, Learning Rate: 0.002470628999999999\n",
      "Epoch [5946/20000], Bound: 0.41449791193008423, Entropy: 138.24229431152344, Temp: 2.474940776824951, KL: 77.80953979492188, Loss: 0.02289745956659317, Learning Rate: 0.002470628999999999\n",
      "Epoch [5947/20000], Bound: 0.3605230450630188, Entropy: 137.60438537597656, Temp: 2.474958896636963, KL: 62.158203125, Loss: 0.023541247472167015, Learning Rate: 0.002470628999999999\n",
      "Epoch [5948/20000], Bound: 0.38954973220825195, Entropy: 135.8216552734375, Temp: 2.4748640060424805, KL: 72.16677856445312, Loss: 0.019682997837662697, Learning Rate: 0.002470628999999999\n",
      "Epoch [5949/20000], Bound: 0.41386356949806213, Entropy: 138.6601104736328, Temp: 2.4748120307922363, KL: 77.3848876953125, Loss: 0.023375486955046654, Learning Rate: 0.002470628999999999\n",
      "Epoch [5950/20000], Bound: 0.42969369888305664, Entropy: 137.8083953857422, Temp: 2.474748134613037, KL: 84.04295349121094, Loss: 0.019468264654278755, Learning Rate: 0.002470628999999999\n",
      "Epoch [5951/20000], Bound: 0.430803120136261, Entropy: 138.06375122070312, Temp: 2.474794626235962, KL: 81.29545593261719, Loss: 0.025697464123368263, Learning Rate: 0.002470628999999999\n",
      "Epoch [5952/20000], Bound: 0.38418158888816833, Entropy: 136.6130828857422, Temp: 2.4747912883758545, KL: 69.05984497070312, Loss: 0.022882657125592232, Learning Rate: 0.002470628999999999\n",
      "Epoch [5953/20000], Bound: 0.4325336217880249, Entropy: 135.7707977294922, Temp: 2.474733591079712, KL: 84.21841430664062, Loss: 0.02085013873875141, Learning Rate: 0.002470628999999999\n",
      "Epoch [5954/20000], Bound: 0.3763200044631958, Entropy: 138.6385955810547, Temp: 2.474756956100464, KL: 65.637451171875, Loss: 0.025333108380436897, Learning Rate: 0.002470628999999999\n",
      "Epoch [5955/20000], Bound: 0.3758043646812439, Entropy: 137.79917907714844, Temp: 2.4746484756469727, KL: 68.48388671875, Loss: 0.019289808347821236, Learning Rate: 0.002470628999999999\n",
      "Epoch [5956/20000], Bound: 0.4406227469444275, Entropy: 137.505126953125, Temp: 2.474571704864502, KL: 85.25212097167969, Loss: 0.023746900260448456, Learning Rate: 0.002470628999999999\n",
      "Epoch [5957/20000], Bound: 0.4185188412666321, Entropy: 139.21913146972656, Temp: 2.4745216369628906, KL: 78.96282958984375, Loss: 0.022967226803302765, Learning Rate: 0.002470628999999999\n",
      "Epoch [5958/20000], Bound: 0.4116862118244171, Entropy: 138.609619140625, Temp: 2.474477529525757, KL: 77.37312316894531, Loss: 0.022099191322922707, Learning Rate: 0.002470628999999999\n",
      "Epoch [5959/20000], Bound: 0.38615882396698, Entropy: 138.27108764648438, Temp: 2.4744479656219482, KL: 68.86576843261719, Loss: 0.02440163865685463, Learning Rate: 0.002470628999999999\n",
      "Epoch [5960/20000], Bound: 0.39279407262802124, Entropy: 138.4937286376953, Temp: 2.474332094192505, KL: 72.27911376953125, Loss: 0.021320277824997902, Learning Rate: 0.002470628999999999\n",
      "Epoch [5961/20000], Bound: 0.41500356793403625, Entropy: 139.89944458007812, Temp: 2.4742259979248047, KL: 79.14341735839844, Loss: 0.020494045689702034, Learning Rate: 0.002470628999999999\n",
      "Epoch [5962/20000], Bound: 0.414309024810791, Entropy: 139.0913848876953, Temp: 2.474186420440674, KL: 76.97154235839844, Loss: 0.024468116462230682, Learning Rate: 0.002470628999999999\n",
      "Epoch [5963/20000], Bound: 0.3833574652671814, Entropy: 139.085205078125, Temp: 2.474107503890991, KL: 69.76507568359375, Loss: 0.020979609340429306, Learning Rate: 0.002470628999999999\n",
      "Epoch [5964/20000], Bound: 0.4208224415779114, Entropy: 138.50180053710938, Temp: 2.4740281105041504, KL: 78.60502624511719, Loss: 0.025068415328860283, Learning Rate: 0.002470628999999999\n",
      "Epoch [5965/20000], Bound: 0.3873421251773834, Entropy: 138.65708923339844, Temp: 2.473909854888916, KL: 70.54226684570312, Loss: 0.021685868501663208, Learning Rate: 0.002470628999999999\n",
      "Epoch [5966/20000], Bound: 0.4203397333621979, Entropy: 139.72467041015625, Temp: 2.4737837314605713, KL: 81.33500671386719, Loss: 0.01925678923726082, Learning Rate: 0.002470628999999999\n",
      "Epoch [5967/20000], Bound: 0.418081134557724, Entropy: 137.7145233154297, Temp: 2.4737656116485596, KL: 79.03663635253906, Loss: 0.02254495956003666, Learning Rate: 0.002470628999999999\n",
      "Epoch [5968/20000], Bound: 0.4018489718437195, Entropy: 138.3831024169922, Temp: 2.473759889602661, KL: 74.2724609375, Loss: 0.02255331724882126, Learning Rate: 0.002470628999999999\n",
      "Epoch [5969/20000], Bound: 0.4073770344257355, Entropy: 138.71267700195312, Temp: 2.473738193511963, KL: 77.22050476074219, Loss: 0.01984531059861183, Learning Rate: 0.002470628999999999\n",
      "Epoch [5970/20000], Bound: 0.399539053440094, Entropy: 139.5247802734375, Temp: 2.473778009414673, KL: 74.10356140136719, Loss: 0.021544285118579865, Learning Rate: 0.002470628999999999\n",
      "Epoch [5971/20000], Bound: 0.40397730469703674, Entropy: 139.1952362060547, Temp: 2.47381854057312, KL: 75.03616333007812, Loss: 0.022259023040533066, Learning Rate: 0.002470628999999999\n",
      "Epoch [5972/20000], Bound: 0.4052911698818207, Entropy: 138.20660400390625, Temp: 2.473849058151245, KL: 77.09947204589844, Loss: 0.018861807882785797, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5973/20000], Bound: 0.40815800428390503, Entropy: 138.6337890625, Temp: 2.473957061767578, KL: 76.09625244140625, Loss: 0.022581912577152252, Learning Rate: 0.002470628999999999\n",
      "Epoch [5974/20000], Bound: 0.3729643225669861, Entropy: 138.26292419433594, Temp: 2.4740476608276367, KL: 65.90939331054688, Loss: 0.022886116057634354, Learning Rate: 0.002470628999999999\n",
      "Epoch [5975/20000], Bound: 0.40447431802749634, Entropy: 137.8999786376953, Temp: 2.4740548133850098, KL: 75.47689819335938, Loss: 0.02166340872645378, Learning Rate: 0.002470628999999999\n",
      "Epoch [5976/20000], Bound: 0.4198271930217743, Entropy: 138.00697326660156, Temp: 2.4740707874298096, KL: 80.89595031738281, Loss: 0.019840244203805923, Learning Rate: 0.002470628999999999\n",
      "Epoch [5977/20000], Bound: 0.4150852560997009, Entropy: 140.23452758789062, Temp: 2.474165678024292, KL: 78.92970275878906, Loss: 0.020973870530724525, Learning Rate: 0.002470628999999999\n",
      "Epoch [5978/20000], Bound: 0.4063941538333893, Entropy: 137.88726806640625, Temp: 2.4742953777313232, KL: 76.61883544921875, Loss: 0.020489167422056198, Learning Rate: 0.002470628999999999\n",
      "Epoch [5979/20000], Bound: 0.3902226388454437, Entropy: 137.87564086914062, Temp: 2.474454164505005, KL: 72.20053100585938, Loss: 0.01999693363904953, Learning Rate: 0.002470628999999999\n",
      "Epoch [5980/20000], Bound: 0.3816765248775482, Entropy: 137.7183380126953, Temp: 2.4746243953704834, KL: 69.24539184570312, Loss: 0.02107815258204937, Learning Rate: 0.002470628999999999\n",
      "Epoch [5981/20000], Bound: 0.39346668124198914, Entropy: 138.6318359375, Temp: 2.4747633934020996, KL: 72.4066162109375, Loss: 0.021456867456436157, Learning Rate: 0.002470628999999999\n",
      "Epoch [5982/20000], Bound: 0.44611692428588867, Entropy: 137.31576538085938, Temp: 2.474884510040283, KL: 85.34768676757812, Loss: 0.026981739327311516, Learning Rate: 0.002470628999999999\n",
      "Epoch [5983/20000], Bound: 0.3762351870536804, Entropy: 137.0712432861328, Temp: 2.4749433994293213, KL: 68.75430297851562, Loss: 0.018990090116858482, Learning Rate: 0.002470628999999999\n",
      "Epoch [5984/20000], Bound: 0.3975362777709961, Entropy: 141.3380889892578, Temp: 2.4750258922576904, KL: 73.52085876464844, Loss: 0.021570224314928055, Learning Rate: 0.002470628999999999\n",
      "Epoch [5985/20000], Bound: 0.3903464376926422, Entropy: 139.0467987060547, Temp: 2.475100040435791, KL: 70.91595458984375, Loss: 0.022671321406960487, Learning Rate: 0.002470628999999999\n",
      "Epoch [5986/20000], Bound: 0.4067803919315338, Entropy: 138.90493774414062, Temp: 2.4751265048980713, KL: 76.67179870605469, Loss: 0.02062128111720085, Learning Rate: 0.002470628999999999\n",
      "Epoch [5987/20000], Bound: 0.38567063212394714, Entropy: 139.90577697753906, Temp: 2.475189447402954, KL: 70.37278747558594, Loss: 0.02108578197658062, Learning Rate: 0.002470628999999999\n",
      "Epoch [5988/20000], Bound: 0.36614251136779785, Entropy: 139.84742736816406, Temp: 2.4752378463745117, KL: 64.61039733886719, Loss: 0.02170649543404579, Learning Rate: 0.002470628999999999\n",
      "Epoch [5989/20000], Bound: 0.39767444133758545, Entropy: 139.22183227539062, Temp: 2.4752249717712402, KL: 74.48480224609375, Loss: 0.01970595121383667, Learning Rate: 0.002470628999999999\n",
      "Epoch [5990/20000], Bound: 0.4220416247844696, Entropy: 139.626220703125, Temp: 2.4752602577209473, KL: 79.31803894042969, Loss: 0.024379074573516846, Learning Rate: 0.002470628999999999\n",
      "Epoch [5991/20000], Bound: 0.3985321819782257, Entropy: 138.8892364501953, Temp: 2.4752631187438965, KL: 72.19258117675781, Loss: 0.0248362198472023, Learning Rate: 0.002470628999999999\n",
      "Epoch [5992/20000], Bound: 0.4002170264720917, Entropy: 140.80039978027344, Temp: 2.475184917449951, KL: 75.31973266601562, Loss: 0.01950126327574253, Learning Rate: 0.002470628999999999\n",
      "Epoch [5993/20000], Bound: 0.4075571894645691, Entropy: 141.40966796875, Temp: 2.475170612335205, KL: 76.55770874023438, Loss: 0.02131071500480175, Learning Rate: 0.002470628999999999\n",
      "Epoch [5994/20000], Bound: 0.3883574903011322, Entropy: 139.87582397460938, Temp: 2.4751811027526855, KL: 70.87461853027344, Loss: 0.02161196619272232, Learning Rate: 0.002470628999999999\n",
      "Epoch [5995/20000], Bound: 0.35775527358055115, Entropy: 137.9862823486328, Temp: 2.475173234939575, KL: 63.46516418457031, Loss: 0.019376980140805244, Learning Rate: 0.002470628999999999\n",
      "Epoch [5996/20000], Bound: 0.4094618260860443, Entropy: 140.00534057617188, Temp: 2.4751555919647217, KL: 76.62730407714844, Loss: 0.022295977920293808, Learning Rate: 0.002470628999999999\n",
      "Epoch [5997/20000], Bound: 0.37691599130630493, Entropy: 142.06808471679688, Temp: 2.4751412868499756, KL: 70.45576477050781, Loss: 0.01593991369009018, Learning Rate: 0.002470628999999999\n",
      "Epoch [5998/20000], Bound: 0.37900227308273315, Entropy: 140.67608642578125, Temp: 2.4752368927001953, KL: 70.03569030761719, Loss: 0.017970344051718712, Learning Rate: 0.002470628999999999\n",
      "Epoch [5999/20000], Bound: 0.3973757028579712, Entropy: 140.12741088867188, Temp: 2.4753830432891846, KL: 72.09809875488281, Loss: 0.02435510978102684, Learning Rate: 0.002470628999999999\n",
      "Epoch [6000/20000], Bound: 0.41026487946510315, Entropy: 140.51515197753906, Temp: 2.4754438400268555, KL: 75.36383056640625, Loss: 0.02532748132944107, Learning Rate: 0.002470628999999999\n",
      "Epoch [6001/20000], Bound: 0.40950894355773926, Entropy: 139.18885803222656, Temp: 2.4754252433776855, KL: 75.35258483886719, Loss: 0.024902191013097763, Learning Rate: 0.002470628999999999\n",
      "Epoch [6002/20000], Bound: 0.3671893775463104, Entropy: 142.09915161132812, Temp: 2.475344657897949, KL: 64.56172180175781, Loss: 0.022389041259884834, Learning Rate: 0.002470628999999999\n",
      "Epoch [6003/20000], Bound: 0.39231032133102417, Entropy: 141.3713836669922, Temp: 2.4751994609832764, KL: 72.41105651855469, Loss: 0.020784873515367508, Learning Rate: 0.002470628999999999\n",
      "Epoch [6004/20000], Bound: 0.3872080147266388, Entropy: 140.23684692382812, Temp: 2.475079298019409, KL: 69.09658813476562, Loss: 0.024543074890971184, Learning Rate: 0.002470628999999999\n",
      "Epoch [6005/20000], Bound: 0.39386892318725586, Entropy: 141.896240234375, Temp: 2.4748778343200684, KL: 73.15188598632812, Loss: 0.020185407251119614, Learning Rate: 0.002470628999999999\n",
      "Epoch [6006/20000], Bound: 0.43060436844825745, Entropy: 141.1549530029297, Temp: 2.4747250080108643, KL: 83.09347534179688, Loss: 0.02194228209555149, Learning Rate: 0.002470628999999999\n",
      "Epoch [6007/20000], Bound: 0.3775482773780823, Entropy: 142.3234405517578, Temp: 2.4746341705322266, KL: 67.46139526367188, Loss: 0.022340860217809677, Learning Rate: 0.002470628999999999\n",
      "Epoch [6008/20000], Bound: 0.422197550535202, Entropy: 142.41526794433594, Temp: 2.474498748779297, KL: 81.6396484375, Loss: 0.019771745428442955, Learning Rate: 0.002470628999999999\n",
      "Epoch [6009/20000], Bound: 0.3860929310321808, Entropy: 141.74339294433594, Temp: 2.474463701248169, KL: 71.25975036621094, Loss: 0.019526707008481026, Learning Rate: 0.002470628999999999\n",
      "Epoch [6010/20000], Bound: 0.41205915808677673, Entropy: 139.69178771972656, Temp: 2.4744648933410645, KL: 77.46504211425781, Loss: 0.0221349336206913, Learning Rate: 0.002470628999999999\n",
      "Epoch [6011/20000], Bound: 0.4121943414211273, Entropy: 140.50982666015625, Temp: 2.4744765758514404, KL: 76.911865234375, Loss: 0.023333227261900902, Learning Rate: 0.002470628999999999\n",
      "Epoch [6012/20000], Bound: 0.4313128590583801, Entropy: 138.41876220703125, Temp: 2.4744675159454346, KL: 82.93107604980469, Loss: 0.02269967459142208, Learning Rate: 0.002470628999999999\n",
      "Epoch [6013/20000], Bound: 0.4073750376701355, Entropy: 138.20916748046875, Temp: 2.4744889736175537, KL: 76.17340087890625, Loss: 0.021970484405755997, Learning Rate: 0.002470628999999999\n",
      "Epoch [6014/20000], Bound: 0.38168200850486755, Entropy: 139.85482788085938, Temp: 2.474514961242676, KL: 68.29977416992188, Loss: 0.02299073152244091, Learning Rate: 0.002470628999999999\n",
      "Epoch [6015/20000], Bound: 0.4088251292705536, Entropy: 140.05760192871094, Temp: 2.4744749069213867, KL: 77.27500915527344, Loss: 0.02060132473707199, Learning Rate: 0.002470628999999999\n",
      "Epoch [6016/20000], Bound: 0.39223408699035645, Entropy: 140.7903289794922, Temp: 2.474483013153076, KL: 72.03953552246094, Loss: 0.021482666954398155, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6017/20000], Bound: 0.4368816614151001, Entropy: 138.07737731933594, Temp: 2.474483013153076, KL: 85.2078857421875, Loss: 0.021520303562283516, Learning Rate: 0.002470628999999999\n",
      "Epoch [6018/20000], Bound: 0.39488649368286133, Entropy: 139.16876220703125, Temp: 2.4745516777038574, KL: 73.71893310546875, Loss: 0.019624944776296616, Learning Rate: 0.002470628999999999\n",
      "Epoch [6019/20000], Bound: 0.425514817237854, Entropy: 137.07550048828125, Temp: 2.474658727645874, KL: 79.90219116210938, Loss: 0.02529120072722435, Learning Rate: 0.002470628999999999\n",
      "Epoch [6020/20000], Bound: 0.39777153730392456, Entropy: 137.5640106201172, Temp: 2.4747092723846436, KL: 74.93489074707031, Loss: 0.018846236169338226, Learning Rate: 0.002470628999999999\n",
      "Epoch [6021/20000], Bound: 0.37805601954460144, Entropy: 138.59234619140625, Temp: 2.474824905395508, KL: 69.68060302734375, Loss: 0.01814662478864193, Learning Rate: 0.002470628999999999\n",
      "Epoch [6022/20000], Bound: 0.4267956018447876, Entropy: 138.3097686767578, Temp: 2.4749836921691895, KL: 81.60816955566406, Loss: 0.022626450285315514, Learning Rate: 0.002470628999999999\n",
      "Epoch [6023/20000], Bound: 0.42845088243484497, Entropy: 136.26046752929688, Temp: 2.4751501083374023, KL: 82.75091552734375, Loss: 0.02132725715637207, Learning Rate: 0.002470628999999999\n",
      "Epoch [6024/20000], Bound: 0.40435656905174255, Entropy: 138.2045135498047, Temp: 2.475358486175537, KL: 74.21261596679688, Loss: 0.024165024980902672, Learning Rate: 0.002470628999999999\n",
      "Epoch [6025/20000], Bound: 0.4276271164417267, Entropy: 136.22312927246094, Temp: 2.4754912853240967, KL: 82.54920959472656, Loss: 0.02123858593404293, Learning Rate: 0.002470628999999999\n",
      "Epoch [6026/20000], Bound: 0.41043779253959656, Entropy: 136.64881896972656, Temp: 2.475670099258423, KL: 77.49754333496094, Loss: 0.021123314276337624, Learning Rate: 0.002470628999999999\n",
      "Epoch [6027/20000], Bound: 0.42851361632347107, Entropy: 138.0524444580078, Temp: 2.4758639335632324, KL: 82.64959716796875, Loss: 0.021581020206212997, Learning Rate: 0.002470628999999999\n",
      "Epoch [6028/20000], Bound: 0.40057745575904846, Entropy: 139.05203247070312, Temp: 2.476090669631958, KL: 74.60069274902344, Loss: 0.0211760476231575, Learning Rate: 0.002470628999999999\n",
      "Epoch [6029/20000], Bound: 0.38380125164985657, Entropy: 137.0330810546875, Temp: 2.476309061050415, KL: 70.30581665039062, Loss: 0.020166391506791115, Learning Rate: 0.002470628999999999\n",
      "Epoch [6030/20000], Bound: 0.4090900123119354, Entropy: 137.1180419921875, Temp: 2.476517677307129, KL: 77.51544189453125, Loss: 0.020300881937146187, Learning Rate: 0.002470628999999999\n",
      "Epoch [6031/20000], Bound: 0.399228572845459, Entropy: 139.48123168945312, Temp: 2.4767563343048096, KL: 74.99380493164062, Loss: 0.01960386149585247, Learning Rate: 0.002470628999999999\n",
      "Epoch [6032/20000], Bound: 0.3946448266506195, Entropy: 136.4443359375, Temp: 2.477023124694824, KL: 74.41268920898438, Loss: 0.018116896972060204, Learning Rate: 0.002470628999999999\n",
      "Epoch [6033/20000], Bound: 0.40170976519584656, Entropy: 139.56719970703125, Temp: 2.4773457050323486, KL: 75.5982666015625, Loss: 0.019840773195028305, Learning Rate: 0.002470628999999999\n",
      "Epoch [6034/20000], Bound: 0.42028316855430603, Entropy: 137.18045043945312, Temp: 2.4776859283447266, KL: 78.99598693847656, Loss: 0.02400313876569271, Learning Rate: 0.002470628999999999\n",
      "Epoch [6035/20000], Bound: 0.3872687816619873, Entropy: 139.3220977783203, Temp: 2.477968215942383, KL: 72.23033142089844, Loss: 0.018284831196069717, Learning Rate: 0.002470628999999999\n",
      "Epoch [6036/20000], Bound: 0.4123347997665405, Entropy: 137.58067321777344, Temp: 2.4782874584198, KL: 77.65316772460938, Loss: 0.021970782428979874, Learning Rate: 0.002470628999999999\n",
      "Epoch [6037/20000], Bound: 0.4044707715511322, Entropy: 138.69631958007812, Temp: 2.478588104248047, KL: 75.37960815429688, Loss: 0.021916737779974937, Learning Rate: 0.002470628999999999\n",
      "Epoch [6038/20000], Bound: 0.4077019989490509, Entropy: 138.4765625, Temp: 2.4788594245910645, KL: 76.36724853515625, Loss: 0.021829882636666298, Learning Rate: 0.002470628999999999\n",
      "Epoch [6039/20000], Bound: 0.4098086655139923, Entropy: 138.39732360839844, Temp: 2.479112148284912, KL: 77.27841186523438, Loss: 0.021240195259451866, Learning Rate: 0.002470628999999999\n",
      "Epoch [6040/20000], Bound: 0.4113944470882416, Entropy: 141.01145935058594, Temp: 2.4793667793273926, KL: 79.65029907226562, Loss: 0.017399782314896584, Learning Rate: 0.002470628999999999\n",
      "Epoch [6041/20000], Bound: 0.38473808765411377, Entropy: 138.20101928710938, Temp: 2.4797234535217285, KL: 71.683837890625, Loss: 0.017962679266929626, Learning Rate: 0.002470628999999999\n",
      "Epoch [6042/20000], Bound: 0.3992340862751007, Entropy: 138.51104736328125, Temp: 2.4801130294799805, KL: 73.31031799316406, Loss: 0.023045964539051056, Learning Rate: 0.002470628999999999\n",
      "Epoch [6043/20000], Bound: 0.40519627928733826, Entropy: 136.42103576660156, Temp: 2.480426073074341, KL: 74.85758972167969, Loss: 0.023418709635734558, Learning Rate: 0.002470628999999999\n",
      "Epoch [6044/20000], Bound: 0.40415889024734497, Entropy: 137.7578582763672, Temp: 2.4806699752807617, KL: 73.47444152832031, Loss: 0.02560056932270527, Learning Rate: 0.002470628999999999\n",
      "Epoch [6045/20000], Bound: 0.41543740034103394, Entropy: 138.43251037597656, Temp: 2.4807941913604736, KL: 78.66706848144531, Loss: 0.021807340905070305, Learning Rate: 0.002470628999999999\n",
      "Epoch [6046/20000], Bound: 0.39361098408699036, Entropy: 137.74600219726562, Temp: 2.4809272289276123, KL: 72.63575744628906, Loss: 0.021153368055820465, Learning Rate: 0.002470628999999999\n",
      "Epoch [6047/20000], Bound: 0.3787105083465576, Entropy: 138.8004150390625, Temp: 2.4810473918914795, KL: 67.2882080078125, Loss: 0.023414233699440956, Learning Rate: 0.002470628999999999\n",
      "Epoch [6048/20000], Bound: 0.3765947222709656, Entropy: 137.1609649658203, Temp: 2.4810714721679688, KL: 67.30607604980469, Loss: 0.022183461114764214, Learning Rate: 0.002470628999999999\n",
      "Epoch [6049/20000], Bound: 0.3778758645057678, Entropy: 138.23597717285156, Temp: 2.481037139892578, KL: 69.20697021484375, Loss: 0.019075436517596245, Learning Rate: 0.002470628999999999\n",
      "Epoch [6050/20000], Bound: 0.4074908494949341, Entropy: 138.25225830078125, Temp: 2.48103404045105, KL: 77.12399291992188, Loss: 0.020208988338708878, Learning Rate: 0.002470628999999999\n",
      "Epoch [6051/20000], Bound: 0.4012381136417389, Entropy: 138.78761291503906, Temp: 2.481079578399658, KL: 74.27421569824219, Loss: 0.022283844649791718, Learning Rate: 0.002470628999999999\n",
      "Epoch [6052/20000], Bound: 0.40371203422546387, Entropy: 138.68284606933594, Temp: 2.481105089187622, KL: 75.76496887207031, Loss: 0.02072754316031933, Learning Rate: 0.002470628999999999\n",
      "Epoch [6053/20000], Bound: 0.38354820013046265, Entropy: 139.32418823242188, Temp: 2.481156349182129, KL: 68.98895263671875, Loss: 0.022733544930815697, Learning Rate: 0.002470628999999999\n",
      "Epoch [6054/20000], Bound: 0.37596243619918823, Entropy: 141.74310302734375, Temp: 2.481144428253174, KL: 68.52548217773438, Loss: 0.01937044970691204, Learning Rate: 0.002470628999999999\n",
      "Epoch [6055/20000], Bound: 0.40741556882858276, Entropy: 142.3614044189453, Temp: 2.4811501502990723, KL: 76.58174133300781, Loss: 0.021258993074297905, Learning Rate: 0.002470628999999999\n",
      "Epoch [6056/20000], Bound: 0.39349573850631714, Entropy: 140.4794464111328, Temp: 2.4811768531799316, KL: 72.0380859375, Loss: 0.022294294089078903, Learning Rate: 0.002470628999999999\n",
      "Epoch [6057/20000], Bound: 0.42912557721138, Entropy: 139.64895629882812, Temp: 2.481171131134033, KL: 83.25904846191406, Loss: 0.02080557495355606, Learning Rate: 0.002470628999999999\n",
      "Epoch [6058/20000], Bound: 0.38103538751602173, Entropy: 141.0017547607422, Temp: 2.481236457824707, KL: 68.9639892578125, Loss: 0.021356303244829178, Learning Rate: 0.002470628999999999\n",
      "Epoch [6059/20000], Bound: 0.3945719599723816, Entropy: 140.80966186523438, Temp: 2.4812684059143066, KL: 73.35029602050781, Loss: 0.02027326449751854, Learning Rate: 0.002470628999999999\n",
      "Epoch [6060/20000], Bound: 0.3806096017360687, Entropy: 139.59169006347656, Temp: 2.4813220500946045, KL: 69.47880554199219, Loss: 0.02007831446826458, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6061/20000], Bound: 0.4042302668094635, Entropy: 140.07498168945312, Temp: 2.4813761711120605, KL: 76.304931640625, Loss: 0.019946957007050514, Learning Rate: 0.002470628999999999\n",
      "Epoch [6062/20000], Bound: 0.3730437755584717, Entropy: 140.55599975585938, Temp: 2.4814746379852295, KL: 65.10066223144531, Loss: 0.02463408187031746, Learning Rate: 0.002470628999999999\n",
      "Epoch [6063/20000], Bound: 0.4004724323749542, Entropy: 139.1119842529297, Temp: 2.481436252593994, KL: 74.02668762207031, Loss: 0.022340072318911552, Learning Rate: 0.002470628999999999\n",
      "Epoch [6064/20000], Bound: 0.3722435534000397, Entropy: 141.36961364746094, Temp: 2.4813826084136963, KL: 68.298583984375, Loss: 0.017740599811077118, Learning Rate: 0.002470628999999999\n",
      "Epoch [6065/20000], Bound: 0.37512877583503723, Entropy: 142.01104736328125, Temp: 2.4813880920410156, KL: 68.06024169921875, Loss: 0.019841309636831284, Learning Rate: 0.002470628999999999\n",
      "Epoch [6066/20000], Bound: 0.4098603129386902, Entropy: 143.63624572753906, Temp: 2.4813954830169678, KL: 78.43310546875, Loss: 0.018975015729665756, Learning Rate: 0.002470628999999999\n",
      "Epoch [6067/20000], Bound: 0.4328884780406952, Entropy: 140.86874389648438, Temp: 2.4814867973327637, KL: 83.77091979980469, Loss: 0.022075751796364784, Learning Rate: 0.002470628999999999\n",
      "Epoch [6068/20000], Bound: 0.38359925150871277, Entropy: 142.02015686035156, Temp: 2.4816133975982666, KL: 71.59999084472656, Loss: 0.0175067987293005, Learning Rate: 0.002470628999999999\n",
      "Epoch [6069/20000], Bound: 0.399428129196167, Entropy: 144.85714721679688, Temp: 2.4818058013916016, KL: 74.11038208007812, Loss: 0.021567322313785553, Learning Rate: 0.002470628999999999\n",
      "Epoch [6070/20000], Bound: 0.37941277027130127, Entropy: 144.2308349609375, Temp: 2.481978416442871, KL: 65.48423767089844, Loss: 0.027455048635601997, Learning Rate: 0.002470628999999999\n",
      "Epoch [6071/20000], Bound: 0.4003884196281433, Entropy: 139.69454956054688, Temp: 2.481943130493164, KL: 70.64503479003906, Loss: 0.029109839349985123, Learning Rate: 0.002470628999999999\n",
      "Epoch [6072/20000], Bound: 0.3967365622520447, Entropy: 144.98684692382812, Temp: 2.481715679168701, KL: 71.773193359375, Loss: 0.02471027709543705, Learning Rate: 0.002470628999999999\n",
      "Epoch [6073/20000], Bound: 0.37204501032829285, Entropy: 143.296142578125, Temp: 2.4814236164093018, KL: 68.97410583496094, Loss: 0.016268683597445488, Learning Rate: 0.002470628999999999\n",
      "Epoch [6074/20000], Bound: 0.40206700563430786, Entropy: 141.0723876953125, Temp: 2.481252670288086, KL: 75.72071838378906, Loss: 0.019855549558997154, Learning Rate: 0.002470628999999999\n",
      "Epoch [6075/20000], Bound: 0.3932173252105713, Entropy: 141.38294982910156, Temp: 2.481147527694702, KL: 71.60446166992188, Loss: 0.023006996139883995, Learning Rate: 0.002470628999999999\n",
      "Epoch [6076/20000], Bound: 0.39577096700668335, Entropy: 143.46861267089844, Temp: 2.481003761291504, KL: 72.73529052734375, Loss: 0.022203542292118073, Learning Rate: 0.002470628999999999\n",
      "Epoch [6077/20000], Bound: 0.42277708649635315, Entropy: 141.581787109375, Temp: 2.480851173400879, KL: 82.45968627929688, Loss: 0.018565597012639046, Learning Rate: 0.002470628999999999\n",
      "Epoch [6078/20000], Bound: 0.3967669904232025, Entropy: 141.67633056640625, Temp: 2.4808313846588135, KL: 74.40878295898438, Loss: 0.019406266510486603, Learning Rate: 0.002470628999999999\n",
      "Epoch [6079/20000], Bound: 0.4196375906467438, Entropy: 140.9917755126953, Temp: 2.480865001678467, KL: 78.84321594238281, Loss: 0.02396608144044876, Learning Rate: 0.002470628999999999\n",
      "Epoch [6080/20000], Bound: 0.4036484360694885, Entropy: 138.0109100341797, Temp: 2.480868339538574, KL: 75.07296752929688, Loss: 0.02208176627755165, Learning Rate: 0.002470628999999999\n",
      "Epoch [6081/20000], Bound: 0.39707499742507935, Entropy: 138.69346618652344, Temp: 2.480865240097046, KL: 73.41560363769531, Loss: 0.021587194874882698, Learning Rate: 0.002470628999999999\n",
      "Epoch [6082/20000], Bound: 0.38223060965538025, Entropy: 140.7190399169922, Temp: 2.4808573722839355, KL: 69.48591613769531, Loss: 0.020979031920433044, Learning Rate: 0.002470628999999999\n",
      "Epoch [6083/20000], Bound: 0.3842138946056366, Entropy: 140.3872528076172, Temp: 2.480835437774658, KL: 71.61338806152344, Loss: 0.017819978296756744, Learning Rate: 0.002470628999999999\n",
      "Epoch [6084/20000], Bound: 0.428749680519104, Entropy: 137.08021545410156, Temp: 2.4808878898620605, KL: 84.74818420410156, Loss: 0.017571216449141502, Learning Rate: 0.002470628999999999\n",
      "Epoch [6085/20000], Bound: 0.3796001076698303, Entropy: 140.38514709472656, Temp: 2.481088638305664, KL: 68.61257934570312, Loss: 0.02124914526939392, Learning Rate: 0.002470628999999999\n",
      "Epoch [6086/20000], Bound: 0.40641915798187256, Entropy: 141.91790771484375, Temp: 2.4812428951263428, KL: 76.41873168945312, Loss: 0.021001895889639854, Learning Rate: 0.002470628999999999\n",
      "Epoch [6087/20000], Bound: 0.4007374048233032, Entropy: 138.7130584716797, Temp: 2.4814083576202393, KL: 74.31040954589844, Loss: 0.021922627463936806, Learning Rate: 0.002470628999999999\n",
      "Epoch [6088/20000], Bound: 0.4134432077407837, Entropy: 139.806640625, Temp: 2.4815497398376465, KL: 78.81411743164062, Loss: 0.020334133878350258, Learning Rate: 0.002470628999999999\n",
      "Epoch [6089/20000], Bound: 0.3933465778827667, Entropy: 140.52099609375, Temp: 2.4817328453063965, KL: 71.658935546875, Loss: 0.02297860011458397, Learning Rate: 0.002470628999999999\n",
      "Epoch [6090/20000], Bound: 0.3825627565383911, Entropy: 138.1411895751953, Temp: 2.481849193572998, KL: 71.15916442871094, Loss: 0.01780816540122032, Learning Rate: 0.002470628999999999\n",
      "Epoch [6091/20000], Bound: 0.4330040514469147, Entropy: 141.4806671142578, Temp: 2.482023239135742, KL: 84.09184265136719, Loss: 0.02150820754468441, Learning Rate: 0.002470628999999999\n",
      "Epoch [6092/20000], Bound: 0.40567776560783386, Entropy: 140.57867431640625, Temp: 2.4822394847869873, KL: 75.21429443359375, Loss: 0.023005345836281776, Learning Rate: 0.002470628999999999\n",
      "Epoch [6093/20000], Bound: 0.3649858832359314, Entropy: 138.59353637695312, Temp: 2.4824061393737793, KL: 66.97149658203125, Loss: 0.01637938618659973, Learning Rate: 0.002470628999999999\n",
      "Epoch [6094/20000], Bound: 0.3974708020687103, Entropy: 139.5928497314453, Temp: 2.4826338291168213, KL: 75.66281127929688, Loss: 0.017312902957201004, Learning Rate: 0.002470628999999999\n",
      "Epoch [6095/20000], Bound: 0.40177178382873535, Entropy: 139.15818786621094, Temp: 2.4829459190368652, KL: 76.83708190917969, Loss: 0.01745770312845707, Learning Rate: 0.002470628999999999\n",
      "Epoch [6096/20000], Bound: 0.4058365523815155, Entropy: 139.8721466064453, Temp: 2.483337640762329, KL: 75.71401977539062, Loss: 0.02210618183016777, Learning Rate: 0.002470628999999999\n",
      "Epoch [6097/20000], Bound: 0.41178420186042786, Entropy: 140.6876220703125, Temp: 2.4836854934692383, KL: 78.15594482421875, Loss: 0.02070486545562744, Learning Rate: 0.002470628999999999\n",
      "Epoch [6098/20000], Bound: 0.41518181562423706, Entropy: 141.9611358642578, Temp: 2.4840409755706787, KL: 77.60848999023438, Loss: 0.023830534890294075, Learning Rate: 0.002470628999999999\n",
      "Epoch [6099/20000], Bound: 0.39702725410461426, Entropy: 140.91526794433594, Temp: 2.484327793121338, KL: 72.64836120605469, Loss: 0.023146241903305054, Learning Rate: 0.002470628999999999\n",
      "Epoch [6100/20000], Bound: 0.38509392738342285, Entropy: 140.34579467773438, Temp: 2.4845378398895264, KL: 68.79301452636719, Loss: 0.024044612422585487, Learning Rate: 0.002470628999999999\n",
      "Epoch [6101/20000], Bound: 0.43288519978523254, Entropy: 141.0176544189453, Temp: 2.484633445739746, KL: 83.15682983398438, Loss: 0.023357464000582695, Learning Rate: 0.002470628999999999\n",
      "Epoch [6102/20000], Bound: 0.38115766644477844, Entropy: 138.16685485839844, Temp: 2.484729766845703, KL: 69.10459899902344, Loss: 0.0211813822388649, Learning Rate: 0.002470628999999999\n",
      "Epoch [6103/20000], Bound: 0.3744337856769562, Entropy: 139.00267028808594, Temp: 2.4847919940948486, KL: 66.08558654785156, Loss: 0.02346242591738701, Learning Rate: 0.002470628999999999\n",
      "Epoch [6104/20000], Bound: 0.38022109866142273, Entropy: 139.93125915527344, Temp: 2.484750747680664, KL: 69.31401062011719, Loss: 0.020229529589414597, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6105/20000], Bound: 0.4113444983959198, Entropy: 140.8292694091797, Temp: 2.484713315963745, KL: 78.6612548828125, Loss: 0.019441787153482437, Learning Rate: 0.002470628999999999\n",
      "Epoch [6106/20000], Bound: 0.3789873421192169, Entropy: 139.28570556640625, Temp: 2.4847536087036133, KL: 68.23480224609375, Loss: 0.02170318365097046, Learning Rate: 0.002470628999999999\n",
      "Epoch [6107/20000], Bound: 0.39755263924598694, Entropy: 140.9886474609375, Temp: 2.4847474098205566, KL: 71.90974426269531, Loss: 0.024942247197031975, Learning Rate: 0.002470628999999999\n",
      "Epoch [6108/20000], Bound: 0.4124750792980194, Entropy: 140.2921905517578, Temp: 2.484647274017334, KL: 78.5115966796875, Loss: 0.02041209116578102, Learning Rate: 0.002470628999999999\n",
      "Epoch [6109/20000], Bound: 0.37826916575431824, Entropy: 141.53387451171875, Temp: 2.484607696533203, KL: 68.62077331542969, Loss: 0.020519118756055832, Learning Rate: 0.002470628999999999\n",
      "Epoch [6110/20000], Bound: 0.39711615443229675, Entropy: 140.505615234375, Temp: 2.484560489654541, KL: 72.79934692382812, Loss: 0.022896666079759598, Learning Rate: 0.002470628999999999\n",
      "Epoch [6111/20000], Bound: 0.40557998418807983, Entropy: 137.58009338378906, Temp: 2.484476089477539, KL: 77.67147827148438, Loss: 0.01803065650165081, Learning Rate: 0.002470628999999999\n",
      "Epoch [6112/20000], Bound: 0.4059370756149292, Entropy: 138.76596069335938, Temp: 2.484501838684082, KL: 74.770751953125, Loss: 0.024078449234366417, Learning Rate: 0.002470628999999999\n",
      "Epoch [6113/20000], Bound: 0.43536487221717834, Entropy: 140.52157592773438, Temp: 2.4844679832458496, KL: 84.76171875, Loss: 0.021644718945026398, Learning Rate: 0.002470628999999999\n",
      "Epoch [6114/20000], Bound: 0.4058096408843994, Entropy: 139.84365844726562, Temp: 2.484496593475342, KL: 75.99851989746094, Loss: 0.02153264917433262, Learning Rate: 0.002470628999999999\n",
      "Epoch [6115/20000], Bound: 0.39383694529533386, Entropy: 140.9767608642578, Temp: 2.4845316410064697, KL: 71.95401000976562, Loss: 0.022699760273098946, Learning Rate: 0.002470628999999999\n",
      "Epoch [6116/20000], Bound: 0.4168015420436859, Entropy: 139.48728942871094, Temp: 2.48452091217041, KL: 78.4537353515625, Loss: 0.023101620376110077, Learning Rate: 0.002470628999999999\n",
      "Epoch [6117/20000], Bound: 0.3695875108242035, Entropy: 139.2875213623047, Temp: 2.484499454498291, KL: 65.89323425292969, Loss: 0.02113349922001362, Learning Rate: 0.002470628999999999\n",
      "Epoch [6118/20000], Bound: 0.3919103741645813, Entropy: 139.11410522460938, Temp: 2.484436511993408, KL: 71.90190124511719, Loss: 0.021692678332328796, Learning Rate: 0.002470628999999999\n",
      "Epoch [6119/20000], Bound: 0.3929077386856079, Entropy: 139.6835479736328, Temp: 2.484360694885254, KL: 72.67308044433594, Loss: 0.020714405924081802, Learning Rate: 0.002470628999999999\n",
      "Epoch [6120/20000], Bound: 0.42703646421432495, Entropy: 139.63192749023438, Temp: 2.4843015670776367, KL: 81.880859375, Loss: 0.022358978167176247, Learning Rate: 0.002470628999999999\n",
      "Epoch [6121/20000], Bound: 0.39823830127716064, Entropy: 138.94921875, Temp: 2.484273910522461, KL: 75.78335571289062, Loss: 0.01753903366625309, Learning Rate: 0.002470628999999999\n",
      "Epoch [6122/20000], Bound: 0.37270963191986084, Entropy: 140.15554809570312, Temp: 2.4843521118164062, KL: 67.05477905273438, Loss: 0.02054017223417759, Learning Rate: 0.002470628999999999\n",
      "Epoch [6123/20000], Bound: 0.4118708074092865, Entropy: 137.48568725585938, Temp: 2.484400510787964, KL: 79.54158020019531, Loss: 0.017977457493543625, Learning Rate: 0.002470628999999999\n",
      "Epoch [6124/20000], Bound: 0.37775254249572754, Entropy: 141.10433959960938, Temp: 2.484558582305908, KL: 66.97413635253906, Loss: 0.023540610447525978, Learning Rate: 0.002470628999999999\n",
      "Epoch [6125/20000], Bound: 0.412412166595459, Entropy: 137.99143981933594, Temp: 2.4846067428588867, KL: 77.22413635253906, Loss: 0.022965092211961746, Learning Rate: 0.002470628999999999\n",
      "Epoch [6126/20000], Bound: 0.4111267030239105, Entropy: 137.41360473632812, Temp: 2.4846339225769043, KL: 79.66529846191406, Loss: 0.01729116402566433, Learning Rate: 0.002470628999999999\n",
      "Epoch [6127/20000], Bound: 0.3935788571834564, Entropy: 138.92202758789062, Temp: 2.4847896099090576, KL: 73.19497680664062, Loss: 0.02005663514137268, Learning Rate: 0.002470628999999999\n",
      "Epoch [6128/20000], Bound: 0.38291579484939575, Entropy: 139.7606201171875, Temp: 2.484957218170166, KL: 69.1522216796875, Loss: 0.022086162120103836, Learning Rate: 0.002470628999999999\n",
      "Epoch [6129/20000], Bound: 0.41562318801879883, Entropy: 137.51527404785156, Temp: 2.4850621223449707, KL: 79.3406982421875, Loss: 0.020621415227651596, Learning Rate: 0.002470628999999999\n",
      "Epoch [6130/20000], Bound: 0.3931829631328583, Entropy: 138.83433532714844, Temp: 2.4852075576782227, KL: 74.34567260742188, Loss: 0.0175184179097414, Learning Rate: 0.002470628999999999\n",
      "Epoch [6131/20000], Bound: 0.40045255422592163, Entropy: 141.02169799804688, Temp: 2.4854331016540527, KL: 73.98971557617188, Loss: 0.02245187759399414, Learning Rate: 0.002470628999999999\n",
      "Epoch [6132/20000], Bound: 0.43679821491241455, Entropy: 138.6293487548828, Temp: 2.4856114387512207, KL: 86.31680297851562, Loss: 0.01941506750881672, Learning Rate: 0.002470628999999999\n",
      "Epoch [6133/20000], Bound: 0.4080142676830292, Entropy: 139.88565063476562, Temp: 2.485891580581665, KL: 76.14540100097656, Loss: 0.0225521232932806, Learning Rate: 0.002470628999999999\n",
      "Epoch [6134/20000], Bound: 0.4241439998149872, Entropy: 138.7068328857422, Temp: 2.4861292839050293, KL: 79.47482299804688, Loss: 0.025475451722741127, Learning Rate: 0.002470628999999999\n",
      "Epoch [6135/20000], Bound: 0.41313689947128296, Entropy: 139.25511169433594, Temp: 2.486280918121338, KL: 77.31465148925781, Loss: 0.023234758526086807, Learning Rate: 0.002470628999999999\n",
      "Epoch [6136/20000], Bound: 0.393845796585083, Entropy: 139.39927673339844, Temp: 2.486394166946411, KL: 70.96275329589844, Loss: 0.024719668552279472, Learning Rate: 0.002470628999999999\n",
      "Epoch [6137/20000], Bound: 0.42067041993141174, Entropy: 140.0679473876953, Temp: 2.486398220062256, KL: 81.1068115234375, Loss: 0.020106801763176918, Learning Rate: 0.002470628999999999\n",
      "Epoch [6138/20000], Bound: 0.4233259856700897, Entropy: 138.88316345214844, Temp: 2.4864745140075684, KL: 79.67292785644531, Loss: 0.024588320404291153, Learning Rate: 0.002470628999999999\n",
      "Epoch [6139/20000], Bound: 0.38946714997291565, Entropy: 137.78163146972656, Temp: 2.486502170562744, KL: 72.54666137695312, Loss: 0.01901685632765293, Learning Rate: 0.002470628999999999\n",
      "Epoch [6140/20000], Bound: 0.40690112113952637, Entropy: 137.88748168945312, Temp: 2.486574649810791, KL: 75.939697265625, Loss: 0.022319283336400986, Learning Rate: 0.002470628999999999\n",
      "Epoch [6141/20000], Bound: 0.39199674129486084, Entropy: 138.67044067382812, Temp: 2.4866294860839844, KL: 71.61459350585938, Loss: 0.02234589122235775, Learning Rate: 0.002470628999999999\n",
      "Epoch [6142/20000], Bound: 0.4132891893386841, Entropy: 139.91610717773438, Temp: 2.4866409301757812, KL: 79.05084228515625, Loss: 0.019838744774460793, Learning Rate: 0.002470628999999999\n",
      "Epoch [6143/20000], Bound: 0.3743118643760681, Entropy: 136.1888427734375, Temp: 2.486717939376831, KL: 67.11590576171875, Loss: 0.021340951323509216, Learning Rate: 0.002470628999999999\n",
      "Epoch [6144/20000], Bound: 0.3809850215911865, Entropy: 138.01963806152344, Temp: 2.4867453575134277, KL: 67.82546997070312, Loss: 0.02367781102657318, Learning Rate: 0.002470628999999999\n",
      "Epoch [6145/20000], Bound: 0.39663076400756836, Entropy: 138.11459350585938, Temp: 2.48667573928833, KL: 75.71697998046875, Loss: 0.016773585230112076, Learning Rate: 0.002470628999999999\n",
      "Epoch [6146/20000], Bound: 0.413125216960907, Entropy: 137.2371826171875, Temp: 2.4867334365844727, KL: 77.8780517578125, Loss: 0.022100793197751045, Learning Rate: 0.002470628999999999\n",
      "Epoch [6147/20000], Bound: 0.3686135709285736, Entropy: 138.68959045410156, Temp: 2.4867918491363525, KL: 66.5501708984375, Loss: 0.019293388351798058, Learning Rate: 0.002470628999999999\n",
      "Epoch [6148/20000], Bound: 0.4068535268306732, Entropy: 137.80502319335938, Temp: 2.4868478775024414, KL: 76.27444458007812, Loss: 0.02162175439298153, Learning Rate: 0.002470628999999999\n",
      "Epoch [6149/20000], Bound: 0.3962162435054779, Entropy: 136.7484130859375, Temp: 2.4869062900543213, KL: 73.99295043945312, Loss: 0.020003007724881172, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6150/20000], Bound: 0.39978018403053284, Entropy: 138.68170166015625, Temp: 2.4869914054870605, KL: 75.6788330078125, Loss: 0.01868339069187641, Learning Rate: 0.002470628999999999\n",
      "Epoch [6151/20000], Bound: 0.39523613452911377, Entropy: 141.90216064453125, Temp: 2.487142562866211, KL: 73.28565979003906, Loss: 0.020860828459262848, Learning Rate: 0.002470628999999999\n",
      "Epoch [6152/20000], Bound: 0.40146759152412415, Entropy: 140.22071838378906, Temp: 2.487285852432251, KL: 74.05274963378906, Loss: 0.022939328104257584, Learning Rate: 0.002470628999999999\n",
      "Epoch [6153/20000], Bound: 0.41724899411201477, Entropy: 139.6411895751953, Temp: 2.487377643585205, KL: 78.45341491699219, Loss: 0.02340724505484104, Learning Rate: 0.002470628999999999\n",
      "Epoch [6154/20000], Bound: 0.4018594026565552, Entropy: 139.3683319091797, Temp: 2.487438917160034, KL: 74.16221618652344, Loss: 0.022949766367673874, Learning Rate: 0.002470628999999999\n",
      "Epoch [6155/20000], Bound: 0.40540871024131775, Entropy: 140.19143676757812, Temp: 2.487457036972046, KL: 76.53926086425781, Loss: 0.020248817279934883, Learning Rate: 0.002470628999999999\n",
      "Epoch [6156/20000], Bound: 0.3939596116542816, Entropy: 138.52333068847656, Temp: 2.4875152111053467, KL: 71.17439270019531, Loss: 0.024371905252337456, Learning Rate: 0.002470628999999999\n",
      "Epoch [6157/20000], Bound: 0.39437630772590637, Entropy: 140.62132263183594, Temp: 2.4874775409698486, KL: 72.08726501464844, Loss: 0.022777074947953224, Learning Rate: 0.002470628999999999\n",
      "Epoch [6158/20000], Bound: 0.4163861572742462, Entropy: 141.15560913085938, Temp: 2.4873979091644287, KL: 80.09629821777344, Loss: 0.01959032565355301, Learning Rate: 0.002470628999999999\n",
      "Epoch [6159/20000], Bound: 0.41004717350006104, Entropy: 141.96493530273438, Temp: 2.4874050617218018, KL: 77.27220153808594, Loss: 0.021505365148186684, Learning Rate: 0.002470628999999999\n",
      "Epoch [6160/20000], Bound: 0.3911725878715515, Entropy: 141.52003479003906, Temp: 2.4874279499053955, KL: 72.3223876953125, Loss: 0.02045835368335247, Learning Rate: 0.002470628999999999\n",
      "Epoch [6161/20000], Bound: 0.4029046297073364, Entropy: 141.31771850585938, Temp: 2.487459659576416, KL: 75.99153137207031, Loss: 0.01988343708217144, Learning Rate: 0.002470628999999999\n",
      "Epoch [6162/20000], Bound: 0.4280393123626709, Entropy: 139.22720336914062, Temp: 2.4875357151031494, KL: 79.89755249023438, Loss: 0.027000363916158676, Learning Rate: 0.002470628999999999\n",
      "Epoch [6163/20000], Bound: 0.3699541389942169, Entropy: 142.57968139648438, Temp: 2.487506866455078, KL: 65.5870361328125, Loss: 0.02198416367173195, Learning Rate: 0.002470628999999999\n",
      "Epoch [6164/20000], Bound: 0.384964257478714, Entropy: 142.55995178222656, Temp: 2.487412214279175, KL: 68.218017578125, Loss: 0.025155983865261078, Learning Rate: 0.002470628999999999\n",
      "Epoch [6165/20000], Bound: 0.3767620623111725, Entropy: 142.10032653808594, Temp: 2.487198829650879, KL: 69.19087219238281, Loss: 0.018552187830209732, Learning Rate: 0.002470628999999999\n",
      "Epoch [6166/20000], Bound: 0.3911861777305603, Entropy: 140.5939178466797, Temp: 2.4870445728302, KL: 72.18142700195312, Loss: 0.020744843408465385, Learning Rate: 0.002470628999999999\n",
      "Epoch [6167/20000], Bound: 0.4007071554660797, Entropy: 140.5792999267578, Temp: 2.4869093894958496, KL: 74.5780029296875, Loss: 0.02143535017967224, Learning Rate: 0.002470628999999999\n",
      "Epoch [6168/20000], Bound: 0.4133059084415436, Entropy: 140.84986877441406, Temp: 2.4867894649505615, KL: 78.45088195800781, Loss: 0.021057086065411568, Learning Rate: 0.002470628999999999\n",
      "Epoch [6169/20000], Bound: 0.38982143998146057, Entropy: 142.25205993652344, Temp: 2.4867165088653564, KL: 72.21766662597656, Loss: 0.019884292036294937, Learning Rate: 0.002470628999999999\n",
      "Epoch [6170/20000], Bound: 0.39621180295944214, Entropy: 142.16812133789062, Temp: 2.486675500869751, KL: 74.38380432128906, Loss: 0.01921156235039234, Learning Rate: 0.002470628999999999\n",
      "Epoch [6171/20000], Bound: 0.4063548743724823, Entropy: 143.04246520996094, Temp: 2.4866933822631836, KL: 76.3482666015625, Loss: 0.02117815986275673, Learning Rate: 0.002470628999999999\n",
      "Epoch [6172/20000], Bound: 0.4035767614841461, Entropy: 140.01080322265625, Temp: 2.4867284297943115, KL: 74.99122619628906, Loss: 0.022278064861893654, Learning Rate: 0.002470628999999999\n",
      "Epoch [6173/20000], Bound: 0.3894255459308624, Entropy: 141.78565979003906, Temp: 2.4867444038391113, KL: 72.43569946289062, Loss: 0.019219180569052696, Learning Rate: 0.002470628999999999\n",
      "Epoch [6174/20000], Bound: 0.3710561990737915, Entropy: 142.8227081298828, Temp: 2.4868013858795166, KL: 65.55488586425781, Loss: 0.022657189518213272, Learning Rate: 0.002470628999999999\n",
      "Epoch [6175/20000], Bound: 0.37500908970832825, Entropy: 144.83763122558594, Temp: 2.486767053604126, KL: 67.63023376464844, Loss: 0.020698875188827515, Learning Rate: 0.002470628999999999\n",
      "Epoch [6176/20000], Bound: 0.3900074362754822, Entropy: 143.71755981445312, Temp: 2.4867124557495117, KL: 72.63259887695312, Loss: 0.0191566813737154, Learning Rate: 0.002470628999999999\n",
      "Epoch [6177/20000], Bound: 0.4030989110469818, Entropy: 141.36705017089844, Temp: 2.486708402633667, KL: 76.69325256347656, Loss: 0.018576065078377724, Learning Rate: 0.002470628999999999\n",
      "Epoch [6178/20000], Bound: 0.4097878634929657, Entropy: 142.8324737548828, Temp: 2.486788749694824, KL: 76.01686096191406, Loss: 0.02386801689863205, Learning Rate: 0.002470628999999999\n",
      "Epoch [6179/20000], Bound: 0.4073258638381958, Entropy: 143.10665893554688, Temp: 2.48681378364563, KL: 74.95272827148438, Loss: 0.02455657348036766, Learning Rate: 0.002470628999999999\n",
      "Epoch [6180/20000], Bound: 0.39944711327552795, Entropy: 141.8512420654297, Temp: 2.4867656230926514, KL: 75.32421875, Loss: 0.01919952780008316, Learning Rate: 0.002470628999999999\n",
      "Epoch [6181/20000], Bound: 0.410625159740448, Entropy: 140.49954223632812, Temp: 2.486783027648926, KL: 77.33256530761719, Loss: 0.02171735279262066, Learning Rate: 0.002470628999999999\n",
      "Epoch [6182/20000], Bound: 0.41909676790237427, Entropy: 140.97320556640625, Temp: 2.4868109226226807, KL: 79.26800537109375, Loss: 0.02286676876246929, Learning Rate: 0.002470628999999999\n",
      "Epoch [6183/20000], Bound: 0.40224766731262207, Entropy: 143.66317749023438, Temp: 2.486832618713379, KL: 76.48626708984375, Loss: 0.018496442586183548, Learning Rate: 0.002470628999999999\n",
      "Epoch [6184/20000], Bound: 0.3994230628013611, Entropy: 142.0142364501953, Temp: 2.4869375228881836, KL: 74.42549133300781, Loss: 0.020994769409298897, Learning Rate: 0.002470628999999999\n",
      "Epoch [6185/20000], Bound: 0.3740892708301544, Entropy: 140.94029235839844, Temp: 2.4870433807373047, KL: 67.24723815917969, Loss: 0.020955413579940796, Learning Rate: 0.002470628999999999\n",
      "Epoch [6186/20000], Bound: 0.37132972478866577, Entropy: 140.00657653808594, Temp: 2.4871058464050293, KL: 66.5904541015625, Loss: 0.020731121301651, Learning Rate: 0.002470628999999999\n",
      "Epoch [6187/20000], Bound: 0.4241199195384979, Entropy: 137.99179077148438, Temp: 2.4871299266815186, KL: 81.69308471679688, Loss: 0.021014420315623283, Learning Rate: 0.002470628999999999\n",
      "Epoch [6188/20000], Bound: 0.4054265320301056, Entropy: 140.95718383789062, Temp: 2.4872076511383057, KL: 74.88232421875, Loss: 0.02358684502542019, Learning Rate: 0.002470628999999999\n",
      "Epoch [6189/20000], Bound: 0.4192075729370117, Entropy: 138.5642852783203, Temp: 2.487229108810425, KL: 80.55891418457031, Loss: 0.02034369483590126, Learning Rate: 0.002470628999999999\n",
      "Epoch [6190/20000], Bound: 0.43185707926750183, Entropy: 140.21180725097656, Temp: 2.4873135089874268, KL: 84.02291870117188, Loss: 0.02102775312960148, Learning Rate: 0.002470628999999999\n",
      "Epoch [6191/20000], Bound: 0.3783983588218689, Entropy: 137.1309814453125, Temp: 2.4874587059020996, KL: 70.18544006347656, Loss: 0.017478592693805695, Learning Rate: 0.002470628999999999\n",
      "Epoch [6192/20000], Bound: 0.3949253261089325, Entropy: 140.28211975097656, Temp: 2.4876601696014404, KL: 74.23849487304688, Loss: 0.01877247542142868, Learning Rate: 0.002470628999999999\n",
      "Epoch [6193/20000], Bound: 0.4285883903503418, Entropy: 138.9203338623047, Temp: 2.487905740737915, KL: 81.81552124023438, Loss: 0.023483654484152794, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6194/20000], Bound: 0.38634732365608215, Entropy: 139.95384216308594, Temp: 2.488123893737793, KL: 71.77020263671875, Loss: 0.018813209608197212, Learning Rate: 0.002470628999999999\n",
      "Epoch [6195/20000], Bound: 0.3878426253795624, Entropy: 139.8195343017578, Temp: 2.488368034362793, KL: 70.42169189453125, Loss: 0.022380081936717033, Learning Rate: 0.002470628999999999\n",
      "Epoch [6196/20000], Bound: 0.4092274308204651, Entropy: 136.90493774414062, Temp: 2.488539218902588, KL: 77.73941040039062, Loss: 0.02009795419871807, Learning Rate: 0.002470628999999999\n",
      "Epoch [6197/20000], Bound: 0.41112640500068665, Entropy: 136.57444763183594, Temp: 2.4887466430664062, KL: 77.42529296875, Loss: 0.021853405982255936, Learning Rate: 0.002470628999999999\n",
      "Epoch [6198/20000], Bound: 0.4213491976261139, Entropy: 138.59202575683594, Temp: 2.4889416694641113, KL: 79.37734985351562, Loss: 0.024026082828640938, Learning Rate: 0.002470628999999999\n",
      "Epoch [6199/20000], Bound: 0.41393545269966125, Entropy: 137.64096069335938, Temp: 2.4890854358673096, KL: 77.64344787597656, Loss: 0.023084275424480438, Learning Rate: 0.002470628999999999\n",
      "Epoch [6200/20000], Bound: 0.4199202060699463, Entropy: 138.27345275878906, Temp: 2.489194393157959, KL: 80.90582275390625, Loss: 0.02010217122733593, Learning Rate: 0.002470628999999999\n",
      "Epoch [6201/20000], Bound: 0.41154465079307556, Entropy: 136.5911102294922, Temp: 2.4893646240234375, KL: 79.0711669921875, Loss: 0.018803145736455917, Learning Rate: 0.002470628999999999\n",
      "Epoch [6202/20000], Bound: 0.4154699742794037, Entropy: 136.63430786132812, Temp: 2.489609718322754, KL: 79.8101806640625, Loss: 0.01965169794857502, Learning Rate: 0.002470628999999999\n",
      "Epoch [6203/20000], Bound: 0.4167284667491913, Entropy: 132.76576232910156, Temp: 2.4899067878723145, KL: 81.05903625488281, Loss: 0.01789766363799572, Learning Rate: 0.002470628999999999\n",
      "Epoch [6204/20000], Bound: 0.4124329686164856, Entropy: 134.08999633789062, Temp: 2.490299701690674, KL: 79.57191467285156, Loss: 0.01833709143102169, Learning Rate: 0.002470628999999999\n",
      "Epoch [6205/20000], Bound: 0.40539631247520447, Entropy: 136.78302001953125, Temp: 2.4907596111297607, KL: 76.36392211914062, Loss: 0.020637692883610725, Learning Rate: 0.002470628999999999\n",
      "Epoch [6206/20000], Bound: 0.40194839239120483, Entropy: 136.32693481445312, Temp: 2.491203784942627, KL: 75.5433349609375, Loss: 0.020274750888347626, Learning Rate: 0.002470628999999999\n",
      "Epoch [6207/20000], Bound: 0.39646968245506287, Entropy: 134.6565704345703, Temp: 2.4916374683380127, KL: 74.60279846191406, Loss: 0.0189858116209507, Learning Rate: 0.002470628999999999\n",
      "Epoch [6208/20000], Bound: 0.4019896984100342, Entropy: 137.9967041015625, Temp: 2.4920871257781982, KL: 76.54765319824219, Loss: 0.018295330926775932, Learning Rate: 0.002470628999999999\n",
      "Epoch [6209/20000], Bound: 0.40720558166503906, Entropy: 135.7220458984375, Temp: 2.492579698562622, KL: 76.84814453125, Loss: 0.02075173333287239, Learning Rate: 0.002470628999999999\n",
      "Epoch [6210/20000], Bound: 0.40762096643447876, Entropy: 137.80433654785156, Temp: 2.4930522441864014, KL: 76.46981811523438, Loss: 0.021760806441307068, Learning Rate: 0.002470628999999999\n",
      "Epoch [6211/20000], Bound: 0.4182227849960327, Entropy: 137.26954650878906, Temp: 2.4934797286987305, KL: 79.07057189941406, Loss: 0.022830404341220856, Learning Rate: 0.002470628999999999\n",
      "Epoch [6212/20000], Bound: 0.3603350520133972, Entropy: 138.51380920410156, Temp: 2.49385666847229, KL: 65.13792419433594, Loss: 0.017625628039240837, Learning Rate: 0.002470628999999999\n",
      "Epoch [6213/20000], Bound: 0.424395889043808, Entropy: 137.15283203125, Temp: 2.494227886199951, KL: 83.61763000488281, Loss: 0.017426494508981705, Learning Rate: 0.002470628999999999\n",
      "Epoch [6214/20000], Bound: 0.395006388425827, Entropy: 138.16555786132812, Temp: 2.4947118759155273, KL: 73.95326232910156, Loss: 0.019482778385281563, Learning Rate: 0.002470628999999999\n",
      "Epoch [6215/20000], Bound: 0.4078211486339569, Entropy: 139.31494140625, Temp: 2.4951889514923096, KL: 77.09126281738281, Loss: 0.02066051959991455, Learning Rate: 0.002470628999999999\n",
      "Epoch [6216/20000], Bound: 0.41282781958580017, Entropy: 138.6241455078125, Temp: 2.495650053024292, KL: 76.49705505371094, Loss: 0.024809421971440315, Learning Rate: 0.002470628999999999\n",
      "Epoch [6217/20000], Bound: 0.41324663162231445, Entropy: 141.42356872558594, Temp: 2.495990753173828, KL: 77.34130859375, Loss: 0.023370232433080673, Learning Rate: 0.002470628999999999\n",
      "Epoch [6218/20000], Bound: 0.3805399239063263, Entropy: 140.24452209472656, Temp: 2.4962637424468994, KL: 70.74354553222656, Loss: 0.01767602004110813, Learning Rate: 0.002470628999999999\n",
      "Epoch [6219/20000], Bound: 0.3946072459220886, Entropy: 136.71401977539062, Temp: 2.496574640274048, KL: 74.28968811035156, Loss: 0.018602533265948296, Learning Rate: 0.002470628999999999\n",
      "Epoch [6220/20000], Bound: 0.38828790187835693, Entropy: 139.3309326171875, Temp: 2.4969184398651123, KL: 71.65449523925781, Loss: 0.020259449258446693, Learning Rate: 0.002470628999999999\n",
      "Epoch [6221/20000], Bound: 0.4063904881477356, Entropy: 141.4031524658203, Temp: 2.4972338676452637, KL: 74.46621704101562, Loss: 0.025104187428951263, Learning Rate: 0.002470628999999999\n",
      "Epoch [6222/20000], Bound: 0.3887234330177307, Entropy: 140.94491577148438, Temp: 2.497422456741333, KL: 72.55647277832031, Loss: 0.01870826631784439, Learning Rate: 0.002470628999999999\n",
      "Epoch [6223/20000], Bound: 0.4294787645339966, Entropy: 139.3354034423828, Temp: 2.4976422786712646, KL: 81.94126892089844, Loss: 0.02390763908624649, Learning Rate: 0.002470628999999999\n",
      "Epoch [6224/20000], Bound: 0.39154332876205444, Entropy: 138.62042236328125, Temp: 2.4978208541870117, KL: 73.62515258789062, Loss: 0.01818789169192314, Learning Rate: 0.002470628999999999\n",
      "Epoch [6225/20000], Bound: 0.38877415657043457, Entropy: 139.78143310546875, Temp: 2.4980506896972656, KL: 73.01747131347656, Loss: 0.017822301015257835, Learning Rate: 0.002470628999999999\n",
      "Epoch [6226/20000], Bound: 0.396584689617157, Entropy: 139.90756225585938, Temp: 2.4983322620391846, KL: 75.30978393554688, Loss: 0.017724227160215378, Learning Rate: 0.002470628999999999\n",
      "Epoch [6227/20000], Bound: 0.4189331531524658, Entropy: 139.93617248535156, Temp: 2.4986767768859863, KL: 79.91120910644531, Loss: 0.021640725433826447, Learning Rate: 0.002470628999999999\n",
      "Epoch [6228/20000], Bound: 0.37917569279670715, Entropy: 140.68002319335938, Temp: 2.499009370803833, KL: 69.80122375488281, Loss: 0.01882561482489109, Learning Rate: 0.002470628999999999\n",
      "Epoch [6229/20000], Bound: 0.40585920214653015, Entropy: 140.3095245361328, Temp: 2.499337673187256, KL: 75.35113525390625, Loss: 0.023046566173434258, Learning Rate: 0.002470628999999999\n",
      "Epoch [6230/20000], Bound: 0.4288872480392456, Entropy: 138.21835327148438, Temp: 2.4995925426483154, KL: 83.09526062011719, Loss: 0.021267695352435112, Learning Rate: 0.002470628999999999\n",
      "Epoch [6231/20000], Bound: 0.38722050189971924, Entropy: 137.71238708496094, Temp: 2.4998722076416016, KL: 70.52275085449219, Loss: 0.021949196234345436, Learning Rate: 0.002470628999999999\n",
      "Epoch [6232/20000], Bound: 0.4169776141643524, Entropy: 138.35121154785156, Temp: 2.5000789165496826, KL: 79.03153991699219, Loss: 0.022254381328821182, Learning Rate: 0.002470628999999999\n",
      "Epoch [6233/20000], Bound: 0.38845589756965637, Entropy: 138.61875915527344, Temp: 2.5002663135528564, KL: 70.79792785644531, Loss: 0.022107494994997978, Learning Rate: 0.002470628999999999\n",
      "Epoch [6234/20000], Bound: 0.40601322054862976, Entropy: 138.14767456054688, Temp: 2.500387668609619, KL: 77.09223937988281, Loss: 0.019667645916342735, Learning Rate: 0.002470628999999999\n",
      "Epoch [6235/20000], Bound: 0.3814694583415985, Entropy: 139.5996856689453, Temp: 2.50054931640625, KL: 70.06831359863281, Loss: 0.0196030605584383, Learning Rate: 0.002470628999999999\n",
      "Epoch [6236/20000], Bound: 0.38651353120803833, Entropy: 138.35366821289062, Temp: 2.5007052421569824, KL: 71.21900939941406, Loss: 0.020163726061582565, Learning Rate: 0.002470628999999999\n",
      "Epoch [6237/20000], Bound: 0.38961169123649597, Entropy: 137.4098663330078, Temp: 2.5008487701416016, KL: 71.58889770507812, Loss: 0.021192384883761406, Learning Rate: 0.002470628999999999\n",
      "Epoch [6238/20000], Bound: 0.39633676409721375, Entropy: 137.39816284179688, Temp: 2.50095796585083, KL: 73.89007568359375, Loss: 0.02045462094247341, Learning Rate: 0.002470628999999999\n",
      "Epoch [6239/20000], Bound: 0.4106920659542084, Entropy: 136.87359619140625, Temp: 2.5010690689086914, KL: 74.38804626464844, Loss: 0.027829961851239204, Learning Rate: 0.002470628999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6240/20000], Bound: 0.39679110050201416, Entropy: 140.74853515625, Temp: 2.5010030269622803, KL: 75.43817138671875, Loss: 0.017622431740164757, Learning Rate: 0.002470628999999999\n",
      "Epoch [6241/20000], Bound: 0.4175983667373657, Entropy: 137.7939453125, Temp: 2.5010366439819336, KL: 79.88249206542969, Loss: 0.02093518152832985, Learning Rate: 0.002470628999999999\n",
      "Epoch [6242/20000], Bound: 0.4157850444316864, Entropy: 140.45217895507812, Temp: 2.5011050701141357, KL: 80.67068481445312, Loss: 0.018282558768987656, Learning Rate: 0.002470628999999999\n",
      "Epoch [6243/20000], Bound: 0.39538058638572693, Entropy: 137.4535675048828, Temp: 2.501275062561035, KL: 73.99217224121094, Loss: 0.019703157246112823, Learning Rate: 0.002470628999999999\n",
      "Epoch [6244/20000], Bound: 0.3915313184261322, Entropy: 138.69483947753906, Temp: 2.501460313796997, KL: 71.73959350585938, Loss: 0.02199648879468441, Learning Rate: 0.002470628999999999\n",
      "Epoch [6245/20000], Bound: 0.4136106073856354, Entropy: 138.1759490966797, Temp: 2.501587390899658, KL: 76.46406555175781, Loss: 0.025409016758203506, Learning Rate: 0.002470628999999999\n",
      "Epoch [6246/20000], Bound: 0.4402783215045929, Entropy: 137.08892822265625, Temp: 2.501607894897461, KL: 86.06759643554688, Loss: 0.02231188304722309, Learning Rate: 0.002470628999999999\n",
      "Epoch [6247/20000], Bound: 0.41359028220176697, Entropy: 137.99241638183594, Temp: 2.5016682147979736, KL: 79.52204895019531, Loss: 0.01928604580461979, Learning Rate: 0.002470628999999999\n",
      "Epoch [6248/20000], Bound: 0.3978198766708374, Entropy: 137.78729248046875, Temp: 2.5017988681793213, KL: 72.5855712890625, Loss: 0.02392849139869213, Learning Rate: 0.002470628999999999\n",
      "Epoch [6249/20000], Bound: 0.43369176983833313, Entropy: 138.0023956298828, Temp: 2.501833915710449, KL: 85.46360778808594, Loss: 0.019486136734485626, Learning Rate: 0.002470628999999999\n",
      "Epoch [6250/20000], Bound: 0.3754293620586395, Entropy: 138.65869140625, Temp: 2.5019726753234863, KL: 66.27813720703125, Loss: 0.023797200992703438, Learning Rate: 0.002470628999999999\n",
      "Epoch [6251/20000], Bound: 0.4025050103664398, Entropy: 137.3605194091797, Temp: 2.501976251602173, KL: 76.9713134765625, Loss: 0.0178834181278944, Learning Rate: 0.001729440299999999\n",
      "Epoch [6252/20000], Bound: 0.3668119013309479, Entropy: 137.70578002929688, Temp: 2.502075672149658, KL: 66.27761840820312, Loss: 0.0190030075609684, Learning Rate: 0.001729440299999999\n",
      "Epoch [6253/20000], Bound: 0.37992095947265625, Entropy: 139.3647003173828, Temp: 2.502164840698242, KL: 68.35934448242188, Loss: 0.022162994369864464, Learning Rate: 0.001729440299999999\n",
      "Epoch [6254/20000], Bound: 0.39155635237693787, Entropy: 138.3339080810547, Temp: 2.50217866897583, KL: 72.95643615722656, Loss: 0.019587336108088493, Learning Rate: 0.001729440299999999\n",
      "Epoch [6255/20000], Bound: 0.4078265130519867, Entropy: 137.7443389892578, Temp: 2.5022192001342773, KL: 76.88351440429688, Loss: 0.02117125876247883, Learning Rate: 0.001729440299999999\n",
      "Epoch [6256/20000], Bound: 0.39947962760925293, Entropy: 139.96299743652344, Temp: 2.5022685527801514, KL: 75.28054809570312, Loss: 0.01950911432504654, Learning Rate: 0.001729440299999999\n",
      "Epoch [6257/20000], Bound: 0.3621309995651245, Entropy: 139.518310546875, Temp: 2.5023577213287354, KL: 62.7003173828125, Loss: 0.023572726175189018, Learning Rate: 0.001729440299999999\n",
      "Epoch [6258/20000], Bound: 0.3948197066783905, Entropy: 140.3460235595703, Temp: 2.5022974014282227, KL: 74.3843994140625, Loss: 0.018609199672937393, Learning Rate: 0.001729440299999999\n",
      "Epoch [6259/20000], Bound: 0.3835112750530243, Entropy: 140.0319366455078, Temp: 2.502304792404175, KL: 70.93162536621094, Loss: 0.019052676856517792, Learning Rate: 0.001729440299999999\n",
      "Epoch [6260/20000], Bound: 0.40173450112342834, Entropy: 139.49630737304688, Temp: 2.502340316772461, KL: 76.36857604980469, Loss: 0.018644629046320915, Learning Rate: 0.001729440299999999\n",
      "Epoch [6261/20000], Bound: 0.4273710250854492, Entropy: 140.07699584960938, Temp: 2.5024449825286865, KL: 83.02359008789062, Loss: 0.020536229014396667, Learning Rate: 0.001729440299999999\n",
      "Epoch [6262/20000], Bound: 0.39554116129875183, Entropy: 140.37596130371094, Temp: 2.502606153488159, KL: 72.1265869140625, Loss: 0.023539461195468903, Learning Rate: 0.001729440299999999\n",
      "Epoch [6263/20000], Bound: 0.3820199966430664, Entropy: 140.57806396484375, Temp: 2.5026743412017822, KL: 69.83134460449219, Loss: 0.02041166089475155, Learning Rate: 0.001729440299999999\n",
      "Epoch [6264/20000], Bound: 0.40673792362213135, Entropy: 136.233154296875, Temp: 2.50272274017334, KL: 77.27311706542969, Loss: 0.019761519506573677, Learning Rate: 0.001729440299999999\n",
      "Epoch [6265/20000], Bound: 0.38625001907348633, Entropy: 142.56703186035156, Temp: 2.502816677093506, KL: 71.51914978027344, Loss: 0.019438721239566803, Learning Rate: 0.001729440299999999\n",
      "Epoch [6266/20000], Bound: 0.4185393750667572, Entropy: 140.74685668945312, Temp: 2.5029234886169434, KL: 79.02989196777344, Loss: 0.023224668577313423, Learning Rate: 0.001729440299999999\n",
      "Epoch [6267/20000], Bound: 0.4075857996940613, Entropy: 142.0606231689453, Temp: 2.502995014190674, KL: 77.46159362792969, Loss: 0.019885357469320297, Learning Rate: 0.001729440299999999\n",
      "Epoch [6268/20000], Bound: 0.4140593707561493, Entropy: 140.00746154785156, Temp: 2.503107786178589, KL: 80.73356628417969, Loss: 0.0171638336032629, Learning Rate: 0.001729440299999999\n",
      "Epoch [6269/20000], Bound: 0.432798832654953, Entropy: 140.40074157714844, Temp: 2.5033459663391113, KL: 85.15171813964844, Loss: 0.019589053466916084, Learning Rate: 0.001729440299999999\n",
      "Epoch [6270/20000], Bound: 0.35445576906204224, Entropy: 144.93972778320312, Temp: 2.5036628246307373, KL: 62.83448791503906, Loss: 0.0191205982118845, Learning Rate: 0.001729440299999999\n",
      "Epoch [6271/20000], Bound: 0.40004608035087585, Entropy: 139.4281463623047, Temp: 2.5039215087890625, KL: 74.20915222167969, Loss: 0.02199794165790081, Learning Rate: 0.001729440299999999\n",
      "Epoch [6272/20000], Bound: 0.3646489381790161, Entropy: 140.43966674804688, Temp: 2.5041286945343018, KL: 65.40997314453125, Loss: 0.019562752917408943, Learning Rate: 0.001729440299999999\n",
      "Epoch [6273/20000], Bound: 0.37932226061820984, Entropy: 141.0734100341797, Temp: 2.504293918609619, KL: 69.92611694335938, Loss: 0.018719565123319626, Learning Rate: 0.001729440299999999\n",
      "Epoch [6274/20000], Bound: 0.3882717490196228, Entropy: 138.3920135498047, Temp: 2.504472255706787, KL: 73.36061096191406, Loss: 0.016932252794504166, Learning Rate: 0.001729440299999999\n",
      "Epoch [6275/20000], Bound: 0.3769412338733673, Entropy: 141.92489624023438, Temp: 2.5047295093536377, KL: 69.01145935058594, Loss: 0.019213153049349785, Learning Rate: 0.001729440299999999\n",
      "Epoch [6276/20000], Bound: 0.40445008873939514, Entropy: 142.1103515625, Temp: 2.504971981048584, KL: 76.49110412597656, Loss: 0.0200157780200243, Learning Rate: 0.001729440299999999\n",
      "Epoch [6277/20000], Bound: 0.38972046971321106, Entropy: 141.89828491210938, Temp: 2.5052285194396973, KL: 73.5985107421875, Loss: 0.01729339174926281, Learning Rate: 0.001729440299999999\n",
      "Epoch [6278/20000], Bound: 0.3905116617679596, Entropy: 140.7247314453125, Temp: 2.5055480003356934, KL: 70.69404602050781, Loss: 0.023545486852526665, Learning Rate: 0.001729440299999999\n",
      "Epoch [6279/20000], Bound: 0.38025200366973877, Entropy: 142.0829620361328, Temp: 2.505746841430664, KL: 68.76878356933594, Loss: 0.021568940952420235, Learning Rate: 0.001729440299999999\n",
      "Epoch [6280/20000], Bound: 0.39949944615364075, Entropy: 141.32061767578125, Temp: 2.5058741569519043, KL: 73.98875427246094, Loss: 0.02214425802230835, Learning Rate: 0.001729440299999999\n",
      "Epoch [6281/20000], Bound: 0.3991365134716034, Entropy: 142.27171325683594, Temp: 2.5059566497802734, KL: 75.52085876464844, Loss: 0.018878282979130745, Learning Rate: 0.001729440299999999\n",
      "Epoch [6282/20000], Bound: 0.4038580060005188, Entropy: 138.80697631835938, Temp: 2.5060911178588867, KL: 76.90158081054688, Loss: 0.01886627823114395, Learning Rate: 0.001729440299999999\n",
      "Epoch [6283/20000], Bound: 0.38940224051475525, Entropy: 140.78652954101562, Temp: 2.5062808990478516, KL: 72.24729919433594, Loss: 0.019821029156446457, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6284/20000], Bound: 0.4060027301311493, Entropy: 138.63467407226562, Temp: 2.506467342376709, KL: 77.17791748046875, Loss: 0.01957133039832115, Learning Rate: 0.001729440299999999\n",
      "Epoch [6285/20000], Bound: 0.37379369139671326, Entropy: 140.3759002685547, Temp: 2.506687641143799, KL: 68.57928466796875, Loss: 0.018336886540055275, Learning Rate: 0.001729440299999999\n",
      "Epoch [6286/20000], Bound: 0.38441202044487, Entropy: 141.04476928710938, Temp: 2.506915330886841, KL: 71.06788635253906, Loss: 0.019344640895724297, Learning Rate: 0.001729440299999999\n",
      "Epoch [6287/20000], Bound: 0.3856314420700073, Entropy: 141.2611083984375, Temp: 2.5071396827697754, KL: 69.76072692871094, Loss: 0.022645296528935432, Learning Rate: 0.001729440299999999\n",
      "Epoch [6288/20000], Bound: 0.3815765082836151, Entropy: 141.57321166992188, Temp: 2.5072684288024902, KL: 70.08537292480469, Loss: 0.019705545157194138, Learning Rate: 0.001729440299999999\n",
      "Epoch [6289/20000], Bound: 0.3927460312843323, Entropy: 142.25779724121094, Temp: 2.507387638092041, KL: 70.74613952636719, Loss: 0.02473912201821804, Learning Rate: 0.001729440299999999\n",
      "Epoch [6290/20000], Bound: 0.4152858853340149, Entropy: 139.5109100341797, Temp: 2.5073747634887695, KL: 79.90814208984375, Loss: 0.019599180668592453, Learning Rate: 0.001729440299999999\n",
      "Epoch [6291/20000], Bound: 0.3728209435939789, Entropy: 141.19532775878906, Temp: 2.5074315071105957, KL: 66.85302734375, Loss: 0.021245192736387253, Learning Rate: 0.001729440299999999\n",
      "Epoch [6292/20000], Bound: 0.3860425055027008, Entropy: 140.27035522460938, Temp: 2.507425308227539, KL: 71.4442138671875, Loss: 0.01952446810901165, Learning Rate: 0.001729440299999999\n",
      "Epoch [6293/20000], Bound: 0.38826262950897217, Entropy: 139.37075805664062, Temp: 2.507436752319336, KL: 70.79533386230469, Loss: 0.02208060398697853, Learning Rate: 0.001729440299999999\n",
      "Epoch [6294/20000], Bound: 0.36365455389022827, Entropy: 142.43655395507812, Temp: 2.507394313812256, KL: 64.08827209472656, Loss: 0.021683847531676292, Learning Rate: 0.001729440299999999\n",
      "Epoch [6295/20000], Bound: 0.3816356658935547, Entropy: 140.80364990234375, Temp: 2.5072689056396484, KL: 71.29721069335938, Loss: 0.017322266474366188, Learning Rate: 0.001729440299999999\n",
      "Epoch [6296/20000], Bound: 0.42792385816574097, Entropy: 142.23822021484375, Temp: 2.507228374481201, KL: 83.62275695800781, Loss: 0.01974533498287201, Learning Rate: 0.001729440299999999\n",
      "Epoch [6297/20000], Bound: 0.40788576006889343, Entropy: 141.62156677246094, Temp: 2.507280111312866, KL: 76.59638977050781, Loss: 0.02184336632490158, Learning Rate: 0.001729440299999999\n",
      "Epoch [6298/20000], Bound: 0.42343375086784363, Entropy: 142.67799377441406, Temp: 2.507317543029785, KL: 82.11883544921875, Loss: 0.020044824108481407, Learning Rate: 0.001729440299999999\n",
      "Epoch [6299/20000], Bound: 0.3791567385196686, Entropy: 139.73231506347656, Temp: 2.50742244720459, KL: 69.47822570800781, Loss: 0.019555579870939255, Learning Rate: 0.001729440299999999\n",
      "Epoch [6300/20000], Bound: 0.40711212158203125, Entropy: 140.94097900390625, Temp: 2.5075204372406006, KL: 75.29740905761719, Loss: 0.023983614519238472, Learning Rate: 0.001729440299999999\n",
      "Epoch [6301/20000], Bound: 0.36419814825057983, Entropy: 141.29876708984375, Temp: 2.5075368881225586, KL: 64.90962219238281, Loss: 0.020346475765109062, Learning Rate: 0.001729440299999999\n",
      "Epoch [6302/20000], Bound: 0.42273545265197754, Entropy: 142.01158142089844, Temp: 2.5075042247772217, KL: 82.21470642089844, Loss: 0.01943797990679741, Learning Rate: 0.001729440299999999\n",
      "Epoch [6303/20000], Bound: 0.3854987621307373, Entropy: 140.7995147705078, Temp: 2.5075619220733643, KL: 70.75323486328125, Loss: 0.020595375448465347, Learning Rate: 0.001729440299999999\n",
      "Epoch [6304/20000], Bound: 0.39533382654190063, Entropy: 141.27500915527344, Temp: 2.507598638534546, KL: 72.48895263671875, Loss: 0.022751783952116966, Learning Rate: 0.001729440299999999\n",
      "Epoch [6305/20000], Bound: 0.3772810399532318, Entropy: 142.1237335205078, Temp: 2.507572650909424, KL: 68.54521179199219, Loss: 0.020364811643958092, Learning Rate: 0.001729440299999999\n",
      "Epoch [6306/20000], Bound: 0.39692050218582153, Entropy: 139.444580078125, Temp: 2.5075254440307617, KL: 75.57177734375, Loss: 0.017517508938908577, Learning Rate: 0.001729440299999999\n",
      "Epoch [6307/20000], Bound: 0.3890136778354645, Entropy: 141.29837036132812, Temp: 2.507577896118164, KL: 71.92939758300781, Loss: 0.02024868130683899, Learning Rate: 0.001729440299999999\n",
      "Epoch [6308/20000], Bound: 0.40924006700515747, Entropy: 141.3829345703125, Temp: 2.507627010345459, KL: 77.18988037109375, Loss: 0.021458372473716736, Learning Rate: 0.001729440299999999\n",
      "Epoch [6309/20000], Bound: 0.4034653902053833, Entropy: 140.10462951660156, Temp: 2.5076751708984375, KL: 76.31352233886719, Loss: 0.019831407815217972, Learning Rate: 0.001729440299999999\n",
      "Epoch [6310/20000], Bound: 0.4078384041786194, Entropy: 141.16062927246094, Temp: 2.5077590942382812, KL: 76.96499633789062, Loss: 0.021086659282445908, Learning Rate: 0.001729440299999999\n",
      "Epoch [6311/20000], Bound: 0.4280741214752197, Entropy: 138.20339965820312, Temp: 2.5078468322753906, KL: 84.19706726074219, Loss: 0.018700288608670235, Learning Rate: 0.001729440299999999\n",
      "Epoch [6312/20000], Bound: 0.42205727100372314, Entropy: 140.0849151611328, Temp: 2.5080437660217285, KL: 81.44132995605469, Loss: 0.020581867545843124, Learning Rate: 0.001729440299999999\n",
      "Epoch [6313/20000], Bound: 0.4312167465686798, Entropy: 141.26231384277344, Temp: 2.508274555206299, KL: 85.568359375, Loss: 0.017874689772725105, Learning Rate: 0.001729440299999999\n",
      "Epoch [6314/20000], Bound: 0.38071566820144653, Entropy: 140.8361358642578, Temp: 2.5086288452148438, KL: 70.99491882324219, Loss: 0.017422908917069435, Learning Rate: 0.001729440299999999\n",
      "Epoch [6315/20000], Bound: 0.38210561871528625, Entropy: 141.33010864257812, Temp: 2.5090157985687256, KL: 70.39132690429688, Loss: 0.019413819536566734, Learning Rate: 0.001729440299999999\n",
      "Epoch [6316/20000], Bound: 0.42104440927505493, Entropy: 139.15736389160156, Temp: 2.5093765258789062, KL: 79.92732238769531, Loss: 0.023012246936559677, Learning Rate: 0.001729440299999999\n",
      "Epoch [6317/20000], Bound: 0.38567107915878296, Entropy: 139.466552734375, Temp: 2.5096824169158936, KL: 70.64833068847656, Loss: 0.02092568762600422, Learning Rate: 0.001729440299999999\n",
      "Epoch [6318/20000], Bound: 0.36259010434150696, Entropy: 141.13636779785156, Temp: 2.509932041168213, KL: 66.22685241699219, Loss: 0.016861939802765846, Learning Rate: 0.001729440299999999\n",
      "Epoch [6319/20000], Bound: 0.3971375524997711, Entropy: 140.63536071777344, Temp: 2.5102078914642334, KL: 74.59365844726562, Loss: 0.01962643302977085, Learning Rate: 0.001729440299999999\n",
      "Epoch [6320/20000], Bound: 0.4027750492095947, Entropy: 138.7488250732422, Temp: 2.5104892253875732, KL: 76.84355163574219, Loss: 0.01841074228286743, Learning Rate: 0.001729440299999999\n",
      "Epoch [6321/20000], Bound: 0.4105723202228546, Entropy: 137.38426208496094, Temp: 2.5108211040496826, KL: 79.02499389648438, Loss: 0.018626566976308823, Learning Rate: 0.001729440299999999\n",
      "Epoch [6322/20000], Bound: 0.38783782720565796, Entropy: 138.78451538085938, Temp: 2.5112063884735107, KL: 73.37020874023438, Loss: 0.016752811148762703, Learning Rate: 0.001729440299999999\n",
      "Epoch [6323/20000], Bound: 0.4147855341434479, Entropy: 137.31629943847656, Temp: 2.5116522312164307, KL: 80.78019714355469, Loss: 0.01762685924768448, Learning Rate: 0.001729440299999999\n",
      "Epoch [6324/20000], Bound: 0.40663713216781616, Entropy: 136.0493621826172, Temp: 2.512176036834717, KL: 76.55555725097656, Loss: 0.02125554345548153, Learning Rate: 0.001729440299999999\n",
      "Epoch [6325/20000], Bound: 0.3911024332046509, Entropy: 137.64537048339844, Temp: 2.51265025138855, KL: 73.74836730957031, Loss: 0.017878234386444092, Learning Rate: 0.001729440299999999\n",
      "Epoch [6326/20000], Bound: 0.403015673160553, Entropy: 138.7391815185547, Temp: 2.513148784637451, KL: 76.07844543457031, Loss: 0.02010810375213623, Learning Rate: 0.001729440299999999\n",
      "Epoch [6327/20000], Bound: 0.4269874095916748, Entropy: 138.96829223632812, Temp: 2.5136256217956543, KL: 81.56771850585938, Loss: 0.02336476743221283, Learning Rate: 0.001729440299999999\n",
      "Epoch [6328/20000], Bound: 0.38353070616722107, Entropy: 137.89999389648438, Temp: 2.5140347480773926, KL: 70.0675048828125, Loss: 0.02091827429831028, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6329/20000], Bound: 0.41220250725746155, Entropy: 137.30015563964844, Temp: 2.5143706798553467, KL: 79.29351806640625, Loss: 0.019099904224276543, Learning Rate: 0.001729440299999999\n",
      "Epoch [6330/20000], Bound: 0.3868655562400818, Entropy: 139.84344482421875, Temp: 2.514746904373169, KL: 71.95796203613281, Loss: 0.019054090604186058, Learning Rate: 0.001729440299999999\n",
      "Epoch [6331/20000], Bound: 0.4222751259803772, Entropy: 140.58355712890625, Temp: 2.5151140689849854, KL: 80.23190307617188, Loss: 0.02321566268801689, Learning Rate: 0.001729440299999999\n",
      "Epoch [6332/20000], Bound: 0.3825620114803314, Entropy: 139.7688751220703, Temp: 2.515418291091919, KL: 70.26188659667969, Loss: 0.02000041864812374, Learning Rate: 0.001729440299999999\n",
      "Epoch [6333/20000], Bound: 0.40192869305610657, Entropy: 139.27621459960938, Temp: 2.515684127807617, KL: 75.64935302734375, Loss: 0.020362477749586105, Learning Rate: 0.001729440299999999\n",
      "Epoch [6334/20000], Bound: 0.41562026739120483, Entropy: 137.3405303955078, Temp: 2.5159406661987305, KL: 80.672607421875, Loss: 0.018396368250250816, Learning Rate: 0.001729440299999999\n",
      "Epoch [6335/20000], Bound: 0.38559427857398987, Entropy: 138.19090270996094, Temp: 2.5162713527679443, KL: 70.68968200683594, Loss: 0.02087203413248062, Learning Rate: 0.001729440299999999\n",
      "Epoch [6336/20000], Bound: 0.4041332006454468, Entropy: 138.282470703125, Temp: 2.51654052734375, KL: 77.63430786132812, Loss: 0.017708489671349525, Learning Rate: 0.001729440299999999\n",
      "Epoch [6337/20000], Bound: 0.3917262852191925, Entropy: 139.25360107421875, Temp: 2.516880989074707, KL: 73.03828430175781, Loss: 0.019697751849889755, Learning Rate: 0.001729440299999999\n",
      "Epoch [6338/20000], Bound: 0.3892618417739868, Entropy: 136.65280151367188, Temp: 2.517204761505127, KL: 71.10511779785156, Loss: 0.022137373685836792, Learning Rate: 0.001729440299999999\n",
      "Epoch [6339/20000], Bound: 0.39633709192276, Entropy: 138.78787231445312, Temp: 2.5174367427825928, KL: 75.05262756347656, Loss: 0.0183437280356884, Learning Rate: 0.001729440299999999\n",
      "Epoch [6340/20000], Bound: 0.40600407123565674, Entropy: 138.39459228515625, Temp: 2.5177104473114014, KL: 74.51597595214844, Loss: 0.025005709379911423, Learning Rate: 0.001729440299999999\n",
      "Epoch [6341/20000], Bound: 0.40526920557022095, Entropy: 139.88442993164062, Temp: 2.517845392227173, KL: 76.53643798828125, Loss: 0.020566925406455994, Learning Rate: 0.001729440299999999\n",
      "Epoch [6342/20000], Bound: 0.40664100646972656, Entropy: 137.9825439453125, Temp: 2.5179834365844727, KL: 77.66795349121094, Loss: 0.019120944663882256, Learning Rate: 0.001729440299999999\n",
      "Epoch [6343/20000], Bound: 0.41707879304885864, Entropy: 137.42735290527344, Temp: 2.5181684494018555, KL: 81.75141906738281, Loss: 0.017148785293102264, Learning Rate: 0.001729440299999999\n",
      "Epoch [6344/20000], Bound: 0.36951926350593567, Entropy: 138.94659423828125, Temp: 2.518472194671631, KL: 67.95658874511719, Loss: 0.01733279600739479, Learning Rate: 0.001729440299999999\n",
      "Epoch [6345/20000], Bound: 0.3946952819824219, Entropy: 138.289306640625, Temp: 2.5187911987304688, KL: 74.18229675292969, Loss: 0.019146759063005447, Learning Rate: 0.001729440299999999\n",
      "Epoch [6346/20000], Bound: 0.4060039222240448, Entropy: 137.01498413085938, Temp: 2.519115924835205, KL: 77.74324035644531, Loss: 0.018615202978253365, Learning Rate: 0.001729440299999999\n",
      "Epoch [6347/20000], Bound: 0.38569948077201843, Entropy: 139.55760192871094, Temp: 2.519482374191284, KL: 70.5792236328125, Loss: 0.021185487508773804, Learning Rate: 0.001729440299999999\n",
      "Epoch [6348/20000], Bound: 0.3654722571372986, Entropy: 140.18031311035156, Temp: 2.5197722911834717, KL: 66.9140625, Loss: 0.017185162752866745, Learning Rate: 0.001729440299999999\n",
      "Epoch [6349/20000], Bound: 0.40250593423843384, Entropy: 139.3438262939453, Temp: 2.5200753211975098, KL: 76.950927734375, Loss: 0.018168406561017036, Learning Rate: 0.001729440299999999\n",
      "Epoch [6350/20000], Bound: 0.42353513836860657, Entropy: 139.96484375, Temp: 2.520427942276001, KL: 81.40139770507812, Loss: 0.021716279909014702, Learning Rate: 0.001729440299999999\n",
      "Epoch [6351/20000], Bound: 0.38603174686431885, Entropy: 140.86300659179688, Temp: 2.520761251449585, KL: 71.63218688964844, Loss: 0.01929844729602337, Learning Rate: 0.001729440299999999\n",
      "Epoch [6352/20000], Bound: 0.3959716856479645, Entropy: 140.73846435546875, Temp: 2.5210776329040527, KL: 75.89559936523438, Loss: 0.01650826446712017, Learning Rate: 0.001729440299999999\n",
      "Epoch [6353/20000], Bound: 0.37640467286109924, Entropy: 139.47103881835938, Temp: 2.5214786529541016, KL: 70.06524658203125, Loss: 0.01700684428215027, Learning Rate: 0.001729440299999999\n",
      "Epoch [6354/20000], Bound: 0.3872232437133789, Entropy: 140.82579040527344, Temp: 2.5219056606292725, KL: 72.36062622070312, Loss: 0.018541552126407623, Learning Rate: 0.001729440299999999\n",
      "Epoch [6355/20000], Bound: 0.4163982570171356, Entropy: 138.08230590820312, Temp: 2.5223302841186523, KL: 78.84991455078125, Loss: 0.02255958691239357, Learning Rate: 0.001729440299999999\n",
      "Epoch [6356/20000], Bound: 0.421630859375, Entropy: 140.2461395263672, Temp: 2.522688865661621, KL: 81.50715637207031, Loss: 0.020401472225785255, Learning Rate: 0.001729440299999999\n",
      "Epoch [6357/20000], Bound: 0.39876964688301086, Entropy: 140.64796447753906, Temp: 2.5230607986450195, KL: 76.29145812988281, Loss: 0.017357641831040382, Learning Rate: 0.001729440299999999\n",
      "Epoch [6358/20000], Bound: 0.3740703761577606, Entropy: 140.35255432128906, Temp: 2.523491144180298, KL: 68.1527099609375, Loss: 0.019520260393619537, Learning Rate: 0.001729440299999999\n",
      "Epoch [6359/20000], Bound: 0.400391161441803, Entropy: 138.843505859375, Temp: 2.523864269256592, KL: 75.35887145996094, Loss: 0.02015015482902527, Learning Rate: 0.001729440299999999\n",
      "Epoch [6360/20000], Bound: 0.43755850195884705, Entropy: 139.35601806640625, Temp: 2.5242156982421875, KL: 87.00631713867188, Loss: 0.01912694424390793, Learning Rate: 0.001729440299999999\n",
      "Epoch [6361/20000], Bound: 0.41640064120292664, Entropy: 141.73199462890625, Temp: 2.5246469974517822, KL: 80.62423706054688, Loss: 0.019075943157076836, Learning Rate: 0.001729440299999999\n",
      "Epoch [6362/20000], Bound: 0.3968218266963959, Entropy: 141.23272705078125, Temp: 2.5251119136810303, KL: 73.78416442871094, Loss: 0.021230049431324005, Learning Rate: 0.001729440299999999\n",
      "Epoch [6363/20000], Bound: 0.3762345016002655, Entropy: 139.12509155273438, Temp: 2.5255069732666016, KL: 68.72665405273438, Loss: 0.01960866153240204, Learning Rate: 0.001729440299999999\n",
      "Epoch [6364/20000], Bound: 0.37133482098579407, Entropy: 140.1792449951172, Temp: 2.525848150253296, KL: 67.90802001953125, Loss: 0.01851271651685238, Learning Rate: 0.001729440299999999\n",
      "Epoch [6365/20000], Bound: 0.405799925327301, Entropy: 138.83883666992188, Temp: 2.5261647701263428, KL: 77.77935791015625, Loss: 0.018518295139074326, Learning Rate: 0.001729440299999999\n",
      "Epoch [6366/20000], Bound: 0.4121911823749542, Entropy: 137.82920837402344, Temp: 2.526522397994995, KL: 77.76101684570312, Loss: 0.022291047498583794, Learning Rate: 0.001729440299999999\n",
      "Epoch [6367/20000], Bound: 0.3714217245578766, Entropy: 138.61705017089844, Temp: 2.526817560195923, KL: 67.78802490234375, Loss: 0.01880844309926033, Learning Rate: 0.001729440299999999\n",
      "Epoch [6368/20000], Bound: 0.38930803537368774, Entropy: 141.299072265625, Temp: 2.527083396911621, KL: 72.6025390625, Loss: 0.019304567947983742, Learning Rate: 0.001729440299999999\n",
      "Epoch [6369/20000], Bound: 0.3745264410972595, Entropy: 139.3161163330078, Temp: 2.527340888977051, KL: 65.36247253417969, Loss: 0.025333406403660774, Learning Rate: 0.001729440299999999\n",
      "Epoch [6370/20000], Bound: 0.38267067074775696, Entropy: 138.62648010253906, Temp: 2.5273818969726562, KL: 70.39480590820312, Loss: 0.01992829330265522, Learning Rate: 0.001729440299999999\n",
      "Epoch [6371/20000], Bound: 0.3913079500198364, Entropy: 140.62062072753906, Temp: 2.5274055004119873, KL: 72.06069946289062, Loss: 0.02151636593043804, Learning Rate: 0.001729440299999999\n",
      "Epoch [6372/20000], Bound: 0.4235265254974365, Entropy: 139.53257751464844, Temp: 2.5273826122283936, KL: 81.89448547363281, Loss: 0.020828958600759506, Learning Rate: 0.001729440299999999\n",
      "Epoch [6373/20000], Bound: 0.40875130891799927, Entropy: 142.82041931152344, Temp: 2.5273993015289307, KL: 78.37936401367188, Loss: 0.01906600408256054, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6374/20000], Bound: 0.3662063479423523, Entropy: 141.86529541015625, Temp: 2.527475595474243, KL: 66.58688354492188, Loss: 0.01831742189824581, Learning Rate: 0.001729440299999999\n",
      "Epoch [6375/20000], Bound: 0.38313060998916626, Entropy: 141.49700927734375, Temp: 2.5275492668151855, KL: 66.49403381347656, Loss: 0.02790515311062336, Learning Rate: 0.001729440299999999\n",
      "Epoch [6376/20000], Bound: 0.4301144778728485, Entropy: 141.5192413330078, Temp: 2.5274195671081543, KL: 84.46450805664062, Loss: 0.019694682210683823, Learning Rate: 0.001729440299999999\n",
      "Epoch [6377/20000], Bound: 0.4134675860404968, Entropy: 143.22409057617188, Temp: 2.527360677719116, KL: 78.26681518554688, Loss: 0.022049928084015846, Learning Rate: 0.001729440299999999\n",
      "Epoch [6378/20000], Bound: 0.3931417465209961, Entropy: 139.92312622070312, Temp: 2.5272953510284424, KL: 72.99116516113281, Loss: 0.02071899175643921, Learning Rate: 0.001729440299999999\n",
      "Epoch [6379/20000], Bound: 0.37926656007766724, Entropy: 142.72506713867188, Temp: 2.5272247791290283, KL: 69.63381958007812, Loss: 0.019523875787854195, Learning Rate: 0.001729440299999999\n",
      "Epoch [6380/20000], Bound: 0.38214918971061707, Entropy: 141.72706604003906, Temp: 2.527155876159668, KL: 70.14750671386719, Loss: 0.020122207701206207, Learning Rate: 0.001729440299999999\n",
      "Epoch [6381/20000], Bound: 0.39139172434806824, Entropy: 142.41831970214844, Temp: 2.5270802974700928, KL: 73.99980163574219, Loss: 0.01772385649383068, Learning Rate: 0.001729440299999999\n",
      "Epoch [6382/20000], Bound: 0.41394367814064026, Entropy: 141.4886932373047, Temp: 2.5270605087280273, KL: 77.01333618164062, Loss: 0.02480621263384819, Learning Rate: 0.001729440299999999\n",
      "Epoch [6383/20000], Bound: 0.3945378363132477, Entropy: 140.60159301757812, Temp: 2.5269742012023926, KL: 73.85403442382812, Loss: 0.019805265590548515, Learning Rate: 0.001729440299999999\n",
      "Epoch [6384/20000], Bound: 0.4214732050895691, Entropy: 141.59852600097656, Temp: 2.5269057750701904, KL: 81.42623901367188, Loss: 0.020525794476270676, Learning Rate: 0.001729440299999999\n",
      "Epoch [6385/20000], Bound: 0.40661177039146423, Entropy: 142.35693359375, Temp: 2.526873826980591, KL: 77.69891357421875, Loss: 0.019158845767378807, Learning Rate: 0.001729440299999999\n",
      "Epoch [6386/20000], Bound: 0.4096094071865082, Entropy: 140.71458435058594, Temp: 2.526884078979492, KL: 77.134765625, Loss: 0.02202305942773819, Learning Rate: 0.001729440299999999\n",
      "Epoch [6387/20000], Bound: 0.43018925189971924, Entropy: 142.080810546875, Temp: 2.526876211166382, KL: 84.11164855957031, Loss: 0.020429959520697594, Learning Rate: 0.001729440299999999\n",
      "Epoch [6388/20000], Bound: 0.4015920162200928, Entropy: 138.17691040039062, Temp: 2.5269126892089844, KL: 76.90855407714844, Loss: 0.017813576385378838, Learning Rate: 0.001729440299999999\n",
      "Epoch [6389/20000], Bound: 0.397517591714859, Entropy: 140.18844604492188, Temp: 2.527005434036255, KL: 74.3336181640625, Loss: 0.020563649013638496, Learning Rate: 0.001729440299999999\n",
      "Epoch [6390/20000], Bound: 0.40442875027656555, Entropy: 137.934814453125, Temp: 2.5270862579345703, KL: 75.49166870117188, Loss: 0.022260818630456924, Learning Rate: 0.001729440299999999\n",
      "Epoch [6391/20000], Bound: 0.398960143327713, Entropy: 137.4021453857422, Temp: 2.527130126953125, KL: 74.70008850097656, Loss: 0.020669011399149895, Learning Rate: 0.001729440299999999\n",
      "Epoch [6392/20000], Bound: 0.4126032590866089, Entropy: 136.31593322753906, Temp: 2.5271666049957275, KL: 79.06343078613281, Loss: 0.019963860511779785, Learning Rate: 0.001729440299999999\n",
      "Epoch [6393/20000], Bound: 0.40189820528030396, Entropy: 137.50741577148438, Temp: 2.527229070663452, KL: 75.86495971679688, Loss: 0.02005927450954914, Learning Rate: 0.001729440299999999\n",
      "Epoch [6394/20000], Bound: 0.3964685797691345, Entropy: 137.55308532714844, Temp: 2.527299165725708, KL: 74.52546691894531, Loss: 0.01958584412932396, Learning Rate: 0.001729440299999999\n",
      "Epoch [6395/20000], Bound: 0.38871487975120544, Entropy: 137.5718536376953, Temp: 2.5273783206939697, KL: 72.48220825195312, Loss: 0.0192097295075655, Learning Rate: 0.001729440299999999\n",
      "Epoch [6396/20000], Bound: 0.4101811349391937, Entropy: 136.10264587402344, Temp: 2.527463674545288, KL: 77.57475280761719, Loss: 0.021493827924132347, Learning Rate: 0.001729440299999999\n",
      "Epoch [6397/20000], Bound: 0.38062340021133423, Entropy: 136.41049194335938, Temp: 2.5275349617004395, KL: 68.69357299804688, Loss: 0.022146649658679962, Learning Rate: 0.001729440299999999\n",
      "Epoch [6398/20000], Bound: 0.3969481885433197, Entropy: 138.58596801757812, Temp: 2.5275399684906006, KL: 73.96878051757812, Loss: 0.020964888855814934, Learning Rate: 0.001729440299999999\n",
      "Epoch [6399/20000], Bound: 0.40064147114753723, Entropy: 137.88099670410156, Temp: 2.5275325775146484, KL: 75.97956848144531, Loss: 0.01911095529794693, Learning Rate: 0.001729440299999999\n",
      "Epoch [6400/20000], Bound: 0.4030340015888214, Entropy: 136.8793487548828, Temp: 2.527557373046875, KL: 76.88310241699219, Loss: 0.018705902621150017, Learning Rate: 0.001729440299999999\n",
      "Epoch [6401/20000], Bound: 0.40626269578933716, Entropy: 137.89263916015625, Temp: 2.527623414993286, KL: 76.49751281738281, Loss: 0.021342115476727486, Learning Rate: 0.001729440299999999\n",
      "Epoch [6402/20000], Bound: 0.354056715965271, Entropy: 140.49026489257812, Temp: 2.527674913406372, KL: 62.71315002441406, Loss: 0.019367076456546783, Learning Rate: 0.001729440299999999\n",
      "Epoch [6403/20000], Bound: 0.3921785056591034, Entropy: 138.21804809570312, Temp: 2.52768611907959, KL: 73.25224304199219, Loss: 0.019657930359244347, Learning Rate: 0.001729440299999999\n",
      "Epoch [6404/20000], Bound: 0.3964342772960663, Entropy: 137.59552001953125, Temp: 2.527704954147339, KL: 74.94047546386719, Loss: 0.018750084564089775, Learning Rate: 0.001729440299999999\n",
      "Epoch [6405/20000], Bound: 0.36084413528442383, Entropy: 139.0704803466797, Temp: 2.5277557373046875, KL: 64.47669982910156, Loss: 0.01956121250987053, Learning Rate: 0.001729440299999999\n",
      "Epoch [6406/20000], Bound: 0.36782369017601013, Entropy: 138.88583374023438, Temp: 2.52777099609375, KL: 66.9078369140625, Loss: 0.018574513494968414, Learning Rate: 0.001729440299999999\n",
      "Epoch [6407/20000], Bound: 0.4271557629108429, Entropy: 138.29039001464844, Temp: 2.5277843475341797, KL: 84.20953369140625, Loss: 0.018425898626446724, Learning Rate: 0.001729440299999999\n",
      "Epoch [6408/20000], Bound: 0.39608368277549744, Entropy: 140.263427734375, Temp: 2.5278773307800293, KL: 72.77928161621094, Loss: 0.022826066240668297, Learning Rate: 0.001729440299999999\n",
      "Epoch [6409/20000], Bound: 0.3975032567977905, Entropy: 139.967041015625, Temp: 2.5279083251953125, KL: 73.31033325195312, Loss: 0.022589916363358498, Learning Rate: 0.001729440299999999\n",
      "Epoch [6410/20000], Bound: 0.38463079929351807, Entropy: 139.4511260986328, Temp: 2.527890205383301, KL: 72.16868591308594, Loss: 0.017527995631098747, Learning Rate: 0.001729440299999999\n",
      "Epoch [6411/20000], Bound: 0.39104437828063965, Entropy: 142.00791931152344, Temp: 2.5279181003570557, KL: 72.34274291992188, Loss: 0.02081414684653282, Learning Rate: 0.001729440299999999\n",
      "Epoch [6412/20000], Bound: 0.3717157244682312, Entropy: 141.91366577148438, Temp: 2.527925968170166, KL: 67.89720153808594, Loss: 0.018766626715660095, Learning Rate: 0.001729440299999999\n",
      "Epoch [6413/20000], Bound: 0.41268911957740784, Entropy: 141.4538116455078, Temp: 2.5279340744018555, KL: 79.593017578125, Loss: 0.018976803869009018, Learning Rate: 0.001729440299999999\n",
      "Epoch [6414/20000], Bound: 0.39859023690223694, Entropy: 143.35769653320312, Temp: 2.527991533279419, KL: 74.53579711914062, Loss: 0.02079140581190586, Learning Rate: 0.001729440299999999\n",
      "Epoch [6415/20000], Bound: 0.3838566243648529, Entropy: 143.58457946777344, Temp: 2.5280370712280273, KL: 70.89425659179688, Loss: 0.019614391028881073, Learning Rate: 0.001729440299999999\n",
      "Epoch [6416/20000], Bound: 0.37080851197242737, Entropy: 141.20721435546875, Temp: 2.528076648712158, KL: 68.62020874023438, Loss: 0.01683666743338108, Learning Rate: 0.001729440299999999\n",
      "Epoch [6417/20000], Bound: 0.41600826382637024, Entropy: 139.55447387695312, Temp: 2.52815318107605, KL: 79.92475891113281, Loss: 0.020276235416531563, Learning Rate: 0.001729440299999999\n",
      "Epoch [6418/20000], Bound: 0.39319294691085815, Entropy: 141.19081115722656, Temp: 2.528249740600586, KL: 73.4561767578125, Loss: 0.01983928307890892, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6419/20000], Bound: 0.37035107612609863, Entropy: 143.60403442382812, Temp: 2.5283429622650146, KL: 67.36642456054688, Loss: 0.01906643435359001, Learning Rate: 0.001729440299999999\n",
      "Epoch [6420/20000], Bound: 0.4158242642879486, Entropy: 139.9279022216797, Temp: 2.528419256210327, KL: 78.39935302734375, Loss: 0.02318781614303589, Learning Rate: 0.001729440299999999\n",
      "Epoch [6421/20000], Bound: 0.4045770764350891, Entropy: 142.2283172607422, Temp: 2.528454303741455, KL: 77.95510864257812, Loss: 0.017491158097982407, Learning Rate: 0.001729440299999999\n",
      "Epoch [6422/20000], Bound: 0.3840728998184204, Entropy: 142.00198364257812, Temp: 2.5285565853118896, KL: 70.005859375, Loss: 0.02149856463074684, Learning Rate: 0.001729440299999999\n",
      "Epoch [6423/20000], Bound: 0.38830655813217163, Entropy: 142.37657165527344, Temp: 2.5286073684692383, KL: 71.66270446777344, Loss: 0.020613035187125206, Learning Rate: 0.001729440299999999\n",
      "Epoch [6424/20000], Bound: 0.36967000365257263, Entropy: 140.2021026611328, Temp: 2.5286359786987305, KL: 66.68672180175781, Loss: 0.02003759890794754, Learning Rate: 0.001729440299999999\n",
      "Epoch [6425/20000], Bound: 0.40284889936447144, Entropy: 139.94049072265625, Temp: 2.528632164001465, KL: 75.54060363769531, Loss: 0.021267177537083626, Learning Rate: 0.001729440299999999\n",
      "Epoch [6426/20000], Bound: 0.41587942838668823, Entropy: 139.8406982421875, Temp: 2.5286173820495605, KL: 79.71173095703125, Loss: 0.020627669990062714, Learning Rate: 0.001729440299999999\n",
      "Epoch [6427/20000], Bound: 0.3954194486141205, Entropy: 142.0952911376953, Temp: 2.5286240577697754, KL: 73.57499694824219, Loss: 0.020880388095974922, Learning Rate: 0.001729440299999999\n",
      "Epoch [6428/20000], Bound: 0.3838317096233368, Entropy: 140.43077087402344, Temp: 2.5286169052124023, KL: 69.61090087890625, Loss: 0.02214437909424305, Learning Rate: 0.001729440299999999\n",
      "Epoch [6429/20000], Bound: 0.3659326136112213, Entropy: 139.9620819091797, Temp: 2.528554677963257, KL: 67.624267578125, Loss: 0.016126815229654312, Learning Rate: 0.001729440299999999\n",
      "Epoch [6430/20000], Bound: 0.38669872283935547, Entropy: 140.87657165527344, Temp: 2.5285487174987793, KL: 70.43070983886719, Loss: 0.022139109671115875, Learning Rate: 0.001729440299999999\n",
      "Epoch [6431/20000], Bound: 0.3784506618976593, Entropy: 140.74632263183594, Temp: 2.528491497039795, KL: 71.08651733398438, Loss: 0.016208834946155548, Learning Rate: 0.001729440299999999\n",
      "Epoch [6432/20000], Bound: 0.4102630913257599, Entropy: 142.14491271972656, Temp: 2.5285046100616455, KL: 78.42753601074219, Loss: 0.01986817829310894, Learning Rate: 0.001729440299999999\n",
      "Epoch [6433/20000], Bound: 0.3809853196144104, Entropy: 143.17454528808594, Temp: 2.5285449028015137, KL: 71.84033203125, Loss: 0.016136767342686653, Learning Rate: 0.001729440299999999\n",
      "Epoch [6434/20000], Bound: 0.40736961364746094, Entropy: 138.95022583007812, Temp: 2.5286507606506348, KL: 77.36848449707031, Loss: 0.02027612179517746, Learning Rate: 0.001729440299999999\n",
      "Epoch [6435/20000], Bound: 0.4210592806339264, Entropy: 139.43365478515625, Temp: 2.528761863708496, KL: 81.2391357421875, Loss: 0.02067502960562706, Learning Rate: 0.001729440299999999\n",
      "Epoch [6436/20000], Bound: 0.39787665009498596, Entropy: 141.52784729003906, Temp: 2.528887987136841, KL: 76.24992370605469, Loss: 0.017002856358885765, Learning Rate: 0.001729440299999999\n",
      "Epoch [6437/20000], Bound: 0.39294421672821045, Entropy: 139.73377990722656, Temp: 2.5290746688842773, KL: 74.22836303710938, Loss: 0.01818045601248741, Learning Rate: 0.001729440299999999\n",
      "Epoch [6438/20000], Bound: 0.40572184324264526, Entropy: 139.14202880859375, Temp: 2.5292837619781494, KL: 76.41928100585938, Loss: 0.02120254933834076, Learning Rate: 0.001729440299999999\n",
      "Epoch [6439/20000], Bound: 0.39495357871055603, Entropy: 140.74354553222656, Temp: 2.529465675354004, KL: 73.68408203125, Loss: 0.020407967269420624, Learning Rate: 0.001729440299999999\n",
      "Epoch [6440/20000], Bound: 0.39951562881469727, Entropy: 140.58348083496094, Temp: 2.529625415802002, KL: 72.31809997558594, Loss: 0.02572592906653881, Learning Rate: 0.001729440299999999\n",
      "Epoch [6441/20000], Bound: 0.4053956866264343, Entropy: 138.9216766357422, Temp: 2.5296576023101807, KL: 75.6378173828125, Loss: 0.02256234548985958, Learning Rate: 0.001729440299999999\n",
      "Epoch [6442/20000], Bound: 0.42275863885879517, Entropy: 141.3839874267578, Temp: 2.5296506881713867, KL: 80.37968444824219, Loss: 0.0233964491635561, Learning Rate: 0.001729440299999999\n",
      "Epoch [6443/20000], Bound: 0.3840999901294708, Entropy: 141.92333984375, Temp: 2.5296151638031006, KL: 70.45259094238281, Loss: 0.02064160257577896, Learning Rate: 0.001729440299999999\n",
      "Epoch [6444/20000], Bound: 0.36296460032463074, Entropy: 141.174072265625, Temp: 2.529559373855591, KL: 64.71630859375, Loss: 0.020261073485016823, Learning Rate: 0.001729440299999999\n",
      "Epoch [6445/20000], Bound: 0.39176201820373535, Entropy: 139.96636962890625, Temp: 2.5294647216796875, KL: 72.8333740234375, Loss: 0.02026950567960739, Learning Rate: 0.001729440299999999\n",
      "Epoch [6446/20000], Bound: 0.3963272273540497, Entropy: 139.1320037841797, Temp: 2.529374122619629, KL: 73.08006286621094, Loss: 0.022386856377124786, Learning Rate: 0.001729440299999999\n",
      "Epoch [6447/20000], Bound: 0.394156277179718, Entropy: 140.80177307128906, Temp: 2.529247999191284, KL: 73.52761840820312, Loss: 0.02025926485657692, Learning Rate: 0.001729440299999999\n",
      "Epoch [6448/20000], Bound: 0.3831539750099182, Entropy: 138.74693298339844, Temp: 2.529132604598999, KL: 72.13470458984375, Loss: 0.016778966411948204, Learning Rate: 0.001729440299999999\n",
      "Epoch [6449/20000], Bound: 0.3885074555873871, Entropy: 141.49618530273438, Temp: 2.5290870666503906, KL: 72.04541015625, Loss: 0.01997542381286621, Learning Rate: 0.001729440299999999\n",
      "Epoch [6450/20000], Bound: 0.37989285588264465, Entropy: 140.47898864746094, Temp: 2.529042959213257, KL: 69.13381958007812, Loss: 0.02088199183344841, Learning Rate: 0.001729440299999999\n",
      "Epoch [6451/20000], Bound: 0.40126481652259827, Entropy: 142.6127471923828, Temp: 2.528968572616577, KL: 76.65057373046875, Loss: 0.01816168799996376, Learning Rate: 0.001729440299999999\n",
      "Epoch [6452/20000], Bound: 0.3505438268184662, Entropy: 141.71563720703125, Temp: 2.52895450592041, KL: 61.438323974609375, Loss: 0.02000737190246582, Learning Rate: 0.001729440299999999\n",
      "Epoch [6453/20000], Bound: 0.377638578414917, Entropy: 143.42503356933594, Temp: 2.528886318206787, KL: 70.55982971191406, Loss: 0.01680174097418785, Learning Rate: 0.001729440299999999\n",
      "Epoch [6454/20000], Bound: 0.3956722915172577, Entropy: 143.9351043701172, Temp: 2.5288760662078857, KL: 73.89227294921875, Loss: 0.020400628447532654, Learning Rate: 0.001729440299999999\n",
      "Epoch [6455/20000], Bound: 0.39965957403182983, Entropy: 141.13706970214844, Temp: 2.5288641452789307, KL: 75.2440185546875, Loss: 0.020016290247440338, Learning Rate: 0.001729440299999999\n",
      "Epoch [6456/20000], Bound: 0.38510605692863464, Entropy: 144.4197998046875, Temp: 2.528864622116089, KL: 70.93553161621094, Loss: 0.02024548500776291, Learning Rate: 0.001729440299999999\n",
      "Epoch [6457/20000], Bound: 0.4208986461162567, Entropy: 144.2443389892578, Temp: 2.528851270675659, KL: 82.30232238769531, Loss: 0.018478674814105034, Learning Rate: 0.001729440299999999\n",
      "Epoch [6458/20000], Bound: 0.4007050395011902, Entropy: 144.25599670410156, Temp: 2.528911828994751, KL: 75.57101440429688, Loss: 0.01997256651520729, Learning Rate: 0.001729440299999999\n",
      "Epoch [6459/20000], Bound: 0.396361380815506, Entropy: 142.1845245361328, Temp: 2.528979778289795, KL: 72.64340209960938, Loss: 0.023265492171049118, Learning Rate: 0.001729440299999999\n",
      "Epoch [6460/20000], Bound: 0.39101701974868774, Entropy: 143.1390838623047, Temp: 2.528977394104004, KL: 73.198486328125, Loss: 0.01911839284002781, Learning Rate: 0.001729440299999999\n",
      "Epoch [6461/20000], Bound: 0.4261265993118286, Entropy: 142.7128448486328, Temp: 2.528993844985962, KL: 82.43754577636719, Loss: 0.02133074216544628, Learning Rate: 0.001729440299999999\n",
      "Epoch [6462/20000], Bound: 0.3768022358417511, Entropy: 144.2428741455078, Temp: 2.5290279388427734, KL: 67.90446472167969, Loss: 0.021587004885077477, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6463/20000], Bound: 0.42682376503944397, Entropy: 144.569091796875, Temp: 2.5290040969848633, KL: 83.00814819335938, Loss: 0.020620331168174744, Learning Rate: 0.001729440299999999\n",
      "Epoch [6464/20000], Bound: 0.4204259514808655, Entropy: 143.45384216308594, Temp: 2.529017686843872, KL: 81.86701965332031, Loss: 0.01906106434762478, Learning Rate: 0.001729440299999999\n",
      "Epoch [6465/20000], Bound: 0.4028843343257904, Entropy: 144.10379028320312, Temp: 2.52908992767334, KL: 76.11009216308594, Loss: 0.020167183130979538, Learning Rate: 0.001729440299999999\n",
      "Epoch [6466/20000], Bound: 0.3946545124053955, Entropy: 142.918701171875, Temp: 2.5291671752929688, KL: 74.23226928710938, Loss: 0.01914985477924347, Learning Rate: 0.001729440299999999\n",
      "Epoch [6467/20000], Bound: 0.4010471999645233, Entropy: 142.1895294189453, Temp: 2.529259443283081, KL: 75.39552307128906, Loss: 0.020520955324172974, Learning Rate: 0.001729440299999999\n",
      "Epoch [6468/20000], Bound: 0.40465375781059265, Entropy: 141.35777282714844, Temp: 2.5293447971343994, KL: 75.95062255859375, Loss: 0.02151002362370491, Learning Rate: 0.001729440299999999\n",
      "Epoch [6469/20000], Bound: 0.4094806909561157, Entropy: 140.39242553710938, Temp: 2.529406785964966, KL: 78.46336364746094, Loss: 0.019351933151483536, Learning Rate: 0.001729440299999999\n",
      "Epoch [6470/20000], Bound: 0.39836931228637695, Entropy: 143.58428955078125, Temp: 2.529501438140869, KL: 74.99253845214844, Loss: 0.019779162481427193, Learning Rate: 0.001729440299999999\n",
      "Epoch [6471/20000], Bound: 0.3981308937072754, Entropy: 141.26052856445312, Temp: 2.5296008586883545, KL: 74.85649108886719, Loss: 0.019912375137209892, Learning Rate: 0.001729440299999999\n",
      "Epoch [6472/20000], Bound: 0.39413607120513916, Entropy: 140.77304077148438, Temp: 2.5297012329101562, KL: 73.36325073242188, Loss: 0.02057780884206295, Learning Rate: 0.001729440299999999\n",
      "Epoch [6473/20000], Bound: 0.4003360867500305, Entropy: 140.7630615234375, Temp: 2.529782772064209, KL: 73.67312622070312, Loss: 0.02352161891758442, Learning Rate: 0.001729440299999999\n",
      "Epoch [6474/20000], Bound: 0.4120723307132721, Entropy: 141.60276794433594, Temp: 2.5297915935516357, KL: 78.02046203613281, Loss: 0.0217482540756464, Learning Rate: 0.001729440299999999\n",
      "Epoch [6475/20000], Bound: 0.39132413268089294, Entropy: 140.84519958496094, Temp: 2.529790163040161, KL: 73.34745788574219, Loss: 0.01900804229080677, Learning Rate: 0.001729440299999999\n",
      "Epoch [6476/20000], Bound: 0.37669554352760315, Entropy: 139.032958984375, Temp: 2.5298101902008057, KL: 68.55343627929688, Loss: 0.020252469927072525, Learning Rate: 0.001729440299999999\n",
      "Epoch [6477/20000], Bound: 0.3991345465183258, Entropy: 139.67318725585938, Temp: 2.5298023223876953, KL: 75.3548583984375, Loss: 0.019506458193063736, Learning Rate: 0.001729440299999999\n",
      "Epoch [6478/20000], Bound: 0.39587756991386414, Entropy: 139.40061950683594, Temp: 2.529816150665283, KL: 73.84690856933594, Loss: 0.020618587732315063, Learning Rate: 0.001729440299999999\n",
      "Epoch [6479/20000], Bound: 0.4118105471134186, Entropy: 139.19786071777344, Temp: 2.5298211574554443, KL: 78.78721618652344, Loss: 0.020079782232642174, Learning Rate: 0.001729440299999999\n",
      "Epoch [6480/20000], Bound: 0.38638532161712646, Entropy: 140.85922241210938, Temp: 2.5298514366149902, KL: 70.83050537109375, Loss: 0.021185245364904404, Learning Rate: 0.001729440299999999\n",
      "Epoch [6481/20000], Bound: 0.3665362298488617, Entropy: 141.81494140625, Temp: 2.529845714569092, KL: 64.65657043457031, Loss: 0.0223376527428627, Learning Rate: 0.001729440299999999\n",
      "Epoch [6482/20000], Bound: 0.3903762698173523, Entropy: 139.2377166748047, Temp: 2.529754400253296, KL: 72.35829162597656, Loss: 0.020424140617251396, Learning Rate: 0.001729440299999999\n",
      "Epoch [6483/20000], Bound: 0.4000507593154907, Entropy: 142.43927001953125, Temp: 2.529660940170288, KL: 74.43708801269531, Loss: 0.021845946088433266, Learning Rate: 0.001729440299999999\n",
      "Epoch [6484/20000], Bound: 0.366854727268219, Entropy: 141.73158264160156, Temp: 2.5295488834381104, KL: 65.271728515625, Loss: 0.021294033154845238, Learning Rate: 0.001729440299999999\n",
      "Epoch [6485/20000], Bound: 0.4121670424938202, Entropy: 141.69735717773438, Temp: 2.5293848514556885, KL: 80.24185180664062, Loss: 0.017407625913619995, Learning Rate: 0.001729440299999999\n",
      "Epoch [6486/20000], Bound: 0.40445223450660706, Entropy: 142.3636474609375, Temp: 2.5293219089508057, KL: 77.70146179199219, Loss: 0.017931876704096794, Learning Rate: 0.001729440299999999\n",
      "Epoch [6487/20000], Bound: 0.41044774651527405, Entropy: 139.9701690673828, Temp: 2.5293281078338623, KL: 79.11155700683594, Loss: 0.018634604290127754, Learning Rate: 0.001729440299999999\n",
      "Epoch [6488/20000], Bound: 0.39299681782722473, Entropy: 142.74417114257812, Temp: 2.5293896198272705, KL: 73.04295349121094, Loss: 0.020557550713419914, Learning Rate: 0.001729440299999999\n",
      "Epoch [6489/20000], Bound: 0.38963741064071655, Entropy: 141.50668334960938, Temp: 2.5294349193573, KL: 72.25213623046875, Loss: 0.02021106146275997, Learning Rate: 0.001729440299999999\n",
      "Epoch [6490/20000], Bound: 0.4072176218032837, Entropy: 140.4456024169922, Temp: 2.529468536376953, KL: 78.35427856445312, Loss: 0.018249323591589928, Learning Rate: 0.001729440299999999\n",
      "Epoch [6491/20000], Bound: 0.37722378969192505, Entropy: 140.13711547851562, Temp: 2.5295584201812744, KL: 71.61366271972656, Loss: 0.014495260082185268, Learning Rate: 0.001729440299999999\n",
      "Epoch [6492/20000], Bound: 0.3972984254360199, Entropy: 141.10816955566406, Temp: 2.5297415256500244, KL: 76.36932373046875, Loss: 0.016446245834231377, Learning Rate: 0.001729440299999999\n",
      "Epoch [6493/20000], Bound: 0.3978807032108307, Entropy: 139.99073791503906, Temp: 2.52999210357666, KL: 74.45320129394531, Loss: 0.020570440217852592, Learning Rate: 0.001729440299999999\n",
      "Epoch [6494/20000], Bound: 0.4072493612766266, Entropy: 139.56834411621094, Temp: 2.5302140712738037, KL: 77.93583679199219, Loss: 0.01910458691418171, Learning Rate: 0.001729440299999999\n",
      "Epoch [6495/20000], Bound: 0.39359787106513977, Entropy: 139.06944274902344, Temp: 2.530454635620117, KL: 72.98091125488281, Loss: 0.02103475108742714, Learning Rate: 0.001729440299999999\n",
      "Epoch [6496/20000], Bound: 0.394326776266098, Entropy: 138.37522888183594, Temp: 2.5306508541107178, KL: 73.90382385253906, Loss: 0.019629400223493576, Learning Rate: 0.001729440299999999\n",
      "Epoch [6497/20000], Bound: 0.3855407238006592, Entropy: 139.8253936767578, Temp: 2.530839443206787, KL: 72.1409912109375, Loss: 0.018130164593458176, Learning Rate: 0.001729440299999999\n",
      "Epoch [6498/20000], Bound: 0.3717457354068756, Entropy: 140.5600128173828, Temp: 2.5310416221618652, KL: 67.89794921875, Loss: 0.018814068287611008, Learning Rate: 0.001729440299999999\n",
      "Epoch [6499/20000], Bound: 0.40490439534187317, Entropy: 138.0802459716797, Temp: 2.531221866607666, KL: 75.81216430664062, Loss: 0.021950924769043922, Learning Rate: 0.001729440299999999\n",
      "Epoch [6500/20000], Bound: 0.40837767720222473, Entropy: 139.78623962402344, Temp: 2.5313594341278076, KL: 75.91995239257812, Loss: 0.023758037015795708, Learning Rate: 0.001729440299999999\n",
      "Epoch [6501/20000], Bound: 0.41175034642219543, Entropy: 139.27783203125, Temp: 2.531424045562744, KL: 76.507568359375, Loss: 0.024567989632487297, Learning Rate: 0.001729440299999999\n",
      "Epoch [6502/20000], Bound: 0.3707664906978607, Entropy: 143.42556762695312, Temp: 2.531409740447998, KL: 67.86552429199219, Loss: 0.01834101974964142, Learning Rate: 0.001729440299999999\n",
      "Epoch [6503/20000], Bound: 0.38223639130592346, Entropy: 140.540771484375, Temp: 2.531404495239258, KL: 69.85110473632812, Loss: 0.020801637321710587, Learning Rate: 0.001729440299999999\n",
      "Epoch [6504/20000], Bound: 0.4075203239917755, Entropy: 140.91746520996094, Temp: 2.5313682556152344, KL: 77.69276428222656, Loss: 0.019757265225052834, Learning Rate: 0.001729440299999999\n",
      "Epoch [6505/20000], Bound: 0.4020465612411499, Entropy: 140.69366455078125, Temp: 2.531362295150757, KL: 75.40855407714844, Loss: 0.021096672862768173, Learning Rate: 0.001729440299999999\n",
      "Epoch [6506/20000], Bound: 0.40034255385398865, Entropy: 141.68045043945312, Temp: 2.5313467979431152, KL: 74.93876647949219, Loss: 0.021042045205831528, Learning Rate: 0.001729440299999999\n",
      "Epoch [6507/20000], Bound: 0.3991602063179016, Entropy: 139.73123168945312, Temp: 2.5313212871551514, KL: 74.35906982421875, Loss: 0.021506590768694878, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6508/20000], Bound: 0.4002942740917206, Entropy: 141.97547912597656, Temp: 2.5312750339508057, KL: 73.82717895507812, Loss: 0.0232091061770916, Learning Rate: 0.001729440299999999\n",
      "Epoch [6509/20000], Bound: 0.380693644285202, Entropy: 142.98291015625, Temp: 2.5311741828918457, KL: 70.43263244628906, Loss: 0.018785949796438217, Learning Rate: 0.001729440299999999\n",
      "Epoch [6510/20000], Bound: 0.3787957429885864, Entropy: 141.674560546875, Temp: 2.531094789505005, KL: 69.57418823242188, Loss: 0.019419897347688675, Learning Rate: 0.001729440299999999\n",
      "Epoch [6511/20000], Bound: 0.42926642298698425, Entropy: 138.990966796875, Temp: 2.531017780303955, KL: 83.22032165527344, Loss: 0.02169511467218399, Learning Rate: 0.001729440299999999\n",
      "Epoch [6512/20000], Bound: 0.3979746699333191, Entropy: 141.12258911132812, Temp: 2.53096342086792, KL: 74.71611022949219, Loss: 0.020116247236728668, Learning Rate: 0.001729440299999999\n",
      "Epoch [6513/20000], Bound: 0.3871663212776184, Entropy: 140.59506225585938, Temp: 2.5309205055236816, KL: 71.81492614746094, Loss: 0.0196926761418581, Learning Rate: 0.001729440299999999\n",
      "Epoch [6514/20000], Bound: 0.39675000309944153, Entropy: 139.13748168945312, Temp: 2.530881881713867, KL: 74.27317810058594, Loss: 0.020288173109292984, Learning Rate: 0.001729440299999999\n",
      "Epoch [6515/20000], Bound: 0.4180710017681122, Entropy: 140.9947509765625, Temp: 2.5308473110198975, KL: 79.77200317382812, Loss: 0.021831154823303223, Learning Rate: 0.001729440299999999\n",
      "Epoch [6516/20000], Bound: 0.39186805486679077, Entropy: 142.3683319091797, Temp: 2.530812978744507, KL: 75.356201171875, Loss: 0.015360774472355843, Learning Rate: 0.001729440299999999\n",
      "Epoch [6517/20000], Bound: 0.39618879556655884, Entropy: 139.84548950195312, Temp: 2.5308847427368164, KL: 74.84719848632812, Loss: 0.018832780420780182, Learning Rate: 0.001729440299999999\n",
      "Epoch [6518/20000], Bound: 0.36902838945388794, Entropy: 141.54653930664062, Temp: 2.5309810638427734, KL: 66.69537353515625, Loss: 0.019689802080392838, Learning Rate: 0.001729440299999999\n",
      "Epoch [6519/20000], Bound: 0.4242728650569916, Entropy: 140.75546264648438, Temp: 2.5310428142547607, KL: 81.28956604003906, Loss: 0.022518891841173172, Learning Rate: 0.001729440299999999\n",
      "Epoch [6520/20000], Bound: 0.3867635726928711, Entropy: 141.970947265625, Temp: 2.5310888290405273, KL: 71.10551452636719, Loss: 0.02086849883198738, Learning Rate: 0.001729440299999999\n",
      "Epoch [6521/20000], Bound: 0.3718283474445343, Entropy: 141.82884216308594, Temp: 2.5311033725738525, KL: 68.3768310546875, Loss: 0.017914380878210068, Learning Rate: 0.001729440299999999\n",
      "Epoch [6522/20000], Bound: 0.3912082612514496, Entropy: 141.6297607421875, Temp: 2.531135320663452, KL: 71.701171875, Loss: 0.02221006155014038, Learning Rate: 0.001729440299999999\n",
      "Epoch [6523/20000], Bound: 0.38737496733665466, Entropy: 140.43914794921875, Temp: 2.531113862991333, KL: 71.971923828125, Loss: 0.019502634182572365, Learning Rate: 0.001729440299999999\n",
      "Epoch [6524/20000], Bound: 0.37861359119415283, Entropy: 141.49998474121094, Temp: 2.5310988426208496, KL: 68.96258544921875, Loss: 0.02052639238536358, Learning Rate: 0.001729440299999999\n",
      "Epoch [6525/20000], Bound: 0.37701258063316345, Entropy: 142.66700744628906, Temp: 2.531054973602295, KL: 69.37248229980469, Loss: 0.018823610618710518, Learning Rate: 0.001729440299999999\n",
      "Epoch [6526/20000], Bound: 0.40309709310531616, Entropy: 141.17909240722656, Temp: 2.5310208797454834, KL: 76.18789672851562, Loss: 0.020160011947155, Learning Rate: 0.001729440299999999\n",
      "Epoch [6527/20000], Bound: 0.4087162911891937, Entropy: 140.18878173828125, Temp: 2.5310022830963135, KL: 76.66439819335938, Loss: 0.022480756044387817, Learning Rate: 0.001729440299999999\n",
      "Epoch [6528/20000], Bound: 0.39027559757232666, Entropy: 142.6532745361328, Temp: 2.530954122543335, KL: 72.48464965820312, Loss: 0.020130710676312447, Learning Rate: 0.001729440299999999\n",
      "Epoch [6529/20000], Bound: 0.41500794887542725, Entropy: 141.19073486328125, Temp: 2.5309057235717773, KL: 80.38864135742188, Loss: 0.01880711503326893, Learning Rate: 0.001729440299999999\n",
      "Epoch [6530/20000], Bound: 0.3874523341655731, Entropy: 142.57470703125, Temp: 2.5309207439422607, KL: 70.48989868164062, Loss: 0.022472001612186432, Learning Rate: 0.001729440299999999\n",
      "Epoch [6531/20000], Bound: 0.4154030680656433, Entropy: 142.9310760498047, Temp: 2.530872344970703, KL: 78.91331481933594, Loss: 0.021953877061605453, Learning Rate: 0.001729440299999999\n",
      "Epoch [6532/20000], Bound: 0.39632079005241394, Entropy: 139.7709503173828, Temp: 2.5308189392089844, KL: 75.94197082519531, Loss: 0.016744650900363922, Learning Rate: 0.001729440299999999\n",
      "Epoch [6533/20000], Bound: 0.37512102723121643, Entropy: 142.36361694335938, Temp: 2.5308494567871094, KL: 67.0379638671875, Loss: 0.022381450980901718, Learning Rate: 0.001729440299999999\n",
      "Epoch [6534/20000], Bound: 0.4089087247848511, Entropy: 140.76365661621094, Temp: 2.530799627304077, KL: 79.52418518066406, Loss: 0.016940603032708168, Learning Rate: 0.001729440299999999\n",
      "Epoch [6535/20000], Bound: 0.3865799605846405, Entropy: 143.0026397705078, Temp: 2.530846118927002, KL: 72.509033203125, Loss: 0.01798940636217594, Learning Rate: 0.001729440299999999\n",
      "Epoch [6536/20000], Bound: 0.3992158770561218, Entropy: 141.30616760253906, Temp: 2.530925989151001, KL: 73.87554931640625, Loss: 0.022489352151751518, Learning Rate: 0.001729440299999999\n",
      "Epoch [6537/20000], Bound: 0.40221306681632996, Entropy: 139.9220733642578, Temp: 2.5309524536132812, KL: 76.18515014648438, Loss: 0.019653793424367905, Learning Rate: 0.001729440299999999\n",
      "Epoch [6538/20000], Bound: 0.41626986861228943, Entropy: 141.1957550048828, Temp: 2.5309982299804688, KL: 78.99650573730469, Loss: 0.022301802411675453, Learning Rate: 0.001729440299999999\n",
      "Epoch [6539/20000], Bound: 0.40679243206977844, Entropy: 140.50198364257812, Temp: 2.5310230255126953, KL: 78.06683349609375, Loss: 0.018590550869703293, Learning Rate: 0.001729440299999999\n",
      "Epoch [6540/20000], Bound: 0.38263437151908875, Entropy: 140.21470642089844, Temp: 2.531097412109375, KL: 70.81407165527344, Loss: 0.019119540229439735, Learning Rate: 0.001729440299999999\n",
      "Epoch [6541/20000], Bound: 0.41077950596809387, Entropy: 140.32254028320312, Temp: 2.5311713218688965, KL: 78.99494934082031, Loss: 0.019083566963672638, Learning Rate: 0.001729440299999999\n",
      "Epoch [6542/20000], Bound: 0.3940211534500122, Entropy: 140.73831176757812, Temp: 2.5312843322753906, KL: 70.38633728027344, Loss: 0.026410412043333054, Learning Rate: 0.001729440299999999\n",
      "Epoch [6543/20000], Bound: 0.3943825364112854, Entropy: 139.92100524902344, Temp: 2.531245231628418, KL: 72.4102783203125, Loss: 0.022618422284722328, Learning Rate: 0.001729440299999999\n",
      "Epoch [6544/20000], Bound: 0.4084208309650421, Entropy: 139.78643798828125, Temp: 2.5311548709869385, KL: 77.99711608886719, Loss: 0.01967770978808403, Learning Rate: 0.001729440299999999\n",
      "Epoch [6545/20000], Bound: 0.38696959614753723, Entropy: 140.86587524414062, Temp: 2.5311036109924316, KL: 72.56166076660156, Loss: 0.01810850016772747, Learning Rate: 0.001729440299999999\n",
      "Epoch [6546/20000], Bound: 0.3618036210536957, Entropy: 140.37319946289062, Temp: 2.531093120574951, KL: 64.7598876953125, Loss: 0.019555510953068733, Learning Rate: 0.001729440299999999\n",
      "Epoch [6547/20000], Bound: 0.38777124881744385, Entropy: 141.33941650390625, Temp: 2.5310511589050293, KL: 72.89448547363281, Loss: 0.017903465777635574, Learning Rate: 0.001729440299999999\n",
      "Epoch [6548/20000], Bound: 0.4065304100513458, Entropy: 141.3057861328125, Temp: 2.531054735183716, KL: 77.82186889648438, Loss: 0.0189225971698761, Learning Rate: 0.001729440299999999\n",
      "Epoch [6549/20000], Bound: 0.3876287341117859, Entropy: 139.22909545898438, Temp: 2.531102418899536, KL: 73.54246520996094, Loss: 0.01654345542192459, Learning Rate: 0.001729440299999999\n",
      "Epoch [6550/20000], Bound: 0.38342955708503723, Entropy: 140.0929718017578, Temp: 2.531216859817505, KL: 70.89749145507812, Loss: 0.019402695819735527, Learning Rate: 0.001729440299999999\n",
      "Epoch [6551/20000], Bound: 0.3964681625366211, Entropy: 140.67466735839844, Temp: 2.5313212871551514, KL: 74.41242980957031, Loss: 0.019856760278344154, Learning Rate: 0.001729440299999999\n",
      "Epoch [6552/20000], Bound: 0.37146568298339844, Entropy: 140.457763671875, Temp: 2.5314249992370605, KL: 68.55374145507812, Loss: 0.017367927357554436, Learning Rate: 0.001729440299999999\n",
      "Epoch [6553/20000], Bound: 0.393556147813797, Entropy: 139.8639373779297, Temp: 2.5315489768981934, KL: 73.76547241210938, Loss: 0.019473573192954063, Learning Rate: 0.001729440299999999\n",
      "Epoch [6554/20000], Bound: 0.3725125789642334, Entropy: 139.80567932128906, Temp: 2.531674385070801, KL: 66.90267944335938, Loss: 0.0212103221565485, Learning Rate: 0.001729440299999999\n",
      "Epoch [6555/20000], Bound: 0.41165682673454285, Entropy: 139.3827667236328, Temp: 2.5317318439483643, KL: 78.6600341796875, Loss: 0.02026567980647087, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6556/20000], Bound: 0.415894091129303, Entropy: 142.4325408935547, Temp: 2.5318055152893066, KL: 80.63262939453125, Loss: 0.01885923556983471, Learning Rate: 0.001729440299999999\n",
      "Epoch [6557/20000], Bound: 0.3942340016365051, Entropy: 140.64627075195312, Temp: 2.5319302082061768, KL: 73.67533874511719, Loss: 0.02004259265959263, Learning Rate: 0.001729440299999999\n",
      "Epoch [6558/20000], Bound: 0.39821097254753113, Entropy: 141.385986328125, Temp: 2.5320446491241455, KL: 73.85989379882812, Loss: 0.021955357864499092, Learning Rate: 0.001729440299999999\n",
      "Epoch [6559/20000], Bound: 0.4053889214992523, Entropy: 137.3314971923828, Temp: 2.5321121215820312, KL: 76.20751953125, Loss: 0.021461453288793564, Learning Rate: 0.001729440299999999\n",
      "Epoch [6560/20000], Bound: 0.411774218082428, Entropy: 141.5743865966797, Temp: 2.532158374786377, KL: 80.16348266601562, Loss: 0.017371172085404396, Learning Rate: 0.001729440299999999\n",
      "Epoch [6561/20000], Bound: 0.3931911885738373, Entropy: 139.90086364746094, Temp: 2.5322864055633545, KL: 72.54989624023438, Loss: 0.021674370393157005, Learning Rate: 0.001729440299999999\n",
      "Epoch [6562/20000], Bound: 0.3958585262298584, Entropy: 139.39254760742188, Temp: 2.532364845275879, KL: 74.7921142578125, Loss: 0.018770596012473106, Learning Rate: 0.001729440299999999\n",
      "Epoch [6563/20000], Bound: 0.38572385907173157, Entropy: 139.74803161621094, Temp: 2.532468318939209, KL: 72.12379455566406, Loss: 0.018286269158124924, Learning Rate: 0.001729440299999999\n",
      "Epoch [6564/20000], Bound: 0.38123440742492676, Entropy: 141.32766723632812, Temp: 2.532590627670288, KL: 69.6988525390625, Loss: 0.020552944391965866, Learning Rate: 0.001729440299999999\n",
      "Epoch [6565/20000], Bound: 0.43430545926094055, Entropy: 139.91439819335938, Temp: 2.53267240524292, KL: 85.102783203125, Loss: 0.021041298285126686, Learning Rate: 0.001729440299999999\n",
      "Epoch [6566/20000], Bound: 0.38983312249183655, Entropy: 141.2364959716797, Temp: 2.5327823162078857, KL: 73.29615783691406, Loss: 0.0182981975376606, Learning Rate: 0.001729440299999999\n",
      "Epoch [6567/20000], Bound: 0.39962366223335266, Entropy: 140.37210083007812, Temp: 2.5329160690307617, KL: 73.63177490234375, Loss: 0.02322668582201004, Learning Rate: 0.001729440299999999\n",
      "Epoch [6568/20000], Bound: 0.39367881417274475, Entropy: 142.83233642578125, Temp: 2.532973527908325, KL: 73.02609252929688, Loss: 0.021019596606492996, Learning Rate: 0.001729440299999999\n",
      "Epoch [6569/20000], Bound: 0.421442449092865, Entropy: 141.4341278076172, Temp: 2.533003807067871, KL: 82.520751953125, Loss: 0.01842965930700302, Learning Rate: 0.001729440299999999\n",
      "Epoch [6570/20000], Bound: 0.4066503345966339, Entropy: 139.96861267089844, Temp: 2.533107042312622, KL: 76.46072387695312, Loss: 0.021705487743020058, Learning Rate: 0.001729440299999999\n",
      "Epoch [6571/20000], Bound: 0.4146496057510376, Entropy: 142.43417358398438, Temp: 2.5331814289093018, KL: 79.78327941894531, Loss: 0.01982237957417965, Learning Rate: 0.001729440299999999\n",
      "Epoch [6572/20000], Bound: 0.413689523935318, Entropy: 142.0925750732422, Temp: 2.5332837104797363, KL: 78.89859008789062, Loss: 0.021005947142839432, Learning Rate: 0.001729440299999999\n",
      "Epoch [6573/20000], Bound: 0.420744389295578, Entropy: 139.30484008789062, Temp: 2.5333831310272217, KL: 81.28118896484375, Loss: 0.020467299968004227, Learning Rate: 0.001729440299999999\n",
      "Epoch [6574/20000], Bound: 0.3959118723869324, Entropy: 140.47320556640625, Temp: 2.5335021018981934, KL: 72.99462890625, Loss: 0.022362299263477325, Learning Rate: 0.001729440299999999\n",
      "Epoch [6575/20000], Bound: 0.39793044328689575, Entropy: 139.23231506347656, Temp: 2.533559799194336, KL: 73.55613708496094, Loss: 0.022410666570067406, Learning Rate: 0.001729440299999999\n",
      "Epoch [6576/20000], Bound: 0.3908243775367737, Entropy: 140.71395874023438, Temp: 2.533564805984497, KL: 73.15283203125, Loss: 0.01915256679058075, Learning Rate: 0.001729440299999999\n",
      "Epoch [6577/20000], Bound: 0.4018256366252899, Entropy: 140.8363037109375, Temp: 2.533585548400879, KL: 75.41195678710938, Loss: 0.02098850905895233, Learning Rate: 0.001729440299999999\n",
      "Epoch [6578/20000], Bound: 0.37157726287841797, Entropy: 138.80467224121094, Temp: 2.533594846725464, KL: 67.23287963867188, Loss: 0.020059799775481224, Learning Rate: 0.001729440299999999\n",
      "Epoch [6579/20000], Bound: 0.42144545912742615, Entropy: 138.1279754638672, Temp: 2.533571243286133, KL: 81.29788208007812, Loss: 0.02085288055241108, Learning Rate: 0.001729440299999999\n",
      "Epoch [6580/20000], Bound: 0.3741028606891632, Entropy: 140.8507537841797, Temp: 2.53357195854187, KL: 67.73008728027344, Loss: 0.02047569490969181, Learning Rate: 0.001729440299999999\n",
      "Epoch [6581/20000], Bound: 0.41606059670448303, Entropy: 138.79150390625, Temp: 2.5335350036621094, KL: 80.998779296875, Loss: 0.01825835183262825, Learning Rate: 0.001729440299999999\n",
      "Epoch [6582/20000], Bound: 0.40345191955566406, Entropy: 139.33741760253906, Temp: 2.533574104309082, KL: 76.56500244140625, Loss: 0.019651884213089943, Learning Rate: 0.001729440299999999\n",
      "Epoch [6583/20000], Bound: 0.4188239872455597, Entropy: 137.08901977539062, Temp: 2.533632278442383, KL: 81.43132019042969, Loss: 0.019037002697587013, Learning Rate: 0.001729440299999999\n",
      "Epoch [6584/20000], Bound: 0.39828041195869446, Entropy: 138.06466674804688, Temp: 2.533743381500244, KL: 75.19888305664062, Loss: 0.019371632486581802, Learning Rate: 0.001729440299999999\n",
      "Epoch [6585/20000], Bound: 0.40645378828048706, Entropy: 139.0321502685547, Temp: 2.5338656902313232, KL: 76.21835327148438, Loss: 0.02207855135202408, Learning Rate: 0.001729440299999999\n",
      "Epoch [6586/20000], Bound: 0.3873879611492157, Entropy: 139.09669494628906, Temp: 2.5339479446411133, KL: 69.60540771484375, Loss: 0.024211253970861435, Learning Rate: 0.001729440299999999\n",
      "Epoch [6587/20000], Bound: 0.3969945013523102, Entropy: 136.67063903808594, Temp: 2.5339176654815674, KL: 73.30224609375, Loss: 0.022379187867045403, Learning Rate: 0.001729440299999999\n",
      "Epoch [6588/20000], Bound: 0.4153611958026886, Entropy: 139.43663024902344, Temp: 2.533842086791992, KL: 80.83973693847656, Loss: 0.01816479116678238, Learning Rate: 0.001729440299999999\n",
      "Epoch [6589/20000], Bound: 0.38354355096817017, Entropy: 135.8947296142578, Temp: 2.5338473320007324, KL: 71.78057861328125, Loss: 0.017752889543771744, Learning Rate: 0.001729440299999999\n",
      "Epoch [6590/20000], Bound: 0.40238726139068604, Entropy: 139.15103149414062, Temp: 2.53389048576355, KL: 76.50657653808594, Loss: 0.019156165421009064, Learning Rate: 0.001729440299999999\n",
      "Epoch [6591/20000], Bound: 0.40843939781188965, Entropy: 136.80206298828125, Temp: 2.5339620113372803, KL: 79.00906372070312, Loss: 0.017727583646774292, Learning Rate: 0.001729440299999999\n",
      "Epoch [6592/20000], Bound: 0.38743019104003906, Entropy: 138.9120330810547, Temp: 2.53410005569458, KL: 73.64253234863281, Loss: 0.016270916908979416, Learning Rate: 0.001729440299999999\n",
      "Epoch [6593/20000], Bound: 0.43089112639427185, Entropy: 138.22021484375, Temp: 2.534301996231079, KL: 84.40841674804688, Loss: 0.020373186096549034, Learning Rate: 0.001729440299999999\n",
      "Epoch [6594/20000], Bound: 0.4051324427127838, Entropy: 138.1906280517578, Temp: 2.5345299243927, KL: 78.08958435058594, Loss: 0.017628515139222145, Learning Rate: 0.001729440299999999\n",
      "Epoch [6595/20000], Bound: 0.3656223714351654, Entropy: 139.63458251953125, Temp: 2.534806251525879, KL: 66.0882568359375, Loss: 0.019055200740695, Learning Rate: 0.001729440299999999\n",
      "Epoch [6596/20000], Bound: 0.39119952917099, Entropy: 137.59620666503906, Temp: 2.5350372791290283, KL: 73.17185974121094, Loss: 0.019344886764883995, Learning Rate: 0.001729440299999999\n",
      "Epoch [6597/20000], Bound: 0.3908871114253998, Entropy: 138.40618896484375, Temp: 2.535257339477539, KL: 73.13032531738281, Loss: 0.019252033904194832, Learning Rate: 0.001729440299999999\n",
      "Epoch [6598/20000], Bound: 0.38713037967681885, Entropy: 137.94383239746094, Temp: 2.5354692935943604, KL: 74.01034545898438, Loss: 0.015393382869660854, Learning Rate: 0.001729440299999999\n",
      "Epoch [6599/20000], Bound: 0.4083713889122009, Entropy: 136.92315673828125, Temp: 2.535756826400757, KL: 75.66658020019531, Loss: 0.024302853271365166, Learning Rate: 0.001729440299999999\n",
      "Epoch [6600/20000], Bound: 0.4078850746154785, Entropy: 139.11073303222656, Temp: 2.5359392166137695, KL: 78.005859375, Loss: 0.0194096639752388, Learning Rate: 0.001729440299999999\n",
      "Epoch [6601/20000], Bound: 0.4111272394657135, Entropy: 139.09536743164062, Temp: 2.536137580871582, KL: 78.1112060546875, Loss: 0.021094322204589844, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6602/20000], Bound: 0.3647049367427826, Entropy: 140.2252197265625, Temp: 2.536316156387329, KL: 65.94219970703125, Loss: 0.018855975940823555, Learning Rate: 0.001729440299999999\n",
      "Epoch [6603/20000], Bound: 0.3934585452079773, Entropy: 141.4942626953125, Temp: 2.5364623069763184, KL: 73.45889282226562, Loss: 0.020079435780644417, Learning Rate: 0.001729440299999999\n",
      "Epoch [6604/20000], Bound: 0.4184863865375519, Entropy: 139.6308135986328, Temp: 2.5365912914276123, KL: 81.22593688964844, Loss: 0.019283201545476913, Learning Rate: 0.001729440299999999\n",
      "Epoch [6605/20000], Bound: 0.4381365478038788, Entropy: 138.34326171875, Temp: 2.536759376525879, KL: 84.86212158203125, Loss: 0.023898670449852943, Learning Rate: 0.001729440299999999\n",
      "Epoch [6606/20000], Bound: 0.4070208966732025, Entropy: 139.5874786376953, Temp: 2.5368871688842773, KL: 77.96931457519531, Loss: 0.01899169385433197, Learning Rate: 0.001729440299999999\n",
      "Epoch [6607/20000], Bound: 0.3994295299053192, Entropy: 139.28245544433594, Temp: 2.537044048309326, KL: 75.0675048828125, Loss: 0.020329443737864494, Learning Rate: 0.001729440299999999\n",
      "Epoch [6608/20000], Bound: 0.3689619302749634, Entropy: 139.47267150878906, Temp: 2.5371856689453125, KL: 67.31791687011719, Loss: 0.018486717715859413, Learning Rate: 0.001729440299999999\n",
      "Epoch [6609/20000], Bound: 0.435443639755249, Entropy: 137.9769287109375, Temp: 2.5373122692108154, KL: 85.87088012695312, Loss: 0.020283136516809464, Learning Rate: 0.001729440299999999\n",
      "Epoch [6610/20000], Bound: 0.3898540139198303, Entropy: 140.67205810546875, Temp: 2.537479877471924, KL: 72.6934814453125, Loss: 0.019552910700440407, Learning Rate: 0.001729440299999999\n",
      "Epoch [6611/20000], Bound: 0.4153297245502472, Entropy: 136.2642059326172, Temp: 2.53763484954834, KL: 80.06303405761719, Loss: 0.019729144871234894, Learning Rate: 0.001729440299999999\n",
      "Epoch [6612/20000], Bound: 0.4059862196445465, Entropy: 137.25375366210938, Temp: 2.537811279296875, KL: 77.118896484375, Loss: 0.02007884904742241, Learning Rate: 0.001729440299999999\n",
      "Epoch [6613/20000], Bound: 0.40732213854789734, Entropy: 140.17724609375, Temp: 2.53798508644104, KL: 76.31608581542969, Loss: 0.02243764139711857, Learning Rate: 0.001729440299999999\n",
      "Epoch [6614/20000], Bound: 0.3870738744735718, Entropy: 140.08053588867188, Temp: 2.538104772567749, KL: 71.29191589355469, Loss: 0.02075018361210823, Learning Rate: 0.001729440299999999\n",
      "Epoch [6615/20000], Bound: 0.38036537170410156, Entropy: 138.684814453125, Temp: 2.538184881210327, KL: 70.78070068359375, Loss: 0.017993424087762833, Learning Rate: 0.001729440299999999\n",
      "Epoch [6616/20000], Bound: 0.38237327337265015, Entropy: 136.66796875, Temp: 2.538283109664917, KL: 70.28253173828125, Loss: 0.020098809152841568, Learning Rate: 0.001729440299999999\n",
      "Epoch [6617/20000], Bound: 0.41129085421562195, Entropy: 139.37513732910156, Temp: 2.5383520126342773, KL: 77.56404113769531, Loss: 0.022294891998171806, Learning Rate: 0.001729440299999999\n",
      "Epoch [6618/20000], Bound: 0.40629085898399353, Entropy: 138.06341552734375, Temp: 2.538386106491089, KL: 77.84159851074219, Loss: 0.018838981166481972, Learning Rate: 0.001729440299999999\n",
      "Epoch [6619/20000], Bound: 0.3744472563266754, Entropy: 139.43898010253906, Temp: 2.5384604930877686, KL: 69.29255676269531, Loss: 0.01763640157878399, Learning Rate: 0.001729440299999999\n",
      "Epoch [6620/20000], Bound: 0.4248679280281067, Entropy: 137.2436065673828, Temp: 2.5385541915893555, KL: 83.92463684082031, Loss: 0.017779991030693054, Learning Rate: 0.001729440299999999\n",
      "Epoch [6621/20000], Bound: 0.4041585326194763, Entropy: 138.6479949951172, Temp: 2.5387327671051025, KL: 77.38313293457031, Loss: 0.01851210743188858, Learning Rate: 0.001729440299999999\n",
      "Epoch [6622/20000], Bound: 0.39671093225479126, Entropy: 138.12437438964844, Temp: 2.5389418601989746, KL: 76.26823425292969, Loss: 0.016429800540208817, Learning Rate: 0.001729440299999999\n",
      "Epoch [6623/20000], Bound: 0.41701605916023254, Entropy: 138.07818603515625, Temp: 2.5392158031463623, KL: 80.28482055664062, Loss: 0.02030511386692524, Learning Rate: 0.001729440299999999\n",
      "Epoch [6624/20000], Bound: 0.3843100666999817, Entropy: 139.35733032226562, Temp: 2.539487600326538, KL: 69.85823059082031, Loss: 0.022032704204320908, Learning Rate: 0.001729440299999999\n",
      "Epoch [6625/20000], Bound: 0.3957715928554535, Entropy: 137.26515197753906, Temp: 2.539669990539551, KL: 75.71710205078125, Loss: 0.01698778197169304, Learning Rate: 0.001729440299999999\n",
      "Epoch [6626/20000], Bound: 0.4018687903881073, Entropy: 138.59866333007812, Temp: 2.539905071258545, KL: 75.15982055664062, Loss: 0.021583089604973793, Learning Rate: 0.001729440299999999\n",
      "Epoch [6627/20000], Bound: 0.3890671730041504, Entropy: 139.40591430664062, Temp: 2.540090322494507, KL: 73.63670349121094, Loss: 0.017280513420701027, Learning Rate: 0.001729440299999999\n",
      "Epoch [6628/20000], Bound: 0.4057828485965729, Entropy: 139.09469604492188, Temp: 2.5403120517730713, KL: 76.49093627929688, Loss: 0.021227717399597168, Learning Rate: 0.001729440299999999\n",
      "Epoch [6629/20000], Bound: 0.39605507254600525, Entropy: 140.39041137695312, Temp: 2.540498971939087, KL: 75.45721435546875, Loss: 0.017671551555395126, Learning Rate: 0.001729440299999999\n",
      "Epoch [6630/20000], Bound: 0.35637277364730835, Entropy: 139.4601287841797, Temp: 2.540722370147705, KL: 64.67387390136719, Loss: 0.01687115989625454, Learning Rate: 0.001729440299999999\n",
      "Epoch [6631/20000], Bound: 0.37491053342819214, Entropy: 138.42713928222656, Temp: 2.5409419536590576, KL: 69.97708129882812, Loss: 0.01657269150018692, Learning Rate: 0.001729440299999999\n",
      "Epoch [6632/20000], Bound: 0.41115108132362366, Entropy: 139.96754455566406, Temp: 2.5411906242370605, KL: 78.48617553710938, Loss: 0.020432407036423683, Learning Rate: 0.001729440299999999\n",
      "Epoch [6633/20000], Bound: 0.3868694007396698, Entropy: 137.64962768554688, Temp: 2.5414276123046875, KL: 72.29298400878906, Loss: 0.018700430169701576, Learning Rate: 0.001729440299999999\n",
      "Epoch [6634/20000], Bound: 0.4294058680534363, Entropy: 139.5224609375, Temp: 2.541658878326416, KL: 84.9404296875, Loss: 0.018539318814873695, Learning Rate: 0.001729440299999999\n",
      "Epoch [6635/20000], Bound: 0.3862437903881073, Entropy: 141.08001708984375, Temp: 2.541949510574341, KL: 71.17286682128906, Loss: 0.020557386800646782, Learning Rate: 0.001729440299999999\n",
      "Epoch [6636/20000], Bound: 0.3859287202358246, Entropy: 141.21597290039062, Temp: 2.542184829711914, KL: 71.4622802734375, Loss: 0.01981344446539879, Learning Rate: 0.001729440299999999\n",
      "Epoch [6637/20000], Bound: 0.4260331690311432, Entropy: 138.9569549560547, Temp: 2.542387008666992, KL: 84.48561096191406, Loss: 0.017427438870072365, Learning Rate: 0.001729440299999999\n",
      "Epoch [6638/20000], Bound: 0.4064759612083435, Entropy: 142.25535583496094, Temp: 2.5426716804504395, KL: 77.7371826171875, Loss: 0.019205983728170395, Learning Rate: 0.001729440299999999\n",
      "Epoch [6639/20000], Bound: 0.421193927526474, Entropy: 139.21115112304688, Temp: 2.542961835861206, KL: 82.00566101074219, Loss: 0.019436249509453773, Learning Rate: 0.001729440299999999\n",
      "Epoch [6640/20000], Bound: 0.3518907427787781, Entropy: 143.00303649902344, Temp: 2.5432729721069336, KL: 62.750274658203125, Loss: 0.018267648294568062, Learning Rate: 0.001729440299999999\n",
      "Epoch [6641/20000], Bound: 0.39194151759147644, Entropy: 142.3717803955078, Temp: 2.543530225753784, KL: 72.93536376953125, Loss: 0.020326882600784302, Learning Rate: 0.001729440299999999\n",
      "Epoch [6642/20000], Bound: 0.3893625736236572, Entropy: 141.30030822753906, Temp: 2.543748617172241, KL: 72.29609680175781, Loss: 0.02012627013027668, Learning Rate: 0.001729440299999999\n",
      "Epoch [6643/20000], Bound: 0.3968372941017151, Entropy: 138.33331298828125, Temp: 2.5439319610595703, KL: 74.31263732910156, Loss: 0.020409978926181793, Learning Rate: 0.001729440299999999\n",
      "Epoch [6644/20000], Bound: 0.4017948508262634, Entropy: 139.03785705566406, Temp: 2.54408860206604, KL: 75.45538330078125, Loss: 0.021006805822253227, Learning Rate: 0.001729440299999999\n",
      "Epoch [6645/20000], Bound: 0.37801235914230347, Entropy: 139.71681213378906, Temp: 2.5442140102386475, KL: 70.35054016113281, Loss: 0.01759554073214531, Learning Rate: 0.001729440299999999\n",
      "Epoch [6646/20000], Bound: 0.38879430294036865, Entropy: 140.4667205810547, Temp: 2.5443572998046875, KL: 71.46279907226562, Loss: 0.021449565887451172, Learning Rate: 0.001729440299999999\n",
      "Epoch [6647/20000], Bound: 0.4336470365524292, Entropy: 139.35963439941406, Temp: 2.5444412231445312, KL: 84.93782043457031, Loss: 0.021135181188583374, Learning Rate: 0.001729440299999999\n",
      "Epoch [6648/20000], Bound: 0.40326133370399475, Entropy: 142.49496459960938, Temp: 2.5445449352264404, KL: 78.77728271484375, Loss: 0.015328683890402317, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6649/20000], Bound: 0.4201451241970062, Entropy: 138.94163513183594, Temp: 2.544757127761841, KL: 80.03349304199219, Loss: 0.022715428844094276, Learning Rate: 0.001729440299999999\n",
      "Epoch [6650/20000], Bound: 0.38919180631637573, Entropy: 139.9204864501953, Temp: 2.544919967651367, KL: 73.96455383300781, Loss: 0.016764508560299873, Learning Rate: 0.001729440299999999\n",
      "Epoch [6651/20000], Bound: 0.39722102880477905, Entropy: 138.26026916503906, Temp: 2.5451319217681885, KL: 73.43917846679688, Loss: 0.022358529269695282, Learning Rate: 0.001729440299999999\n",
      "Epoch [6652/20000], Bound: 0.397289901971817, Entropy: 142.11375427246094, Temp: 2.5452682971954346, KL: 75.64625549316406, Loss: 0.01806366816163063, Learning Rate: 0.001729440299999999\n",
      "Epoch [6653/20000], Bound: 0.3671438694000244, Entropy: 139.05625915527344, Temp: 2.5454375743865967, KL: 63.358001708984375, Loss: 0.02535189501941204, Learning Rate: 0.001729440299999999\n",
      "Epoch [6654/20000], Bound: 0.42395949363708496, Entropy: 140.58599853515625, Temp: 2.545419216156006, KL: 83.61376953125, Loss: 0.017950741574168205, Learning Rate: 0.001729440299999999\n",
      "Epoch [6655/20000], Bound: 0.38890454173088074, Entropy: 139.796142578125, Temp: 2.5454893112182617, KL: 71.75384521484375, Loss: 0.02095170132815838, Learning Rate: 0.001729440299999999\n",
      "Epoch [6656/20000], Bound: 0.4072383642196655, Entropy: 140.9959259033203, Temp: 2.5455188751220703, KL: 76.8623046875, Loss: 0.021401673555374146, Learning Rate: 0.001729440299999999\n",
      "Epoch [6657/20000], Bound: 0.3774517774581909, Entropy: 141.07598876953125, Temp: 2.5455281734466553, KL: 69.31333923339844, Loss: 0.019335949793457985, Learning Rate: 0.001729440299999999\n",
      "Epoch [6658/20000], Bound: 0.4257451593875885, Entropy: 140.1647186279297, Temp: 2.5455238819122314, KL: 83.42996215820312, Loss: 0.019375719130039215, Learning Rate: 0.001729440299999999\n",
      "Epoch [6659/20000], Bound: 0.3906666040420532, Entropy: 141.87118530273438, Temp: 2.545576810836792, KL: 72.37385559082031, Loss: 0.020730316638946533, Learning Rate: 0.001729440299999999\n",
      "Epoch [6660/20000], Bound: 0.3985200822353363, Entropy: 138.94747924804688, Temp: 2.5455985069274902, KL: 73.91886901855469, Loss: 0.022163832560181618, Learning Rate: 0.001729440299999999\n",
      "Epoch [6661/20000], Bound: 0.4012545943260193, Entropy: 140.63465881347656, Temp: 2.545569658279419, KL: 75.09721374511719, Loss: 0.021416708827018738, Learning Rate: 0.001729440299999999\n",
      "Epoch [6662/20000], Bound: 0.39491668343544006, Entropy: 141.0061492919922, Temp: 2.5455172061920166, KL: 73.07192993164062, Loss: 0.02177014760673046, Learning Rate: 0.001729440299999999\n",
      "Epoch [6663/20000], Bound: 0.3865815997123718, Entropy: 139.1098175048828, Temp: 2.5454254150390625, KL: 70.64381408691406, Loss: 0.0218228567391634, Learning Rate: 0.001729440299999999\n",
      "Epoch [6664/20000], Bound: 0.4154534339904785, Entropy: 140.4221649169922, Temp: 2.5452849864959717, KL: 78.91099548339844, Loss: 0.02216474898159504, Learning Rate: 0.001729440299999999\n",
      "Epoch [6665/20000], Bound: 0.4009940028190613, Entropy: 142.00347900390625, Temp: 2.545135736465454, KL: 77.38499450683594, Loss: 0.016767747700214386, Learning Rate: 0.001729440299999999\n",
      "Epoch [6666/20000], Bound: 0.3841339945793152, Entropy: 139.13336181640625, Temp: 2.5450832843780518, KL: 72.12197875976562, Loss: 0.017541225999593735, Learning Rate: 0.001729440299999999\n",
      "Epoch [6667/20000], Bound: 0.37934499979019165, Entropy: 136.69931030273438, Temp: 2.545076370239258, KL: 69.57185363769531, Loss: 0.019875943660736084, Learning Rate: 0.001729440299999999\n",
      "Epoch [6668/20000], Bound: 0.39735522866249084, Entropy: 140.46197509765625, Temp: 2.5450477600097656, KL: 76.05891418457031, Loss: 0.017287563532590866, Learning Rate: 0.001729440299999999\n",
      "Epoch [6669/20000], Bound: 0.41358157992362976, Entropy: 137.49501037597656, Temp: 2.5450868606567383, KL: 79.84602355957031, Loss: 0.019228730350732803, Learning Rate: 0.001729440299999999\n",
      "Epoch [6670/20000], Bound: 0.3886113464832306, Entropy: 139.85707092285156, Temp: 2.5451650619506836, KL: 72.72825622558594, Loss: 0.01886865682899952, Learning Rate: 0.001729440299999999\n",
      "Epoch [6671/20000], Bound: 0.3814108669757843, Entropy: 138.6041259765625, Temp: 2.545250415802002, KL: 71.043212890625, Loss: 0.018139012157917023, Learning Rate: 0.001729440299999999\n",
      "Epoch [6672/20000], Bound: 0.3780006468296051, Entropy: 139.29898071289062, Temp: 2.54534912109375, KL: 70.21797180175781, Loss: 0.017861884087324142, Learning Rate: 0.001729440299999999\n",
      "Epoch [6673/20000], Bound: 0.40907004475593567, Entropy: 138.48191833496094, Temp: 2.545461654663086, KL: 76.79029846191406, Loss: 0.022605372592806816, Learning Rate: 0.001729440299999999\n",
      "Epoch [6674/20000], Bound: 0.3719906210899353, Entropy: 140.49095153808594, Temp: 2.5455198287963867, KL: 66.11360168457031, Loss: 0.022600822150707245, Learning Rate: 0.001729440299999999\n",
      "Epoch [6675/20000], Bound: 0.41209933161735535, Entropy: 137.91363525390625, Temp: 2.545473575592041, KL: 80.72114562988281, Loss: 0.01664857752621174, Learning Rate: 0.001729440299999999\n",
      "Epoch [6676/20000], Bound: 0.3817771375179291, Entropy: 140.84591674804688, Temp: 2.5455329418182373, KL: 71.85459899902344, Loss: 0.016752896830439568, Learning Rate: 0.001729440299999999\n",
      "Epoch [6677/20000], Bound: 0.3884630501270294, Entropy: 140.56085205078125, Temp: 2.5456416606903076, KL: 70.05279541015625, Loss: 0.024045327678322792, Learning Rate: 0.001729440299999999\n",
      "Epoch [6678/20000], Bound: 0.3824331760406494, Entropy: 140.67965698242188, Temp: 2.5456314086914062, KL: 70.66203308105469, Loss: 0.019463008269667625, Learning Rate: 0.001729440299999999\n",
      "Epoch [6679/20000], Bound: 0.3964032232761383, Entropy: 140.01327514648438, Temp: 2.5456137657165527, KL: 74.72921752929688, Loss: 0.019362976774573326, Learning Rate: 0.001729440299999999\n",
      "Epoch [6680/20000], Bound: 0.40957289934158325, Entropy: 140.78302001953125, Temp: 2.545612335205078, KL: 77.68559265136719, Loss: 0.021140923723578453, Learning Rate: 0.001729440299999999\n",
      "Epoch [6681/20000], Bound: 0.36861908435821533, Entropy: 141.44664001464844, Temp: 2.5456032752990723, KL: 67.89743041992188, Loss: 0.017244869843125343, Learning Rate: 0.001729440299999999\n",
      "Epoch [6682/20000], Bound: 0.41681233048439026, Entropy: 142.09071350097656, Temp: 2.5456197261810303, KL: 79.17744445800781, Loss: 0.022443393245339394, Learning Rate: 0.001729440299999999\n",
      "Epoch [6683/20000], Bound: 0.407227486371994, Entropy: 141.45423889160156, Temp: 2.545607089996338, KL: 78.16259765625, Loss: 0.018842384219169617, Learning Rate: 0.001729440299999999\n",
      "Epoch [6684/20000], Bound: 0.38842520117759705, Entropy: 139.56231689453125, Temp: 2.5456383228302, KL: 73.26165771484375, Loss: 0.017721321433782578, Learning Rate: 0.001729440299999999\n",
      "Epoch [6685/20000], Bound: 0.4065112769603729, Entropy: 141.8878631591797, Temp: 2.545707941055298, KL: 77.40707397460938, Loss: 0.01991277001798153, Learning Rate: 0.001729440299999999\n",
      "Epoch [6686/20000], Bound: 0.39648717641830444, Entropy: 142.33062744140625, Temp: 2.5457873344421387, KL: 75.24948120117188, Loss: 0.018391085788607597, Learning Rate: 0.001729440299999999\n",
      "Epoch [6687/20000], Bound: 0.40099993348121643, Entropy: 139.96926879882812, Temp: 2.545896291732788, KL: 77.00193786621094, Loss: 0.017533330246806145, Learning Rate: 0.001729440299999999\n",
      "Epoch [6688/20000], Bound: 0.39713162183761597, Entropy: 140.36373901367188, Temp: 2.5460588932037354, KL: 75.43473815917969, Loss: 0.018398301675915718, Learning Rate: 0.001729440299999999\n",
      "Epoch [6689/20000], Bound: 0.37219110131263733, Entropy: 140.52085876464844, Temp: 2.546243667602539, KL: 69.20054626464844, Loss: 0.016655687242746353, Learning Rate: 0.001729440299999999\n",
      "Epoch [6690/20000], Bound: 0.40632373094558716, Entropy: 137.84783935546875, Temp: 2.5464539527893066, KL: 78.50187683105469, Loss: 0.017663685604929924, Learning Rate: 0.001729440299999999\n",
      "Epoch [6691/20000], Bound: 0.4007173478603363, Entropy: 140.71075439453125, Temp: 2.5467121601104736, KL: 76.86494445800781, Loss: 0.017650509253144264, Learning Rate: 0.001729440299999999\n",
      "Epoch [6692/20000], Bound: 0.3739931583404541, Entropy: 141.08978271484375, Temp: 2.5470056533813477, KL: 67.98638916015625, Loss: 0.020041337236762047, Learning Rate: 0.001729440299999999\n",
      "Epoch [6693/20000], Bound: 0.41202571988105774, Entropy: 139.07406616210938, Temp: 2.547234296798706, KL: 78.7218017578125, Loss: 0.0205546785145998, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6694/20000], Bound: 0.3839382529258728, Entropy: 140.44842529296875, Temp: 2.5474491119384766, KL: 71.583740234375, Loss: 0.018514983355998993, Learning Rate: 0.001729440299999999\n",
      "Epoch [6695/20000], Bound: 0.4092642366886139, Entropy: 139.32305908203125, Temp: 2.5476579666137695, KL: 78.04713439941406, Loss: 0.020276421681046486, Learning Rate: 0.001729440299999999\n",
      "Epoch [6696/20000], Bound: 0.3825836181640625, Entropy: 140.7425079345703, Temp: 2.5478572845458984, KL: 71.11158752441406, Loss: 0.01868826523423195, Learning Rate: 0.001729440299999999\n",
      "Epoch [6697/20000], Bound: 0.3778282105922699, Entropy: 139.3377685546875, Temp: 2.5480458736419678, KL: 68.09207153320312, Loss: 0.021966969594359398, Learning Rate: 0.001729440299999999\n",
      "Epoch [6698/20000], Bound: 0.3453398048877716, Entropy: 141.3341827392578, Temp: 2.5481390953063965, KL: 61.75007629394531, Loss: 0.016781479120254517, Learning Rate: 0.001729440299999999\n",
      "Epoch [6699/20000], Bound: 0.4121020436286926, Entropy: 140.09251403808594, Temp: 2.548224687576294, KL: 78.51380920410156, Loss: 0.021019525825977325, Learning Rate: 0.001729440299999999\n",
      "Epoch [6700/20000], Bound: 0.4104755222797394, Entropy: 140.50022888183594, Temp: 2.5482988357543945, KL: 79.26171875, Loss: 0.018605340272188187, Learning Rate: 0.001729440299999999\n",
      "Epoch [6701/20000], Bound: 0.4105491638183594, Entropy: 139.87928771972656, Temp: 2.548417806625366, KL: 79.12138366699219, Loss: 0.018925074487924576, Learning Rate: 0.001729440299999999\n",
      "Epoch [6702/20000], Bound: 0.40181228518486023, Entropy: 140.09027099609375, Temp: 2.548569440841675, KL: 76.63441467285156, Loss: 0.01875464990735054, Learning Rate: 0.001729440299999999\n",
      "Epoch [6703/20000], Bound: 0.40327098965644836, Entropy: 140.3637237548828, Temp: 2.548741340637207, KL: 74.21955871582031, Loss: 0.02433297038078308, Learning Rate: 0.001729440299999999\n",
      "Epoch [6704/20000], Bound: 0.39735478162765503, Entropy: 140.70533752441406, Temp: 2.5488014221191406, KL: 74.57566833496094, Loss: 0.02024388685822487, Learning Rate: 0.001729440299999999\n",
      "Epoch [6705/20000], Bound: 0.3590112328529358, Entropy: 142.29827880859375, Temp: 2.548848867416382, KL: 64.51319885253906, Loss: 0.018688997253775597, Learning Rate: 0.001729440299999999\n",
      "Epoch [6706/20000], Bound: 0.40251556038856506, Entropy: 138.77980041503906, Temp: 2.548866033554077, KL: 75.42204284667969, Loss: 0.021540755406022072, Learning Rate: 0.001729440299999999\n",
      "Epoch [6707/20000], Bound: 0.3956901431083679, Entropy: 138.53382873535156, Temp: 2.548851728439331, KL: 74.16415405273438, Loss: 0.020102791488170624, Learning Rate: 0.001729440299999999\n",
      "Epoch [6708/20000], Bound: 0.37030425667762756, Entropy: 140.5593719482422, Temp: 2.548832893371582, KL: 65.43989562988281, Loss: 0.023023562505841255, Learning Rate: 0.001729440299999999\n",
      "Epoch [6709/20000], Bound: 0.4030904471874237, Entropy: 139.25338745117188, Temp: 2.548701763153076, KL: 75.10102844238281, Loss: 0.022499358281493187, Learning Rate: 0.001729440299999999\n",
      "Epoch [6710/20000], Bound: 0.4014423191547394, Entropy: 140.74560546875, Temp: 2.5485317707061768, KL: 75.89634704589844, Loss: 0.019989686086773872, Learning Rate: 0.001729440299999999\n",
      "Epoch [6711/20000], Bound: 0.40310558676719666, Entropy: 141.8646240234375, Temp: 2.548384666442871, KL: 74.78446960449219, Loss: 0.023125719279050827, Learning Rate: 0.001729440299999999\n",
      "Epoch [6712/20000], Bound: 0.41613689064979553, Entropy: 140.837646484375, Temp: 2.548185348510742, KL: 80.395751953125, Loss: 0.01968703791499138, Learning Rate: 0.001729440299999999\n",
      "Epoch [6713/20000], Bound: 0.4146745800971985, Entropy: 140.8498077392578, Temp: 2.5480411052703857, KL: 79.48089599609375, Loss: 0.020623115822672844, Learning Rate: 0.001729440299999999\n",
      "Epoch [6714/20000], Bound: 0.3738963305950165, Entropy: 141.60215759277344, Temp: 2.547921895980835, KL: 64.48579406738281, Loss: 0.0268662478774786, Learning Rate: 0.001729440299999999\n",
      "Epoch [6715/20000], Bound: 0.4326801598072052, Entropy: 140.71224975585938, Temp: 2.547612428665161, KL: 85.33413696289062, Loss: 0.019819226115942, Learning Rate: 0.001729440299999999\n",
      "Epoch [6716/20000], Bound: 0.4040554463863373, Entropy: 142.06040954589844, Temp: 2.547390937805176, KL: 72.65843200683594, Loss: 0.0278354249894619, Learning Rate: 0.001729440299999999\n",
      "Epoch [6717/20000], Bound: 0.4068593382835388, Entropy: 139.17665100097656, Temp: 2.5470149517059326, KL: 76.514404296875, Loss: 0.02188258059322834, Learning Rate: 0.001729440299999999\n",
      "Epoch [6718/20000], Bound: 0.3928048312664032, Entropy: 139.52413940429688, Temp: 2.5466456413269043, KL: 73.130615234375, Loss: 0.020467009395360947, Learning Rate: 0.001729440299999999\n",
      "Epoch [6719/20000], Bound: 0.3930952847003937, Entropy: 138.3758087158203, Temp: 2.546295642852783, KL: 72.83442687988281, Loss: 0.021209588274359703, Learning Rate: 0.001729440299999999\n",
      "Epoch [6720/20000], Bound: 0.4044347405433655, Entropy: 139.5059356689453, Temp: 2.545945644378662, KL: 78.09355163574219, Loss: 0.017367374151945114, Learning Rate: 0.001729440299999999\n",
      "Epoch [6721/20000], Bound: 0.39210063219070435, Entropy: 139.10902404785156, Temp: 2.5457046031951904, KL: 74.67439270019531, Loss: 0.01702522486448288, Learning Rate: 0.001729440299999999\n",
      "Epoch [6722/20000], Bound: 0.39860910177230835, Entropy: 139.4290313720703, Temp: 2.5455520153045654, KL: 76.55184936523438, Loss: 0.01704251766204834, Learning Rate: 0.001729440299999999\n",
      "Epoch [6723/20000], Bound: 0.4201939105987549, Entropy: 138.08799743652344, Temp: 2.5454883575439453, KL: 80.77476501464844, Loss: 0.021297156810760498, Learning Rate: 0.001729440299999999\n",
      "Epoch [6724/20000], Bound: 0.41699716448783875, Entropy: 138.6263885498047, Temp: 2.5454354286193848, KL: 78.71638488769531, Loss: 0.02345547080039978, Learning Rate: 0.001729440299999999\n",
      "Epoch [6725/20000], Bound: 0.398786336183548, Entropy: 138.96925354003906, Temp: 2.5453360080718994, KL: 76.84712219238281, Loss: 0.016561197116971016, Learning Rate: 0.001729440299999999\n",
      "Epoch [6726/20000], Bound: 0.40436872839927673, Entropy: 138.14096069335938, Temp: 2.5453319549560547, KL: 76.04598999023438, Loss: 0.021343445405364037, Learning Rate: 0.001729440299999999\n",
      "Epoch [6727/20000], Bound: 0.3866001069545746, Entropy: 141.7471923828125, Temp: 2.5453078746795654, KL: 71.57347106933594, Loss: 0.020005851984024048, Learning Rate: 0.001729440299999999\n",
      "Epoch [6728/20000], Bound: 0.40140053629875183, Entropy: 138.17811584472656, Temp: 2.5452709197998047, KL: 77.32293701171875, Loss: 0.01712493784725666, Learning Rate: 0.001729440299999999\n",
      "Epoch [6729/20000], Bound: 0.3916477859020233, Entropy: 139.4943084716797, Temp: 2.545313596725464, KL: 74.41012573242188, Loss: 0.017282942309975624, Learning Rate: 0.001729440299999999\n",
      "Epoch [6730/20000], Bound: 0.3781944215297699, Entropy: 140.96229553222656, Temp: 2.54541015625, KL: 69.57247924804688, Loss: 0.01923815719783306, Learning Rate: 0.001729440299999999\n",
      "Epoch [6731/20000], Bound: 0.39753466844558716, Entropy: 139.86956787109375, Temp: 2.545487642288208, KL: 76.59765625, Loss: 0.016337335109710693, Learning Rate: 0.001729440299999999\n",
      "Epoch [6732/20000], Bound: 0.4126003682613373, Entropy: 138.9471893310547, Temp: 2.5456466674804688, KL: 78.87710571289062, Loss: 0.02056552842259407, Learning Rate: 0.001729440299999999\n",
      "Epoch [6733/20000], Bound: 0.3858029842376709, Entropy: 140.6297607421875, Temp: 2.5458004474639893, KL: 72.99497985839844, Loss: 0.01677122712135315, Learning Rate: 0.001729440299999999\n",
      "Epoch [6734/20000], Bound: 0.3931923806667328, Entropy: 138.9677276611328, Temp: 2.5460002422332764, KL: 73.48025512695312, Loss: 0.019993262365460396, Learning Rate: 0.001729440299999999\n",
      "Epoch [6735/20000], Bound: 0.40545859932899475, Entropy: 138.70152282714844, Temp: 2.546175003051758, KL: 77.80319213867188, Loss: 0.018531793728470802, Learning Rate: 0.001729440299999999\n",
      "Epoch [6736/20000], Bound: 0.40029987692832947, Entropy: 137.99842834472656, Temp: 2.5463802814483643, KL: 75.50471496582031, Loss: 0.020077699795365334, Learning Rate: 0.001729440299999999\n",
      "Epoch [6737/20000], Bound: 0.39964380860328674, Entropy: 138.25372314453125, Temp: 2.546567916870117, KL: 75.42984008789062, Loss: 0.019850831478834152, Learning Rate: 0.001729440299999999\n",
      "Epoch [6738/20000], Bound: 0.3880840837955475, Entropy: 139.61436462402344, Temp: 2.5467443466186523, KL: 73.34136962890625, Loss: 0.0173854511231184, Learning Rate: 0.001729440299999999\n",
      "Epoch [6739/20000], Bound: 0.41495952010154724, Entropy: 138.923828125, Temp: 2.5469532012939453, KL: 80.13400268554688, Loss: 0.01949431374669075, Learning Rate: 0.001729440299999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6740/20000], Bound: 0.36963951587677, Entropy: 140.20361328125, Temp: 2.547179698944092, KL: 67.61505126953125, Loss: 0.018375251442193985, Learning Rate: 0.001729440299999999\n",
      "Epoch [6741/20000], Bound: 0.3790144920349121, Entropy: 138.84005737304688, Temp: 2.547382116317749, KL: 68.93333435058594, Loss: 0.02096879854798317, Learning Rate: 0.001729440299999999\n",
      "Epoch [6742/20000], Bound: 0.3932170271873474, Entropy: 140.9635772705078, Temp: 2.547513008117676, KL: 74.0137939453125, Loss: 0.018976891413331032, Learning Rate: 0.001729440299999999\n",
      "Epoch [6743/20000], Bound: 0.39518576860427856, Entropy: 140.03372192382812, Temp: 2.547649383544922, KL: 74.56033325195312, Loss: 0.019024593755602837, Learning Rate: 0.001729440299999999\n",
      "Epoch [6744/20000], Bound: 0.4104255437850952, Entropy: 141.31698608398438, Temp: 2.547792434692383, KL: 79.85093688964844, Loss: 0.017413321882486343, Learning Rate: 0.001729440299999999\n",
      "Epoch [6745/20000], Bound: 0.40350207686424255, Entropy: 139.29010009765625, Temp: 2.5480029582977295, KL: 77.30929565429688, Loss: 0.018395479768514633, Learning Rate: 0.001729440299999999\n",
      "Epoch [6746/20000], Bound: 0.3980790078639984, Entropy: 140.4835662841797, Temp: 2.5482397079467773, KL: 75.38607788085938, Loss: 0.01906098984181881, Learning Rate: 0.001729440299999999\n",
      "Epoch [6747/20000], Bound: 0.3921785056591034, Entropy: 140.44398498535156, Temp: 2.548476457595825, KL: 72.22805786132812, Loss: 0.02190270833671093, Learning Rate: 0.001729440299999999\n",
      "Epoch [6748/20000], Bound: 0.41138070821762085, Entropy: 141.03207397460938, Temp: 2.5486350059509277, KL: 78.13172912597656, Loss: 0.021353570744395256, Learning Rate: 0.001729440299999999\n",
      "Epoch [6749/20000], Bound: 0.4193881154060364, Entropy: 138.2977294921875, Temp: 2.5487656593322754, KL: 80.74929809570312, Loss: 0.020913343876600266, Learning Rate: 0.001729440299999999\n",
      "Epoch [6750/20000], Bound: 0.3942594826221466, Entropy: 140.72976684570312, Temp: 2.548894166946411, KL: 75.14907836914062, Loss: 0.017357535660266876, Learning Rate: 0.001729440299999999\n",
      "Epoch [6751/20000], Bound: 0.4047936201095581, Entropy: 140.8876953125, Temp: 2.5490684509277344, KL: 74.05026245117188, Loss: 0.025545839220285416, Learning Rate: 0.001729440299999999\n",
      "Epoch [6752/20000], Bound: 0.3784983456134796, Entropy: 139.86106872558594, Temp: 2.5491015911102295, KL: 71.18367004394531, Loss: 0.016284609213471413, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6753/20000], Bound: 0.37927427887916565, Entropy: 141.63316345214844, Temp: 2.5491933822631836, KL: 69.70622253417969, Loss: 0.019614750519394875, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6754/20000], Bound: 0.3800410032272339, Entropy: 138.92222595214844, Temp: 2.549257278442383, KL: 70.63894653320312, Loss: 0.01821254938840866, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6755/20000], Bound: 0.42210909724235535, Entropy: 139.93856811523438, Temp: 2.5493316650390625, KL: 80.46316528320312, Loss: 0.02308916486799717, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6756/20000], Bound: 0.4022933542728424, Entropy: 139.2073211669922, Temp: 2.549360752105713, KL: 77.45387268066406, Loss: 0.017433535307645798, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6757/20000], Bound: 0.40381669998168945, Entropy: 140.79212951660156, Temp: 2.5494558811187744, KL: 76.69699096679688, Loss: 0.019795697182416916, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6758/20000], Bound: 0.3642979860305786, Entropy: 141.22854614257812, Temp: 2.5495548248291016, KL: 66.53372192382812, Loss: 0.01760067418217659, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6759/20000], Bound: 0.3865153193473816, Entropy: 140.9740447998047, Temp: 2.5496528148651123, KL: 71.941650390625, Loss: 0.019282257184386253, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6760/20000], Bound: 0.3992954194545746, Entropy: 139.02479553222656, Temp: 2.549740791320801, KL: 77.39340209960938, Loss: 0.0158377755433321, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6761/20000], Bound: 0.3965834081172943, Entropy: 140.9625701904297, Temp: 2.5499229431152344, KL: 74.96435546875, Loss: 0.01905451901257038, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6762/20000], Bound: 0.38328230381011963, Entropy: 140.90655517578125, Temp: 2.550107717514038, KL: 71.7723388671875, Loss: 0.01780746690928936, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6763/20000], Bound: 0.39001888036727905, Entropy: 140.0041046142578, Temp: 2.5503053665161133, KL: 74.23251342773438, Loss: 0.016770632937550545, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6764/20000], Bound: 0.4112085998058319, Entropy: 141.5087432861328, Temp: 2.5505495071411133, KL: 79.53977966308594, Loss: 0.018515946343541145, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6765/20000], Bound: 0.38002753257751465, Entropy: 138.93704223632812, Temp: 2.5508246421813965, KL: 70.79135131835938, Loss: 0.017923185601830482, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6766/20000], Bound: 0.4091247022151947, Entropy: 139.71669006347656, Temp: 2.551095724105835, KL: 75.98582458496094, Loss: 0.02427735924720764, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6767/20000], Bound: 0.39486730098724365, Entropy: 141.7042694091797, Temp: 2.5512521266937256, KL: 74.61235046386719, Loss: 0.018783360719680786, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6768/20000], Bound: 0.41090521216392517, Entropy: 142.9398193359375, Temp: 2.5514166355133057, KL: 78.04534912109375, Loss: 0.02127927727997303, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6769/20000], Bound: 0.40914270281791687, Entropy: 138.84046936035156, Temp: 2.5515525341033936, KL: 78.70317077636719, Loss: 0.018967734649777412, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6770/20000], Bound: 0.35801398754119873, Entropy: 141.45774841308594, Temp: 2.551715612411499, KL: 63.54643249511719, Loss: 0.020071132108569145, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6771/20000], Bound: 0.3708195090293884, Entropy: 140.48736572265625, Temp: 2.5517988204956055, KL: 66.07278442382812, Loss: 0.022090546786785126, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6772/20000], Bound: 0.36439400911331177, Entropy: 141.12738037109375, Temp: 2.551779270172119, KL: 67.13870239257812, Loss: 0.01648947224020958, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6773/20000], Bound: 0.3861803710460663, Entropy: 141.45933532714844, Temp: 2.5517969131469727, KL: 71.56587219238281, Loss: 0.01985369250178337, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6774/20000], Bound: 0.42198488116264343, Entropy: 141.9189453125, Temp: 2.551797389984131, KL: 81.64666748046875, Loss: 0.020726555958390236, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6775/20000], Bound: 0.4131900370121002, Entropy: 141.5810089111328, Temp: 2.5518155097961426, KL: 80.10212707519531, Loss: 0.01858541928231716, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6776/20000], Bound: 0.3435250222682953, Entropy: 141.79576110839844, Temp: 2.5518877506256104, KL: 60.76959228515625, Loss: 0.017775138840079308, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6777/20000], Bound: 0.4026033282279968, Entropy: 141.25196838378906, Temp: 2.5519251823425293, KL: 76.69525146484375, Loss: 0.01913059689104557, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6778/20000], Bound: 0.37346190214157104, Entropy: 138.26629638671875, Temp: 2.551985740661621, KL: 69.62663269042969, Loss: 0.016582729294896126, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6779/20000], Bound: 0.3905593454837799, Entropy: 141.5176544189453, Temp: 2.552086591720581, KL: 73.82347106933594, Loss: 0.01789855770766735, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6780/20000], Bound: 0.38867852091789246, Entropy: 141.30746459960938, Temp: 2.5522165298461914, KL: 72.3082275390625, Loss: 0.019807986915111542, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6781/20000], Bound: 0.4065849781036377, Entropy: 140.8005828857422, Temp: 2.5523223876953125, KL: 77.46224975585938, Loss: 0.019927358254790306, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6782/20000], Bound: 0.39552223682403564, Entropy: 142.47811889648438, Temp: 2.552431344985962, KL: 74.47927856445312, Loss: 0.01942998170852661, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6783/20000], Bound: 0.40639275312423706, Entropy: 139.21780395507812, Temp: 2.5525379180908203, KL: 78.77093505859375, Loss: 0.017255373299121857, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6784/20000], Bound: 0.37642642855644226, Entropy: 141.72579956054688, Temp: 2.5527119636535645, KL: 69.591796875, Loss: 0.018295656889677048, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6785/20000], Bound: 0.39968550205230713, Entropy: 142.99159240722656, Temp: 2.5528767108917236, KL: 74.58650207519531, Loss: 0.021599488332867622, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6786/20000], Bound: 0.4046071767807007, Entropy: 140.21482849121094, Temp: 2.5529863834381104, KL: 77.68473815917969, Loss: 0.018358396366238594, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6787/20000], Bound: 0.4089822471141815, Entropy: 141.04476928710938, Temp: 2.553133726119995, KL: 76.8824462890625, Loss: 0.02246030792593956, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6788/20000], Bound: 0.37766069173812866, Entropy: 142.5824432373047, Temp: 2.553220748901367, KL: 70.37188720703125, Loss: 0.01745673082768917, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6789/20000], Bound: 0.38895800709724426, Entropy: 141.5285186767578, Temp: 2.5533297061920166, KL: 71.91688537597656, Loss: 0.020743632689118385, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6790/20000], Bound: 0.4093543589115143, Entropy: 142.9979248046875, Temp: 2.5533933639526367, KL: 78.63851928710938, Loss: 0.019240327179431915, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6791/20000], Bound: 0.40899625420570374, Entropy: 142.4792938232422, Temp: 2.5534844398498535, KL: 77.07449340820312, Loss: 0.02209632098674774, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6792/20000], Bound: 0.4183492064476013, Entropy: 141.15830993652344, Temp: 2.553530216217041, KL: 82.26840209960938, Loss: 0.017387989908456802, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6793/20000], Bound: 0.3908363878726959, Entropy: 140.49220275878906, Temp: 2.553663492202759, KL: 74.08966064453125, Loss: 0.017552057281136513, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6794/20000], Bound: 0.3893294036388397, Entropy: 142.5523681640625, Temp: 2.5538313388824463, KL: 72.21794128417969, Loss: 0.020368613302707672, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6795/20000], Bound: 0.39589548110961914, Entropy: 141.20201110839844, Temp: 2.553957462310791, KL: 75.224609375, Loss: 0.018200373277068138, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6796/20000], Bound: 0.4025193452835083, Entropy: 140.729736328125, Temp: 2.554110050201416, KL: 76.97785949707031, Loss: 0.01855538971722126, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6797/20000], Bound: 0.38156503438949585, Entropy: 141.29986572265625, Temp: 2.5542876720428467, KL: 70.88821411132812, Loss: 0.018626660108566284, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6798/20000], Bound: 0.40436941385269165, Entropy: 141.40606689453125, Temp: 2.5544540882110596, KL: 77.18101501464844, Loss: 0.019225770607590675, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6799/20000], Bound: 0.40634050965309143, Entropy: 142.48611450195312, Temp: 2.5546300411224365, KL: 77.80345153808594, Loss: 0.019146058708429337, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6800/20000], Bound: 0.41154178977012634, Entropy: 140.71612548828125, Temp: 2.5548195838928223, KL: 78.63578796386719, Loss: 0.020534204319119453, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6801/20000], Bound: 0.4146728813648224, Entropy: 141.92416381835938, Temp: 2.55499529838562, KL: 82.27532958984375, Loss: 0.015239925123751163, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6802/20000], Bound: 0.412540078163147, Entropy: 137.288818359375, Temp: 2.5552926063537598, KL: 80.55503845214844, Loss: 0.017365731298923492, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6803/20000], Bound: 0.38047558069229126, Entropy: 140.66380310058594, Temp: 2.5556437969207764, KL: 72.21279907226562, Loss: 0.015443331561982632, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6804/20000], Bound: 0.37311989068984985, Entropy: 141.5753631591797, Temp: 2.556044340133667, KL: 69.53463745117188, Loss: 0.016618480905890465, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6805/20000], Bound: 0.36372214555740356, Entropy: 141.8058319091797, Temp: 2.556448459625244, KL: 66.66249084472656, Loss: 0.017103679478168488, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6806/20000], Bound: 0.3999316096305847, Entropy: 140.34446716308594, Temp: 2.5568292140960693, KL: 77.82498168945312, Loss: 0.015449793078005314, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6807/20000], Bound: 0.377409428358078, Entropy: 141.3082275390625, Temp: 2.557283878326416, KL: 70.43118286132812, Loss: 0.017245503142476082, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6808/20000], Bound: 0.382599413394928, Entropy: 139.33233642578125, Temp: 2.5577268600463867, KL: 71.80137634277344, Loss: 0.01745411567389965, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6809/20000], Bound: 0.38556814193725586, Entropy: 141.85430908203125, Temp: 2.558161735534668, KL: 72.48158264160156, Loss: 0.01778767630457878, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6810/20000], Bound: 0.3875880539417267, Entropy: 140.48939514160156, Temp: 2.5585849285125732, KL: 71.96055603027344, Loss: 0.019942661747336388, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6811/20000], Bound: 0.3522438406944275, Entropy: 141.35154724121094, Temp: 2.5589466094970703, KL: 63.4063720703125, Loss: 0.01731118932366371, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6812/20000], Bound: 0.37484392523765564, Entropy: 141.8436279296875, Temp: 2.5592660903930664, KL: 69.03970336914062, Loss: 0.018568996340036392, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6813/20000], Bound: 0.40581879019737244, Entropy: 142.97080993652344, Temp: 2.55954909324646, KL: 77.58726501464844, Loss: 0.01932770013809204, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6814/20000], Bound: 0.39134421944618225, Entropy: 142.56622314453125, Temp: 2.5598278045654297, KL: 75.09068298339844, Loss: 0.015955066308379173, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6815/20000], Bound: 0.39434462785720825, Entropy: 141.4872283935547, Temp: 2.560163736343384, KL: 73.34585571289062, Loss: 0.02106303907930851, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6816/20000], Bound: 0.3372170329093933, Entropy: 142.152587890625, Temp: 2.5604286193847656, KL: 59.49189758300781, Loss: 0.017026908695697784, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6817/20000], Bound: 0.41499024629592896, Entropy: 139.7872314453125, Temp: 2.56064510345459, KL: 79.91053771972656, Loss: 0.02012394368648529, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6818/20000], Bound: 0.40723854303359985, Entropy: 142.389892578125, Temp: 2.5608572959899902, KL: 76.76060485839844, Loss: 0.021776150912046432, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6819/20000], Bound: 0.42383745312690735, Entropy: 142.1146240234375, Temp: 2.5610125064849854, KL: 82.6505126953125, Loss: 0.019979696720838547, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6820/20000], Bound: 0.39683324098587036, Entropy: 140.2022247314453, Temp: 2.5611863136291504, KL: 75.56124877929688, Loss: 0.018160982057452202, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6821/20000], Bound: 0.418137788772583, Entropy: 142.1459197998047, Temp: 2.5613808631896973, KL: 82.15106201171875, Loss: 0.017601536586880684, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6822/20000], Bound: 0.42053133249282837, Entropy: 140.4365234375, Temp: 2.5616400241851807, KL: 83.33355712890625, Loss: 0.016703536733984947, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6823/20000], Bound: 0.407507061958313, Entropy: 141.39340209960938, Temp: 2.5619826316833496, KL: 79.34684753417969, Loss: 0.016896329820156097, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6824/20000], Bound: 0.4094873368740082, Entropy: 140.83438110351562, Temp: 2.5623762607574463, KL: 78.10995483398438, Loss: 0.020459914579987526, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6825/20000], Bound: 0.38710370659828186, Entropy: 139.45838928222656, Temp: 2.562730312347412, KL: 72.34808349609375, Loss: 0.018958264961838722, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6826/20000], Bound: 0.4346644878387451, Entropy: 138.57498168945312, Temp: 2.563051462173462, KL: 87.76553344726562, Loss: 0.016484910622239113, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6827/20000], Bound: 0.39208298921585083, Entropy: 138.6474609375, Temp: 2.5634758472442627, KL: 74.24406433105469, Loss: 0.018067549914121628, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6828/20000], Bound: 0.3996177017688751, Entropy: 140.5041046142578, Temp: 2.563890218734741, KL: 76.77555847167969, Loss: 0.01740960404276848, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6829/20000], Bound: 0.3557003438472748, Entropy: 139.75341796875, Temp: 2.5643229484558105, KL: 65.32456970214844, Loss: 0.01546709518879652, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6830/20000], Bound: 0.39203476905822754, Entropy: 140.36526489257812, Temp: 2.564756155014038, KL: 73.67337036132812, Loss: 0.01916753686964512, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6831/20000], Bound: 0.35681331157684326, Entropy: 139.88255310058594, Temp: 2.565150022506714, KL: 64.10198974609375, Loss: 0.018455075100064278, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6832/20000], Bound: 0.3999593257904053, Entropy: 140.04278564453125, Temp: 2.5654730796813965, KL: 77.18315124511719, Loss: 0.016829360276460648, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6833/20000], Bound: 0.3915308117866516, Entropy: 140.43431091308594, Temp: 2.565838098526001, KL: 74.20346069335938, Loss: 0.017862360924482346, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6834/20000], Bound: 0.408810019493103, Entropy: 138.93568420410156, Temp: 2.566202163696289, KL: 79.26017761230469, Loss: 0.017872290685772896, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6835/20000], Bound: 0.38090893626213074, Entropy: 141.25778198242188, Temp: 2.5665907859802246, KL: 71.32916259765625, Loss: 0.017532361671328545, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6836/20000], Bound: 0.4066302180290222, Entropy: 139.49610900878906, Temp: 2.5669684410095215, KL: 79.97825622558594, Loss: 0.015226083807647228, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6837/20000], Bound: 0.37839651107788086, Entropy: 140.9467315673828, Temp: 2.5674314498901367, KL: 69.40898132324219, Loss: 0.01988978497684002, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6838/20000], Bound: 0.3944167196750641, Entropy: 141.240234375, Temp: 2.567811965942383, KL: 74.646484375, Loss: 0.018650691956281662, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6839/20000], Bound: 0.39910438656806946, Entropy: 139.5097198486328, Temp: 2.568173408508301, KL: 75.35041809082031, Loss: 0.019943999126553535, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6840/20000], Bound: 0.4054532051086426, Entropy: 138.13853454589844, Temp: 2.5684921741485596, KL: 77.54118347167969, Loss: 0.019313940778374672, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6841/20000], Bound: 0.39978548884391785, Entropy: 139.53448486328125, Temp: 2.5687978267669678, KL: 76.50291442871094, Loss: 0.018095532432198524, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6842/20000], Bound: 0.40724825859069824, Entropy: 139.78973388671875, Temp: 2.5691137313842773, KL: 77.66264343261719, Loss: 0.020117567852139473, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6843/20000], Bound: 0.3752674162387848, Entropy: 140.4285125732422, Temp: 2.56939959526062, KL: 70.09486389160156, Loss: 0.016849244013428688, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6844/20000], Bound: 0.3757922053337097, Entropy: 143.29678344726562, Temp: 2.5696918964385986, KL: 68.57965087890625, Loss: 0.02008923515677452, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6845/20000], Bound: 0.3919428884983063, Entropy: 138.79701232910156, Temp: 2.5699081420898438, KL: 74.36325073242188, Loss: 0.017829637974500656, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6846/20000], Bound: 0.3587820827960968, Entropy: 140.16412353515625, Temp: 2.5701377391815186, KL: 63.27470397949219, Loss: 0.021164393052458763, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6847/20000], Bound: 0.4025989770889282, Entropy: 140.6879425048828, Temp: 2.5702433586120605, KL: 75.97622680664062, Loss: 0.020743055269122124, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6848/20000], Bound: 0.3709370791912079, Entropy: 139.54617309570312, Temp: 2.5703160762786865, KL: 67.91596984863281, Loss: 0.018723752349615097, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6849/20000], Bound: 0.3867013156414032, Entropy: 140.5392608642578, Temp: 2.570361614227295, KL: 72.01788330078125, Loss: 0.01945672743022442, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6850/20000], Bound: 0.3939390480518341, Entropy: 142.1156768798828, Temp: 2.5703883171081543, KL: 74.72018432617188, Loss: 0.01826641708612442, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6851/20000], Bound: 0.3745798170566559, Entropy: 141.15008544921875, Temp: 2.570439100265503, KL: 69.97013854980469, Loss: 0.016724957153201103, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6852/20000], Bound: 0.3715984523296356, Entropy: 140.2176971435547, Temp: 2.570521831512451, KL: 67.95536804199219, Loss: 0.019010623916983604, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6853/20000], Bound: 0.38739803433418274, Entropy: 142.16946411132812, Temp: 2.5705699920654297, KL: 72.74397277832031, Loss: 0.018436085432767868, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6854/20000], Bound: 0.3965405523777008, Entropy: 141.2300567626953, Temp: 2.5706257820129395, KL: 75.81500244140625, Loss: 0.01761152409017086, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6855/20000], Bound: 0.3950307369232178, Entropy: 140.74008178710938, Temp: 2.570723295211792, KL: 75.58758544921875, Loss: 0.017200103029608727, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6856/20000], Bound: 0.4081372916698456, Entropy: 141.50303649902344, Temp: 2.570866346359253, KL: 79.06211853027344, Loss: 0.01792832650244236, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6857/20000], Bound: 0.4153476059436798, Entropy: 142.24790954589844, Temp: 2.571052074432373, KL: 81.72808837890625, Loss: 0.016925308853387833, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6858/20000], Bound: 0.37182149291038513, Entropy: 141.19627380371094, Temp: 2.5713114738464355, KL: 67.73001098632812, Loss: 0.01957833208143711, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6859/20000], Bound: 0.40552037954330444, Entropy: 141.96702575683594, Temp: 2.5715043544769287, KL: 77.08676147460938, Loss: 0.0202715415507555, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6860/20000], Bound: 0.4055507183074951, Entropy: 140.6246795654297, Temp: 2.5716712474823, KL: 77.73199462890625, Loss: 0.01903640665113926, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6861/20000], Bound: 0.4182429611682892, Entropy: 141.76731872558594, Temp: 2.5718462467193604, KL: 79.95199584960938, Loss: 0.02207949385046959, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6862/20000], Bound: 0.39129123091697693, Entropy: 141.70736694335938, Temp: 2.5719709396362305, KL: 72.63847351074219, Loss: 0.02083943597972393, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6863/20000], Bound: 0.4000256061553955, Entropy: 141.76419067382812, Temp: 2.57204008102417, KL: 75.68197631835938, Loss: 0.019866513088345528, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6864/20000], Bound: 0.4005051851272583, Entropy: 140.08033752441406, Temp: 2.572096824645996, KL: 75.25778198242188, Loss: 0.020964927971363068, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6865/20000], Bound: 0.3900500237941742, Entropy: 141.73666381835938, Temp: 2.572115421295166, KL: 72.65879821777344, Loss: 0.020104045048356056, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6866/20000], Bound: 0.3774814307689667, Entropy: 143.250244140625, Temp: 2.572105646133423, KL: 69.45010375976562, Loss: 0.01934978924691677, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6867/20000], Bound: 0.34699687361717224, Entropy: 141.290283203125, Temp: 2.5720696449279785, KL: 61.32432556152344, Loss: 0.01869221217930317, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6868/20000], Bound: 0.37612804770469666, Entropy: 140.8399200439453, Temp: 2.5719807147979736, KL: 70.38877868652344, Loss: 0.016778329387307167, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6869/20000], Bound: 0.368845671415329, Entropy: 143.17955017089844, Temp: 2.5719380378723145, KL: 67.27964782714844, Loss: 0.018835384398698807, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6870/20000], Bound: 0.43117523193359375, Entropy: 141.9923553466797, Temp: 2.5718727111816406, KL: 83.888427734375, Loss: 0.022069092839956284, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6871/20000], Bound: 0.3850310742855072, Entropy: 141.6163787841797, Temp: 2.5718019008636475, KL: 72.53829956054688, Loss: 0.01752784289419651, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6872/20000], Bound: 0.3972473740577698, Entropy: 142.68385314941406, Temp: 2.571769952774048, KL: 71.98670959472656, Loss: 0.025468606501817703, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6873/20000], Bound: 0.41399720311164856, Entropy: 140.65196228027344, Temp: 2.5715889930725098, KL: 80.084716796875, Loss: 0.01934169977903366, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6874/20000], Bound: 0.4151829779148102, Entropy: 142.2107696533203, Temp: 2.571455955505371, KL: 79.55583190917969, Loss: 0.02105860598385334, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6875/20000], Bound: 0.3860693871974945, Entropy: 140.75511169433594, Temp: 2.571324348449707, KL: 71.30839538574219, Loss: 0.02049335651099682, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6876/20000], Bound: 0.3748660683631897, Entropy: 142.70130920410156, Temp: 2.5711634159088135, KL: 69.75311279296875, Loss: 0.017311856150627136, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6877/20000], Bound: 0.3658764660358429, Entropy: 141.25331115722656, Temp: 2.571040630340576, KL: 67.55894470214844, Loss: 0.01666964776813984, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6878/20000], Bound: 0.3995351493358612, Entropy: 140.3451385498047, Temp: 2.570955276489258, KL: 76.32133483886719, Loss: 0.01833168976008892, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6879/20000], Bound: 0.391360878944397, Entropy: 139.4224853515625, Temp: 2.5709121227264404, KL: 74.67704772949219, Loss: 0.016903212293982506, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6880/20000], Bound: 0.36139458417892456, Entropy: 141.13319396972656, Temp: 2.5709311962127686, KL: 65.25592041015625, Loss: 0.018723463639616966, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6881/20000], Bound: 0.3460805416107178, Entropy: 141.41024780273438, Temp: 2.570913553237915, KL: 62.864471435546875, Loss: 0.015203622169792652, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6882/20000], Bound: 0.4144437611103058, Entropy: 142.67015075683594, Temp: 2.5709314346313477, KL: 81.30810546875, Loss: 0.0172140933573246, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6883/20000], Bound: 0.3353899121284485, Entropy: 143.06298828125, Temp: 2.571032762527466, KL: 58.20133972167969, Loss: 0.018669720739126205, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6884/20000], Bound: 0.40831106901168823, Entropy: 141.59298706054688, Temp: 2.5710501670837402, KL: 77.59123229980469, Loss: 0.02089129388332367, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6885/20000], Bound: 0.4267362058162689, Entropy: 140.9971466064453, Temp: 2.571047782897949, KL: 83.33845520019531, Loss: 0.0204874649643898, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6886/20000], Bound: 0.4008689522743225, Entropy: 141.10009765625, Temp: 2.5710668563842773, KL: 76.52505493164062, Loss: 0.01869683712720871, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6887/20000], Bound: 0.3777305483818054, Entropy: 141.18824768066406, Temp: 2.571110725402832, KL: 70.06295776367188, Loss: 0.018285708501935005, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6888/20000], Bound: 0.39414164423942566, Entropy: 143.4640655517578, Temp: 2.5711514949798584, KL: 75.53300476074219, Loss: 0.016808826476335526, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6889/20000], Bound: 0.3952310383319855, Entropy: 141.90634155273438, Temp: 2.5712528228759766, KL: 75.08424377441406, Loss: 0.018298417329788208, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6890/20000], Bound: 0.3878956437110901, Entropy: 142.539794921875, Temp: 2.5713717937469482, KL: 73.57035827636719, Loss: 0.017116200178861618, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6891/20000], Bound: 0.41285622119903564, Entropy: 140.98841857910156, Temp: 2.571526288986206, KL: 79.68977355957031, Loss: 0.019445955753326416, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6892/20000], Bound: 0.4011985659599304, Entropy: 141.46324157714844, Temp: 2.5716910362243652, KL: 76.63912963867188, Loss: 0.01867028884589672, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6893/20000], Bound: 0.4082894027233124, Entropy: 141.34800720214844, Temp: 2.571866750717163, KL: 79.64225769042969, Loss: 0.01690066047012806, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6894/20000], Bound: 0.3935706317424774, Entropy: 141.7274627685547, Temp: 2.572108507156372, KL: 74.71621704101562, Loss: 0.018085667863488197, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6895/20000], Bound: 0.42456740140914917, Entropy: 140.7461395263672, Temp: 2.572356700897217, KL: 83.98553466796875, Loss: 0.01796339638531208, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6896/20000], Bound: 0.3564627766609192, Entropy: 142.38938903808594, Temp: 2.5726611614227295, KL: 64.62675476074219, Loss: 0.017312651500105858, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6897/20000], Bound: 0.4041110575199127, Entropy: 139.74623107910156, Temp: 2.5729286670684814, KL: 77.84086608886719, Loss: 0.01801416277885437, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6898/20000], Bound: 0.39996805787086487, Entropy: 140.8056640625, Temp: 2.5732178688049316, KL: 75.24763488769531, Loss: 0.02069074660539627, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6899/20000], Bound: 0.42658281326293945, Entropy: 138.38296508789062, Temp: 2.573451042175293, KL: 84.12202453613281, Loss: 0.01890525035560131, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6900/20000], Bound: 0.4049874246120453, Entropy: 140.40931701660156, Temp: 2.573720693588257, KL: 77.73989868164062, Loss: 0.01872214674949646, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6901/20000], Bound: 0.3758622407913208, Entropy: 140.27976989746094, Temp: 2.5739946365356445, KL: 69.00814819335938, Loss: 0.019335320219397545, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6902/20000], Bound: 0.37720489501953125, Entropy: 138.04698181152344, Temp: 2.5742108821868896, KL: 70.23106384277344, Loss: 0.017700864002108574, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6903/20000], Bound: 0.3767015039920807, Entropy: 139.195556640625, Temp: 2.5744199752807617, KL: 71.21784973144531, Loss: 0.015509351156651974, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6904/20000], Bound: 0.40418586134910583, Entropy: 138.90867614746094, Temp: 2.574678897857666, KL: 75.373779296875, Loss: 0.022869262844324112, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6905/20000], Bound: 0.38623347878456116, Entropy: 139.03189086914062, Temp: 2.5748345851898193, KL: 72.19140625, Loss: 0.018904659897089005, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6906/20000], Bound: 0.40317589044570923, Entropy: 139.8373260498047, Temp: 2.5749714374542236, KL: 77.36064147949219, Loss: 0.01843632012605667, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6907/20000], Bound: 0.41087132692337036, Entropy: 140.09329223632812, Temp: 2.575129747390747, KL: 79.33145141601562, Loss: 0.01903625763952732, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6908/20000], Bound: 0.3900454640388489, Entropy: 139.84848022460938, Temp: 2.575303554534912, KL: 74.61068725585938, Loss: 0.016344504430890083, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6909/20000], Bound: 0.39973434805870056, Entropy: 139.71592712402344, Temp: 2.575528860092163, KL: 77.55021667480469, Loss: 0.01611248217523098, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6910/20000], Bound: 0.3746815323829651, Entropy: 138.195068359375, Temp: 2.5758209228515625, KL: 66.42756652832031, Loss: 0.02371368370950222, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6911/20000], Bound: 0.4051682651042938, Entropy: 139.5382537841797, Temp: 2.5759356021881104, KL: 77.83631896972656, Loss: 0.018664933741092682, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6912/20000], Bound: 0.38781172037124634, Entropy: 141.06321716308594, Temp: 2.576070785522461, KL: 73.35252380371094, Loss: 0.017544982954859734, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6913/20000], Bound: 0.37534624338150024, Entropy: 141.35726928710938, Temp: 2.576226234436035, KL: 69.64122009277344, Loss: 0.017844462767243385, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6914/20000], Bound: 0.39185598492622375, Entropy: 139.1898651123047, Temp: 2.5763728618621826, KL: 73.43789672851562, Loss: 0.019649531692266464, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6915/20000], Bound: 0.3910302221775055, Entropy: 138.32037353515625, Temp: 2.5764901638031006, KL: 73.30079650878906, Loss: 0.019452892243862152, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6916/20000], Bound: 0.4178880453109741, Entropy: 137.2330780029297, Temp: 2.576584577560425, KL: 82.12615966796875, Loss: 0.017707977443933487, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6917/20000], Bound: 0.40599000453948975, Entropy: 138.76930236816406, Temp: 2.5767455101013184, KL: 78.09112548828125, Loss: 0.018651336431503296, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6918/20000], Bound: 0.37756067514419556, Entropy: 138.3590850830078, Temp: 2.576923370361328, KL: 69.87449645996094, Loss: 0.01861634850502014, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6919/20000], Bound: 0.409220814704895, Entropy: 140.17271423339844, Temp: 2.577072858810425, KL: 76.625, Loss: 0.023358101025223732, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6920/20000], Bound: 0.42533189058303833, Entropy: 139.84205627441406, Temp: 2.5771238803863525, KL: 82.32992553710938, Loss: 0.021692072972655296, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6921/20000], Bound: 0.397497296333313, Entropy: 137.0216064453125, Temp: 2.5771548748016357, KL: 74.90863037109375, Loss: 0.019987987354397774, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6922/20000], Bound: 0.3885606527328491, Entropy: 139.0044708251953, Temp: 2.5771677494049072, KL: 72.34136962890625, Loss: 0.019937552511692047, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6923/20000], Bound: 0.3888750970363617, Entropy: 137.96917724609375, Temp: 2.577151298522949, KL: 72.84271240234375, Loss: 0.01914062164723873, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6924/20000], Bound: 0.3741410970687866, Entropy: 140.80532836914062, Temp: 2.57712984085083, KL: 68.17381286621094, Loss: 0.02003953419625759, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6925/20000], Bound: 0.40436482429504395, Entropy: 138.58746337890625, Temp: 2.577057123184204, KL: 75.97984313964844, Loss: 0.021819956600666046, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6926/20000], Bound: 0.3928180932998657, Entropy: 138.91323852539062, Temp: 2.576939582824707, KL: 74.21018981933594, Loss: 0.018698135390877724, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6927/20000], Bound: 0.39106273651123047, Entropy: 138.69732666015625, Temp: 2.576845169067383, KL: 72.80316162109375, Loss: 0.020440462976694107, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6928/20000], Bound: 0.3924189805984497, Entropy: 138.7987060546875, Temp: 2.5767228603363037, KL: 73.16429138183594, Loss: 0.020500678569078445, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6929/20000], Bound: 0.39538484811782837, Entropy: 136.51332092285156, Temp: 2.5765764713287354, KL: 75.83316040039062, Loss: 0.01699226163327694, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6930/20000], Bound: 0.3850708305835724, Entropy: 141.12213134765625, Temp: 2.5765044689178467, KL: 71.1546630859375, Loss: 0.020286230370402336, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6931/20000], Bound: 0.36562013626098633, Entropy: 140.79261779785156, Temp: 2.576397180557251, KL: 67.56887817382812, Loss: 0.01656448096036911, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6932/20000], Bound: 0.42352330684661865, Entropy: 141.46661376953125, Temp: 2.5763261318206787, KL: 83.0333251953125, Loss: 0.01925009861588478, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6933/20000], Bound: 0.41010811924934387, Entropy: 140.56668090820312, Temp: 2.576308012008667, KL: 80.09300231933594, Loss: 0.01713165082037449, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6934/20000], Bound: 0.3969663977622986, Entropy: 137.02413940429688, Temp: 2.5763707160949707, KL: 75.04725646972656, Loss: 0.019409725442528725, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6935/20000], Bound: 0.38074570894241333, Entropy: 139.99569702148438, Temp: 2.5764267444610596, KL: 70.90803527832031, Loss: 0.01836380735039711, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6936/20000], Bound: 0.4039277732372284, Entropy: 138.285400390625, Temp: 2.5764784812927246, KL: 75.82669067382812, Loss: 0.021860897541046143, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6937/20000], Bound: 0.3983041048049927, Entropy: 139.95144653320312, Temp: 2.5764713287353516, KL: 75.98416137695312, Loss: 0.018350951373577118, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6938/20000], Bound: 0.42204779386520386, Entropy: 140.16624450683594, Temp: 2.5764942169189453, KL: 82.14793395996094, Loss: 0.02010215073823929, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6939/20000], Bound: 0.4106825590133667, Entropy: 139.78562927246094, Temp: 2.576536178588867, KL: 79.44149780273438, Loss: 0.018730726093053818, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6940/20000], Bound: 0.3965684771537781, Entropy: 138.92510986328125, Temp: 2.5766124725341797, KL: 73.70327758789062, Loss: 0.0217951200902462, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6941/20000], Bound: 0.40396830439567566, Entropy: 140.57875061035156, Temp: 2.5766172409057617, KL: 77.906005859375, Loss: 0.01785062812268734, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6942/20000], Bound: 0.3976652920246124, Entropy: 139.51724243164062, Temp: 2.5766725540161133, KL: 75.08738708496094, Loss: 0.019731123000383377, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6943/20000], Bound: 0.392536461353302, Entropy: 138.80662536621094, Temp: 2.576714277267456, KL: 73.62696838378906, Loss: 0.019668888300657272, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6944/20000], Bound: 0.38920339941978455, Entropy: 137.74319458007812, Temp: 2.57673716545105, KL: 71.86991882324219, Loss: 0.02120770886540413, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6945/20000], Bound: 0.3838813006877899, Entropy: 139.9180908203125, Temp: 2.57669734954834, KL: 71.45513916015625, Loss: 0.01904343254864216, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6946/20000], Bound: 0.3988897204399109, Entropy: 136.9835968017578, Temp: 2.5766499042510986, KL: 73.65274047851562, Loss: 0.02320955879986286, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6947/20000], Bound: 0.4154634475708008, Entropy: 137.94793701171875, Temp: 2.576509475708008, KL: 80.68287658691406, Loss: 0.019094061106443405, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6948/20000], Bound: 0.3833461403846741, Entropy: 139.2960662841797, Temp: 2.5764200687408447, KL: 69.71287536621094, Loss: 0.02212454192340374, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6949/20000], Bound: 0.40902888774871826, Entropy: 138.96359252929688, Temp: 2.5762453079223633, KL: 78.07081604003906, Loss: 0.02043280564248562, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6950/20000], Bound: 0.3573972284793854, Entropy: 141.6172332763672, Temp: 2.576079845428467, KL: 64.20921325683594, Loss: 0.018653906881809235, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6951/20000], Bound: 0.4079608917236328, Entropy: 140.73045349121094, Temp: 2.575888156890869, KL: 78.95970153808594, Loss: 0.018088042736053467, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6952/20000], Bound: 0.3785761296749115, Entropy: 138.69406127929688, Temp: 2.5757670402526855, KL: 69.71492004394531, Loss: 0.019474416971206665, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6953/20000], Bound: 0.3862467110157013, Entropy: 140.4333953857422, Temp: 2.5756266117095947, KL: 72.70249938964844, Loss: 0.017928138375282288, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6954/20000], Bound: 0.37066686153411865, Entropy: 137.15428161621094, Temp: 2.5755224227905273, KL: 69.95852661132812, Loss: 0.014660162851214409, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6955/20000], Bound: 0.42265674471855164, Entropy: 138.18551635742188, Temp: 2.5755136013031006, KL: 82.9420166015625, Loss: 0.018906399607658386, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6956/20000], Bound: 0.3880664110183716, Entropy: 140.46783447265625, Temp: 2.575559616088867, KL: 72.52610778808594, Loss: 0.01928607001900673, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6957/20000], Bound: 0.3725939989089966, Entropy: 141.33013916015625, Temp: 2.575589895248413, KL: 67.42942810058594, Loss: 0.020623382180929184, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6958/20000], Bound: 0.42779988050460815, Entropy: 139.82740783691406, Temp: 2.5755457878112793, KL: 84.56735229492188, Loss: 0.01879022642970085, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6959/20000], Bound: 0.36900991201400757, Entropy: 139.65304565429688, Temp: 2.575571060180664, KL: 67.09332275390625, Loss: 0.019320111721754074, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6960/20000], Bound: 0.3869291841983795, Entropy: 142.69873046875, Temp: 2.5755515098571777, KL: 73.24473571777344, Loss: 0.017255447804927826, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6961/20000], Bound: 0.34862393140792847, Entropy: 140.4600372314453, Temp: 2.5755748748779297, KL: 61.80525207519531, Loss: 0.01864611729979515, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6962/20000], Bound: 0.39716601371765137, Entropy: 141.40542602539062, Temp: 2.5755393505096436, KL: 75.84648132324219, Loss: 0.01796211488544941, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6963/20000], Bound: 0.38190969824790955, Entropy: 139.85313415527344, Temp: 2.5755457878112793, KL: 72.3336181640625, Loss: 0.016231931746006012, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6964/20000], Bound: 0.36338090896606445, Entropy: 141.97865295410156, Temp: 2.5756120681762695, KL: 66.9384765625, Loss: 0.016569456085562706, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6965/20000], Bound: 0.36980652809143066, Entropy: 141.25360107421875, Temp: 2.5756947994232178, KL: 68.36396789550781, Loss: 0.017288407310843468, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6966/20000], Bound: 0.3853684961795807, Entropy: 141.66879272460938, Temp: 2.575782299041748, KL: 71.64956665039062, Loss: 0.019484220072627068, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6967/20000], Bound: 0.3977753221988678, Entropy: 141.1234893798828, Temp: 2.575840473175049, KL: 74.00704956054688, Loss: 0.021881507709622383, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6968/20000], Bound: 0.40854573249816895, Entropy: 142.35739135742188, Temp: 2.5758283138275146, KL: 79.81974792480469, Loss: 0.01675466261804104, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6969/20000], Bound: 0.4170505702495575, Entropy: 141.71255493164062, Temp: 2.5759057998657227, KL: 80.2442626953125, Loss: 0.020862936973571777, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6970/20000], Bound: 0.370585560798645, Entropy: 141.96359252929688, Temp: 2.5759689807891846, KL: 68.51765441894531, Loss: 0.017417453229427338, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6971/20000], Bound: 0.39200735092163086, Entropy: 143.03036499023438, Temp: 2.5760369300842285, KL: 74.14279174804688, Loss: 0.01836291141808033, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6972/20000], Bound: 0.3807080388069153, Entropy: 142.07073974609375, Temp: 2.576117753982544, KL: 71.90312194824219, Loss: 0.016408421099185944, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6973/20000], Bound: 0.38806137442588806, Entropy: 140.00143432617188, Temp: 2.576244354248047, KL: 71.93634033203125, Loss: 0.020435012876987457, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6974/20000], Bound: 0.4223179817199707, Entropy: 143.46046447753906, Temp: 2.5763163566589355, KL: 83.74005126953125, Loss: 0.01716885343194008, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6975/20000], Bound: 0.41464924812316895, Entropy: 139.30535888671875, Temp: 2.57647967338562, KL: 80.06170654296875, Loss: 0.019825570285320282, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6976/20000], Bound: 0.39870303869247437, Entropy: 141.013427734375, Temp: 2.576643705368042, KL: 76.42410278320312, Loss: 0.017725646495819092, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6977/20000], Bound: 0.3872848451137543, Entropy: 141.37767028808594, Temp: 2.5768380165100098, KL: 72.39991760253906, Loss: 0.01910751685500145, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6978/20000], Bound: 0.4014410078525543, Entropy: 139.17535400390625, Temp: 2.5770046710968018, KL: 76.90005493164062, Loss: 0.018363870680332184, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6979/20000], Bound: 0.373891681432724, Entropy: 142.642333984375, Temp: 2.5771889686584473, KL: 68.53570556640625, Loss: 0.01920134760439396, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6980/20000], Bound: 0.37472444772720337, Entropy: 142.01626586914062, Temp: 2.577322244644165, KL: 68.69398498535156, Loss: 0.019351935014128685, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6981/20000], Bound: 0.4047280251979828, Entropy: 137.80075073242188, Temp: 2.577406883239746, KL: 78.25479125976562, Loss: 0.017618322744965553, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6982/20000], Bound: 0.4283429682254791, Entropy: 141.45750427246094, Temp: 2.5775420665740967, KL: 83.73951721191406, Loss: 0.020745281130075455, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6983/20000], Bound: 0.3923949599266052, Entropy: 137.23648071289062, Temp: 2.5776779651641846, KL: 74.50518798828125, Loss: 0.017895953729748726, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6984/20000], Bound: 0.38161757588386536, Entropy: 142.50685119628906, Temp: 2.5778322219848633, KL: 70.23146057128906, Loss: 0.020172877237200737, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6985/20000], Bound: 0.3899933993816376, Entropy: 140.42295837402344, Temp: 2.5779244899749756, KL: 73.0587158203125, Loss: 0.019355982542037964, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6986/20000], Bound: 0.397217720746994, Entropy: 140.4787139892578, Temp: 2.577996253967285, KL: 75.14755249023438, Loss: 0.019375324249267578, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6987/20000], Bound: 0.38983649015426636, Entropy: 138.54791259765625, Temp: 2.5780608654022217, KL: 71.71646118164062, Loss: 0.02187271974980831, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6988/20000], Bound: 0.4024044871330261, Entropy: 140.02035522460938, Temp: 2.57804012298584, KL: 75.97380065917969, Loss: 0.020721817389130592, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6989/20000], Bound: 0.420726478099823, Entropy: 139.0504608154297, Temp: 2.577993392944336, KL: 80.95512390136719, Loss: 0.021658839657902718, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6990/20000], Bound: 0.3969835638999939, Entropy: 140.50669860839844, Temp: 2.5779290199279785, KL: 75.88713073730469, Loss: 0.017807556316256523, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6991/20000], Bound: 0.3824344277381897, Entropy: 141.47792053222656, Temp: 2.5779125690460205, KL: 71.8643798828125, Loss: 0.01745910756289959, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6992/20000], Bound: 0.40051448345184326, Entropy: 138.47515869140625, Temp: 2.577925682067871, KL: 76.45628356933594, Loss: 0.018707746639847755, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6993/20000], Bound: 0.3791642189025879, Entropy: 139.6036834716797, Temp: 2.577960252761841, KL: 68.18873596191406, Loss: 0.022779932245612144, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6994/20000], Bound: 0.3942107558250427, Entropy: 139.72216796875, Temp: 2.5778698921203613, KL: 75.14935302734375, Loss: 0.01767117902636528, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6995/20000], Bound: 0.3964339792728424, Entropy: 139.30885314941406, Temp: 2.577829599380493, KL: 73.61601257324219, Loss: 0.021900471299886703, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6996/20000], Bound: 0.3816113770008087, Entropy: 142.89111328125, Temp: 2.577723979949951, KL: 70.517333984375, Loss: 0.01961391419172287, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6997/20000], Bound: 0.3865818977355957, Entropy: 136.79010009765625, Temp: 2.5775973796844482, KL: 71.88484191894531, Loss: 0.019722284749150276, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6998/20000], Bound: 0.40659672021865845, Entropy: 137.31382751464844, Temp: 2.5774567127227783, KL: 78.42713928222656, Loss: 0.018356282263994217, Learning Rate: 0.0012106082099999993\n",
      "Epoch [6999/20000], Bound: 0.38112902641296387, Entropy: 138.29763793945312, Temp: 2.5773727893829346, KL: 71.68559265136719, Loss: 0.017077099531888962, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7000/20000], Bound: 0.38118088245391846, Entropy: 140.05935668945312, Temp: 2.577333688735962, KL: 69.77578735351562, Loss: 0.020810401067137718, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7001/20000], Bound: 0.4021792709827423, Entropy: 139.27716064453125, Temp: 2.5772335529327393, KL: 77.82286071777344, Loss: 0.01699725352227688, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7002/20000], Bound: 0.3639456033706665, Entropy: 140.63279724121094, Temp: 2.5772154331207275, KL: 66.70401000976562, Loss: 0.017344843596220016, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7003/20000], Bound: 0.3791145384311676, Entropy: 138.55404663085938, Temp: 2.5772013664245605, KL: 69.26815795898438, Loss: 0.020652007311582565, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7004/20000], Bound: 0.4103100597858429, Entropy: 139.6355438232422, Temp: 2.577125072479248, KL: 79.18974304199219, Loss: 0.019011186435818672, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7005/20000], Bound: 0.38142213225364685, Entropy: 141.08477783203125, Temp: 2.57708740234375, KL: 70.07127380371094, Loss: 0.02036832459270954, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7006/20000], Bound: 0.4218335747718811, Entropy: 139.45458984375, Temp: 2.5770010948181152, KL: 81.93893432617188, Loss: 0.02038814313709736, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7007/20000], Bound: 0.39724355936050415, Entropy: 140.18312072753906, Temp: 2.576936960220337, KL: 75.90409851074219, Loss: 0.017910383641719818, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7008/20000], Bound: 0.4020315408706665, Entropy: 142.65208435058594, Temp: 2.5769193172454834, KL: 77.32225036621094, Loss: 0.017880458384752274, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7009/20000], Bound: 0.39163148403167725, Entropy: 142.44271850585938, Temp: 2.576951742172241, KL: 74.62190246582031, Loss: 0.01723216101527214, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7010/20000], Bound: 0.3772888779640198, Entropy: 139.5111083984375, Temp: 2.5770301818847656, KL: 69.15353393554688, Loss: 0.019866589456796646, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7011/20000], Bound: 0.3938571810722351, Entropy: 141.65859985351562, Temp: 2.5770552158355713, KL: 74.15596008300781, Loss: 0.019389914348721504, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7012/20000], Bound: 0.38818421959877014, Entropy: 141.704345703125, Temp: 2.5770723819732666, KL: 73.85137939453125, Loss: 0.016796376556158066, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7013/20000], Bound: 0.3918280005455017, Entropy: 141.35287475585938, Temp: 2.577143430709839, KL: 73.57673645019531, Loss: 0.019372517243027687, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7014/20000], Bound: 0.37508344650268555, Entropy: 143.05433654785156, Temp: 2.5771992206573486, KL: 69.46781921386719, Loss: 0.0180464256554842, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7015/20000], Bound: 0.35972878336906433, Entropy: 143.44032287597656, Temp: 2.5772500038146973, KL: 65.44560241699219, Loss: 0.017515305429697037, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7016/20000], Bound: 0.3890341818332672, Entropy: 141.5499267578125, Temp: 2.5772860050201416, KL: 73.93217468261719, Loss: 0.017117487266659737, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7017/20000], Bound: 0.3939243257045746, Entropy: 144.39976501464844, Temp: 2.577366828918457, KL: 73.09323120117188, Loss: 0.021492743864655495, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7018/20000], Bound: 0.39021036028862, Entropy: 141.740966796875, Temp: 2.5773770809173584, KL: 72.26055908203125, Loss: 0.02102024480700493, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7019/20000], Bound: 0.38791897892951965, Entropy: 144.0624237060547, Temp: 2.5773303508758545, KL: 74.16853332519531, Loss: 0.01603573001921177, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7020/20000], Bound: 0.3904542028903961, Entropy: 142.3764190673828, Temp: 2.577364206314087, KL: 73.08804321289062, Loss: 0.01955161802470684, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7021/20000], Bound: 0.4297778308391571, Entropy: 143.48605346679688, Temp: 2.577378988265991, KL: 85.07441711425781, Loss: 0.019005905836820602, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7022/20000], Bound: 0.38154664635658264, Entropy: 143.4374237060547, Temp: 2.577455520629883, KL: 71.63648986816406, Loss: 0.01740439049899578, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7023/20000], Bound: 0.3820783793926239, Entropy: 143.24935913085938, Temp: 2.5775527954101562, KL: 69.98236083984375, Loss: 0.020908724516630173, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7024/20000], Bound: 0.3939341902732849, Entropy: 143.4435272216797, Temp: 2.5775740146636963, KL: 73.80908203125, Loss: 0.02011173591017723, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7025/20000], Bound: 0.42255881428718567, Entropy: 143.44593811035156, Temp: 2.5775675773620605, KL: 82.57456970214844, Loss: 0.019588535651564598, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7026/20000], Bound: 0.40841734409332275, Entropy: 144.34927368164062, Temp: 2.5775978565216064, KL: 78.82901000976562, Loss: 0.018625296652317047, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7027/20000], Bound: 0.4011651277542114, Entropy: 143.56788635253906, Temp: 2.5776634216308594, KL: 75.65261840820312, Loss: 0.02063407562673092, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7028/20000], Bound: 0.39360174536705017, Entropy: 141.6571044921875, Temp: 2.577694892883301, KL: 75.20823669433594, Loss: 0.017211703583598137, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7029/20000], Bound: 0.38095325231552124, Entropy: 141.46231079101562, Temp: 2.5777764320373535, KL: 69.76240539550781, Loss: 0.020714491605758667, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7030/20000], Bound: 0.35279983282089233, Entropy: 145.13433837890625, Temp: 2.577786445617676, KL: 61.82781982421875, Loss: 0.02083459123969078, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7031/20000], Bound: 0.3617897927761078, Entropy: 144.38107299804688, Temp: 2.57768177986145, KL: 65.93356323242188, Loss: 0.017681030556559563, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7032/20000], Bound: 0.38278061151504517, Entropy: 143.42127990722656, Temp: 2.5775763988494873, KL: 72.29049682617188, Loss: 0.016820941120386124, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7033/20000], Bound: 0.39413103461265564, Entropy: 145.82691955566406, Temp: 2.577528238296509, KL: 75.28135681152344, Loss: 0.017366256564855576, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7034/20000], Bound: 0.4137120842933655, Entropy: 144.01158142089844, Temp: 2.5775344371795654, KL: 78.85136413574219, Loss: 0.021641943603754044, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7035/20000], Bound: 0.401006281375885, Entropy: 139.66073608398438, Temp: 2.5775063037872314, KL: 76.32987976074219, Loss: 0.019228149205446243, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7036/20000], Bound: 0.3926854133605957, Entropy: 145.90830993652344, Temp: 2.577491283416748, KL: 74.30992126464844, Loss: 0.018436014652252197, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7037/20000], Bound: 0.3895719349384308, Entropy: 144.34991455078125, Temp: 2.57749605178833, KL: 73.1697998046875, Loss: 0.018899856135249138, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7038/20000], Bound: 0.3864029347896576, Entropy: 143.0167999267578, Temp: 2.577501058578491, KL: 72.28536987304688, Loss: 0.01884455233812332, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7039/20000], Bound: 0.40619412064552307, Entropy: 139.73480224609375, Temp: 2.577502489089966, KL: 78.9287109375, Loss: 0.017152708023786545, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7040/20000], Bound: 0.4127974212169647, Entropy: 143.14553833007812, Temp: 2.5775787830352783, KL: 79.43511962890625, Loss: 0.01997961290180683, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7041/20000], Bound: 0.42291298508644104, Entropy: 141.54116821289062, Temp: 2.5776569843292236, KL: 82.64228820800781, Loss: 0.019666654989123344, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7042/20000], Bound: 0.3890897035598755, Entropy: 139.3782196044922, Temp: 2.5777618885040283, KL: 73.21173095703125, Loss: 0.018551360815763474, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7043/20000], Bound: 0.4059900939464569, Entropy: 141.38075256347656, Temp: 2.5778656005859375, KL: 76.97181701660156, Loss: 0.02083573490381241, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7044/20000], Bound: 0.4171983301639557, Entropy: 139.68368530273438, Temp: 2.5779337882995605, KL: 80.00584411621094, Loss: 0.021435674279928207, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7045/20000], Bound: 0.3982606530189514, Entropy: 140.76736450195312, Temp: 2.577972173690796, KL: 76.07872009277344, Loss: 0.018160102888941765, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7046/20000], Bound: 0.4030226171016693, Entropy: 139.21347045898438, Temp: 2.5780415534973145, KL: 76.84548950195312, Loss: 0.019384123384952545, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7047/20000], Bound: 0.3933183550834656, Entropy: 138.7525634765625, Temp: 2.578113079071045, KL: 74.14485168457031, Loss: 0.019119251519441605, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7048/20000], Bound: 0.3883068263530731, Entropy: 140.26187133789062, Temp: 2.5781779289245605, KL: 72.90553283691406, Loss: 0.018711742013692856, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7049/20000], Bound: 0.4090314209461212, Entropy: 139.50289916992188, Temp: 2.578239679336548, KL: 78.74845886230469, Loss: 0.019142938777804375, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7050/20000], Bound: 0.4039836525917053, Entropy: 138.94488525390625, Temp: 2.5783207416534424, KL: 77.12391662597656, Loss: 0.019396619871258736, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7051/20000], Bound: 0.3752184212207794, Entropy: 140.8835906982422, Temp: 2.578403949737549, KL: 67.92988586425781, Loss: 0.021114708855748177, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7052/20000], Bound: 0.4061405062675476, Entropy: 139.0973663330078, Temp: 2.5783936977386475, KL: 78.0606689453125, Loss: 0.018816372379660606, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7053/20000], Bound: 0.37937986850738525, Entropy: 138.29473876953125, Temp: 2.57841420173645, KL: 70.01974487304688, Loss: 0.01935206539928913, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7054/20000], Bound: 0.37569907307624817, Entropy: 139.43637084960938, Temp: 2.5784034729003906, KL: 69.49110412597656, Loss: 0.018351048231124878, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7055/20000], Bound: 0.38505250215530396, Entropy: 140.30857849121094, Temp: 2.5783863067626953, KL: 71.7650146484375, Loss: 0.019110748544335365, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7056/20000], Bound: 0.36818015575408936, Entropy: 139.16000366210938, Temp: 2.578357696533203, KL: 66.90913391113281, Loss: 0.019251400604844093, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7057/20000], Bound: 0.3837556540966034, Entropy: 137.32777404785156, Temp: 2.57828688621521, KL: 70.99668884277344, Loss: 0.019878854975104332, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7058/20000], Bound: 0.40981391072273254, Entropy: 139.5258331298828, Temp: 2.5781867504119873, KL: 77.35470581054688, Loss: 0.02229645662009716, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7059/20000], Bound: 0.394771009683609, Entropy: 139.62400817871094, Temp: 2.5780370235443115, KL: 75.60966491699219, Loss: 0.017096340656280518, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7060/20000], Bound: 0.39265960454940796, Entropy: 140.59536743164062, Temp: 2.577960968017578, KL: 73.89193725585938, Loss: 0.019237376749515533, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7061/20000], Bound: 0.39583075046539307, Entropy: 139.78546142578125, Temp: 2.5778887271881104, KL: 73.94766235351562, Loss: 0.02091670036315918, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7062/20000], Bound: 0.40072154998779297, Entropy: 141.27333068847656, Temp: 2.5777788162231445, KL: 74.45840454101562, Loss: 0.022699078544974327, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7063/20000], Bound: 0.4113557040691376, Entropy: 139.2994842529297, Temp: 2.57759428024292, KL: 78.63482666015625, Loss: 0.02069755829870701, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7064/20000], Bound: 0.3974875509738922, Entropy: 138.7682647705078, Temp: 2.5774152278900146, KL: 73.69924926757812, Loss: 0.022331396117806435, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7065/20000], Bound: 0.4040512442588806, Entropy: 138.2591552734375, Temp: 2.5771734714508057, KL: 77.35556030273438, Loss: 0.018972720950841904, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7066/20000], Bound: 0.42373713850975037, Entropy: 140.99412536621094, Temp: 2.5769782066345215, KL: 82.74130249023438, Loss: 0.01995118521153927, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7067/20000], Bound: 0.38520440459251404, Entropy: 140.16290283203125, Temp: 2.576831579208374, KL: 73.45333862304688, Loss: 0.015903465449810028, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7068/20000], Bound: 0.40265873074531555, Entropy: 140.60121154785156, Temp: 2.5767765045166016, KL: 78.00343322753906, Loss: 0.016914958134293556, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7069/20000], Bound: 0.3870731592178345, Entropy: 140.20968627929688, Temp: 2.5768041610717773, KL: 72.42979431152344, Loss: 0.018931027501821518, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7070/20000], Bound: 0.38279300928115845, Entropy: 142.4787139892578, Temp: 2.5768251419067383, KL: 70.21794128417969, Loss: 0.020841097459197044, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7071/20000], Bound: 0.4043305516242981, Entropy: 141.29501342773438, Temp: 2.57677960395813, KL: 76.80072021484375, Loss: 0.020204568281769753, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7072/20000], Bound: 0.4060421288013458, Entropy: 138.54644775390625, Temp: 2.576728105545044, KL: 78.19500732421875, Loss: 0.01847950741648674, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7073/20000], Bound: 0.38280123472213745, Entropy: 140.34750366210938, Temp: 2.576721668243408, KL: 70.74526977539062, Loss: 0.01982145942747593, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7074/20000], Bound: 0.40080490708351135, Entropy: 141.03111267089844, Temp: 2.5766797065734863, KL: 76.05360412597656, Loss: 0.019640175625681877, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7075/20000], Bound: 0.3798515200614929, Entropy: 141.15835571289062, Temp: 2.57664155960083, KL: 71.66328430175781, Loss: 0.016406020149588585, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7076/20000], Bound: 0.40317898988723755, Entropy: 140.77243041992188, Temp: 2.5766618251800537, KL: 76.43049621582031, Loss: 0.020263008773326874, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7077/20000], Bound: 0.410698801279068, Entropy: 140.07737731933594, Temp: 2.5766658782958984, KL: 79.26898193359375, Loss: 0.019076477736234665, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7078/20000], Bound: 0.40528616309165955, Entropy: 139.0948028564453, Temp: 2.5767009258270264, KL: 78.22721862792969, Loss: 0.01798303611576557, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7079/20000], Bound: 0.3789152503013611, Entropy: 138.69873046875, Temp: 2.576785087585449, KL: 69.66172790527344, Loss: 0.01977451704442501, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7080/20000], Bound: 0.40732869505882263, Entropy: 140.73446655273438, Temp: 2.57681941986084, KL: 79.08467102050781, Loss: 0.017493311315774918, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7081/20000], Bound: 0.34016236662864685, Entropy: 140.61373901367188, Temp: 2.5769200325012207, KL: 59.58787536621094, Loss: 0.018509013578295708, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7082/20000], Bound: 0.3832031786441803, Entropy: 140.13282775878906, Temp: 2.5769400596618652, KL: 73.06906127929688, Loss: 0.015537815168499947, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7083/20000], Bound: 0.38176774978637695, Entropy: 139.91957092285156, Temp: 2.5770423412323, KL: 71.51071166992188, Loss: 0.017766539007425308, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7084/20000], Bound: 0.4179818034172058, Entropy: 139.47183227539062, Temp: 2.5771543979644775, KL: 80.25996398925781, Loss: 0.02139091119170189, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7085/20000], Bound: 0.39036864042282104, Entropy: 141.5795440673828, Temp: 2.577234983444214, KL: 72.54814147949219, Loss: 0.02054968848824501, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7086/20000], Bound: 0.4115232825279236, Entropy: 138.65606689453125, Temp: 2.577263355255127, KL: 80.10791015625, Loss: 0.017932767048478127, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7087/20000], Bound: 0.4194386601448059, Entropy: 137.8109893798828, Temp: 2.577353000640869, KL: 81.48710632324219, Loss: 0.019864575937390327, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7088/20000], Bound: 0.40002262592315674, Entropy: 140.45663452148438, Temp: 2.577457904815674, KL: 76.58879089355469, Loss: 0.01816549338400364, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7089/20000], Bound: 0.40383005142211914, Entropy: 141.67633056640625, Temp: 2.5775907039642334, KL: 76.03456115722656, Loss: 0.021413546055555344, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7090/20000], Bound: 0.373430073261261, Entropy: 142.0689697265625, Temp: 2.577665090560913, KL: 68.25434875488281, Loss: 0.019498784095048904, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7091/20000], Bound: 0.42486444115638733, Entropy: 141.1088409423828, Temp: 2.577688217163086, KL: 84.2613525390625, Loss: 0.017676495015621185, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7092/20000], Bound: 0.3821054995059967, Entropy: 140.78866577148438, Temp: 2.5778024196624756, KL: 70.6009521484375, Loss: 0.019726235419511795, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7093/20000], Bound: 0.38546887040138245, Entropy: 140.6117401123047, Temp: 2.5778698921203613, KL: 71.85972595214844, Loss: 0.019153542816638947, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7094/20000], Bound: 0.3810996115207672, Entropy: 140.40582275390625, Temp: 2.5779170989990234, KL: 70.77986145019531, Loss: 0.018823346123099327, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7095/20000], Bound: 0.40088146924972534, Entropy: 140.45957946777344, Temp: 2.5779478549957275, KL: 76.69171142578125, Loss: 0.018460256978869438, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7096/20000], Bound: 0.37196090817451477, Entropy: 141.6244659423828, Temp: 2.578007221221924, KL: 68.28717041015625, Loss: 0.018635045737028122, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7097/20000], Bound: 0.3505801558494568, Entropy: 140.16322326660156, Temp: 2.5780389308929443, KL: 63.90736389160156, Loss: 0.015624542720615864, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7098/20000], Bound: 0.3896225094795227, Entropy: 141.60403442382812, Temp: 2.578096389770508, KL: 72.19914245605469, Loss: 0.020817050710320473, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7099/20000], Bound: 0.3757762312889099, Entropy: 140.53616333007812, Temp: 2.578094482421875, KL: 68.96304321289062, Loss: 0.019414477050304413, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7100/20000], Bound: 0.3888682425022125, Entropy: 139.35208129882812, Temp: 2.5780556201934814, KL: 74.59904479980469, Loss: 0.015739955008029938, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7101/20000], Bound: 0.39543306827545166, Entropy: 138.15579223632812, Temp: 2.578108549118042, KL: 73.77420043945312, Loss: 0.021030664443969727, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7102/20000], Bound: 0.37659403681755066, Entropy: 140.563720703125, Temp: 2.57810640335083, KL: 70.77716064453125, Loss: 0.01634560339152813, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7103/20000], Bound: 0.4024520814418793, Entropy: 138.74105834960938, Temp: 2.578155517578125, KL: 77.34786987304688, Loss: 0.01808539405465126, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7104/20000], Bound: 0.40718117356300354, Entropy: 140.82635498046875, Temp: 2.57824444770813, KL: 79.66004943847656, Loss: 0.016310477629303932, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7105/20000], Bound: 0.3969305455684662, Entropy: 139.31179809570312, Temp: 2.5784261226654053, KL: 75.55171203613281, Loss: 0.018433677032589912, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7106/20000], Bound: 0.3888207972049713, Entropy: 143.52516174316406, Temp: 2.578615427017212, KL: 71.63499450683594, Loss: 0.021467382088303566, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7107/20000], Bound: 0.35131943225860596, Entropy: 141.59341430664062, Temp: 2.578712224960327, KL: 61.64537048339844, Loss: 0.020408758893609047, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7108/20000], Bound: 0.35934922099113464, Entropy: 139.9716033935547, Temp: 2.5786914825439453, KL: 64.52255249023438, Loss: 0.019114447757601738, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7109/20000], Bound: 0.38553112745285034, Entropy: 142.0109100341797, Temp: 2.578615665435791, KL: 72.66188049316406, Loss: 0.017640424892306328, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7110/20000], Bound: 0.38227418065071106, Entropy: 142.979248046875, Temp: 2.5785763263702393, KL: 70.1572265625, Loss: 0.02068762108683586, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7111/20000], Bound: 0.4144740700721741, Entropy: 139.8903350830078, Temp: 2.5784778594970703, KL: 80.60743713378906, Loss: 0.018689775839447975, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7112/20000], Bound: 0.3829023241996765, Entropy: 141.30540466308594, Temp: 2.578437089920044, KL: 72.45291137695312, Loss: 0.016582898795604706, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7113/20000], Bound: 0.39145225286483765, Entropy: 142.53079223632812, Temp: 2.5784547328948975, KL: 72.34634399414062, Loss: 0.02156122215092182, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7114/20000], Bound: 0.366599977016449, Entropy: 144.29652404785156, Temp: 2.578399181365967, KL: 66.61189270019531, Loss: 0.018970681354403496, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7115/20000], Bound: 0.40135738253593445, Entropy: 142.88514709472656, Temp: 2.57830810546875, KL: 76.92204284667969, Loss: 0.018288828432559967, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7116/20000], Bound: 0.4001533091068268, Entropy: 142.14505004882812, Temp: 2.578263282775879, KL: 76.92166137695312, Loss: 0.017603633925318718, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7117/20000], Bound: 0.40773701667785645, Entropy: 140.06640625, Temp: 2.578277587890625, KL: 78.69659423828125, Loss: 0.018498850986361504, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7118/20000], Bound: 0.40056103467941284, Entropy: 142.86798095703125, Temp: 2.5783321857452393, KL: 75.14773559570312, Loss: 0.021276457235217094, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7119/20000], Bound: 0.40046319365501404, Entropy: 142.47979736328125, Temp: 2.578333616256714, KL: 77.25041198730469, Loss: 0.01714324951171875, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7120/20000], Bound: 0.39349016547203064, Entropy: 141.79788208007812, Temp: 2.5784027576446533, KL: 74.64369201660156, Loss: 0.018251797184348106, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7121/20000], Bound: 0.4062840938568115, Entropy: 143.05059814453125, Temp: 2.5784900188446045, KL: 78.20339965820312, Loss: 0.01862313412129879, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7122/20000], Bound: 0.3979557454586029, Entropy: 140.8368377685547, Temp: 2.5786046981811523, KL: 73.54905700683594, Loss: 0.02289959415793419, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7123/20000], Bound: 0.3898588716983795, Entropy: 141.8800048828125, Temp: 2.5786092281341553, KL: 73.47943115234375, Loss: 0.018471980467438698, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7124/20000], Bound: 0.3609575629234314, Entropy: 141.97903442382812, Temp: 2.5786259174346924, KL: 66.67953491210938, Loss: 0.015795394778251648, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7125/20000], Bound: 0.3733969032764435, Entropy: 142.1120147705078, Temp: 2.5786821842193604, KL: 69.8973388671875, Loss: 0.01630433276295662, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7126/20000], Bound: 0.38150009512901306, Entropy: 142.10116577148438, Temp: 2.578779458999634, KL: 71.81826782226562, Loss: 0.01704024337232113, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7127/20000], Bound: 0.4168153703212738, Entropy: 141.96824645996094, Temp: 2.578906536102295, KL: 82.37626647949219, Loss: 0.016627948731184006, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7128/20000], Bound: 0.4019288122653961, Entropy: 140.87220764160156, Temp: 2.579130172729492, KL: 74.80731201171875, Loss: 0.022723736241459846, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7129/20000], Bound: 0.38922423124313354, Entropy: 140.8417510986328, Temp: 2.5792441368103027, KL: 72.26054382324219, Loss: 0.020486382767558098, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7130/20000], Bound: 0.3897561728954315, Entropy: 142.0269775390625, Temp: 2.579301118850708, KL: 72.92434692382812, Loss: 0.019497951492667198, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7131/20000], Bound: 0.37744155526161194, Entropy: 141.7244110107422, Temp: 2.5793352127075195, KL: 67.83711242675781, Loss: 0.022523999214172363, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7132/20000], Bound: 0.3997081220149994, Entropy: 142.554443359375, Temp: 2.5792412757873535, KL: 74.97711181640625, Loss: 0.02113180421292782, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7133/20000], Bound: 0.3657914996147156, Entropy: 141.3913116455078, Temp: 2.5791103839874268, KL: 65.39048767089844, Loss: 0.020907087251544, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7134/20000], Bound: 0.39267706871032715, Entropy: 142.1354217529297, Temp: 2.5788938999176025, KL: 73.81886291503906, Loss: 0.019398832693696022, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7135/20000], Bound: 0.39847809076309204, Entropy: 140.44476318359375, Temp: 2.5786900520324707, KL: 75.97648620605469, Loss: 0.018489904701709747, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7136/20000], Bound: 0.403372198343277, Entropy: 138.4547119140625, Temp: 2.578533172607422, KL: 77.8272705078125, Loss: 0.01768566481769085, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7137/20000], Bound: 0.402190238237381, Entropy: 141.4289093017578, Temp: 2.5784497261047363, KL: 75.37553405761719, Loss: 0.02176414616405964, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7138/20000], Bound: 0.35983550548553467, Entropy: 140.1672821044922, Temp: 2.578315019607544, KL: 65.92939758300781, Loss: 0.016644101589918137, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7139/20000], Bound: 0.3765200078487396, Entropy: 140.62428283691406, Temp: 2.5782089233398438, KL: 69.67066955566406, Loss: 0.018451839685440063, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7140/20000], Bound: 0.4170612394809723, Entropy: 141.84877014160156, Temp: 2.578104019165039, KL: 80.65298461914062, Loss: 0.020102644339203835, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7141/20000], Bound: 0.3805566132068634, Entropy: 140.84739685058594, Temp: 2.5780229568481445, KL: 70.28486633300781, Loss: 0.019484128803014755, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7142/20000], Bound: 0.4353315830230713, Entropy: 140.10960388183594, Temp: 2.57791805267334, KL: 87.29304504394531, Loss: 0.018026482313871384, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7143/20000], Bound: 0.40259671211242676, Entropy: 140.1099395751953, Temp: 2.5779266357421875, KL: 77.49525451660156, Loss: 0.017879368737339973, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7144/20000], Bound: 0.405550479888916, Entropy: 140.54039001464844, Temp: 2.5779852867126465, KL: 76.51614379882812, Loss: 0.02146875485777855, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7145/20000], Bound: 0.36670631170272827, Entropy: 140.17019653320312, Temp: 2.577993154525757, KL: 67.53631591796875, Loss: 0.017231792211532593, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7146/20000], Bound: 0.395192950963974, Entropy: 142.25733947753906, Temp: 2.578010082244873, KL: 75.11026000976562, Loss: 0.01830277219414711, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7147/20000], Bound: 0.3847515285015106, Entropy: 141.56044006347656, Temp: 2.578052043914795, KL: 72.10845947265625, Loss: 0.018273815512657166, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7148/20000], Bound: 0.35697877407073975, Entropy: 140.81507873535156, Temp: 2.5780997276306152, KL: 62.661224365234375, Loss: 0.021449368447065353, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7149/20000], Bound: 0.3967699706554413, Entropy: 139.43438720703125, Temp: 2.5780131816864014, KL: 75.24771118164062, Loss: 0.018927739933133125, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7150/20000], Bound: 0.35503339767456055, Entropy: 140.46685791015625, Temp: 2.5779471397399902, KL: 64.2239990234375, Loss: 0.017378002405166626, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7151/20000], Bound: 0.396341472864151, Entropy: 138.33729553222656, Temp: 2.577873468399048, KL: 74.22209167480469, Loss: 0.02067304030060768, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7152/20000], Bound: 0.387583464384079, Entropy: 142.24032592773438, Temp: 2.577768325805664, KL: 72.79328918457031, Loss: 0.01852090284228325, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7153/20000], Bound: 0.42771416902542114, Entropy: 141.49913024902344, Temp: 2.57768177986145, KL: 84.38676452636719, Loss: 0.019118696451187134, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7154/20000], Bound: 0.3969932794570923, Entropy: 140.86911010742188, Temp: 2.5776634216308594, KL: 76.52656555175781, Loss: 0.01656961441040039, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7155/20000], Bound: 0.41708531975746155, Entropy: 141.02862548828125, Temp: 2.5777268409729004, KL: 79.90521240234375, Loss: 0.021562552079558372, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7156/20000], Bound: 0.3867635130882263, Entropy: 143.6724395751953, Temp: 2.577755928039551, KL: 71.52879333496094, Loss: 0.02051583305001259, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7157/20000], Bound: 0.3979153335094452, Entropy: 140.66595458984375, Temp: 2.5777316093444824, KL: 75.55392456054688, Loss: 0.018979469314217567, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7158/20000], Bound: 0.40988579392433167, Entropy: 139.4277801513672, Temp: 2.5777220726013184, KL: 78.39254760742188, Loss: 0.02031981386244297, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7159/20000], Bound: 0.39085420966148376, Entropy: 138.57647705078125, Temp: 2.5777087211608887, KL: 73.40937805175781, Loss: 0.0191563218832016, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7160/20000], Bound: 0.39147627353668213, Entropy: 141.1498565673828, Temp: 2.5776917934417725, KL: 72.2808837890625, Loss: 0.021694302558898926, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7161/20000], Bound: 0.41492921113967896, Entropy: 139.11204528808594, Temp: 2.5776002407073975, KL: 80.16665649414062, Loss: 0.019798288121819496, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7162/20000], Bound: 0.3805564045906067, Entropy: 140.58262634277344, Temp: 2.5775363445281982, KL: 70.27369689941406, Loss: 0.019500935450196266, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7163/20000], Bound: 0.3982929587364197, Entropy: 140.88558959960938, Temp: 2.577446699142456, KL: 74.4779052734375, Loss: 0.021277811378240585, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7164/20000], Bound: 0.4055728018283844, Entropy: 138.7325897216797, Temp: 2.5773134231567383, KL: 76.66647338867188, Loss: 0.02118266560137272, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7165/20000], Bound: 0.3766258955001831, Entropy: 140.87319946289062, Temp: 2.5771565437316895, KL: 68.54177856445312, Loss: 0.020689906552433968, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7166/20000], Bound: 0.3877905607223511, Entropy: 141.37417602539062, Temp: 2.576941967010498, KL: 73.1024169921875, Loss: 0.01802804134786129, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7167/20000], Bound: 0.39555618166923523, Entropy: 140.74923706054688, Temp: 2.5767714977264404, KL: 74.64735412597656, Loss: 0.019392328336834908, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7168/20000], Bound: 0.4193112552165985, Entropy: 137.35748291015625, Temp: 2.576615810394287, KL: 81.27317810058594, Loss: 0.020195918157696724, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7169/20000], Bound: 0.44171521067619324, Entropy: 139.3892059326172, Temp: 2.576490879058838, KL: 89.03715515136719, Loss: 0.018469786271452904, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7170/20000], Bound: 0.37625131011009216, Entropy: 139.4755859375, Temp: 2.576481580734253, KL: 69.23927307128906, Loss: 0.01912427693605423, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7171/20000], Bound: 0.3881887197494507, Entropy: 141.45518493652344, Temp: 2.576444625854492, KL: 73.13662719726562, Loss: 0.018178820610046387, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7172/20000], Bound: 0.396718829870224, Entropy: 139.99559020996094, Temp: 2.5764312744140625, KL: 73.80815124511719, Loss: 0.02167491428554058, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7173/20000], Bound: 0.39788302779197693, Entropy: 140.2615203857422, Temp: 2.576352834701538, KL: 75.85702514648438, Loss: 0.01835748180747032, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7174/20000], Bound: 0.3874727785587311, Entropy: 139.37850952148438, Temp: 2.5763134956359863, KL: 73.57347106933594, Loss: 0.01692948117852211, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7175/20000], Bound: 0.39683347940444946, Entropy: 140.162353515625, Temp: 2.5763325691223145, KL: 76.15446472167969, Loss: 0.017185235396027565, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7176/20000], Bound: 0.38218244910240173, Entropy: 140.06398010253906, Temp: 2.576413154602051, KL: 69.8216552734375, Loss: 0.021267661824822426, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7177/20000], Bound: 0.3906274437904358, Entropy: 138.54238891601562, Temp: 2.5764050483703613, KL: 73.74221801757812, Loss: 0.018369274213910103, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7178/20000], Bound: 0.40237513184547424, Entropy: 139.5228729248047, Temp: 2.576416254043579, KL: 77.18997192382812, Loss: 0.01832723617553711, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7179/20000], Bound: 0.41149863600730896, Entropy: 140.83090209960938, Temp: 2.5764663219451904, KL: 79.7384033203125, Loss: 0.018625551834702492, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7180/20000], Bound: 0.3939138650894165, Entropy: 140.81106567382812, Temp: 2.5765583515167236, KL: 73.63096618652344, Loss: 0.020435333251953125, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7181/20000], Bound: 0.37136751413345337, Entropy: 140.7645721435547, Temp: 2.5766055583953857, KL: 68.66604614257812, Loss: 0.017562467604875565, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7182/20000], Bound: 0.39308255910873413, Entropy: 139.37600708007812, Temp: 2.5766565799713135, KL: 72.74368286132812, Loss: 0.021689720451831818, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7183/20000], Bound: 0.40690675377845764, Entropy: 139.14340209960938, Temp: 2.576629161834717, KL: 78.70509338378906, Loss: 0.017985016107559204, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7184/20000], Bound: 0.3845696747303009, Entropy: 139.6266326904297, Temp: 2.5766618251800537, KL: 70.837890625, Loss: 0.020623555406928062, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7185/20000], Bound: 0.39838385581970215, Entropy: 139.38673400878906, Temp: 2.5766334533691406, KL: 76.87973022460938, Loss: 0.01666020043194294, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7186/20000], Bound: 0.40767720341682434, Entropy: 139.69532775878906, Temp: 2.5766890048980713, KL: 79.47517395019531, Loss: 0.016934387385845184, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7187/20000], Bound: 0.3847726285457611, Entropy: 139.97889709472656, Temp: 2.5768277645111084, KL: 70.92570495605469, Loss: 0.020567692816257477, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7188/20000], Bound: 0.37928298115730286, Entropy: 139.01376342773438, Temp: 2.576896905899048, KL: 69.80435180664062, Loss: 0.019701775163412094, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7189/20000], Bound: 0.38324466347694397, Entropy: 142.33387756347656, Temp: 2.5769190788269043, KL: 69.43247985839844, Loss: 0.022616667672991753, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7190/20000], Bound: 0.3919502794742584, Entropy: 141.05455017089844, Temp: 2.5768203735351562, KL: 73.71519470214844, Loss: 0.019169149920344353, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7191/20000], Bound: 0.393991619348526, Entropy: 141.45704650878906, Temp: 2.5767290592193604, KL: 74.9029541015625, Loss: 0.018012691289186478, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7192/20000], Bound: 0.3668401539325714, Entropy: 140.63316345214844, Temp: 2.576680898666382, KL: 66.15321350097656, Loss: 0.01997561939060688, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7193/20000], Bound: 0.39180463552474976, Entropy: 140.85719299316406, Temp: 2.5765676498413086, KL: 74.80058288574219, Loss: 0.0169783066958189, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7194/20000], Bound: 0.41748934984207153, Entropy: 140.6112823486328, Temp: 2.576526403427124, KL: 82.43547058105469, Loss: 0.01687416061758995, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7195/20000], Bound: 0.3701692521572113, Entropy: 141.4367218017578, Temp: 2.576596975326538, KL: 69.74830627441406, Loss: 0.014808575622737408, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7196/20000], Bound: 0.4008706212043762, Entropy: 139.95977783203125, Temp: 2.5767483711242676, KL: 78.45292663574219, Loss: 0.015022631734609604, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7197/20000], Bound: 0.40694093704223633, Entropy: 141.84092712402344, Temp: 2.577017068862915, KL: 76.76747131347656, Loss: 0.021768828853964806, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7198/20000], Bound: 0.3933362662792206, Entropy: 138.5054168701172, Temp: 2.577207565307617, KL: 73.90341186523438, Loss: 0.01958797685801983, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7199/20000], Bound: 0.3820047676563263, Entropy: 142.74636840820312, Temp: 2.577366590499878, KL: 72.23526000976562, Loss: 0.016495659947395325, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7200/20000], Bound: 0.4277249574661255, Entropy: 138.32733154296875, Temp: 2.5775673389434814, KL: 86.03950500488281, Loss: 0.015917552635073662, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7201/20000], Bound: 0.39063090085983276, Entropy: 140.20726013183594, Temp: 2.577899932861328, KL: 73.900390625, Loss: 0.018080662935972214, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7202/20000], Bound: 0.40818583965301514, Entropy: 141.4674530029297, Temp: 2.5782251358032227, KL: 79.74598693847656, Loss: 0.016721319407224655, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7203/20000], Bound: 0.37678301334381104, Entropy: 141.02882385253906, Temp: 2.578613519668579, KL: 71.42543029785156, Loss: 0.01519788708537817, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7204/20000], Bound: 0.4126948416233063, Entropy: 139.6497802734375, Temp: 2.5790493488311768, KL: 78.87840270996094, Loss: 0.021016959100961685, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7205/20000], Bound: 0.3970842659473419, Entropy: 140.21502685546875, Temp: 2.579420804977417, KL: 76.48977661132812, Loss: 0.016713542863726616, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7206/20000], Bound: 0.3937091529369354, Entropy: 140.27883911132812, Temp: 2.57983136177063, KL: 74.7261962890625, Loss: 0.018231067806482315, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7207/20000], Bound: 0.39260411262512207, Entropy: 141.84754943847656, Temp: 2.5802268981933594, KL: 73.30287170410156, Loss: 0.02037186175584793, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7208/20000], Bound: 0.39024949073791504, Entropy: 141.392333984375, Temp: 2.5805444717407227, KL: 75.59950256347656, Loss: 0.014603760093450546, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7209/20000], Bound: 0.3813360631465912, Entropy: 142.3306121826172, Temp: 2.580955743789673, KL: 70.15487670898438, Loss: 0.020195210352540016, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7210/20000], Bound: 0.38698890805244446, Entropy: 139.88218688964844, Temp: 2.5812723636627197, KL: 71.30155944824219, Loss: 0.021115824580192566, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7211/20000], Bound: 0.39525502920150757, Entropy: 139.5926971435547, Temp: 2.581486701965332, KL: 74.54287719726562, Loss: 0.019475672394037247, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7212/20000], Bound: 0.3882712721824646, Entropy: 140.34812927246094, Temp: 2.5816712379455566, KL: 73.59226989746094, Loss: 0.01739864982664585, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7213/20000], Bound: 0.3860965371131897, Entropy: 140.3457794189453, Temp: 2.5818769931793213, KL: 73.63633728027344, Loss: 0.016102978959679604, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7214/20000], Bound: 0.407924622297287, Entropy: 139.95401000976562, Temp: 2.582136392593384, KL: 78.43537902832031, Loss: 0.01915888302028179, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7215/20000], Bound: 0.39005911350250244, Entropy: 140.41604614257812, Temp: 2.5823922157287598, KL: 72.887451171875, Loss: 0.019770827144384384, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7216/20000], Bound: 0.40238121151924133, Entropy: 139.52810668945312, Temp: 2.5825958251953125, KL: 77.852783203125, Loss: 0.017119841650128365, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7217/20000], Bound: 0.3953942358493805, Entropy: 141.61447143554688, Temp: 2.582850933074951, KL: 76.12631225585938, Loss: 0.016503537073731422, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7218/20000], Bound: 0.37205642461776733, Entropy: 140.3815460205078, Temp: 2.583158254623413, KL: 68.30108642578125, Loss: 0.018708670511841774, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7219/20000], Bound: 0.3927997052669525, Entropy: 139.73399353027344, Temp: 2.5834076404571533, KL: 74.99444580078125, Loss: 0.017240090295672417, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7220/20000], Bound: 0.38498756289482117, Entropy: 140.65440368652344, Temp: 2.583683490753174, KL: 72.76863098144531, Loss: 0.017186053097248077, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7221/20000], Bound: 0.3804943859577179, Entropy: 139.38316345214844, Temp: 2.5839715003967285, KL: 70.83497619628906, Loss: 0.018442735075950623, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7222/20000], Bound: 0.4039047658443451, Entropy: 138.9654998779297, Temp: 2.5842254161834717, KL: 76.55810546875, Loss: 0.02051345631480217, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7223/20000], Bound: 0.39354515075683594, Entropy: 140.3834991455078, Temp: 2.5844287872314453, KL: 74.89491271972656, Loss: 0.017863241955637932, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7224/20000], Bound: 0.38117095828056335, Entropy: 141.32305908203125, Temp: 2.584645986557007, KL: 71.47050476074219, Loss: 0.01759365014731884, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7225/20000], Bound: 0.4122365117073059, Entropy: 140.70074462890625, Temp: 2.584861993789673, KL: 81.89492797851562, Loss: 0.014983211643993855, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7226/20000], Bound: 0.3891877233982086, Entropy: 140.1776885986328, Temp: 2.585205554962158, KL: 73.66282653808594, Loss: 0.01781262643635273, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7227/20000], Bound: 0.4033661186695099, Entropy: 140.8862762451172, Temp: 2.585542678833008, KL: 76.63545227050781, Loss: 0.02007085271179676, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7228/20000], Bound: 0.3910002112388611, Entropy: 140.55271911621094, Temp: 2.585832118988037, KL: 73.8575439453125, Loss: 0.018456710502505302, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7229/20000], Bound: 0.39297249913215637, Entropy: 140.87611389160156, Temp: 2.586104393005371, KL: 73.60220336914062, Loss: 0.020059427246451378, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7230/20000], Bound: 0.3751334846019745, Entropy: 140.46975708007812, Temp: 2.5863168239593506, KL: 70.74449157714844, Loss: 0.015695158392190933, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7231/20000], Bound: 0.37433338165283203, Entropy: 141.05331420898438, Temp: 2.586573600769043, KL: 69.72636413574219, Loss: 0.0172282662242651, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7232/20000], Bound: 0.3932472765445709, Entropy: 140.6966552734375, Temp: 2.5868234634399414, KL: 73.2083740234375, Loss: 0.020982326939702034, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7233/20000], Bound: 0.38214701414108276, Entropy: 141.76405334472656, Temp: 2.586988687515259, KL: 71.1993408203125, Loss: 0.018681172281503677, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7234/20000], Bound: 0.3902912735939026, Entropy: 141.6988067626953, Temp: 2.5871264934539795, KL: 74.05259704589844, Loss: 0.01769682765007019, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7235/20000], Bound: 0.38349857926368713, Entropy: 140.37171936035156, Temp: 2.5872819423675537, KL: 70.788818359375, Loss: 0.020225264132022858, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7236/20000], Bound: 0.39869093894958496, Entropy: 139.23611450195312, Temp: 2.587367296218872, KL: 77.26522827148438, Loss: 0.016217775642871857, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7237/20000], Bound: 0.38007742166519165, Entropy: 141.8385772705078, Temp: 2.587533950805664, KL: 70.57649230957031, Loss: 0.01874786987900734, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7238/20000], Bound: 0.3870902955532074, Entropy: 143.1760711669922, Temp: 2.5876665115356445, KL: 71.76576232910156, Loss: 0.020335447043180466, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7239/20000], Bound: 0.41700229048728943, Entropy: 141.08555603027344, Temp: 2.5877339839935303, KL: 80.37322998046875, Loss: 0.020724909380078316, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7240/20000], Bound: 0.41808584332466125, Entropy: 140.89393615722656, Temp: 2.5877845287323, KL: 80.82313537597656, Loss: 0.020487051457166672, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7241/20000], Bound: 0.4111613631248474, Entropy: 141.4799346923828, Temp: 2.5878283977508545, KL: 80.26838684082031, Loss: 0.017545750364661217, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7242/20000], Bound: 0.3816767632961273, Entropy: 141.34280395507812, Temp: 2.587939977645874, KL: 70.75210571289062, Loss: 0.019294898957014084, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7243/20000], Bound: 0.3941583037376404, Entropy: 140.10812377929688, Temp: 2.5880095958709717, KL: 75.44743347167969, Loss: 0.01718013547360897, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7244/20000], Bound: 0.39645662903785706, Entropy: 139.88104248046875, Temp: 2.588125705718994, KL: 75.01860046386719, Loss: 0.019304530695080757, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7245/20000], Bound: 0.39771389961242676, Entropy: 140.49134826660156, Temp: 2.5882248878479004, KL: 73.1341552734375, Loss: 0.02365599013864994, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7246/20000], Bound: 0.36487579345703125, Entropy: 140.11636352539062, Temp: 2.5881826877593994, KL: 67.71182250976562, Loss: 0.016001371666789055, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7247/20000], Bound: 0.38204577565193176, Entropy: 140.70640563964844, Temp: 2.5881829261779785, KL: 71.34835815429688, Loss: 0.018349232152104378, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7248/20000], Bound: 0.3894159197807312, Entropy: 142.1067657470703, Temp: 2.5881810188293457, KL: 73.62052917480469, Loss: 0.018053900450468063, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7249/20000], Bound: 0.37459850311279297, Entropy: 141.74691772460938, Temp: 2.58819842338562, KL: 68.61015319824219, Loss: 0.019545774906873703, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7250/20000], Bound: 0.4200139045715332, Entropy: 141.55349731445312, Temp: 2.588162899017334, KL: 81.10661315917969, Loss: 0.021068882197141647, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7251/20000], Bound: 0.37200233340263367, Entropy: 141.96334838867188, Temp: 2.588115692138672, KL: 68.12823486328125, Loss: 0.019059138372540474, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7252/20000], Bound: 0.37446272373199463, Entropy: 142.7379913330078, Temp: 2.5880322456359863, KL: 67.65797424316406, Loss: 0.021309614181518555, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7253/20000], Bound: 0.37440571188926697, Entropy: 141.50999450683594, Temp: 2.587852716445923, KL: 67.7109375, Loss: 0.021174568682909012, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7254/20000], Bound: 0.41559338569641113, Entropy: 140.9746856689453, Temp: 2.5875909328460693, KL: 81.84800720214844, Loss: 0.01705477200448513, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7255/20000], Bound: 0.3789624273777008, Entropy: 142.25448608398438, Temp: 2.5874500274658203, KL: 70.26678466796875, Loss: 0.018731487914919853, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7256/20000], Bound: 0.4015711545944214, Entropy: 142.98318481445312, Temp: 2.5873043537139893, KL: 76.16983032226562, Loss: 0.0199680607765913, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7257/20000], Bound: 0.3973971903324127, Entropy: 142.0033416748047, Temp: 2.587157726287842, KL: 75.98225402832031, Loss: 0.017962953075766563, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7258/20000], Bound: 0.3835417926311493, Entropy: 144.23841857910156, Temp: 2.587062120437622, KL: 72.24685668945312, Loss: 0.017429206520318985, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7259/20000], Bound: 0.373678058385849, Entropy: 140.82728576660156, Temp: 2.5870046615600586, KL: 69.51786804199219, Loss: 0.017277436330914497, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7260/20000], Bound: 0.40437138080596924, Entropy: 143.05392456054688, Temp: 2.5869686603546143, KL: 77.174072265625, Loss: 0.019619040191173553, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7261/20000], Bound: 0.39739540219306946, Entropy: 141.8401336669922, Temp: 2.5869362354278564, KL: 76.07699584960938, Loss: 0.017776308581233025, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7262/20000], Bound: 0.4100756347179413, Entropy: 143.36160278320312, Temp: 2.586949586868286, KL: 78.10995483398438, Loss: 0.021080829203128815, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7263/20000], Bound: 0.351375013589859, Entropy: 143.1471710205078, Temp: 2.586928606033325, KL: 62.909576416015625, Loss: 0.01805475912988186, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7264/20000], Bound: 0.4096016585826874, Entropy: 141.50164794921875, Temp: 2.5868630409240723, KL: 78.85397338867188, Loss: 0.019368942826986313, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7265/20000], Bound: 0.3656865656375885, Entropy: 142.04173278808594, Temp: 2.5868210792541504, KL: 65.22279357910156, Loss: 0.021236641332507133, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7266/20000], Bound: 0.38545987010002136, Entropy: 141.0233612060547, Temp: 2.586665391921997, KL: 72.21292114257812, Loss: 0.01855463907122612, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7267/20000], Bound: 0.4070492684841156, Entropy: 141.38482666015625, Temp: 2.5865235328674316, KL: 78.72579956054688, Loss: 0.018146635964512825, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7268/20000], Bound: 0.4091980457305908, Entropy: 140.32815551757812, Temp: 2.586444616317749, KL: 79.53590393066406, Loss: 0.017813561484217644, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7269/20000], Bound: 0.40928491950035095, Entropy: 140.7751007080078, Temp: 2.5864357948303223, KL: 75.64004516601562, Loss: 0.025394724681973457, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7270/20000], Bound: 0.38109710812568665, Entropy: 143.05723571777344, Temp: 2.58626651763916, KL: 72.48846435546875, Loss: 0.015601597726345062, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7271/20000], Bound: 0.36474376916885376, Entropy: 141.45652770996094, Temp: 2.586193799972534, KL: 65.67448425292969, Loss: 0.019849427044391632, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7272/20000], Bound: 0.37639278173446655, Entropy: 140.897705078125, Temp: 2.5860509872436523, KL: 70.25103759765625, Loss: 0.017336443066596985, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7273/20000], Bound: 0.38969776034355164, Entropy: 141.71836853027344, Temp: 2.585942029953003, KL: 74.71873474121094, Loss: 0.01606396958231926, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7274/20000], Bound: 0.38509032130241394, Entropy: 140.38209533691406, Temp: 2.5859246253967285, KL: 72.34175109863281, Loss: 0.018092704936861992, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7275/20000], Bound: 0.3904629349708557, Entropy: 140.2165069580078, Temp: 2.585920810699463, KL: 74.99693298339844, Loss: 0.015953797847032547, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7276/20000], Bound: 0.39860063791275024, Entropy: 142.11996459960938, Temp: 2.586002826690674, KL: 75.0755615234375, Loss: 0.020383838564157486, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7277/20000], Bound: 0.39732125401496887, Entropy: 140.59983825683594, Temp: 2.5860443115234375, KL: 75.94992065429688, Loss: 0.017969880253076553, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7278/20000], Bound: 0.3988742530345917, Entropy: 141.35336303710938, Temp: 2.586118698120117, KL: 76.3553466796875, Loss: 0.01806563511490822, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7279/20000], Bound: 0.41449853777885437, Entropy: 140.07582092285156, Temp: 2.5862226486206055, KL: 79.37754821777344, Loss: 0.021177804097533226, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7280/20000], Bound: 0.39226317405700684, Entropy: 140.6931610107422, Temp: 2.5862886905670166, KL: 73.95159912109375, Loss: 0.018987711519002914, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7281/20000], Bound: 0.36995166540145874, Entropy: 141.71438598632812, Temp: 2.58634614944458, KL: 66.33969116210938, Loss: 0.021384643390774727, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7282/20000], Bound: 0.4034736156463623, Entropy: 140.98831176757812, Temp: 2.586282968521118, KL: 77.0914306640625, Loss: 0.019258754327893257, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7283/20000], Bound: 0.3988438844680786, Entropy: 140.5166015625, Temp: 2.586235523223877, KL: 74.76101684570312, Loss: 0.02113216742873192, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7284/20000], Bound: 0.4063577651977539, Entropy: 139.3577880859375, Temp: 2.5861384868621826, KL: 78.21626281738281, Loss: 0.018730856478214264, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7285/20000], Bound: 0.3788018524646759, Entropy: 141.57974243164062, Temp: 2.5860815048217773, KL: 69.81329345703125, Loss: 0.019506581127643585, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7286/20000], Bound: 0.39922186732292175, Entropy: 141.02915954589844, Temp: 2.5859882831573486, KL: 76.55097961425781, Loss: 0.017882894724607468, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7287/20000], Bound: 0.43466731905937195, Entropy: 138.0520477294922, Temp: 2.5859477519989014, KL: 86.80146789550781, Loss: 0.018694719299674034, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7288/20000], Bound: 0.4166400730609894, Entropy: 139.64048767089844, Temp: 2.5859932899475098, KL: 81.42549133300781, Loss: 0.018459204584360123, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7289/20000], Bound: 0.3945082128047943, Entropy: 138.86077880859375, Temp: 2.5860910415649414, KL: 73.97561645507812, Loss: 0.02020084299147129, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7290/20000], Bound: 0.39637115597724915, Entropy: 138.65309143066406, Temp: 2.5861446857452393, KL: 75.36476135253906, Loss: 0.018565725535154343, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7291/20000], Bound: 0.3788397014141083, Entropy: 138.61279296875, Temp: 2.5862104892730713, KL: 71.28634643554688, Loss: 0.016680719330906868, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7292/20000], Bound: 0.39282116293907166, Entropy: 140.81326293945312, Temp: 2.586313486099243, KL: 73.78755187988281, Loss: 0.019618269056081772, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7293/20000], Bound: 0.4021338224411011, Entropy: 140.2321014404297, Temp: 2.58638596534729, KL: 77.25199890136719, Loss: 0.018186179921030998, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7294/20000], Bound: 0.36783766746520996, Entropy: 139.069091796875, Temp: 2.5864903926849365, KL: 66.97512817382812, Loss: 0.019010335206985474, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7295/20000], Bound: 0.40840965509414673, Entropy: 139.69122314453125, Temp: 2.5865378379821777, KL: 76.209716796875, Loss: 0.023791315034031868, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7296/20000], Bound: 0.3808576464653015, Entropy: 137.79901123046875, Temp: 2.586463451385498, KL: 69.83052062988281, Loss: 0.020609837025403976, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7297/20000], Bound: 0.376278281211853, Entropy: 141.40219116210938, Temp: 2.586324453353882, KL: 68.42547607421875, Loss: 0.020805668085813522, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7298/20000], Bound: 0.40207335352897644, Entropy: 139.8748779296875, Temp: 2.586113214492798, KL: 75.90054321289062, Loss: 0.020761489868164062, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7299/20000], Bound: 0.4276714622974396, Entropy: 138.74192810058594, Temp: 2.5858852863311768, KL: 84.62379455566406, Loss: 0.0187442135065794, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7300/20000], Bound: 0.3763727843761444, Entropy: 139.24478149414062, Temp: 2.5857486724853516, KL: 69.74513244628906, Loss: 0.018300676718354225, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7301/20000], Bound: 0.39025968313217163, Entropy: 139.12185668945312, Temp: 2.585615873336792, KL: 73.63348388671875, Loss: 0.018473125994205475, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7302/20000], Bound: 0.3999592065811157, Entropy: 138.6802215576172, Temp: 2.5855064392089844, KL: 77.351318359375, Loss: 0.016747809946537018, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7303/20000], Bound: 0.3913983106613159, Entropy: 137.32479858398438, Temp: 2.585486888885498, KL: 74.32394409179688, Loss: 0.017774095758795738, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7304/20000], Bound: 0.37530192732810974, Entropy: 140.42538452148438, Temp: 2.5855026245117188, KL: 69.45620727539062, Loss: 0.018270084634423256, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7305/20000], Bound: 0.3830840289592743, Entropy: 138.40911865234375, Temp: 2.5855064392089844, KL: 69.56733703613281, Loss: 0.02234097383916378, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7306/20000], Bound: 0.4109114110469818, Entropy: 138.48196411132812, Temp: 2.5853893756866455, KL: 78.30107116699219, Loss: 0.021175656467676163, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7307/20000], Bound: 0.36105650663375854, Entropy: 139.30796813964844, Temp: 2.5852506160736084, KL: 65.87550354003906, Loss: 0.017468063160777092, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7308/20000], Bound: 0.36785754561424255, Entropy: 138.9093475341797, Temp: 2.5851144790649414, KL: 67.52767944335938, Loss: 0.01794014871120453, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7309/20000], Bound: 0.376966655254364, Entropy: 140.97215270996094, Temp: 2.5849785804748535, KL: 71.55424499511719, Loss: 0.015119817107915878, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7310/20000], Bound: 0.3800019323825836, Entropy: 139.72860717773438, Temp: 2.5849452018737793, KL: 69.67982482910156, Loss: 0.02041522040963173, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7311/20000], Bound: 0.4203607141971588, Entropy: 142.93324279785156, Temp: 2.584847927093506, KL: 81.59005737304688, Loss: 0.020297259092330933, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7312/20000], Bound: 0.39333420991897583, Entropy: 141.35671997070312, Temp: 2.584770441055298, KL: 73.63346862792969, Loss: 0.020188581198453903, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7313/20000], Bound: 0.403004914522171, Entropy: 139.4476776123047, Temp: 2.584664821624756, KL: 76.92048645019531, Loss: 0.01930379495024681, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7314/20000], Bound: 0.40211939811706543, Entropy: 141.89817810058594, Temp: 2.584578275680542, KL: 77.48312377929688, Loss: 0.017709828913211823, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7315/20000], Bound: 0.3921557366847992, Entropy: 142.02882385253906, Temp: 2.584555149078369, KL: 74.20640563964844, Loss: 0.01841604895889759, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7316/20000], Bound: 0.39596399664878845, Entropy: 141.64849853515625, Temp: 2.584550380706787, KL: 76.46368408203125, Loss: 0.016192302107810974, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7317/20000], Bound: 0.38814178109169006, Entropy: 141.45753479003906, Temp: 2.5846357345581055, KL: 72.09004211425781, Loss: 0.02026495896279812, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7318/20000], Bound: 0.41469356417655945, Entropy: 140.91265869140625, Temp: 2.584665060043335, KL: 78.37342834472656, Loss: 0.02321568876504898, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7319/20000], Bound: 0.4053860604763031, Entropy: 138.69615173339844, Temp: 2.584604024887085, KL: 78.06341552734375, Loss: 0.018452150747179985, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7320/20000], Bound: 0.40208369493484497, Entropy: 141.50901794433594, Temp: 2.584587574005127, KL: 76.71206665039062, Loss: 0.01918123848736286, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7321/20000], Bound: 0.3813718259334564, Entropy: 140.69102478027344, Temp: 2.584583282470703, KL: 71.84999084472656, Loss: 0.016969827935099602, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7322/20000], Bound: 0.3892974555492401, Entropy: 141.05101013183594, Temp: 2.5846199989318848, KL: 73.30691528320312, Loss: 0.018556104972958565, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7323/20000], Bound: 0.4084601402282715, Entropy: 140.86460876464844, Temp: 2.5846593379974365, KL: 78.65367126464844, Loss: 0.019073855131864548, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7324/20000], Bound: 0.35974225401878357, Entropy: 141.59645080566406, Temp: 2.5847201347351074, KL: 65.55076599121094, Loss: 0.017386361956596375, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7325/20000], Bound: 0.3890542685985565, Entropy: 141.31483459472656, Temp: 2.584764242172241, KL: 74.53327941894531, Loss: 0.016049468889832497, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7326/20000], Bound: 0.4171728491783142, Entropy: 140.43267822265625, Temp: 2.5848865509033203, KL: 82.38150024414062, Loss: 0.016906123608350754, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7327/20000], Bound: 0.391112744808197, Entropy: 141.50440979003906, Temp: 2.5851023197174072, KL: 74.46882629394531, Loss: 0.01732965186238289, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7328/20000], Bound: 0.4158659875392914, Entropy: 141.67591857910156, Temp: 2.585343360900879, KL: 80.40634155273438, Loss: 0.019971944391727448, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7329/20000], Bound: 0.383698046207428, Entropy: 140.1273651123047, Temp: 2.585571765899658, KL: 72.17268371582031, Loss: 0.01764347217977047, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7330/20000], Bound: 0.4112928807735443, Entropy: 141.75279235839844, Temp: 2.585801362991333, KL: 79.0693359375, Loss: 0.0199147816747427, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7331/20000], Bound: 0.36145493388175964, Entropy: 142.9583282470703, Temp: 2.586012363433838, KL: 66.70986938476562, Loss: 0.016075653955340385, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7332/20000], Bound: 0.3815429210662842, Entropy: 141.34234619140625, Temp: 2.5862348079681396, KL: 71.70498657226562, Loss: 0.017362190410494804, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7333/20000], Bound: 0.40314722061157227, Entropy: 141.48861694335938, Temp: 2.586463212966919, KL: 76.256103515625, Loss: 0.020689524710178375, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7334/20000], Bound: 0.3942658007144928, Entropy: 140.9393310546875, Temp: 2.586634397506714, KL: 74.77816772460938, Loss: 0.01851867139339447, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7335/20000], Bound: 0.3995855450630188, Entropy: 139.08273315429688, Temp: 2.586804151535034, KL: 76.22206115722656, Loss: 0.018734227865934372, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7336/20000], Bound: 0.4037420153617859, Entropy: 140.0954132080078, Temp: 2.58697509765625, KL: 77.50588989257812, Loss: 0.01861863024532795, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7337/20000], Bound: 0.39288535714149475, Entropy: 140.66024780273438, Temp: 2.587158203125, KL: 73.37564086914062, Loss: 0.020459135994315147, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7338/20000], Bound: 0.39472609758377075, Entropy: 140.1872100830078, Temp: 2.5872766971588135, KL: 75.34031677246094, Loss: 0.017698293551802635, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7339/20000], Bound: 0.3887399435043335, Entropy: 141.39488220214844, Temp: 2.5874242782592773, KL: 73.67304992675781, Loss: 0.01756688393652439, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7340/20000], Bound: 0.39240315556526184, Entropy: 140.0489044189453, Temp: 2.5875911712646484, KL: 73.07952880859375, Loss: 0.020765036344528198, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7341/20000], Bound: 0.40425917506217957, Entropy: 140.43984985351562, Temp: 2.587683916091919, KL: 78.16291809082031, Loss: 0.01765231415629387, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7342/20000], Bound: 0.3961432874202728, Entropy: 141.09793090820312, Temp: 2.587826728820801, KL: 74.90718078613281, Loss: 0.019339846447110176, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7343/20000], Bound: 0.41864317655563354, Entropy: 140.7616729736328, Temp: 2.5879485607147217, KL: 79.57896423339844, Loss: 0.023217644542455673, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7344/20000], Bound: 0.38226965069770813, Entropy: 140.740234375, Temp: 2.587975025177002, KL: 70.98052978515625, Loss: 0.019181497395038605, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7345/20000], Bound: 0.39132705330848694, Entropy: 140.35821533203125, Temp: 2.5879712104797363, KL: 75.22795104980469, Loss: 0.01601489447057247, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7346/20000], Bound: 0.3782704472541809, Entropy: 141.09512329101562, Temp: 2.5880541801452637, KL: 68.37272644042969, Loss: 0.022016094997525215, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7347/20000], Bound: 0.4058026671409607, Entropy: 138.96324157714844, Temp: 2.588006019592285, KL: 76.94113159179688, Loss: 0.02089855447411537, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7348/20000], Bound: 0.3966374099254608, Entropy: 139.13929748535156, Temp: 2.587925910949707, KL: 74.809326171875, Loss: 0.019808726385235786, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7349/20000], Bound: 0.38566672801971436, Entropy: 141.08880615234375, Temp: 2.5878329277038574, KL: 72.43685913085938, Loss: 0.018248800188302994, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7350/20000], Bound: 0.37652337551116943, Entropy: 141.31646728515625, Temp: 2.587756872177124, KL: 67.8424072265625, Loss: 0.022079121321439743, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7351/20000], Bound: 0.4294237196445465, Entropy: 140.76602172851562, Temp: 2.5875606536865234, KL: 84.22061157226562, Loss: 0.020583033561706543, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7352/20000], Bound: 0.3660850524902344, Entropy: 142.17832946777344, Temp: 2.5874006748199463, KL: 67.66921997070312, Loss: 0.01672889105975628, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7353/20000], Bound: 0.3870526850223541, Entropy: 141.83303833007812, Temp: 2.5872764587402344, KL: 69.56539916992188, Loss: 0.02456304244697094, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7354/20000], Bound: 0.3832913637161255, Entropy: 140.47616577148438, Temp: 2.5869803428649902, KL: 72.27900695800781, Loss: 0.01722750999033451, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7355/20000], Bound: 0.37971019744873047, Entropy: 140.07217407226562, Temp: 2.5867488384246826, KL: 71.27952575683594, Loss: 0.01717887446284294, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7356/20000], Bound: 0.3865026831626892, Entropy: 144.59341430664062, Temp: 2.586570978164673, KL: 73.2415771484375, Loss: 0.01714494824409485, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7357/20000], Bound: 0.3849439024925232, Entropy: 141.64622497558594, Temp: 2.5864546298980713, KL: 72.61599731445312, Loss: 0.017486700788140297, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7358/20000], Bound: 0.37153488397598267, Entropy: 141.22206115722656, Temp: 2.5863802433013916, KL: 66.96952819824219, Loss: 0.02102864906191826, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7359/20000], Bound: 0.35961583256721497, Entropy: 139.74127197265625, Temp: 2.586209297180176, KL: 64.92613220214844, Loss: 0.018539655953645706, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7360/20000], Bound: 0.37816762924194336, Entropy: 140.98841857910156, Temp: 2.586007595062256, KL: 68.96693420410156, Loss: 0.020793423056602478, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7361/20000], Bound: 0.3850112855434418, Entropy: 141.92982482910156, Temp: 2.5857419967651367, KL: 71.0491943359375, Loss: 0.020546285435557365, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7362/20000], Bound: 0.4074728786945343, Entropy: 141.05361938476562, Temp: 2.585439443588257, KL: 77.88092041015625, Loss: 0.02001056633889675, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7363/20000], Bound: 0.37414318323135376, Entropy: 143.87547302246094, Temp: 2.5851616859436035, KL: 68.27342224121094, Loss: 0.01992025040090084, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7364/20000], Bound: 0.3958762586116791, Entropy: 141.1938018798828, Temp: 2.5848474502563477, KL: 75.25883483886719, Loss: 0.0184769444167614, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7365/20000], Bound: 0.40899381041526794, Entropy: 142.88070678710938, Temp: 2.584585428237915, KL: 74.75399780273438, Loss: 0.02692398987710476, Learning Rate: 0.0012106082099999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7366/20000], Bound: 0.4100073575973511, Entropy: 141.50326538085938, Temp: 2.584137201309204, KL: 77.57478332519531, Loss: 0.02204573154449463, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7367/20000], Bound: 0.3885386884212494, Entropy: 142.44203186035156, Temp: 2.583672046661377, KL: 73.26956176757812, Loss: 0.01819443702697754, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7368/20000], Bound: 0.42387592792510986, Entropy: 141.1586456298828, Temp: 2.5832700729370117, KL: 83.50126647949219, Loss: 0.018641849979758263, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7369/20000], Bound: 0.4006475508213043, Entropy: 141.90811157226562, Temp: 2.5829761028289795, KL: 76.55514526367188, Loss: 0.01864957995712757, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7370/20000], Bound: 0.4044991731643677, Entropy: 142.54164123535156, Temp: 2.5827367305755615, KL: 79.12773132324219, Loss: 0.015862273052334785, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7371/20000], Bound: 0.4017364978790283, Entropy: 141.58203125, Temp: 2.582639217376709, KL: 76.22364807128906, Loss: 0.01990709826350212, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7372/20000], Bound: 0.36233073472976685, Entropy: 141.27102661132812, Temp: 2.582540512084961, KL: 65.73208618164062, Loss: 0.01840636320412159, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7373/20000], Bound: 0.3832804262638092, Entropy: 139.59788513183594, Temp: 2.5824146270751953, KL: 70.00682067871094, Loss: 0.021572060883045197, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7374/20000], Bound: 0.39462098479270935, Entropy: 140.4743194580078, Temp: 2.582204580307007, KL: 74.76985168457031, Loss: 0.018686378374695778, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7375/20000], Bound: 0.3695392310619354, Entropy: 139.30511474609375, Temp: 2.582029342651367, KL: 67.72921752929688, Loss: 0.018434341996908188, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7376/20000], Bound: 0.40261420607566833, Entropy: 139.36279296875, Temp: 2.5818469524383545, KL: 77.28298950195312, Loss: 0.018347054719924927, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7377/20000], Bound: 0.40971094369888306, Entropy: 139.8900146484375, Temp: 2.581721782684326, KL: 79.36674499511719, Loss: 0.018378324806690216, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7378/20000], Bound: 0.368589848279953, Entropy: 141.7316436767578, Temp: 2.581660270690918, KL: 66.16923522949219, Loss: 0.020936250686645508, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7379/20000], Bound: 0.40665456652641296, Entropy: 139.10769653320312, Temp: 2.58150053024292, KL: 77.42927551269531, Loss: 0.020370783284306526, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7380/20000], Bound: 0.38078197836875916, Entropy: 139.91700744628906, Temp: 2.58134126663208, KL: 70.80001831054688, Loss: 0.018642909824848175, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7381/20000], Bound: 0.42588770389556885, Entropy: 140.42120361328125, Temp: 2.581188201904297, KL: 83.7408447265625, Loss: 0.01933661848306656, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7382/20000], Bound: 0.3627922534942627, Entropy: 142.44679260253906, Temp: 2.5811026096343994, KL: 65.06988525390625, Loss: 0.019924981519579887, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7383/20000], Bound: 0.39709678292274475, Entropy: 139.28836059570312, Temp: 2.580942153930664, KL: 76.34097290039062, Loss: 0.017027081921696663, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7384/20000], Bound: 0.3844417929649353, Entropy: 139.99037170410156, Temp: 2.5808680057525635, KL: 72.12602233886719, Loss: 0.018097147345542908, Learning Rate: 0.0012106082099999993\n",
      "Epoch [7385/20000], Bound: 0.39762428402900696, Entropy: 139.67593383789062, Temp: 2.580831289291382, KL: 75.54222106933594, Loss: 0.018871836364269257, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7386/20000], Bound: 0.3788242042064667, Entropy: 139.56765747070312, Temp: 2.580808162689209, KL: 70.03070068359375, Loss: 0.019047578796744347, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7387/20000], Bound: 0.40270307660102844, Entropy: 140.40234375, Temp: 2.5807693004608154, KL: 75.57652282714844, Loss: 0.021691273897886276, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7388/20000], Bound: 0.4118705689907074, Entropy: 141.04324340820312, Temp: 2.5806901454925537, KL: 80.60284423828125, Loss: 0.017217721790075302, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7389/20000], Bound: 0.39162224531173706, Entropy: 140.99008178710938, Temp: 2.5806832313537598, KL: 75.69696044921875, Loss: 0.015186461620032787, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7390/20000], Bound: 0.39636126160621643, Entropy: 140.2885284423828, Temp: 2.580759286880493, KL: 74.03208923339844, Loss: 0.021082233637571335, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7391/20000], Bound: 0.3885376751422882, Entropy: 140.8382110595703, Temp: 2.5807883739471436, KL: 73.10244750976562, Loss: 0.01848677545785904, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7392/20000], Bound: 0.40110814571380615, Entropy: 139.3859405517578, Temp: 2.5808210372924805, KL: 76.08815002441406, Loss: 0.019791776314377785, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7393/20000], Bound: 0.3693791925907135, Entropy: 141.6333465576172, Temp: 2.580845355987549, KL: 66.11654663085938, Loss: 0.021460622549057007, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7394/20000], Bound: 0.39067167043685913, Entropy: 139.24029541015625, Temp: 2.5807836055755615, KL: 73.55255126953125, Loss: 0.01880902424454689, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7395/20000], Bound: 0.38347911834716797, Entropy: 141.6830291748047, Temp: 2.580730438232422, KL: 71.52183532714844, Loss: 0.018731845542788506, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7396/20000], Bound: 0.3825880289077759, Entropy: 139.25961303710938, Temp: 2.5806777477264404, KL: 71.1202392578125, Loss: 0.01901528798043728, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7397/20000], Bound: 0.3586060404777527, Entropy: 139.96585083007812, Temp: 2.5806174278259277, KL: 66.49476623535156, Loss: 0.01491105742752552, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7398/20000], Bound: 0.39497455954551697, Entropy: 142.08670043945312, Temp: 2.580610752105713, KL: 74.72293090820312, Loss: 0.018959172070026398, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7399/20000], Bound: 0.3972126543521881, Entropy: 140.4104766845703, Temp: 2.5806097984313965, KL: 74.91233825683594, Loss: 0.01985674537718296, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7400/20000], Bound: 0.42167016863822937, Entropy: 139.55616760253906, Temp: 2.580597162246704, KL: 83.60032653808594, Loss: 0.017117895185947418, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7401/20000], Bound: 0.3839983344078064, Entropy: 141.89112854003906, Temp: 2.58066463470459, KL: 71.45660400390625, Loss: 0.019145721569657326, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7402/20000], Bound: 0.4068492650985718, Entropy: 141.90020751953125, Temp: 2.580712080001831, KL: 78.4356689453125, Loss: 0.018523812294006348, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7403/20000], Bound: 0.37799331545829773, Entropy: 140.87136840820312, Temp: 2.580784320831299, KL: 69.75881958007812, Loss: 0.01911650411784649, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7404/20000], Bound: 0.3996991813182831, Entropy: 141.35751342773438, Temp: 2.5808286666870117, KL: 76.50419616699219, Loss: 0.018184762448072433, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7405/20000], Bound: 0.4107683002948761, Entropy: 140.9178466796875, Temp: 2.5808961391448975, KL: 80.23780822753906, Loss: 0.017290698364377022, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7406/20000], Bound: 0.38187238574028015, Entropy: 141.80972290039062, Temp: 2.5810184478759766, KL: 71.04080200195312, Loss: 0.018776336684823036, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7407/20000], Bound: 0.3805199861526489, Entropy: 144.28196716308594, Temp: 2.581120252609253, KL: 69.99485778808594, Loss: 0.020055703818798065, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7408/20000], Bound: 0.3664664030075073, Entropy: 142.14976501464844, Temp: 2.5811736583709717, KL: 66.49159240722656, Loss: 0.019156187772750854, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7409/20000], Bound: 0.37950599193573, Entropy: 142.97288513183594, Temp: 2.5811846256256104, KL: 70.76414489746094, Loss: 0.018006378784775734, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7410/20000], Bound: 0.3819511830806732, Entropy: 140.49473571777344, Temp: 2.581200122833252, KL: 70.37202453613281, Loss: 0.020117197185754776, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7411/20000], Bound: 0.3850049078464508, Entropy: 141.9528045654297, Temp: 2.5811760425567627, KL: 71.10568237304688, Loss: 0.02038978971540928, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7412/20000], Bound: 0.3942640721797943, Entropy: 142.9931640625, Temp: 2.5811145305633545, KL: 75.04780578613281, Loss: 0.01793481409549713, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7413/20000], Bound: 0.39059004187583923, Entropy: 140.9013214111328, Temp: 2.581085443496704, KL: 73.06890869140625, Loss: 0.01970340497791767, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7414/20000], Bound: 0.3933752775192261, Entropy: 140.90145874023438, Temp: 2.5810418128967285, KL: 74.97953796386719, Loss: 0.017565807327628136, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7415/20000], Bound: 0.39059194922447205, Entropy: 142.36477661132812, Temp: 2.581035614013672, KL: 73.523193359375, Loss: 0.018823882564902306, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7416/20000], Bound: 0.3853546977043152, Entropy: 141.97581481933594, Temp: 2.5810322761535645, KL: 73.39350891113281, Loss: 0.0161509457975626, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7417/20000], Bound: 0.3889634609222412, Entropy: 142.7349853515625, Temp: 2.581083059310913, KL: 73.77165222167969, Loss: 0.01743149198591709, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7418/20000], Bound: 0.3759051561355591, Entropy: 140.3495330810547, Temp: 2.5811593532562256, KL: 67.11737060546875, Loss: 0.023089289665222168, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7419/20000], Bound: 0.3750089108943939, Entropy: 141.58921813964844, Temp: 2.581116199493408, KL: 69.94862365722656, Loss: 0.017112841829657555, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7420/20000], Bound: 0.4009588956832886, Entropy: 142.5498809814453, Temp: 2.581096887588501, KL: 77.44418334960938, Loss: 0.01708303578197956, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7421/20000], Bound: 0.3986327648162842, Entropy: 141.14659118652344, Temp: 2.581132650375366, KL: 75.38815307617188, Loss: 0.01974504068493843, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7422/20000], Bound: 0.37294331192970276, Entropy: 140.72442626953125, Temp: 2.5811574459075928, KL: 68.18722534179688, Loss: 0.01939484104514122, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7423/20000], Bound: 0.40071624517440796, Entropy: 141.76068115234375, Temp: 2.581145763397217, KL: 75.55157470703125, Loss: 0.02061184123158455, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7424/20000], Bound: 0.3770328760147095, Entropy: 141.17689514160156, Temp: 2.581111431121826, KL: 69.38665771484375, Loss: 0.019312288612127304, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7425/20000], Bound: 0.3929253816604614, Entropy: 141.4190216064453, Temp: 2.5810537338256836, KL: 74.44261169433594, Loss: 0.01835297793149948, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7426/20000], Bound: 0.41623449325561523, Entropy: 141.2522430419922, Temp: 2.581017017364502, KL: 81.96298217773438, Loss: 0.017118750140070915, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7427/20000], Bound: 0.3707813024520874, Entropy: 145.67611694335938, Temp: 2.581056594848633, KL: 68.80262756347656, Loss: 0.017022056505084038, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7428/20000], Bound: 0.383864164352417, Entropy: 142.6389617919922, Temp: 2.581108570098877, KL: 71.01995849609375, Loss: 0.019921528175473213, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7429/20000], Bound: 0.40027937293052673, Entropy: 141.27597045898438, Temp: 2.5811238288879395, KL: 77.08047485351562, Loss: 0.01740146614611149, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7430/20000], Bound: 0.37329378724098206, Entropy: 143.34603881835938, Temp: 2.581183671951294, KL: 66.96781921386719, Loss: 0.02194877155125141, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7431/20000], Bound: 0.40173986554145813, Entropy: 142.1559295654297, Temp: 2.5811469554901123, KL: 75.85984802246094, Loss: 0.020597275346517563, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7432/20000], Bound: 0.39403513073921204, Entropy: 142.97592163085938, Temp: 2.5810916423797607, KL: 73.67951965332031, Loss: 0.020456179976463318, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7433/20000], Bound: 0.3973411023616791, Entropy: 142.28128051757812, Temp: 2.58101224899292, KL: 76.224365234375, Loss: 0.017392059788107872, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7434/20000], Bound: 0.36828815937042236, Entropy: 142.74400329589844, Temp: 2.5809834003448486, KL: 67.34120178222656, Loss: 0.01849641092121601, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7435/20000], Bound: 0.3861613869667053, Entropy: 142.75721740722656, Temp: 2.580936908721924, KL: 69.3536376953125, Loss: 0.024425221607089043, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7436/20000], Bound: 0.3791635036468506, Entropy: 144.43463134765625, Temp: 2.580767869949341, KL: 70.27813720703125, Loss: 0.018754886463284492, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7437/20000], Bound: 0.3989102244377136, Entropy: 141.28758239746094, Temp: 2.580603837966919, KL: 77.28378295898438, Loss: 0.01622382551431656, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7438/20000], Bound: 0.37777164578437805, Entropy: 141.96383666992188, Temp: 2.58052659034729, KL: 70.7027587890625, Loss: 0.017163030803203583, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7439/20000], Bound: 0.42100104689598083, Entropy: 141.4244384765625, Temp: 2.580479145050049, KL: 80.89274597167969, Loss: 0.021970059722661972, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7440/20000], Bound: 0.3867890238761902, Entropy: 143.59426879882812, Temp: 2.5804104804992676, KL: 71.02622985839844, Loss: 0.02152969315648079, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7441/20000], Bound: 0.40280425548553467, Entropy: 142.22756958007812, Temp: 2.5802860260009766, KL: 76.09408569335938, Loss: 0.020741056650877, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7442/20000], Bound: 0.39470356702804565, Entropy: 143.54495239257812, Temp: 2.5801503658294678, KL: 76.19480895996094, Loss: 0.015948999673128128, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7443/20000], Bound: 0.38915231823921204, Entropy: 142.39453125, Temp: 2.580099105834961, KL: 71.3974609375, Loss: 0.02212720550596714, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7444/20000], Bound: 0.34829479455947876, Entropy: 142.21734619140625, Temp: 2.5799806118011475, KL: 62.50062561035156, Loss: 0.017159409821033478, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7445/20000], Bound: 0.41689059138298035, Entropy: 143.4987030029297, Temp: 2.5798583030700684, KL: 82.43106079101562, Loss: 0.0165784303098917, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7446/20000], Bound: 0.37847018241882324, Entropy: 142.66207885742188, Temp: 2.579834461212158, KL: 71.59092712402344, Loss: 0.015819178894162178, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7447/20000], Bound: 0.40863093733787537, Entropy: 141.46446228027344, Temp: 2.5798661708831787, KL: 79.19432067871094, Loss: 0.018067602068185806, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7448/20000], Bound: 0.40659525990486145, Entropy: 143.5141143798828, Temp: 2.57993745803833, KL: 77.68911743164062, Loss: 0.01981559954583645, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7449/20000], Bound: 0.41314497590065, Entropy: 142.13336181640625, Temp: 2.580003499984741, KL: 79.96066284179688, Loss: 0.01919129304587841, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7450/20000], Bound: 0.38022321462631226, Entropy: 143.3416748046875, Temp: 2.580087423324585, KL: 70.98686218261719, Loss: 0.017959540709853172, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7451/20000], Bound: 0.3798747956752777, Entropy: 140.61801147460938, Temp: 2.5801706314086914, KL: 70.62333679199219, Loss: 0.018472488969564438, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7452/20000], Bound: 0.3862224221229553, Entropy: 141.72714233398438, Temp: 2.5802416801452637, KL: 72.68489074707031, Loss: 0.01799820177257061, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7453/20000], Bound: 0.37856388092041016, Entropy: 142.83892822265625, Temp: 2.5803205966949463, KL: 70.02876281738281, Loss: 0.018903197720646858, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7454/20000], Bound: 0.40862390398979187, Entropy: 142.6685333251953, Temp: 2.580376386642456, KL: 77.01911926269531, Loss: 0.02228471450507641, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7455/20000], Bound: 0.3966488540172577, Entropy: 142.93568420410156, Temp: 2.580376625061035, KL: 76.59413146972656, Loss: 0.01627647690474987, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7456/20000], Bound: 0.3958466351032257, Entropy: 143.7877655029297, Temp: 2.5804433822631836, KL: 74.61480712890625, Loss: 0.019659169018268585, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7457/20000], Bound: 0.36021897196769714, Entropy: 143.1949920654297, Temp: 2.5804944038391113, KL: 66.56402587890625, Loss: 0.015640728175640106, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7458/20000], Bound: 0.4186094403266907, Entropy: 142.22897338867188, Temp: 2.5805740356445312, KL: 81.66287231445312, Loss: 0.019079364836215973, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7459/20000], Bound: 0.4116547405719757, Entropy: 141.01759338378906, Temp: 2.5806796550750732, KL: 81.35643005371094, Loss: 0.015632783994078636, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7460/20000], Bound: 0.3815642297267914, Entropy: 140.95384216308594, Temp: 2.5808746814727783, KL: 71.59185791015625, Loss: 0.01753677800297737, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7461/20000], Bound: 0.399601012468338, Entropy: 141.12493896484375, Temp: 2.581069231033325, KL: 74.59152221679688, Loss: 0.021836986765265465, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7462/20000], Bound: 0.3896086513996124, Entropy: 141.5592803955078, Temp: 2.5811915397644043, KL: 71.92927551269531, Loss: 0.02136247046291828, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7463/20000], Bound: 0.38796985149383545, Entropy: 140.18386840820312, Temp: 2.5812456607818604, KL: 73.06619262695312, Loss: 0.018244652077555656, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7464/20000], Bound: 0.3864760994911194, Entropy: 140.19601440429688, Temp: 2.581305980682373, KL: 72.24639892578125, Loss: 0.019000191241502762, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7465/20000], Bound: 0.4175560772418976, Entropy: 140.07191467285156, Temp: 2.581352710723877, KL: 82.16567993164062, Loss: 0.0175005029886961, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7466/20000], Bound: 0.39649277925491333, Entropy: 141.76890563964844, Temp: 2.5814614295959473, KL: 74.97164916992188, Loss: 0.019343893975019455, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7467/20000], Bound: 0.3848820626735687, Entropy: 140.8593292236328, Temp: 2.581557035446167, KL: 72.87364196777344, Loss: 0.016900980845093727, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7468/20000], Bound: 0.393197238445282, Entropy: 139.83462524414062, Temp: 2.5816807746887207, KL: 75.02067565917969, Loss: 0.01739320158958435, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7469/20000], Bound: 0.39511287212371826, Entropy: 140.28643798828125, Temp: 2.581829071044922, KL: 73.9580078125, Loss: 0.020531801506876945, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7470/20000], Bound: 0.3992445170879364, Entropy: 138.43798828125, Temp: 2.581932306289673, KL: 77.79493713378906, Loss: 0.015439842827618122, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7471/20000], Bound: 0.388204425573349, Entropy: 142.03085327148438, Temp: 2.582113265991211, KL: 72.90304565429688, Loss: 0.018700797110795975, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7472/20000], Bound: 0.37593361735343933, Entropy: 140.5254669189453, Temp: 2.582277297973633, KL: 69.51960754394531, Loss: 0.01846235990524292, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7473/20000], Bound: 0.3903087079524994, Entropy: 142.35415649414062, Temp: 2.5824153423309326, KL: 73.12864685058594, Loss: 0.0194438137114048, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7474/20000], Bound: 0.41698288917541504, Entropy: 138.53390502929688, Temp: 2.582526683807373, KL: 81.27310180664062, Loss: 0.01891002617776394, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7475/20000], Bound: 0.38190099596977234, Entropy: 142.2704315185547, Temp: 2.5826613903045654, KL: 71.56788635253906, Loss: 0.017788153141736984, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7476/20000], Bound: 0.3602965772151947, Entropy: 141.51625061035156, Temp: 2.5827958583831787, KL: 64.58293151855469, Loss: 0.01954001560807228, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7477/20000], Bound: 0.39729493856430054, Entropy: 140.13084411621094, Temp: 2.5828611850738525, KL: 76.57406616210938, Loss: 0.016710523515939713, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7478/20000], Bound: 0.4100400507450104, Entropy: 138.4486541748047, Temp: 2.582977533340454, KL: 79.16009521484375, Loss: 0.01898328773677349, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7479/20000], Bound: 0.3899247944355011, Entropy: 141.3585662841797, Temp: 2.5831055641174316, KL: 72.99069213867188, Loss: 0.019503049552440643, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7480/20000], Bound: 0.36985525488853455, Entropy: 141.56190490722656, Temp: 2.5832059383392334, KL: 67.93864440917969, Loss: 0.018211746588349342, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7481/20000], Bound: 0.373532235622406, Entropy: 138.33583068847656, Temp: 2.5832836627960205, KL: 68.88034057617188, Loss: 0.018394580110907555, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7482/20000], Bound: 0.3910900354385376, Entropy: 140.19371032714844, Temp: 2.583341598510742, KL: 74.85575866699219, Loss: 0.016548318788409233, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7483/20000], Bound: 0.4316920340061188, Entropy: 142.19322204589844, Temp: 2.583446741104126, KL: 86.78175354003906, Loss: 0.0169219970703125, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7484/20000], Bound: 0.39870092272758484, Entropy: 139.8418731689453, Temp: 2.5836384296417236, KL: 75.76058959960938, Loss: 0.01909003220498562, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7485/20000], Bound: 0.37434694170951843, Entropy: 143.38653564453125, Temp: 2.5838167667388916, KL: 69.93338012695312, Loss: 0.016807418316602707, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7486/20000], Bound: 0.395950049161911, Entropy: 141.17062377929688, Temp: 2.5840020179748535, KL: 76.13241577148438, Loss: 0.016818910837173462, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7487/20000], Bound: 0.4132615029811859, Entropy: 141.83328247070312, Temp: 2.584221601486206, KL: 82.07582092285156, Loss: 0.015217438340187073, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7488/20000], Bound: 0.3996525704860687, Entropy: 141.4571075439453, Temp: 2.5845296382904053, KL: 77.58914184570312, Loss: 0.016101988032460213, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7489/20000], Bound: 0.39871394634246826, Entropy: 142.00350952148438, Temp: 2.5848805904388428, KL: 76.09689331054688, Loss: 0.018460605293512344, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7490/20000], Bound: 0.41471442580223083, Entropy: 139.68084716796875, Temp: 2.5852155685424805, KL: 78.42092895507812, Loss: 0.02314174920320511, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7491/20000], Bound: 0.36242732405662537, Entropy: 141.01101684570312, Temp: 2.585453987121582, KL: 67.75363159179688, Loss: 0.014574427157640457, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7492/20000], Bound: 0.3941487967967987, Entropy: 139.0460205078125, Temp: 2.585728168487549, KL: 74.74598693847656, Loss: 0.018505152314901352, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7493/20000], Bound: 0.38571441173553467, Entropy: 139.65296936035156, Temp: 2.585986852645874, KL: 73.71586608886719, Loss: 0.01578315906226635, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7494/20000], Bound: 0.3933796286582947, Entropy: 140.70291137695312, Temp: 2.5862815380096436, KL: 75.79359436035156, Loss: 0.016053419560194016, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7495/20000], Bound: 0.4082612693309784, Entropy: 140.44935607910156, Temp: 2.5866124629974365, KL: 78.79591369628906, Loss: 0.018707619979977608, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7496/20000], Bound: 0.3597860634326935, Entropy: 141.27569580078125, Temp: 2.5869359970092773, KL: 64.53453063964844, Loss: 0.019393885508179665, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7497/20000], Bound: 0.38948220014572144, Entropy: 139.0312042236328, Temp: 2.587172746658325, KL: 74.01339721679688, Loss: 0.017320849001407623, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7498/20000], Bound: 0.38571101427078247, Entropy: 142.09185791015625, Temp: 2.5874176025390625, KL: 73.0162353515625, Loss: 0.017149489372968674, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7499/20000], Bound: 0.3696705400943756, Entropy: 142.15318298339844, Temp: 2.5876684188842773, KL: 68.54533386230469, Loss: 0.016980791464447975, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7500/20000], Bound: 0.3799353837966919, Entropy: 141.4823760986328, Temp: 2.5879077911376953, KL: 69.76371765136719, Loss: 0.020243577659130096, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7501/20000], Bound: 0.39650630950927734, Entropy: 142.34280395507812, Temp: 2.588075876235962, KL: 74.87226867675781, Loss: 0.019614726305007935, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7502/20000], Bound: 0.4073679447174072, Entropy: 140.8285675048828, Temp: 2.5882160663604736, KL: 78.98103332519531, Loss: 0.01785646565258503, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7503/20000], Bound: 0.3876706063747406, Entropy: 140.54087829589844, Temp: 2.5883851051330566, KL: 73.57521057128906, Loss: 0.017170172184705734, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7504/20000], Bound: 0.3906187415122986, Entropy: 140.4354705810547, Temp: 2.5885696411132812, KL: 73.3050537109375, Loss: 0.01933966390788555, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7505/20000], Bound: 0.37204843759536743, Entropy: 141.66802978515625, Temp: 2.5887229442596436, KL: 69.41117858886719, Loss: 0.016611797735095024, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7506/20000], Bound: 0.40018051862716675, Entropy: 141.31040954589844, Temp: 2.588885545730591, KL: 76.26858520507812, Loss: 0.019004913046956062, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7507/20000], Bound: 0.3794838786125183, Entropy: 140.0740509033203, Temp: 2.589038848876953, KL: 70.8443603515625, Loss: 0.017918424680829048, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7508/20000], Bound: 0.3923780918121338, Entropy: 141.80538940429688, Temp: 2.589181423187256, KL: 74.54931640625, Loss: 0.01792835257947445, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7509/20000], Bound: 0.4024102985858917, Entropy: 140.21348571777344, Temp: 2.5893309116363525, KL: 77.92861938476562, Loss: 0.01707097515463829, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7510/20000], Bound: 0.3873192071914673, Entropy: 143.63587951660156, Temp: 2.5895190238952637, KL: 72.16145324707031, Loss: 0.0197166558355093, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7511/20000], Bound: 0.38334742188453674, Entropy: 142.1621551513672, Temp: 2.5896620750427246, KL: 73.71195983886719, Loss: 0.014520241878926754, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7512/20000], Bound: 0.3866066634654999, Entropy: 142.5021514892578, Temp: 2.589877128601074, KL: 74.75514221191406, Loss: 0.014316503889858723, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7513/20000], Bound: 0.3911980390548706, Entropy: 140.41615295410156, Temp: 2.590165853500366, KL: 74.156982421875, Loss: 0.01803549937903881, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7514/20000], Bound: 0.4260737895965576, Entropy: 139.91905212402344, Temp: 2.590442419052124, KL: 85.01280212402344, Loss: 0.01711120270192623, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7515/20000], Bound: 0.3990112245082855, Entropy: 140.6377716064453, Temp: 2.5907745361328125, KL: 75.7666015625, Loss: 0.019332310184836388, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7516/20000], Bound: 0.35463374853134155, Entropy: 142.5334930419922, Temp: 2.591071128845215, KL: 64.43588256835938, Loss: 0.016870634630322456, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7517/20000], Bound: 0.3917643129825592, Entropy: 140.80274963378906, Temp: 2.5913329124450684, KL: 73.3511962890625, Loss: 0.01991979032754898, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7518/20000], Bound: 0.3931487202644348, Entropy: 140.387939453125, Temp: 2.591542959213257, KL: 75.13955688476562, Loss: 0.01724739745259285, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7519/20000], Bound: 0.3592500686645508, Entropy: 141.74029541015625, Temp: 2.591768503189087, KL: 64.478271484375, Loss: 0.01925509050488472, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7520/20000], Bound: 0.39447370171546936, Entropy: 141.4333953857422, Temp: 2.591917037963867, KL: 73.43194580078125, Loss: 0.02128966711461544, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7521/20000], Bound: 0.3729727268218994, Entropy: 142.60069274902344, Temp: 2.5919973850250244, KL: 69.56065368652344, Loss: 0.016859600320458412, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7522/20000], Bound: 0.3961094915866852, Entropy: 140.23739624023438, Temp: 2.592088222503662, KL: 75.44978332519531, Loss: 0.018319478258490562, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7523/20000], Bound: 0.37988680601119995, Entropy: 143.49851989746094, Temp: 2.592186212539673, KL: 70.82768249511719, Loss: 0.018203718587756157, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7524/20000], Bound: 0.42714428901672363, Entropy: 141.84530639648438, Temp: 2.592271327972412, KL: 83.50448608398438, Loss: 0.02067664824426174, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7525/20000], Bound: 0.42411360144615173, Entropy: 141.67852783203125, Temp: 2.5923526287078857, KL: 84.28326416015625, Loss: 0.017391467466950417, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7526/20000], Bound: 0.3608941435813904, Entropy: 142.60728454589844, Temp: 2.5924999713897705, KL: 63.38165283203125, Loss: 0.02225581556558609, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7527/20000], Bound: 0.3835952877998352, Entropy: 140.53616333007812, Temp: 2.592510223388672, KL: 71.13861083984375, Loss: 0.01965329609811306, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7528/20000], Bound: 0.39132896065711975, Entropy: 139.15066528320312, Temp: 2.5924880504608154, KL: 73.66543579101562, Loss: 0.01908174902200699, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7529/20000], Bound: 0.4212273061275482, Entropy: 142.3126983642578, Temp: 2.5924601554870605, KL: 82.57510375976562, Loss: 0.01899593695998192, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7530/20000], Bound: 0.39145568013191223, Entropy: 140.10650634765625, Temp: 2.5924696922302246, KL: 73.774169921875, Loss: 0.01894276961684227, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7531/20000], Bound: 0.37423765659332275, Entropy: 144.35267639160156, Temp: 2.592473268508911, KL: 67.14529418945312, Loss: 0.022212600335478783, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7532/20000], Bound: 0.4007459282875061, Entropy: 141.4122772216797, Temp: 2.592373847961426, KL: 76.88986206054688, Loss: 0.018165702000260353, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7533/20000], Bound: 0.4155441224575043, Entropy: 143.2327117919922, Temp: 2.5923099517822266, KL: 80.43507385253906, Loss: 0.019812561571598053, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7534/20000], Bound: 0.39805665612220764, Entropy: 141.7677459716797, Temp: 2.5922605991363525, KL: 76.81076049804688, Loss: 0.016794724389910698, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7535/20000], Bound: 0.4322676360607147, Entropy: 141.6201171875, Temp: 2.5922694206237793, KL: 86.79217529296875, Loss: 0.017370330169796944, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7536/20000], Bound: 0.415555864572525, Entropy: 141.880859375, Temp: 2.592362642288208, KL: 80.58132934570312, Loss: 0.019537942484021187, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7537/20000], Bound: 0.3657186031341553, Entropy: 140.54124450683594, Temp: 2.592461347579956, KL: 66.57673645019531, Loss: 0.018686650320887566, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7538/20000], Bound: 0.39515602588653564, Entropy: 139.50753784179688, Temp: 2.5925168991088867, KL: 75.40589904785156, Loss: 0.017872128635644913, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7539/20000], Bound: 0.3439234495162964, Entropy: 141.13916015625, Temp: 2.5925917625427246, KL: 59.65382385253906, Loss: 0.020459704101085663, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7540/20000], Bound: 0.36116963624954224, Entropy: 140.71389770507812, Temp: 2.5925545692443848, KL: 65.99266052246094, Loss: 0.017368201166391373, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7541/20000], Bound: 0.3795742690563202, Entropy: 141.4582061767578, Temp: 2.592512607574463, KL: 70.33721923828125, Loss: 0.018980931490659714, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7542/20000], Bound: 0.3759593665599823, Entropy: 142.98326110839844, Temp: 2.5924532413482666, KL: 68.78329467773438, Loss: 0.01999424211680889, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7543/20000], Bound: 0.3929651081562042, Entropy: 141.7703094482422, Temp: 2.5923500061035156, KL: 74.18612670898438, Loss: 0.01899242401123047, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7544/20000], Bound: 0.3987295627593994, Entropy: 141.08265686035156, Temp: 2.5922534465789795, KL: 76.26004028320312, Loss: 0.018237240612506866, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7545/20000], Bound: 0.37122732400894165, Entropy: 141.09910583496094, Temp: 2.5921878814697266, KL: 69.05874633789062, Loss: 0.01687965914607048, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7546/20000], Bound: 0.40916869044303894, Entropy: 140.14651489257812, Temp: 2.592144727706909, KL: 80.11701965332031, Loss: 0.016745274886488914, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7547/20000], Bound: 0.3953615128993988, Entropy: 139.85687255859375, Temp: 2.592175245285034, KL: 74.64114379882812, Loss: 0.01945904642343521, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7548/20000], Bound: 0.3512209951877594, Entropy: 140.54916381835938, Temp: 2.5921919345855713, KL: 62.708740234375, Loss: 0.018403375521302223, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7549/20000], Bound: 0.38934090733528137, Entropy: 141.1183624267578, Temp: 2.59216046333313, KL: 72.53866577148438, Loss: 0.020141225308179855, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7550/20000], Bound: 0.4277295470237732, Entropy: 138.1837615966797, Temp: 2.592097282409668, KL: 82.82177734375, Loss: 0.0223369300365448, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7551/20000], Bound: 0.4015969932079315, Entropy: 139.92178344726562, Temp: 2.5920093059539795, KL: 76.07962036132812, Loss: 0.020207511261105537, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7552/20000], Bound: 0.36351048946380615, Entropy: 139.79983520507812, Temp: 2.5919103622436523, KL: 65.83453369140625, Loss: 0.018924299627542496, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7553/20000], Bound: 0.38666749000549316, Entropy: 141.1357421875, Temp: 2.5917794704437256, KL: 70.97691345214844, Loss: 0.02166176773607731, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7554/20000], Bound: 0.379285991191864, Entropy: 142.64414978027344, Temp: 2.5915887355804443, KL: 70.73371887207031, Loss: 0.018048541620373726, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7555/20000], Bound: 0.39562681317329407, Entropy: 140.7130889892578, Temp: 2.591417074203491, KL: 74.40042114257812, Loss: 0.02006489224731922, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7556/20000], Bound: 0.3986204266548157, Entropy: 140.82447814941406, Temp: 2.591238021850586, KL: 76.089599609375, Loss: 0.01849295385181904, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7557/20000], Bound: 0.4063822627067566, Entropy: 140.32130432128906, Temp: 2.591093063354492, KL: 78.00482177734375, Loss: 0.01921059936285019, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7558/20000], Bound: 0.4038735032081604, Entropy: 140.61343383789062, Temp: 2.590972661972046, KL: 76.76554870605469, Loss: 0.02016816847026348, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7559/20000], Bound: 0.39581137895584106, Entropy: 142.76808166503906, Temp: 2.5908491611480713, KL: 76.22885131835938, Loss: 0.016634395346045494, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7560/20000], Bound: 0.38932004570961, Entropy: 139.63128662109375, Temp: 2.5907928943634033, KL: 72.52764892578125, Loss: 0.020137397572398186, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7561/20000], Bound: 0.3989027738571167, Entropy: 142.5012969970703, Temp: 2.590708017349243, KL: 76.29132080078125, Loss: 0.01825748197734356, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7562/20000], Bound: 0.3625841736793518, Entropy: 142.74322509765625, Temp: 2.590653419494629, KL: 64.32656860351562, Loss: 0.021325964480638504, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7563/20000], Bound: 0.39711159467697144, Entropy: 142.1876983642578, Temp: 2.590505599975586, KL: 75.80075073242188, Loss: 0.0181898046284914, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7564/20000], Bound: 0.3810340166091919, Entropy: 140.98892211914062, Temp: 2.5903937816619873, KL: 71.10287475585938, Loss: 0.018286913633346558, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7565/20000], Bound: 0.3577045202255249, Entropy: 140.53143310546875, Temp: 2.5902903079986572, KL: 66.5062255859375, Loss: 0.014502818696200848, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7566/20000], Bound: 0.41216275095939636, Entropy: 139.5863494873047, Temp: 2.590251922607422, KL: 79.50003051757812, Loss: 0.019636614248156548, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7567/20000], Bound: 0.39408937096595764, Entropy: 142.37118530273438, Temp: 2.5902259349823, KL: 75.71542358398438, Loss: 0.016649264842271805, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7568/20000], Bound: 0.4255971610546112, Entropy: 139.19085693359375, Temp: 2.5902552604675293, KL: 84.84327697753906, Loss: 0.01715499721467495, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7569/20000], Bound: 0.3809678256511688, Entropy: 142.05235290527344, Temp: 2.590365171432495, KL: 73.10469055175781, Loss: 0.014386155642569065, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7570/20000], Bound: 0.393036425113678, Entropy: 142.73779296875, Temp: 2.5905513763427734, KL: 74.51809692382812, Loss: 0.018372735008597374, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7571/20000], Bound: 0.38344070315361023, Entropy: 142.5753936767578, Temp: 2.590730905532837, KL: 70.54067993164062, Loss: 0.020704645663499832, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7572/20000], Bound: 0.38191720843315125, Entropy: 141.7836151123047, Temp: 2.590836763381958, KL: 71.09396362304688, Loss: 0.018795814365148544, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7573/20000], Bound: 0.3943280279636383, Entropy: 140.8728485107422, Temp: 2.59091854095459, KL: 75.65478515625, Loss: 0.016908401623368263, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7574/20000], Bound: 0.4196150004863739, Entropy: 139.4588623046875, Temp: 2.591038942337036, KL: 83.02496337890625, Loss: 0.01716797612607479, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7575/20000], Bound: 0.3795011043548584, Entropy: 141.08848571777344, Temp: 2.591222047805786, KL: 70.655029296875, Loss: 0.018315093591809273, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7576/20000], Bound: 0.3722076416015625, Entropy: 144.06161499023438, Temp: 2.591381072998047, KL: 69.10504150390625, Loss: 0.017315812408924103, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7577/20000], Bound: 0.3724454343318939, Entropy: 142.3021240234375, Temp: 2.591531991958618, KL: 69.98080444335938, Loss: 0.015757061541080475, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7578/20000], Bound: 0.36781683564186096, Entropy: 140.59231567382812, Temp: 2.5917117595672607, KL: 66.146240234375, Loss: 0.02064422331750393, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7579/20000], Bound: 0.35830461978912354, Entropy: 144.43716430664062, Temp: 2.5917975902557373, KL: 65.85986328125, Loss: 0.016084887087345123, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7580/20000], Bound: 0.3811469078063965, Entropy: 139.2069091796875, Temp: 2.5918922424316406, KL: 71.62124633789062, Loss: 0.01736408658325672, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7581/20000], Bound: 0.3800322711467743, Entropy: 141.4203338623047, Temp: 2.59199595451355, KL: 71.9505615234375, Loss: 0.016115834936499596, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7582/20000], Bound: 0.41356948018074036, Entropy: 139.86911010742188, Temp: 2.592134952545166, KL: 79.66123962402344, Loss: 0.020160000771284103, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7583/20000], Bound: 0.3911234438419342, Entropy: 143.44900512695312, Temp: 2.5922579765319824, KL: 74.77333068847656, Loss: 0.016827469691634178, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7584/20000], Bound: 0.4087248146533966, Entropy: 141.23155212402344, Temp: 2.5924129486083984, KL: 77.98045349121094, Loss: 0.020614659413695335, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7585/20000], Bound: 0.3906182646751404, Entropy: 142.3447265625, Temp: 2.5925331115722656, KL: 74.14970397949219, Loss: 0.017750917002558708, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7586/20000], Bound: 0.38629433512687683, Entropy: 142.5990753173828, Temp: 2.592663049697876, KL: 72.13482666015625, Loss: 0.019229430705308914, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7587/20000], Bound: 0.3854767978191376, Entropy: 140.662109375, Temp: 2.592761516571045, KL: 73.68304443359375, Loss: 0.015791049227118492, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7588/20000], Bound: 0.3964563310146332, Entropy: 142.8878631591797, Temp: 2.5929107666015625, KL: 75.195068359375, Loss: 0.019015081226825714, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7589/20000], Bound: 0.39864233136177063, Entropy: 142.0876922607422, Temp: 2.593045234680176, KL: 77.38218688964844, Loss: 0.016033008694648743, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7590/20000], Bound: 0.37398645281791687, Entropy: 142.769775390625, Temp: 2.593238592147827, KL: 68.11799621582031, Loss: 0.02020617201924324, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7591/20000], Bound: 0.42107176780700684, Entropy: 141.34896850585938, Temp: 2.593353748321533, KL: 83.2864990234375, Loss: 0.017544716596603394, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7592/20000], Bound: 0.40160566568374634, Entropy: 141.27523803710938, Temp: 2.5935251712799072, KL: 77.54251098632812, Loss: 0.017408311367034912, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7593/20000], Bound: 0.4075643718242645, Entropy: 141.17138671875, Temp: 2.5937235355377197, KL: 79.52981567382812, Loss: 0.016977140679955482, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7594/20000], Bound: 0.3775767982006073, Entropy: 141.84449768066406, Temp: 2.5939643383026123, KL: 70.33578491210938, Loss: 0.01790091209113598, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7595/20000], Bound: 0.39335402846336365, Entropy: 142.21905517578125, Temp: 2.5941810607910156, KL: 73.14187622070312, Loss: 0.021242298185825348, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7596/20000], Bound: 0.4251321256160736, Entropy: 141.9989776611328, Temp: 2.5943198204040527, KL: 81.61924743652344, Loss: 0.023150546476244926, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7597/20000], Bound: 0.38502973318099976, Entropy: 141.1766815185547, Temp: 2.5943899154663086, KL: 74.00276184082031, Loss: 0.014945301227271557, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7598/20000], Bound: 0.38524991273880005, Entropy: 142.15211486816406, Temp: 2.594531536102295, KL: 72.31167602539062, Loss: 0.018327858299016953, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7599/20000], Bound: 0.4009592533111572, Entropy: 139.3372344970703, Temp: 2.5946595668792725, KL: 76.86749267578125, Loss: 0.018355662003159523, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7600/20000], Bound: 0.36178725957870483, Entropy: 141.56825256347656, Temp: 2.5947957038879395, KL: 66.76502990722656, Loss: 0.016230856999754906, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7601/20000], Bound: 0.39458900690078735, Entropy: 139.8501739501953, Temp: 2.5949361324310303, KL: 73.83432006835938, Loss: 0.02060837857425213, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7602/20000], Bound: 0.38717830181121826, Entropy: 141.1377716064453, Temp: 2.595022439956665, KL: 72.45481872558594, Loss: 0.01912711188197136, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7603/20000], Bound: 0.4027261435985565, Entropy: 142.8871307373047, Temp: 2.5950841903686523, KL: 77.96910095214844, Loss: 0.017240995541214943, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7604/20000], Bound: 0.40360668301582336, Entropy: 140.61941528320312, Temp: 2.595188856124878, KL: 77.92791748046875, Loss: 0.01782238483428955, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7605/20000], Bound: 0.36452651023864746, Entropy: 142.5279541015625, Temp: 2.5953197479248047, KL: 66.79682922363281, Loss: 0.017645573243498802, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7606/20000], Bound: 0.3611491024494171, Entropy: 142.38536071777344, Temp: 2.5954253673553467, KL: 66.05574035644531, Loss: 0.017261330038309097, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7607/20000], Bound: 0.37623947858810425, Entropy: 143.1881561279297, Temp: 2.595512866973877, KL: 69.22035217285156, Loss: 0.019332654774188995, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7608/20000], Bound: 0.4160710275173187, Entropy: 141.5286865234375, Temp: 2.595555305480957, KL: 80.28529357910156, Loss: 0.020444776862859726, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7609/20000], Bound: 0.408762663602829, Entropy: 141.07289123535156, Temp: 2.5955870151519775, KL: 79.1854248046875, Loss: 0.018349986523389816, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7610/20000], Bound: 0.4056953191757202, Entropy: 139.09632873535156, Temp: 2.595646619796753, KL: 78.09481811523438, Loss: 0.018696622923016548, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7611/20000], Bound: 0.4086766242980957, Entropy: 141.59027099609375, Temp: 2.595719814300537, KL: 80.5469970703125, Loss: 0.01567949540913105, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7612/20000], Bound: 0.3996738791465759, Entropy: 141.99484252929688, Temp: 2.5958786010742188, KL: 77.08819580078125, Loss: 0.017216408625245094, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7613/20000], Bound: 0.39264392852783203, Entropy: 142.26600646972656, Temp: 2.596066951751709, KL: 76.15974426269531, Loss: 0.015050242654979229, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7614/20000], Bound: 0.39480480551719666, Entropy: 141.52716064453125, Temp: 2.596322536468506, KL: 75.44448852539062, Loss: 0.017642458900809288, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7615/20000], Bound: 0.40201514959335327, Entropy: 139.12977600097656, Temp: 2.596581220626831, KL: 77.28108215332031, Loss: 0.018179751932621002, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7616/20000], Bound: 0.40455079078674316, Entropy: 139.67172241210938, Temp: 2.596839666366577, KL: 78.71688842773438, Loss: 0.01685996912419796, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7617/20000], Bound: 0.3956977427005768, Entropy: 141.5274658203125, Temp: 2.5971322059631348, KL: 76.29075622558594, Loss: 0.016523925587534904, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7618/20000], Bound: 0.4077117145061493, Entropy: 139.6640167236328, Temp: 2.5974512100219727, KL: 79.61636352539062, Loss: 0.016940541565418243, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7619/20000], Bound: 0.4189075231552124, Entropy: 139.2578125, Temp: 2.5978000164031982, KL: 83.16200256347656, Loss: 0.01658090576529503, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7620/20000], Bound: 0.3895941376686096, Entropy: 140.18321228027344, Temp: 2.5981993675231934, KL: 73.47224426269531, Loss: 0.018544452264904976, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7621/20000], Bound: 0.3843357264995575, Entropy: 141.519287109375, Temp: 2.598558187484741, KL: 72.585693359375, Loss: 0.01733560487627983, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7622/20000], Bound: 0.41778630018234253, Entropy: 137.51312255859375, Temp: 2.5989019870758057, KL: 81.67433166503906, Loss: 0.018805881962180138, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7623/20000], Bound: 0.3772841691970825, Entropy: 139.03863525390625, Temp: 2.599243402481079, KL: 70.29653930664062, Loss: 0.017867717891931534, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7624/20000], Bound: 0.38321739435195923, Entropy: 137.94073486328125, Temp: 2.599548816680908, KL: 72.50387573242188, Loss: 0.016885612159967422, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7625/20000], Bound: 0.41120046377182007, Entropy: 140.0753936767578, Temp: 2.599853038787842, KL: 80.455810546875, Loss: 0.01735595613718033, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7626/20000], Bound: 0.38467782735824585, Entropy: 138.081298828125, Temp: 2.6001830101013184, KL: 72.76849365234375, Loss: 0.017190055921673775, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7627/20000], Bound: 0.3786475360393524, Entropy: 140.68951416015625, Temp: 2.600503921508789, KL: 70.00115966796875, Loss: 0.019194431602954865, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7628/20000], Bound: 0.37690383195877075, Entropy: 140.4020538330078, Temp: 2.6007609367370605, KL: 70.09005737304688, Loss: 0.018071388825774193, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7629/20000], Bound: 0.38936612010002136, Entropy: 138.71728515625, Temp: 2.600984573364258, KL: 74.50509643554688, Loss: 0.016460606828331947, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7630/20000], Bound: 0.4008859395980835, Entropy: 140.73956298828125, Temp: 2.6012327671051025, KL: 77.19563293457031, Loss: 0.017756983637809753, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7631/20000], Bound: 0.40693390369415283, Entropy: 141.08592224121094, Temp: 2.6014883518218994, KL: 77.97018432617188, Loss: 0.019709276035428047, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7632/20000], Bound: 0.3897618353366852, Entropy: 140.4931640625, Temp: 2.601713180541992, KL: 73.67498779296875, Loss: 0.018284235149621964, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7633/20000], Bound: 0.42581480741500854, Entropy: 139.9823760986328, Temp: 2.6019201278686523, KL: 84.09184265136719, Loss: 0.018884645774960518, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7634/20000], Bound: 0.37460872530937195, Entropy: 141.24842834472656, Temp: 2.6021461486816406, KL: 69.7628173828125, Loss: 0.017461104318499565, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7635/20000], Bound: 0.39676228165626526, Entropy: 139.72998046875, Temp: 2.6023526191711426, KL: 75.20263671875, Loss: 0.019272705540060997, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7636/20000], Bound: 0.421438068151474, Entropy: 138.49681091308594, Temp: 2.6025288105010986, KL: 85.05641174316406, Loss: 0.014477161690592766, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7637/20000], Bound: 0.3667595088481903, Entropy: 139.92308044433594, Temp: 2.6028225421905518, KL: 67.6302490234375, Loss: 0.017312603071331978, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7638/20000], Bound: 0.4013795256614685, Entropy: 140.89833068847656, Temp: 2.6030826568603516, KL: 76.5943603515625, Loss: 0.019212163984775543, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7639/20000], Bound: 0.4149080812931061, Entropy: 140.1793975830078, Temp: 2.6033146381378174, KL: 78.97637939453125, Loss: 0.02237538807094097, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7640/20000], Bound: 0.38017088174819946, Entropy: 141.6918487548828, Temp: 2.603466510772705, KL: 70.0579833984375, Loss: 0.019947927445173264, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7641/20000], Bound: 0.38174164295196533, Entropy: 138.95999145507812, Temp: 2.6035542488098145, KL: 72.28764343261719, Loss: 0.016529930755496025, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7642/20000], Bound: 0.39395612478256226, Entropy: 139.20640563964844, Temp: 2.6036672592163086, KL: 75.58045959472656, Loss: 0.016986489295959473, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7643/20000], Bound: 0.40578046441078186, Entropy: 142.06028747558594, Temp: 2.603808879852295, KL: 79.53314208984375, Loss: 0.016075849533081055, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7644/20000], Bound: 0.3747997283935547, Entropy: 140.7113037109375, Temp: 2.604013204574585, KL: 69.15005493164062, Loss: 0.018759598955512047, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7645/20000], Bound: 0.3855016529560089, Entropy: 141.985107421875, Temp: 2.604168653488159, KL: 71.34922790527344, Loss: 0.020411822944879532, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7646/20000], Bound: 0.38416269421577454, Entropy: 139.95814514160156, Temp: 2.604255437850952, KL: 72.46501159667969, Loss: 0.01753067970275879, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7647/20000], Bound: 0.41518494486808777, Entropy: 139.2412872314453, Temp: 2.604346990585327, KL: 80.24208068847656, Loss: 0.020116223022341728, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7648/20000], Bound: 0.40495163202285767, Entropy: 140.57044982910156, Temp: 2.6044247150421143, KL: 77.93235778808594, Loss: 0.01868480071425438, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7649/20000], Bound: 0.34700068831443787, Entropy: 140.79644775390625, Temp: 2.6045093536376953, KL: 63.31190490722656, Loss: 0.015124027617275715, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7650/20000], Bound: 0.4056927561759949, Entropy: 142.22669982910156, Temp: 2.6046063899993896, KL: 79.25578308105469, Loss: 0.016568148508667946, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7651/20000], Bound: 0.3970854580402374, Entropy: 141.16815185546875, Temp: 2.6047592163085938, KL: 75.17601013183594, Loss: 0.019530612975358963, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7652/20000], Bound: 0.38811859488487244, Entropy: 140.62217712402344, Temp: 2.6048803329467773, KL: 72.75418090820312, Loss: 0.01917128637433052, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7653/20000], Bound: 0.39494678378105164, Entropy: 141.80584716796875, Temp: 2.604968786239624, KL: 75.42750549316406, Loss: 0.017849387601017952, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7654/20000], Bound: 0.40061914920806885, Entropy: 143.85302734375, Temp: 2.6050686836242676, KL: 76.266845703125, Loss: 0.01943216845393181, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7655/20000], Bound: 0.39293715357780457, Entropy: 138.7169952392578, Temp: 2.605149745941162, KL: 74.53128051757812, Loss: 0.01844666711986065, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7656/20000], Bound: 0.37990066409111023, Entropy: 142.0696258544922, Temp: 2.6052255630493164, KL: 70.28286743164062, Loss: 0.019383898004889488, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7657/20000], Bound: 0.3699071407318115, Entropy: 145.11746215820312, Temp: 2.605257034301758, KL: 67.48294067382812, Loss: 0.019315745681524277, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7658/20000], Bound: 0.41547349095344543, Entropy: 140.48204040527344, Temp: 2.6052358150482178, KL: 82.50244140625, Loss: 0.015954913571476936, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7659/20000], Bound: 0.4146454632282257, Entropy: 140.13829040527344, Temp: 2.605309247970581, KL: 81.04327392578125, Loss: 0.018278103321790695, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7660/20000], Bound: 0.3964214622974396, Entropy: 142.01190185546875, Temp: 2.605412721633911, KL: 74.9041748046875, Loss: 0.019685909152030945, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7661/20000], Bound: 0.36452898383140564, Entropy: 140.24742126464844, Temp: 2.6054844856262207, KL: 68.27033996582031, Loss: 0.01490976195782423, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7662/20000], Bound: 0.3886253237724304, Entropy: 140.88494873046875, Temp: 2.6055984497070312, KL: 74.87373352050781, Loss: 0.015392323024570942, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7663/20000], Bound: 0.38694947957992554, Entropy: 141.37271118164062, Temp: 2.6057708263397217, KL: 70.17787170410156, Loss: 0.023475293070077896, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7664/20000], Bound: 0.3707517385482788, Entropy: 141.9279022216797, Temp: 2.6058008670806885, KL: 68.30783081054688, Loss: 0.018194489181041718, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7665/20000], Bound: 0.37350741028785706, Entropy: 141.31842041015625, Temp: 2.605806350708008, KL: 69.91302490234375, Loss: 0.01660902239382267, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7666/20000], Bound: 0.4051464796066284, Entropy: 141.57688903808594, Temp: 2.6058316230773926, KL: 77.38189697265625, Loss: 0.01986761949956417, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7667/20000], Bound: 0.3781287372112274, Entropy: 140.85400390625, Temp: 2.6058404445648193, KL: 69.74923706054688, Loss: 0.019443070515990257, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7668/20000], Bound: 0.39467450976371765, Entropy: 140.34486389160156, Temp: 2.605807304382324, KL: 72.78950500488281, Loss: 0.022767672315239906, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7669/20000], Bound: 0.36739417910575867, Entropy: 141.70387268066406, Temp: 2.605679988861084, KL: 66.83302307128906, Loss: 0.019210273399949074, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7670/20000], Bound: 0.3804865777492523, Entropy: 141.48141479492188, Temp: 2.6055147647857666, KL: 71.13601684570312, Loss: 0.018070833757519722, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7671/20000], Bound: 0.4010435938835144, Entropy: 141.93223571777344, Temp: 2.605360984802246, KL: 78.07221984863281, Loss: 0.016210490837693214, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7672/20000], Bound: 0.38019508123397827, Entropy: 141.90235900878906, Temp: 2.605289936065674, KL: 71.5546875, Loss: 0.01710519939661026, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7673/20000], Bound: 0.3992466330528259, Entropy: 143.8176727294922, Temp: 2.605243682861328, KL: 78.05854797363281, Loss: 0.015220431610941887, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7674/20000], Bound: 0.3950275182723999, Entropy: 141.14564514160156, Temp: 2.605290651321411, KL: 74.65855407714844, Loss: 0.019373876973986626, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7675/20000], Bound: 0.4072082042694092, Entropy: 141.05792236328125, Temp: 2.6053171157836914, KL: 78.47383117675781, Loss: 0.018941283226013184, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7676/20000], Bound: 0.43383899331092834, Entropy: 140.99888610839844, Temp: 2.6053521633148193, KL: 84.89414978027344, Loss: 0.022127356380224228, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7677/20000], Bound: 0.3945831060409546, Entropy: 140.43589782714844, Temp: 2.605358123779297, KL: 75.15745544433594, Loss: 0.018168052658438683, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7678/20000], Bound: 0.40426191687583923, Entropy: 140.80157470703125, Temp: 2.6053755283355713, KL: 79.81553649902344, Loss: 0.01468935701996088, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7679/20000], Bound: 0.3873293101787567, Entropy: 143.5929718017578, Temp: 2.605499267578125, KL: 71.54216003417969, Loss: 0.02106551080942154, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7680/20000], Bound: 0.39497110247612, Entropy: 143.2819061279297, Temp: 2.6055431365966797, KL: 76.26750183105469, Loss: 0.016257263720035553, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7681/20000], Bound: 0.38654857873916626, Entropy: 140.1327362060547, Temp: 2.6056408882141113, KL: 73.93353271484375, Loss: 0.01604546420276165, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7682/20000], Bound: 0.3734295070171356, Entropy: 140.37799072265625, Temp: 2.605780839920044, KL: 70.39537048339844, Loss: 0.015640905126929283, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7683/20000], Bound: 0.4139288067817688, Entropy: 141.26416015625, Temp: 2.605950117111206, KL: 81.92930603027344, Loss: 0.01617255248129368, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7684/20000], Bound: 0.3590738773345947, Entropy: 141.99591064453125, Temp: 2.6061880588531494, KL: 64.8597412109375, Loss: 0.018544288352131844, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7685/20000], Bound: 0.4130363166332245, Entropy: 139.42984008789062, Temp: 2.606355905532837, KL: 80.98722839355469, Loss: 0.017471013590693474, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7686/20000], Bound: 0.43131086230278015, Entropy: 142.07473754882812, Temp: 2.606560707092285, KL: 85.51945495605469, Loss: 0.019443543627858162, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7687/20000], Bound: 0.43403059244155884, Entropy: 140.423095703125, Temp: 2.6067776679992676, KL: 88.229248046875, Loss: 0.015861358493566513, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7688/20000], Bound: 0.3834076523780823, Entropy: 141.85794067382812, Temp: 2.60709285736084, KL: 71.32017517089844, Loss: 0.019338930025696754, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7689/20000], Bound: 0.38861483335494995, Entropy: 140.20848083496094, Temp: 2.607344388961792, KL: 74.45826721191406, Loss: 0.01620291732251644, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7690/20000], Bound: 0.3803199827671051, Entropy: 143.92404174804688, Temp: 2.60762095451355, KL: 70.86419677734375, Loss: 0.018521074205636978, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7691/20000], Bound: 0.3810271918773651, Entropy: 139.8482666015625, Temp: 2.6078529357910156, KL: 73.04145812988281, Loss: 0.014736907556653023, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7692/20000], Bound: 0.41908299922943115, Entropy: 140.26878356933594, Temp: 2.6081361770629883, KL: 83.53071594238281, Loss: 0.01611110009253025, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7693/20000], Bound: 0.38596397638320923, Entropy: 139.46742248535156, Temp: 2.6084845066070557, KL: 72.27752685546875, Loss: 0.01892738603055477, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7694/20000], Bound: 0.3504020571708679, Entropy: 142.7323455810547, Temp: 2.608778476715088, KL: 63.42645263671875, Loss: 0.016725651919841766, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7695/20000], Bound: 0.4060201048851013, Entropy: 139.26637268066406, Temp: 2.6090283393859863, KL: 77.79885864257812, Loss: 0.019599679857492447, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7696/20000], Bound: 0.41599053144454956, Entropy: 140.60215759277344, Temp: 2.6092453002929688, KL: 81.46240234375, Loss: 0.018298497423529625, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7697/20000], Bound: 0.36219722032546997, Entropy: 142.78692626953125, Temp: 2.609477996826172, KL: 65.25726318359375, Loss: 0.019474415108561516, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7698/20000], Bound: 0.41526442766189575, Entropy: 137.59437561035156, Temp: 2.6096208095550537, KL: 80.52131652832031, Loss: 0.019686950370669365, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7699/20000], Bound: 0.3813190162181854, Entropy: 140.89952087402344, Temp: 2.6097524166107178, KL: 72.48184204101562, Loss: 0.01599005237221718, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7700/20000], Bound: 0.39646658301353455, Entropy: 140.62615966796875, Temp: 2.609915256500244, KL: 74.9749755859375, Loss: 0.019621239975094795, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7701/20000], Bound: 0.35232457518577576, Entropy: 140.7283172607422, Temp: 2.6100399494171143, KL: 62.87550354003906, Loss: 0.018803251907229424, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7702/20000], Bound: 0.3728105425834656, Entropy: 140.58705139160156, Temp: 2.6100878715515137, KL: 69.14520263671875, Loss: 0.01774359866976738, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7703/20000], Bound: 0.37949812412261963, Entropy: 141.06637573242188, Temp: 2.610121250152588, KL: 71.93717956542969, Loss: 0.01603912003338337, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7704/20000], Bound: 0.37304869294166565, Entropy: 140.593505859375, Temp: 2.610192060470581, KL: 70.5948486328125, Loss: 0.015096835792064667, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7705/20000], Bound: 0.3860776424407959, Entropy: 140.86273193359375, Temp: 2.6103103160858154, KL: 72.57791137695312, Loss: 0.01843242533504963, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7706/20000], Bound: 0.3866214156150818, Entropy: 141.72171020507812, Temp: 2.6104087829589844, KL: 72.01043701171875, Loss: 0.019820965826511383, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7707/20000], Bound: 0.3942822813987732, Entropy: 138.68930053710938, Temp: 2.6104562282562256, KL: 76.08283996582031, Loss: 0.0162813737988472, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7708/20000], Bound: 0.3828698396682739, Entropy: 142.2357635498047, Temp: 2.610553741455078, KL: 71.796630859375, Loss: 0.018162844702601433, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7709/20000], Bound: 0.42156463861465454, Entropy: 139.7200164794922, Temp: 2.610635280609131, KL: 82.53138732910156, Loss: 0.019500331953167915, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7710/20000], Bound: 0.40098538994789124, Entropy: 141.72662353515625, Temp: 2.610724449157715, KL: 77.30393981933594, Loss: 0.017712289467453957, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7711/20000], Bound: 0.39185893535614014, Entropy: 142.97413635253906, Temp: 2.6108345985412598, KL: 73.87965393066406, Loss: 0.01915232464671135, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7712/20000], Bound: 0.36513587832450867, Entropy: 142.51107788085938, Temp: 2.6109158992767334, KL: 66.01881408691406, Loss: 0.019600171595811844, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7713/20000], Bound: 0.39294499158859253, Entropy: 141.76573181152344, Temp: 2.6109228134155273, KL: 76.1641845703125, Loss: 0.015383887104690075, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7714/20000], Bound: 0.3967496156692505, Entropy: 141.43801879882812, Temp: 2.6110033988952637, KL: 75.97978210449219, Loss: 0.017866890877485275, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7715/20000], Bound: 0.4059322476387024, Entropy: 140.09857177734375, Temp: 2.611095905303955, KL: 79.47650146484375, Loss: 0.016359588131308556, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7716/20000], Bound: 0.3832550048828125, Entropy: 142.3021697998047, Temp: 2.611248016357422, KL: 71.22535705566406, Loss: 0.01947532407939434, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7717/20000], Bound: 0.4025721848011017, Entropy: 142.0561065673828, Temp: 2.611346960067749, KL: 76.34103393554688, Loss: 0.020459897816181183, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7718/20000], Bound: 0.3825671076774597, Entropy: 142.15858459472656, Temp: 2.6114017963409424, KL: 71.66787719726562, Loss: 0.018251197412610054, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7719/20000], Bound: 0.3917708098888397, Entropy: 141.1500244140625, Temp: 2.6114416122436523, KL: 73.10060119628906, Loss: 0.020600952208042145, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7720/20000], Bound: 0.3765435516834259, Entropy: 143.51808166503906, Temp: 2.611424446105957, KL: 70.90046691894531, Loss: 0.016424089670181274, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7721/20000], Bound: 0.3726480007171631, Entropy: 144.60360717773438, Temp: 2.611435890197754, KL: 68.34506225585938, Loss: 0.019199971109628677, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7722/20000], Bound: 0.3738325238227844, Entropy: 139.69140625, Temp: 2.6113998889923096, KL: 67.53237915039062, Loss: 0.021398134529590607, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7723/20000], Bound: 0.41796985268592834, Entropy: 143.04930114746094, Temp: 2.611269235610962, KL: 81.39527893066406, Loss: 0.019595924764871597, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7724/20000], Bound: 0.3865588903427124, Entropy: 142.23056030273438, Temp: 2.611159563064575, KL: 73.88059997558594, Loss: 0.016212334856390953, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7725/20000], Bound: 0.36647793650627136, Entropy: 142.15846252441406, Temp: 2.6111068725585938, KL: 67.86993408203125, Loss: 0.016777403652668, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7726/20000], Bound: 0.3764980137348175, Entropy: 143.48489379882812, Temp: 2.6110639572143555, KL: 68.12245178222656, Loss: 0.02171538583934307, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7727/20000], Bound: 0.3693772554397583, Entropy: 143.2139434814453, Temp: 2.6109230518341064, KL: 68.1976318359375, Loss: 0.017709197476506233, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7728/20000], Bound: 0.38419482111930847, Entropy: 142.831298828125, Temp: 2.6107823848724365, KL: 70.43608093261719, Loss: 0.021500105038285255, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7729/20000], Bound: 0.3932987451553345, Entropy: 141.8369598388672, Temp: 2.610569715499878, KL: 75.36395263671875, Loss: 0.017109951004385948, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7730/20000], Bound: 0.43239545822143555, Entropy: 142.59654235839844, Temp: 2.610412120819092, KL: 86.34835815429688, Loss: 0.018546873703598976, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7731/20000], Bound: 0.36843714118003845, Entropy: 142.47755432128906, Temp: 2.6103241443634033, KL: 68.51414489746094, Loss: 0.016590576618909836, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7732/20000], Bound: 0.40358585119247437, Entropy: 142.15562438964844, Temp: 2.6102569103240967, KL: 74.42817687988281, Loss: 0.02468705363571644, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7733/20000], Bound: 0.4000921845436096, Entropy: 143.27590942382812, Temp: 2.610062837600708, KL: 75.85868835449219, Loss: 0.01996937394142151, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7734/20000], Bound: 0.37775084376335144, Entropy: 142.35568237304688, Temp: 2.6098623275756836, KL: 69.8485107421875, Loss: 0.019082538783550262, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7735/20000], Bound: 0.40143871307373047, Entropy: 142.8554229736328, Temp: 2.6096465587615967, KL: 75.88822937011719, Loss: 0.02066882513463497, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7736/20000], Bound: 0.3973791003227234, Entropy: 141.2340850830078, Temp: 2.6094117164611816, KL: 76.75653076171875, Loss: 0.016714846715331078, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7737/20000], Bound: 0.38384947180747986, Entropy: 143.5590057373047, Temp: 2.6092498302459717, KL: 71.80784606933594, Loss: 0.018667982891201973, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7738/20000], Bound: 0.39911484718322754, Entropy: 143.26144409179688, Temp: 2.6090872287750244, KL: 77.17488098144531, Loss: 0.0168860275298357, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7739/20000], Bound: 0.3948935270309448, Entropy: 140.1083221435547, Temp: 2.6089887619018555, KL: 75.50601196289062, Loss: 0.017712373286485672, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7740/20000], Bound: 0.3916514813899994, Entropy: 141.76763916015625, Temp: 2.6089224815368652, KL: 74.6219482421875, Loss: 0.017594970762729645, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7741/20000], Bound: 0.37138715386390686, Entropy: 143.55325317382812, Temp: 2.6088831424713135, KL: 68.75534057617188, Loss: 0.01770833320915699, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7742/20000], Bound: 0.4048006236553192, Entropy: 143.14170837402344, Temp: 2.608837127685547, KL: 78.4591064453125, Loss: 0.017638757824897766, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7743/20000], Bound: 0.3999880850315094, Entropy: 143.91363525390625, Temp: 2.6088335514068604, KL: 75.25566101074219, Loss: 0.021053766831755638, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7744/20000], Bound: 0.40508103370666504, Entropy: 142.8069610595703, Temp: 2.608779191970825, KL: 77.99888610839844, Loss: 0.01867947354912758, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7745/20000], Bound: 0.3950950503349304, Entropy: 142.5841522216797, Temp: 2.6087429523468018, KL: 75.628173828125, Loss: 0.017588447779417038, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7746/20000], Bound: 0.3962435722351074, Entropy: 141.3569793701172, Temp: 2.608736276626587, KL: 74.2872314453125, Loss: 0.02080230414867401, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7747/20000], Bound: 0.4288748502731323, Entropy: 139.53292846679688, Temp: 2.6086792945861816, KL: 84.76730346679688, Loss: 0.019474385306239128, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7748/20000], Bound: 0.37537726759910583, Entropy: 141.53302001953125, Temp: 2.608656167984009, KL: 70.16061401367188, Loss: 0.01717899926006794, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7749/20000], Bound: 0.40091145038604736, Entropy: 142.0145721435547, Temp: 2.6086432933807373, KL: 77.44573974609375, Loss: 0.01737542264163494, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7750/20000], Bound: 0.40974169969558716, Entropy: 140.92990112304688, Temp: 2.608670711517334, KL: 80.18682861328125, Loss: 0.017143042758107185, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7751/20000], Bound: 0.3975300192832947, Entropy: 140.3878631591797, Temp: 2.6087522506713867, KL: 75.17674255371094, Loss: 0.01982002519071102, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7752/20000], Bound: 0.42168545722961426, Entropy: 139.55027770996094, Temp: 2.6088006496429443, KL: 83.89151000976562, Loss: 0.016941871494054794, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7753/20000], Bound: 0.3995833694934845, Entropy: 140.64505004882812, Temp: 2.6089224815368652, KL: 76.20542907714844, Loss: 0.019006187096238136, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7754/20000], Bound: 0.38264328241348267, Entropy: 142.6378631591797, Temp: 2.609029769897461, KL: 72.14482116699219, Loss: 0.017356066033244133, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7755/20000], Bound: 0.39933690428733826, Entropy: 139.5235137939453, Temp: 2.609139919281006, KL: 74.45199584960938, Loss: 0.022229718044400215, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7756/20000], Bound: 0.39404550194740295, Entropy: 139.514892578125, Temp: 2.6091580390930176, KL: 76.0498046875, Loss: 0.016197605058550835, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7757/20000], Bound: 0.40125980973243713, Entropy: 140.32286071777344, Temp: 2.609231948852539, KL: 75.41844177246094, Loss: 0.021463798359036446, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7758/20000], Bound: 0.4161413013935089, Entropy: 140.870361328125, Temp: 2.609238862991333, KL: 81.23516845703125, Loss: 0.0188209880143404, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7759/20000], Bound: 0.37729576230049133, Entropy: 140.18804931640625, Temp: 2.6092703342437744, KL: 68.8663330078125, Loss: 0.020710811018943787, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7760/20000], Bound: 0.4035373628139496, Entropy: 139.4285888671875, Temp: 2.609222173690796, KL: 77.09429931640625, Loss: 0.019541531801223755, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7761/20000], Bound: 0.4068434238433838, Entropy: 136.80490112304688, Temp: 2.609168767929077, KL: 78.38096618652344, Loss: 0.018954528495669365, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7762/20000], Bound: 0.3667263984680176, Entropy: 139.06858825683594, Temp: 2.6091296672821045, KL: 67.20808410644531, Loss: 0.018161091953516006, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7763/20000], Bound: 0.37362343072891235, Entropy: 139.72654724121094, Temp: 2.6090657711029053, KL: 68.25100708007812, Loss: 0.019888954237103462, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7764/20000], Bound: 0.39730414748191833, Entropy: 139.49620056152344, Temp: 2.6089470386505127, KL: 76.8970947265625, Loss: 0.016398044303059578, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7765/20000], Bound: 0.39929288625717163, Entropy: 139.8473358154297, Temp: 2.6088976860046387, KL: 77.92897033691406, Loss: 0.015538985840976238, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7766/20000], Bound: 0.3878840506076813, Entropy: 140.341796875, Temp: 2.6089348793029785, KL: 72.65390014648438, Loss: 0.01927308924496174, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7767/20000], Bound: 0.37559977173805237, Entropy: 141.15272521972656, Temp: 2.6089425086975098, KL: 71.51719665527344, Loss: 0.014703035354614258, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7768/20000], Bound: 0.3761846721172333, Entropy: 139.92059326171875, Temp: 2.6090188026428223, KL: 71.72758483886719, Loss: 0.014619315974414349, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7769/20000], Bound: 0.4145893454551697, Entropy: 138.6815643310547, Temp: 2.6091599464416504, KL: 80.27296447753906, Loss: 0.01976822316646576, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7770/20000], Bound: 0.4197195768356323, Entropy: 140.29153442382812, Temp: 2.6092875003814697, KL: 81.29904174804688, Loss: 0.02077227458357811, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7771/20000], Bound: 0.3908834457397461, Entropy: 140.54541015625, Temp: 2.6093857288360596, KL: 73.09832763671875, Loss: 0.02009166032075882, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7772/20000], Bound: 0.39107745885849, Entropy: 140.64828491210938, Temp: 2.6094324588775635, KL: 75.156982421875, Loss: 0.016255466267466545, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7773/20000], Bound: 0.39739277958869934, Entropy: 139.60813903808594, Temp: 2.6095268726348877, KL: 76.4083251953125, Loss: 0.017391016706824303, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7774/20000], Bound: 0.40239593386650085, Entropy: 139.01284790039062, Temp: 2.6096456050872803, KL: 76.72383117675781, Loss: 0.01960925944149494, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7775/20000], Bound: 0.37893015146255493, Entropy: 141.29354858398438, Temp: 2.609738826751709, KL: 70.52314758300781, Loss: 0.018433494493365288, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7776/20000], Bound: 0.4106874465942383, Entropy: 139.04180908203125, Temp: 2.609804391860962, KL: 79.83076477050781, Loss: 0.01838039979338646, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7777/20000], Bound: 0.41212254762649536, Entropy: 140.9618377685547, Temp: 2.609891414642334, KL: 81.75959777832031, Loss: 0.015509350225329399, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7778/20000], Bound: 0.39049211144447327, Entropy: 140.83290100097656, Temp: 2.610069513320923, KL: 72.98616027832031, Loss: 0.02009543403983116, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7779/20000], Bound: 0.38447415828704834, Entropy: 140.58184814453125, Temp: 2.61018705368042, KL: 72.10771179199219, Loss: 0.018446870148181915, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7780/20000], Bound: 0.3977356553077698, Entropy: 139.04432678222656, Temp: 2.6102817058563232, KL: 75.608642578125, Loss: 0.019123824313282967, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7781/20000], Bound: 0.37774959206581116, Entropy: 140.33383178710938, Temp: 2.610358238220215, KL: 69.69281005859375, Loss: 0.019384630024433136, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7782/20000], Bound: 0.4083424210548401, Entropy: 140.2331085205078, Temp: 2.6103832721710205, KL: 78.9014892578125, Loss: 0.01882590912282467, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7783/20000], Bound: 0.430454820394516, Entropy: 139.44418334960938, Temp: 2.610419511795044, KL: 85.48454284667969, Loss: 0.01905415579676628, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7784/20000], Bound: 0.4103543162345886, Entropy: 138.85476684570312, Temp: 2.610491991043091, KL: 80.96697998046875, Loss: 0.016021355986595154, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7785/20000], Bound: 0.39273372292518616, Entropy: 140.89894104003906, Temp: 2.6106419563293457, KL: 75.57295227050781, Loss: 0.01639508455991745, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7786/20000], Bound: 0.3960030972957611, Entropy: 136.74853515625, Temp: 2.610827922821045, KL: 74.86457824707031, Loss: 0.019582044333219528, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7787/20000], Bound: 0.3664274215698242, Entropy: 141.02178955078125, Temp: 2.6109726428985596, KL: 67.20863342285156, Loss: 0.018015434965491295, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7788/20000], Bound: 0.4059794247150421, Entropy: 139.31643676757812, Temp: 2.611076831817627, KL: 77.01399230957031, Loss: 0.02110164985060692, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7789/20000], Bound: 0.41268542408943176, Entropy: 137.78961181640625, Temp: 2.6111247539520264, KL: 79.77682495117188, Loss: 0.0196452084928751, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7790/20000], Bound: 0.36988139152526855, Entropy: 140.06907653808594, Temp: 2.6111676692962646, KL: 69.44880676269531, Loss: 0.015587612055242062, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7791/20000], Bound: 0.37690746784210205, Entropy: 141.4611053466797, Temp: 2.6112451553344727, KL: 70.47955322265625, Loss: 0.01742664724588394, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7792/20000], Bound: 0.3980962336063385, Entropy: 139.129150390625, Temp: 2.611318349838257, KL: 76.97352600097656, Loss: 0.016723819077014923, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7793/20000], Bound: 0.3738584816455841, Entropy: 139.90573120117188, Temp: 2.6114344596862793, KL: 69.54829406738281, Loss: 0.017552759498357773, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7794/20000], Bound: 0.3894849717617035, Entropy: 139.79429626464844, Temp: 2.611534357070923, KL: 72.90202331542969, Loss: 0.019710905849933624, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7795/20000], Bound: 0.41572120785713196, Entropy: 139.22108459472656, Temp: 2.611588478088379, KL: 80.54988098144531, Loss: 0.019918298348784447, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7796/20000], Bound: 0.39268332719802856, Entropy: 140.84255981445312, Temp: 2.6116344928741455, KL: 75.05604553222656, Loss: 0.01736760325729847, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7797/20000], Bound: 0.3891223967075348, Entropy: 138.31869506835938, Temp: 2.6117026805877686, KL: 74.39791870117188, Loss: 0.016647499054670334, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7798/20000], Bound: 0.38514888286590576, Entropy: 141.2779541015625, Temp: 2.6118032932281494, KL: 72.52784729003906, Loss: 0.0180303193628788, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7799/20000], Bound: 0.41031453013420105, Entropy: 138.6294403076172, Temp: 2.61189341545105, KL: 79.22874450683594, Loss: 0.019343610852956772, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7800/20000], Bound: 0.42630141973495483, Entropy: 139.37921142578125, Temp: 2.6119778156280518, KL: 84.8511962890625, Loss: 0.017843270674347878, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7801/20000], Bound: 0.3912270665168762, Entropy: 139.6157684326172, Temp: 2.612116575241089, KL: 73.79313659667969, Loss: 0.018978940322995186, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7802/20000], Bound: 0.3914047181606293, Entropy: 140.93484497070312, Temp: 2.6122260093688965, KL: 73.28773498535156, Loss: 0.020046310499310493, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7803/20000], Bound: 0.3844259977340698, Entropy: 143.3365936279297, Temp: 2.6122827529907227, KL: 72.09303283691406, Loss: 0.018468797206878662, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7804/20000], Bound: 0.4214954078197479, Entropy: 140.91851806640625, Temp: 2.612321138381958, KL: 83.34573364257812, Loss: 0.017921559512615204, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7805/20000], Bound: 0.39437004923820496, Entropy: 141.55276489257812, Temp: 2.6124095916748047, KL: 73.50112915039062, Loss: 0.021293576806783676, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7806/20000], Bound: 0.39362064003944397, Entropy: 138.8356475830078, Temp: 2.6124207973480225, KL: 74.74856567382812, Loss: 0.018487535417079926, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7807/20000], Bound: 0.4038829803466797, Entropy: 140.9203338623047, Temp: 2.6124305725097656, KL: 78.69094848632812, Loss: 0.016715658828616142, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7808/20000], Bound: 0.42439886927604675, Entropy: 139.74900817871094, Temp: 2.6124980449676514, KL: 83.63795471191406, Loss: 0.019058138132095337, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7809/20000], Bound: 0.4310007691383362, Entropy: 136.53973388671875, Temp: 2.612589120864868, KL: 86.69764709472656, Loss: 0.01708259992301464, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7810/20000], Bound: 0.40770068764686584, Entropy: 139.6722412109375, Temp: 2.6127588748931885, KL: 78.82966613769531, Loss: 0.018624110147356987, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7811/20000], Bound: 0.37071493268013, Entropy: 140.47621154785156, Temp: 2.6129281520843506, KL: 69.02574157714844, Loss: 0.016864575445652008, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7812/20000], Bound: 0.40065014362335205, Entropy: 138.65728759765625, Temp: 2.613088369369507, KL: 76.10317993164062, Loss: 0.019847095012664795, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7813/20000], Bound: 0.37802621722221375, Entropy: 140.64260864257812, Temp: 2.613208770751953, KL: 70.640625, Loss: 0.017747482284903526, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7814/20000], Bound: 0.37157881259918213, Entropy: 141.05197143554688, Temp: 2.6133129596710205, KL: 67.9583740234375, Loss: 0.01937730796635151, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7815/20000], Bound: 0.40291401743888855, Entropy: 138.73370361328125, Temp: 2.613352060317993, KL: 77.17848205566406, Loss: 0.019071487709879875, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7816/20000], Bound: 0.3724428713321686, Entropy: 138.82620239257812, Temp: 2.6133859157562256, KL: 70.92176818847656, Loss: 0.01417586486786604, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7817/20000], Bound: 0.39591410756111145, Entropy: 141.63121032714844, Temp: 2.6134939193725586, KL: 74.89933776855469, Loss: 0.01949257031083107, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7818/20000], Bound: 0.3827481269836426, Entropy: 140.0217742919922, Temp: 2.6135690212249756, KL: 71.8646240234375, Loss: 0.017995178699493408, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7819/20000], Bound: 0.3753441274166107, Entropy: 140.34182739257812, Temp: 2.6136326789855957, KL: 70.26771545410156, Loss: 0.01700386218726635, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7820/20000], Bound: 0.4112972021102905, Entropy: 138.41542053222656, Temp: 2.6137003898620605, KL: 79.50675964355469, Loss: 0.019394569098949432, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7821/20000], Bound: 0.39026516675949097, Entropy: 140.69287109375, Temp: 2.6137642860412598, KL: 73.89756774902344, Loss: 0.01826094463467598, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7822/20000], Bound: 0.37996935844421387, Entropy: 140.80877685546875, Temp: 2.613821506500244, KL: 71.96591186523438, Loss: 0.016280105337500572, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7823/20000], Bound: 0.39843761920928955, Entropy: 139.99171447753906, Temp: 2.613908529281616, KL: 75.73820495605469, Loss: 0.019307952374219894, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7824/20000], Bound: 0.4283224642276764, Entropy: 140.0947265625, Temp: 2.6139724254608154, KL: 83.76300048828125, Loss: 0.02113693207502365, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7825/20000], Bound: 0.38473406434059143, Entropy: 137.29600524902344, Temp: 2.6140146255493164, KL: 71.0660400390625, Loss: 0.02061970718204975, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7826/20000], Bound: 0.38200509548187256, Entropy: 139.16769409179688, Temp: 2.613985538482666, KL: 71.84555053710938, Loss: 0.01762767881155014, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7827/20000], Bound: 0.3708302676677704, Entropy: 140.48269653320312, Temp: 2.6139633655548096, KL: 68.27520751953125, Loss: 0.018372198566794395, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7828/20000], Bound: 0.4096769392490387, Entropy: 140.8572998046875, Temp: 2.613912582397461, KL: 80.0274658203125, Loss: 0.01747378706932068, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7829/20000], Bound: 0.40161654353141785, Entropy: 137.81809997558594, Temp: 2.6139144897460938, KL: 76.85087585449219, Loss: 0.018970737233757973, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7830/20000], Bound: 0.43062007427215576, Entropy: 142.18589782714844, Temp: 2.613915205001831, KL: 85.89082336425781, Loss: 0.0184192955493927, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7831/20000], Bound: 0.36211249232292175, Entropy: 139.1399383544922, Temp: 2.6139702796936035, KL: 65.98423767089844, Loss: 0.018073944374918938, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7832/20000], Bound: 0.3800099492073059, Entropy: 140.33251953125, Temp: 2.6139841079711914, KL: 70.0296630859375, Loss: 0.02000764012336731, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7833/20000], Bound: 0.3538917601108551, Entropy: 139.32249450683594, Temp: 2.6139373779296875, KL: 63.74066162109375, Loss: 0.018003473058342934, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7834/20000], Bound: 0.3729528784751892, Entropy: 141.65292358398438, Temp: 2.613849639892578, KL: 67.47465515136719, Loss: 0.02105094864964485, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7835/20000], Bound: 0.3964802026748657, Entropy: 139.9286346435547, Temp: 2.613675117492676, KL: 77.230712890625, Loss: 0.015351593494415283, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7836/20000], Bound: 0.37791189551353455, Entropy: 139.82872009277344, Temp: 2.613600015640259, KL: 70.3692626953125, Loss: 0.018207944929599762, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7837/20000], Bound: 0.4130765199661255, Entropy: 138.7427978515625, Temp: 2.613515853881836, KL: 77.62559509277344, Loss: 0.024012304842472076, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7838/20000], Bound: 0.4084899425506592, Entropy: 140.49761962890625, Temp: 2.6133315563201904, KL: 78.46746826171875, Loss: 0.019773637875914574, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7839/20000], Bound: 0.3616339862346649, Entropy: 139.1051788330078, Temp: 2.613154649734497, KL: 66.08546447753906, Loss: 0.017618102952837944, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7840/20000], Bound: 0.3975348472595215, Entropy: 140.63601684570312, Temp: 2.6129703521728516, KL: 76.04319763183594, Loss: 0.018207399174571037, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7841/20000], Bound: 0.38195690512657166, Entropy: 139.90760803222656, Temp: 2.6128170490264893, KL: 70.95149230957031, Loss: 0.019300635904073715, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7842/20000], Bound: 0.4124279022216797, Entropy: 141.642578125, Temp: 2.6126415729522705, KL: 79.73744201660156, Loss: 0.019589798524975777, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7843/20000], Bound: 0.39198315143585205, Entropy: 142.7566680908203, Temp: 2.612483263015747, KL: 75.30477905273438, Loss: 0.01651042141020298, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7844/20000], Bound: 0.3975245952606201, Entropy: 140.73402404785156, Temp: 2.6123881340026855, KL: 76.72195434570312, Loss: 0.01689632423222065, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7845/20000], Bound: 0.38978299498558044, Entropy: 141.1066436767578, Temp: 2.6123478412628174, KL: 74.71788024902344, Loss: 0.01640862040221691, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7846/20000], Bound: 0.3763192296028137, Entropy: 139.87619018554688, Temp: 2.612358570098877, KL: 68.89378356933594, Loss: 0.020151956006884575, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7847/20000], Bound: 0.3668611943721771, Entropy: 142.9300537109375, Temp: 2.6123006343841553, KL: 68.36053466796875, Loss: 0.016055233776569366, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7848/20000], Bound: 0.36907246708869934, Entropy: 141.5883331298828, Temp: 2.612271547317505, KL: 68.5838623046875, Loss: 0.016817716881632805, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7849/20000], Bound: 0.4065358638763428, Entropy: 138.90914916992188, Temp: 2.6122524738311768, KL: 78.26475524902344, Loss: 0.019036170095205307, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7850/20000], Bound: 0.38899126648902893, Entropy: 141.56802368164062, Temp: 2.6122403144836426, KL: 73.31939697265625, Loss: 0.01864486001431942, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7851/20000], Bound: 0.39391735196113586, Entropy: 142.99078369140625, Temp: 2.6122188568115234, KL: 72.21240234375, Loss: 0.02350560389459133, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7852/20000], Bound: 0.3715357780456543, Entropy: 140.58563232421875, Temp: 2.6120738983154297, KL: 68.74208068847656, Loss: 0.01784331537783146, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7853/20000], Bound: 0.38781794905662537, Entropy: 142.3992919921875, Temp: 2.6119277477264404, KL: 72.6348876953125, Loss: 0.01930181123316288, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7854/20000], Bound: 0.38481393456459045, Entropy: 141.758056640625, Temp: 2.611767292022705, KL: 72.36981201171875, Loss: 0.0181477889418602, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7855/20000], Bound: 0.40111058950424194, Entropy: 142.87413024902344, Temp: 2.611618757247925, KL: 75.52067565917969, Loss: 0.021207069978117943, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7856/20000], Bound: 0.40427327156066895, Entropy: 141.49801635742188, Temp: 2.6114282608032227, KL: 76.73660278320312, Loss: 0.02066720277070999, Learning Rate: 0.0008474257469999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7857/20000], Bound: 0.40177109837532043, Entropy: 142.8722686767578, Temp: 2.611218214035034, KL: 77.27883911132812, Loss: 0.01820976287126541, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7858/20000], Bound: 0.3811700642108917, Entropy: 139.63983154296875, Temp: 2.611048698425293, KL: 71.50025939941406, Loss: 0.017801566049456596, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7859/20000], Bound: 0.37435758113861084, Entropy: 142.92835998535156, Temp: 2.610895872116089, KL: 67.77366638183594, Loss: 0.02121729403734207, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7860/20000], Bound: 0.41036662459373474, Entropy: 140.36170959472656, Temp: 2.610661268234253, KL: 80.75570678710938, Loss: 0.01643514633178711, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7861/20000], Bound: 0.36922162771224976, Entropy: 141.2587432861328, Temp: 2.6105265617370605, KL: 70.48143005371094, Loss: 0.013247436843812466, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7862/20000], Bound: 0.3869258463382721, Entropy: 141.22227478027344, Temp: 2.6105048656463623, KL: 73.32142639160156, Loss: 0.01747928000986576, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7863/20000], Bound: 0.4315822720527649, Entropy: 143.70176696777344, Temp: 2.610501766204834, KL: 86.23561096191406, Loss: 0.01828286610543728, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7864/20000], Bound: 0.3709797263145447, Entropy: 142.1390838623047, Temp: 2.610560417175293, KL: 67.8460693359375, Loss: 0.01924484223127365, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7865/20000], Bound: 0.4030851125717163, Entropy: 140.78997802734375, Temp: 2.6105616092681885, KL: 77.75096130371094, Loss: 0.01804185099899769, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7866/20000], Bound: 0.3901512324810028, Entropy: 141.92149353027344, Temp: 2.610588550567627, KL: 75.69186401367188, Loss: 0.014728627167642117, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7867/20000], Bound: 0.4040297269821167, Entropy: 142.36068725585938, Temp: 2.610703706741333, KL: 75.82926940917969, Loss: 0.02225930616259575, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7868/20000], Bound: 0.38214239478111267, Entropy: 141.76864624023438, Temp: 2.6107287406921387, KL: 72.17631530761719, Loss: 0.01703753136098385, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7869/20000], Bound: 0.3754206597805023, Entropy: 141.77960205078125, Temp: 2.6107723712921143, KL: 70.29095458984375, Loss: 0.016973374411463737, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7870/20000], Bound: 0.3993993401527405, Entropy: 142.72811889648438, Temp: 2.6108243465423584, KL: 77.76553344726562, Loss: 0.015934864059090614, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7871/20000], Bound: 0.39079225063323975, Entropy: 141.2733917236328, Temp: 2.6109442710876465, KL: 75.12106323242188, Loss: 0.01618228107690811, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7872/20000], Bound: 0.39332878589630127, Entropy: 141.5272674560547, Temp: 2.611107349395752, KL: 74.62704467773438, Loss: 0.01854364573955536, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7873/20000], Bound: 0.3943626284599304, Entropy: 142.96224975585938, Temp: 2.611253023147583, KL: 75.55546569824219, Loss: 0.01734505407512188, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7874/20000], Bound: 0.38406991958618164, Entropy: 140.39651489257812, Temp: 2.6114144325256348, KL: 72.18852233886719, Loss: 0.018081387504935265, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7875/20000], Bound: 0.4031245708465576, Entropy: 142.17095947265625, Temp: 2.6115567684173584, KL: 77.77345275878906, Loss: 0.01803227886557579, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7876/20000], Bound: 0.3819791078567505, Entropy: 141.02548217773438, Temp: 2.611710786819458, KL: 71.87947082519531, Loss: 0.01752602867782116, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7877/20000], Bound: 0.40789949893951416, Entropy: 140.0492401123047, Temp: 2.6118569374084473, KL: 79.9349365234375, Loss: 0.016611430794000626, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7878/20000], Bound: 0.39662376046180725, Entropy: 139.65066528320312, Temp: 2.6120569705963135, KL: 76.70033264160156, Loss: 0.016428399831056595, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7879/20000], Bound: 0.43561914563179016, Entropy: 140.17726135253906, Temp: 2.6122934818267822, KL: 88.28190612792969, Loss: 0.016783608123660088, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7880/20000], Bound: 0.3958408832550049, Entropy: 137.42996215820312, Temp: 2.612610101699829, KL: 75.35421752929688, Loss: 0.01857205107808113, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7881/20000], Bound: 0.40381577610969543, Entropy: 139.32554626464844, Temp: 2.612896203994751, KL: 76.63935852050781, Loss: 0.02060895785689354, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7882/20000], Bound: 0.37218207120895386, Entropy: 139.0875244140625, Temp: 2.6131153106689453, KL: 70.98635864257812, Loss: 0.013908237218856812, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7883/20000], Bound: 0.4078312814235687, Entropy: 138.0841827392578, Temp: 2.6133978366851807, KL: 78.82241821289062, Loss: 0.018719637766480446, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7884/20000], Bound: 0.3860948383808136, Entropy: 139.06655883789062, Temp: 2.6136670112609863, KL: 73.04640197753906, Loss: 0.017578724771738052, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7885/20000], Bound: 0.3888670802116394, Entropy: 140.73948669433594, Temp: 2.6139206886291504, KL: 73.73249816894531, Loss: 0.017802512273192406, Learning Rate: 0.0008474257469999995\n",
      "Epoch [7886/20000], Bound: 0.35593029856681824, Entropy: 139.29612731933594, Temp: 2.6140875816345215, KL: 63.732025146484375, Loss: 0.019098127260804176, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7887/20000], Bound: 0.4172619581222534, Entropy: 138.5924835205078, Temp: 2.614187240600586, KL: 80.91766357421875, Loss: 0.02013397216796875, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7888/20000], Bound: 0.37452712655067444, Entropy: 139.77113342285156, Temp: 2.614271879196167, KL: 70.10902404785156, Loss: 0.01686963438987732, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7889/20000], Bound: 0.4040086567401886, Entropy: 138.37290954589844, Temp: 2.6143569946289062, KL: 77.25888061523438, Loss: 0.019548317417502403, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7890/20000], Bound: 0.3756304979324341, Entropy: 141.0597686767578, Temp: 2.614424705505371, KL: 69.72218322753906, Loss: 0.018210511654615402, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7891/20000], Bound: 0.39125293493270874, Entropy: 138.72976684570312, Temp: 2.614471673965454, KL: 73.41069030761719, Loss: 0.0197481419891119, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7892/20000], Bound: 0.3961513638496399, Entropy: 139.37156677246094, Temp: 2.614488124847412, KL: 76.24795532226562, Loss: 0.017056312412023544, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7893/20000], Bound: 0.3672724664211273, Entropy: 139.60894775390625, Temp: 2.614530324935913, KL: 67.45530700683594, Loss: 0.01802840642631054, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7894/20000], Bound: 0.37087950110435486, Entropy: 139.31134033203125, Temp: 2.614548921585083, KL: 68.39411926269531, Loss: 0.018176525831222534, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7895/20000], Bound: 0.4014572501182556, Entropy: 139.5210723876953, Temp: 2.61454701423645, KL: 75.944580078125, Loss: 0.020620722323656082, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7896/20000], Bound: 0.4037460684776306, Entropy: 138.0546417236328, Temp: 2.6145145893096924, KL: 78.74394226074219, Loss: 0.016561178490519524, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7897/20000], Bound: 0.3591267466545105, Entropy: 143.24485778808594, Temp: 2.614529609680176, KL: 67.49873352050781, Loss: 0.013592641800642014, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7898/20000], Bound: 0.3849271535873413, Entropy: 141.54649353027344, Temp: 2.614595651626587, KL: 69.871826171875, Loss: 0.02301497384905815, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7899/20000], Bound: 0.39308419823646545, Entropy: 140.53662109375, Temp: 2.6145641803741455, KL: 72.62567138671875, Loss: 0.022270092740654945, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7900/20000], Bound: 0.33367958664894104, Entropy: 140.2056427001953, Temp: 2.6144673824310303, KL: 59.337982177734375, Loss: 0.015911487862467766, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7901/20000], Bound: 0.3891250193119049, Entropy: 141.734130859375, Temp: 2.614365339279175, KL: 73.03813171386719, Loss: 0.019278010353446007, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7902/20000], Bound: 0.4060294032096863, Entropy: 138.2834930419922, Temp: 2.6142539978027344, KL: 78.79104614257812, Loss: 0.017763648182153702, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7903/20000], Bound: 0.3742522895336151, Entropy: 143.16070556640625, Temp: 2.6141786575317383, KL: 69.12606811523438, Loss: 0.018599633127450943, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7904/20000], Bound: 0.3897061347961426, Entropy: 141.599853515625, Temp: 2.614088296890259, KL: 74.69046020507812, Loss: 0.016437267884612083, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7905/20000], Bound: 0.42389214038848877, Entropy: 140.08921813964844, Temp: 2.6140389442443848, KL: 83.80319213867188, Loss: 0.01846507377922535, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7906/20000], Bound: 0.387579083442688, Entropy: 141.54281616210938, Temp: 2.614025592803955, KL: 70.93417358398438, Loss: 0.022442827001214027, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7907/20000], Bound: 0.38414767384529114, Entropy: 142.81202697753906, Temp: 2.613935947418213, KL: 71.1046142578125, Loss: 0.020222239196300507, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7908/20000], Bound: 0.37793856859207153, Entropy: 142.7318572998047, Temp: 2.613813877105713, KL: 70.14324951171875, Loss: 0.018656840547919273, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7909/20000], Bound: 0.36660075187683105, Entropy: 143.65452575683594, Temp: 2.6136841773986816, KL: 66.02391052246094, Loss: 0.020398389548063278, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7910/20000], Bound: 0.39487215876579285, Entropy: 143.4917449951172, Temp: 2.613504409790039, KL: 74.17991638183594, Loss: 0.020285887643694878, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7911/20000], Bound: 0.3726656138896942, Entropy: 142.26348876953125, Temp: 2.613311529159546, KL: 68.77015686035156, Loss: 0.018412362784147263, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7912/20000], Bound: 0.37881776690483093, Entropy: 143.14093017578125, Temp: 2.613117218017578, KL: 71.24240112304688, Loss: 0.017027458176016808, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7913/20000], Bound: 0.40711677074432373, Entropy: 140.2553253173828, Temp: 2.612952947616577, KL: 79.84642028808594, Loss: 0.016347987577319145, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7914/20000], Bound: 0.4330872595310211, Entropy: 139.83421325683594, Temp: 2.612856864929199, KL: 86.64382934570312, Loss: 0.018423298373818398, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7915/20000], Bound: 0.3876207768917084, Entropy: 140.41510009765625, Temp: 2.6128122806549072, KL: 72.79989624023438, Loss: 0.018885424360632896, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7916/20000], Bound: 0.4127301275730133, Entropy: 141.42550659179688, Temp: 2.6127586364746094, KL: 79.5955810546875, Loss: 0.020036105066537857, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7917/20000], Bound: 0.3953513503074646, Entropy: 142.81982421875, Temp: 2.6127023696899414, KL: 76.25706481933594, Loss: 0.016571098938584328, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7918/20000], Bound: 0.3837226331233978, Entropy: 141.90438842773438, Temp: 2.612687826156616, KL: 72.68031311035156, Loss: 0.016961555927991867, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7919/20000], Bound: 0.40154892206192017, Entropy: 142.18368530273438, Temp: 2.612691879272461, KL: 75.82255554199219, Loss: 0.020887402817606926, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7920/20000], Bound: 0.3670867383480072, Entropy: 142.02777099609375, Temp: 2.6126606464385986, KL: 67.16018676757812, Loss: 0.01847703941166401, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7921/20000], Bound: 0.3784172236919403, Entropy: 142.09375, Temp: 2.612605333328247, KL: 71.60804748535156, Loss: 0.016103794798254967, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7922/20000], Bound: 0.4198175072669983, Entropy: 141.89096069335938, Temp: 2.6125826835632324, KL: 82.99470520019531, Loss: 0.017621364444494247, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7923/20000], Bound: 0.3992871046066284, Entropy: 141.45399475097656, Temp: 2.612604856491089, KL: 77.65643310546875, Loss: 0.016101347282528877, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7924/20000], Bound: 0.40988847613334656, Entropy: 142.10299682617188, Temp: 2.612673044204712, KL: 80.283203125, Loss: 0.017090635374188423, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7925/20000], Bound: 0.3697110712528229, Entropy: 142.7061767578125, Temp: 2.612776279449463, KL: 67.44354248046875, Loss: 0.019349008798599243, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7926/20000], Bound: 0.40107280015945435, Entropy: 141.086669921875, Temp: 2.612828254699707, KL: 77.6715087890625, Loss: 0.017081687226891518, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7927/20000], Bound: 0.3863707184791565, Entropy: 139.8778839111328, Temp: 2.612908124923706, KL: 72.44883728027344, Loss: 0.01886686310172081, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7928/20000], Bound: 0.4078153967857361, Entropy: 140.22299194335938, Temp: 2.6129653453826904, KL: 80.16485595703125, Loss: 0.016136879101395607, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7929/20000], Bound: 0.3888814151287079, Entropy: 141.47654724121094, Temp: 2.6130731105804443, KL: 74.80583190917969, Loss: 0.015748031437397003, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7930/20000], Bound: 0.3622174561023712, Entropy: 143.0698699951172, Temp: 2.6132144927978516, KL: 67.55520629882812, Loss: 0.015117821283638477, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7931/20000], Bound: 0.38942426443099976, Entropy: 141.8074951171875, Temp: 2.613370418548584, KL: 74.81668090820312, Loss: 0.016031606122851372, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7932/20000], Bound: 0.3836704194545746, Entropy: 140.1417236328125, Temp: 2.6135501861572266, KL: 72.51351928710938, Loss: 0.01726069115102291, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7933/20000], Bound: 0.4089357852935791, Entropy: 140.75347900390625, Temp: 2.6137237548828125, KL: 79.56764221191406, Loss: 0.01792776957154274, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7934/20000], Bound: 0.3962287902832031, Entropy: 140.76568603515625, Temp: 2.613905191421509, KL: 75.85906982421875, Loss: 0.01783711463212967, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7935/20000], Bound: 0.3900591731071472, Entropy: 141.2048797607422, Temp: 2.6140828132629395, KL: 73.90330505371094, Loss: 0.018138790503144264, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7936/20000], Bound: 0.3928060233592987, Entropy: 143.09173583984375, Temp: 2.6142444610595703, KL: 73.59098815917969, Loss: 0.020265888422727585, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7937/20000], Bound: 0.37361857295036316, Entropy: 140.4172821044922, Temp: 2.614356517791748, KL: 69.18904113769531, Loss: 0.01813696138560772, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7938/20000], Bound: 0.40495872497558594, Entropy: 140.95785522460938, Temp: 2.6144423484802246, KL: 76.61410522460938, Loss: 0.021321050822734833, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7939/20000], Bound: 0.38203009963035583, Entropy: 141.42710876464844, Temp: 2.6144797801971436, KL: 72.10075378417969, Loss: 0.017158223316073418, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7940/20000], Bound: 0.41492682695388794, Entropy: 141.18829345703125, Temp: 2.614525079727173, KL: 81.28060913085938, Loss: 0.018096361309289932, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7941/20000], Bound: 0.3905700147151947, Entropy: 139.7343292236328, Temp: 2.6145942211151123, KL: 74.4044189453125, Loss: 0.0174693763256073, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7942/20000], Bound: 0.3926013112068176, Entropy: 139.89634704589844, Temp: 2.614670753479004, KL: 74.19099426269531, Loss: 0.019008513540029526, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7943/20000], Bound: 0.40106868743896484, Entropy: 142.37799072265625, Temp: 2.6147289276123047, KL: 77.05029296875, Loss: 0.018288858234882355, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7944/20000], Bound: 0.39091938734054565, Entropy: 141.60386657714844, Temp: 2.6147913932800293, KL: 74.4510498046875, Loss: 0.017576444894075394, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7945/20000], Bound: 0.402418315410614, Entropy: 140.86428833007812, Temp: 2.6148605346679688, KL: 76.13414001464844, Loss: 0.020804276689887047, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7946/20000], Bound: 0.3961816132068634, Entropy: 142.59371948242188, Temp: 2.61488938331604, KL: 76.95877075195312, Loss: 0.015718495473265648, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7947/20000], Bound: 0.3820279538631439, Entropy: 142.3576202392578, Temp: 2.6149673461914062, KL: 70.57708740234375, Loss: 0.020075269043445587, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7948/20000], Bound: 0.39754465222358704, Entropy: 140.802734375, Temp: 2.6149957180023193, KL: 77.04209899902344, Loss: 0.016324596479535103, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7949/20000], Bound: 0.4007132649421692, Entropy: 140.23583984375, Temp: 2.615063428878784, KL: 75.62361145019531, Loss: 0.020819885656237602, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7950/20000], Bound: 0.3879127502441406, Entropy: 139.44442749023438, Temp: 2.615088939666748, KL: 73.67778015136719, Loss: 0.017390569671988487, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7951/20000], Bound: 0.39425191283226013, Entropy: 140.43833923339844, Temp: 2.6151247024536133, KL: 75.24810791015625, Loss: 0.01791263557970524, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7952/20000], Bound: 0.39402636885643005, Entropy: 140.07261657714844, Temp: 2.6151671409606934, KL: 74.21408081054688, Loss: 0.01976414956152439, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7953/20000], Bound: 0.37692394852638245, Entropy: 138.9382781982422, Temp: 2.6151814460754395, KL: 70.64056396484375, Loss: 0.01716548390686512, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7954/20000], Bound: 0.37661999464035034, Entropy: 141.83680725097656, Temp: 2.6152002811431885, KL: 71.08033752441406, Loss: 0.01615929789841175, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7955/20000], Bound: 0.3629125654697418, Entropy: 140.60641479492188, Temp: 2.615241289138794, KL: 68.10467529296875, Loss: 0.014457580633461475, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7956/20000], Bound: 0.4218798279762268, Entropy: 141.97076416015625, Temp: 2.61531925201416, KL: 83.81265258789062, Loss: 0.017290115356445312, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7957/20000], Bound: 0.4231499433517456, Entropy: 138.6700897216797, Temp: 2.6154394149780273, KL: 82.62884521484375, Loss: 0.020294584333896637, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7958/20000], Bound: 0.3810911178588867, Entropy: 142.219970703125, Temp: 2.6155452728271484, KL: 71.05081176757812, Loss: 0.018661165609955788, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7959/20000], Bound: 0.3823961317539215, Entropy: 140.6876983642578, Temp: 2.6156232357025146, KL: 70.23968505859375, Loss: 0.020928092300891876, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7960/20000], Bound: 0.4107176661491394, Entropy: 140.14549255371094, Temp: 2.615635871887207, KL: 79.11500549316406, Loss: 0.01983332261443138, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7961/20000], Bound: 0.3730626404285431, Entropy: 142.10098266601562, Temp: 2.6156399250030518, KL: 69.08624267578125, Loss: 0.018043864518404007, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7962/20000], Bound: 0.3835720121860504, Entropy: 141.72711181640625, Temp: 2.615628957748413, KL: 71.96617126464844, Loss: 0.01827380247414112, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7963/20000], Bound: 0.38343191146850586, Entropy: 140.21018981933594, Temp: 2.6156113147735596, KL: 71.87339782714844, Loss: 0.018373943865299225, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7964/20000], Bound: 0.378042072057724, Entropy: 142.2191162109375, Temp: 2.6155858039855957, KL: 73.71022033691406, Loss: 0.011910821311175823, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7965/20000], Bound: 0.374754399061203, Entropy: 141.16799926757812, Temp: 2.6156656742095947, KL: 67.8330078125, Loss: 0.021357139572501183, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7966/20000], Bound: 0.3862420618534088, Entropy: 138.2991485595703, Temp: 2.61566424369812, KL: 72.97613525390625, Loss: 0.01781447045505047, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7967/20000], Bound: 0.3981807231903076, Entropy: 142.3908233642578, Temp: 2.615666151046753, KL: 77.28398132324219, Loss: 0.016226883977651596, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7968/20000], Bound: 0.4140563905239105, Entropy: 139.79283142089844, Temp: 2.6157126426696777, KL: 80.821533203125, Loss: 0.018487215042114258, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7969/20000], Bound: 0.39614132046699524, Entropy: 140.15267944335938, Temp: 2.615774631500244, KL: 76.56317138671875, Loss: 0.016462286934256554, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7970/20000], Bound: 0.3636230528354645, Entropy: 140.03172302246094, Temp: 2.6158688068389893, KL: 66.47529602050781, Loss: 0.017957668751478195, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7971/20000], Bound: 0.38830938935279846, Entropy: 139.2512969970703, Temp: 2.6159307956695557, KL: 71.49729919433594, Loss: 0.021786430850625038, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7972/20000], Bound: 0.3820077180862427, Entropy: 141.68304443359375, Temp: 2.615919589996338, KL: 71.60702514648438, Loss: 0.01810399442911148, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7973/20000], Bound: 0.38013237714767456, Entropy: 141.8994598388672, Temp: 2.615902900695801, KL: 72.66651916503906, Loss: 0.015051335096359253, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7974/20000], Bound: 0.40177324414253235, Entropy: 141.66905212402344, Temp: 2.6159355640411377, KL: 76.32598876953125, Loss: 0.020083937793970108, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7975/20000], Bound: 0.39200320839881897, Entropy: 140.49522399902344, Temp: 2.615943431854248, KL: 74.43058776855469, Loss: 0.01823025941848755, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7976/20000], Bound: 0.40966880321502686, Entropy: 140.61306762695312, Temp: 2.615952491760254, KL: 78.52093505859375, Loss: 0.020372673869132996, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7977/20000], Bound: 0.38318943977355957, Entropy: 139.62506103515625, Temp: 2.6159420013427734, KL: 72.84400939941406, Loss: 0.016388701274991035, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7978/20000], Bound: 0.4035438299179077, Entropy: 141.51388549804688, Temp: 2.6159589290618896, KL: 78.19380187988281, Loss: 0.017515096813440323, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7979/20000], Bound: 0.402622252702713, Entropy: 143.51438903808594, Temp: 2.6160008907318115, KL: 78.31864929199219, Loss: 0.016755608841776848, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7980/20000], Bound: 0.388480007648468, Entropy: 140.51783752441406, Temp: 2.6160786151885986, KL: 74.44044494628906, Loss: 0.01625708118081093, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7981/20000], Bound: 0.3916308879852295, Entropy: 142.2880401611328, Temp: 2.616182327270508, KL: 74.26945495605469, Loss: 0.01833352819085121, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7982/20000], Bound: 0.379406213760376, Entropy: 141.73440551757812, Temp: 2.6162755489349365, KL: 71.40043640136719, Loss: 0.01707790419459343, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7983/20000], Bound: 0.40746545791625977, Entropy: 137.9721221923828, Temp: 2.6163687705993652, KL: 79.32920837402344, Loss: 0.017576005309820175, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7984/20000], Bound: 0.40779173374176025, Entropy: 141.00868225097656, Temp: 2.6164824962615967, KL: 78.36447143554688, Loss: 0.019606687128543854, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7985/20000], Bound: 0.3960025906562805, Entropy: 141.39187622070312, Temp: 2.6165783405303955, KL: 77.05992126464844, Loss: 0.015444369986653328, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7986/20000], Bound: 0.3930121064186096, Entropy: 141.5718994140625, Temp: 2.6167209148406982, KL: 75.13290405273438, Loss: 0.017458055168390274, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7987/20000], Bound: 0.37924838066101074, Entropy: 139.8595428466797, Temp: 2.616866111755371, KL: 71.50886535644531, Loss: 0.01679028384387493, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7988/20000], Bound: 0.3943813145160675, Entropy: 143.5236358642578, Temp: 2.617011308670044, KL: 74.68283081054688, Loss: 0.019084753468632698, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7989/20000], Bound: 0.3900764584541321, Entropy: 142.3273468017578, Temp: 2.6171305179595947, KL: 72.8153076171875, Loss: 0.020257873460650444, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7990/20000], Bound: 0.4188061058521271, Entropy: 139.56028747558594, Temp: 2.6171998977661133, KL: 84.28849792480469, Loss: 0.01462123915553093, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7991/20000], Bound: 0.37949085235595703, Entropy: 142.93162536621094, Temp: 2.617356777191162, KL: 69.78288269042969, Loss: 0.020224791020154953, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7992/20000], Bound: 0.4199453294277191, Entropy: 138.69851684570312, Temp: 2.6174495220184326, KL: 82.42817687988281, Loss: 0.01883842423558235, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7993/20000], Bound: 0.3921964764595032, Entropy: 140.93565368652344, Temp: 2.6175525188446045, KL: 75.00540161132812, Loss: 0.017256224527955055, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7994/20000], Bound: 0.40730559825897217, Entropy: 138.76918029785156, Temp: 2.6176645755767822, KL: 80.09455871582031, Loss: 0.016038106754422188, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7995/20000], Bound: 0.39337748289108276, Entropy: 140.56979370117188, Temp: 2.6178224086761475, KL: 74.343994140625, Loss: 0.01918010041117668, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7996/20000], Bound: 0.38143929839134216, Entropy: 139.3440399169922, Temp: 2.6179497241973877, KL: 71.19267272949219, Loss: 0.018603269010782242, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7997/20000], Bound: 0.37329331040382385, Entropy: 139.19212341308594, Temp: 2.6180472373962402, KL: 70.56794738769531, Loss: 0.015360651537775993, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7998/20000], Bound: 0.39647576212882996, Entropy: 140.38644409179688, Temp: 2.6181697845458984, KL: 74.57456970214844, Loss: 0.020473938435316086, Learning Rate: 0.0005931980228999996\n",
      "Epoch [7999/20000], Bound: 0.3971777558326721, Entropy: 140.0699920654297, Temp: 2.6182446479797363, KL: 78.25430297851562, Loss: 0.013840765692293644, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8000/20000], Bound: 0.3636939823627472, Entropy: 140.51646423339844, Temp: 2.618398666381836, KL: 65.94013977050781, Loss: 0.019038820639252663, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8001/20000], Bound: 0.3994028568267822, Entropy: 140.91183471679688, Temp: 2.6184935569763184, KL: 78.600341796875, Loss: 0.014432031661272049, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8002/20000], Bound: 0.3554694354534149, Entropy: 140.43487548828125, Temp: 2.618656635284424, KL: 65.079345703125, Loss: 0.01631651446223259, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8003/20000], Bound: 0.40382444858551025, Entropy: 142.0124053955078, Temp: 2.618802070617676, KL: 78.57315063476562, Loss: 0.016981665045022964, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8004/20000], Bound: 0.40624645352363586, Entropy: 139.16925048828125, Temp: 2.6189687252044678, KL: 78.97480773925781, Loss: 0.017589688301086426, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8005/20000], Bound: 0.39509865641593933, Entropy: 139.4296875, Temp: 2.6191461086273193, KL: 76.39013671875, Loss: 0.01624734327197075, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8006/20000], Bound: 0.3930146396160126, Entropy: 138.64859008789062, Temp: 2.6193461418151855, KL: 74.82136535644531, Loss: 0.018081825226545334, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8007/20000], Bound: 0.3900239169597626, Entropy: 140.70297241210938, Temp: 2.619530439376831, KL: 73.50929260253906, Loss: 0.018926378339529037, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8008/20000], Bound: 0.40726786851882935, Entropy: 140.43319702148438, Temp: 2.6196820735931396, KL: 78.78767395019531, Loss: 0.01853540539741516, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8009/20000], Bound: 0.3934763967990875, Entropy: 139.76913452148438, Temp: 2.6198298931121826, KL: 75.11355590820312, Loss: 0.017786365002393723, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8010/20000], Bound: 0.4014745354652405, Entropy: 138.4520721435547, Temp: 2.6199729442596436, KL: 78.52947998046875, Loss: 0.01575138047337532, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8011/20000], Bound: 0.3947213590145111, Entropy: 142.77059936523438, Temp: 2.620157241821289, KL: 75.29244995117188, Loss: 0.018142659217119217, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8012/20000], Bound: 0.3416171967983246, Entropy: 142.36654663085938, Temp: 2.620327949523926, KL: 61.537261962890625, Loss: 0.01584717258810997, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8013/20000], Bound: 0.3660047650337219, Entropy: 140.95420837402344, Temp: 2.620474100112915, KL: 68.65994262695312, Loss: 0.015101557597517967, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8014/20000], Bound: 0.39740702509880066, Entropy: 140.44155883789062, Temp: 2.6206369400024414, KL: 76.09391784667969, Loss: 0.018120255321264267, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8015/20000], Bound: 0.3932313621044159, Entropy: 138.74949645996094, Temp: 2.620791435241699, KL: 75.89601135253906, Loss: 0.016167104244232178, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8016/20000], Bound: 0.40369322896003723, Entropy: 142.48558044433594, Temp: 2.620969772338867, KL: 78.16523742675781, Loss: 0.01771044172346592, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8017/20000], Bound: 0.3923626244068146, Entropy: 141.43722534179688, Temp: 2.621152400970459, KL: 75.82626342773438, Loss: 0.01582069881260395, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8018/20000], Bound: 0.40527209639549255, Entropy: 139.6398468017578, Temp: 2.62136173248291, KL: 79.21743774414062, Loss: 0.016601579263806343, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8019/20000], Bound: 0.39032265543937683, Entropy: 140.81112670898438, Temp: 2.621593475341797, KL: 73.57475280761719, Loss: 0.01898718625307083, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8020/20000], Bound: 0.4381764531135559, Entropy: 141.14573669433594, Temp: 2.621786594390869, KL: 87.20610046386719, Loss: 0.020488282665610313, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8021/20000], Bound: 0.3692576587200165, Entropy: 140.48426818847656, Temp: 2.6219682693481445, KL: 69.31990051269531, Loss: 0.01560299377888441, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8022/20000], Bound: 0.4178999960422516, Entropy: 142.44081115722656, Temp: 2.6221566200256348, KL: 79.92149353027344, Loss: 0.02249143272638321, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8023/20000], Bound: 0.3886203467845917, Entropy: 140.09864807128906, Temp: 2.6222758293151855, KL: 74.12382507324219, Loss: 0.017004482448101044, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8024/20000], Bound: 0.3930131196975708, Entropy: 140.19171142578125, Temp: 2.6224019527435303, KL: 75.57931518554688, Loss: 0.0166671983897686, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8025/20000], Bound: 0.40317732095718384, Entropy: 140.5358428955078, Temp: 2.622544765472412, KL: 74.80357360839844, Loss: 0.023845626041293144, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8026/20000], Bound: 0.4158487319946289, Entropy: 138.8107452392578, Temp: 2.6225814819335938, KL: 83.0897216796875, Loss: 0.01527300477027893, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8027/20000], Bound: 0.38704678416252136, Entropy: 140.36854553222656, Temp: 2.6226930618286133, KL: 73.85676574707031, Loss: 0.01664901152253151, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8028/20000], Bound: 0.38630592823028564, Entropy: 140.98291015625, Temp: 2.622817039489746, KL: 72.27626037597656, Loss: 0.01925485208630562, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8029/20000], Bound: 0.3989764153957367, Entropy: 140.13584899902344, Temp: 2.6229031085968018, KL: 75.369384765625, Loss: 0.02040506713092327, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8030/20000], Bound: 0.3695535957813263, Entropy: 140.88580322265625, Temp: 2.6229472160339355, KL: 67.75823974609375, Loss: 0.018748465925455093, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8031/20000], Bound: 0.36162784695625305, Entropy: 140.99636840820312, Temp: 2.622953176498413, KL: 65.94631958007812, Loss: 0.01796255074441433, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8032/20000], Bound: 0.39422595500946045, Entropy: 140.97274780273438, Temp: 2.6229305267333984, KL: 75.31103515625, Loss: 0.01785966381430626, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8033/20000], Bound: 0.3961329758167267, Entropy: 139.94508361816406, Temp: 2.6229186058044434, KL: 76.62969970703125, Loss: 0.016410162672400475, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8034/20000], Bound: 0.37976303696632385, Entropy: 139.38778686523438, Temp: 2.6229453086853027, KL: 70.48947143554688, Loss: 0.019074123352766037, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8035/20000], Bound: 0.39148032665252686, Entropy: 139.6465606689453, Temp: 2.6229400634765625, KL: 73.79452514648438, Loss: 0.019223418086767197, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8036/20000], Bound: 0.397296667098999, Entropy: 139.88189697265625, Temp: 2.622915744781494, KL: 75.77912902832031, Loss: 0.018682517111301422, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8037/20000], Bound: 0.3795787990093231, Entropy: 139.45571899414062, Temp: 2.6228902339935303, KL: 70.83247375488281, Loss: 0.018319183960556984, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8038/20000], Bound: 0.4031854271888733, Entropy: 137.94129943847656, Temp: 2.622852087020874, KL: 78.10592651367188, Loss: 0.017557581886649132, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8039/20000], Bound: 0.3818884789943695, Entropy: 138.9714813232422, Temp: 2.6228411197662354, KL: 72.06137084960938, Loss: 0.01723853126168251, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8040/20000], Bound: 0.38797688484191895, Entropy: 138.7805633544922, Temp: 2.6228384971618652, KL: 73.36659240722656, Loss: 0.018098292872309685, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8041/20000], Bound: 0.38645973801612854, Entropy: 140.6417694091797, Temp: 2.622833728790283, KL: 72.54835510253906, Loss: 0.01882103458046913, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8042/20000], Bound: 0.3724355399608612, Entropy: 139.04751586914062, Temp: 2.622811794281006, KL: 69.76614379882812, Loss: 0.016473127529025078, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8043/20000], Bound: 0.4260246753692627, Entropy: 137.75656127929688, Temp: 2.6228039264678955, KL: 83.08454895019531, Loss: 0.02118801325559616, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8044/20000], Bound: 0.36868828535079956, Entropy: 139.97645568847656, Temp: 2.6227784156799316, KL: 68.77168273925781, Loss: 0.01634984277188778, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8045/20000], Bound: 0.39963340759277344, Entropy: 141.08786010742188, Temp: 2.62276554107666, KL: 73.98959350585938, Loss: 0.023403091356158257, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8046/20000], Bound: 0.41590991616249084, Entropy: 138.6516571044922, Temp: 2.6226658821105957, KL: 81.10122680664062, Loss: 0.0191002506762743, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8047/20000], Bound: 0.39871540665626526, Entropy: 141.36727905273438, Temp: 2.6225850582122803, KL: 74.80656433105469, Loss: 0.021328534930944443, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8048/20000], Bound: 0.38988444209098816, Entropy: 139.73495483398438, Temp: 2.6224613189697266, KL: 73.53062438964844, Loss: 0.018836945295333862, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8049/20000], Bound: 0.4157215654850006, Entropy: 140.94198608398438, Temp: 2.6223361492156982, KL: 83.23336791992188, Loss: 0.014922735281288624, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8050/20000], Bound: 0.3935617208480835, Entropy: 140.98667907714844, Temp: 2.6223089694976807, KL: 76.27566528320312, Loss: 0.015643829479813576, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8051/20000], Bound: 0.41323184967041016, Entropy: 140.716064453125, Temp: 2.6223337650299072, KL: 82.54074096679688, Loss: 0.014813126996159554, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8052/20000], Bound: 0.4097398817539215, Entropy: 139.9990997314453, Temp: 2.622440814971924, KL: 79.6112060546875, Loss: 0.018403159454464912, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8053/20000], Bound: 0.3964190185070038, Entropy: 141.3820037841797, Temp: 2.6225523948669434, KL: 75.49717712402344, Loss: 0.018725253641605377, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8054/20000], Bound: 0.39944741129875183, Entropy: 139.4667510986328, Temp: 2.622648000717163, KL: 75.99488830566406, Loss: 0.019474567845463753, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8055/20000], Bound: 0.38884595036506653, Entropy: 140.5071258544922, Temp: 2.622718334197998, KL: 72.60270690917969, Loss: 0.02003370225429535, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8056/20000], Bound: 0.40170323848724365, Entropy: 140.83523559570312, Temp: 2.622743844985962, KL: 78.05580139160156, Loss: 0.01681571453809738, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8057/20000], Bound: 0.357743501663208, Entropy: 142.30833435058594, Temp: 2.622802972793579, KL: 65.14877319335938, Loss: 0.017420781776309013, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8058/20000], Bound: 0.4046515226364136, Entropy: 139.9914093017578, Temp: 2.6228342056274414, KL: 79.26846313476562, Loss: 0.016170095652341843, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8059/20000], Bound: 0.36846786737442017, Entropy: 141.7888946533203, Temp: 2.6229135990142822, KL: 67.29747009277344, Loss: 0.01904294639825821, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8060/20000], Bound: 0.38783398270606995, Entropy: 140.63311767578125, Temp: 2.622943878173828, KL: 73.98051452636719, Loss: 0.016850152984261513, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8061/20000], Bound: 0.4304329454898834, Entropy: 139.8731231689453, Temp: 2.6229918003082275, KL: 87.20404052734375, Loss: 0.015922963619232178, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8062/20000], Bound: 0.37873393297195435, Entropy: 139.93817138671875, Temp: 2.623117208480835, KL: 70.97581481933594, Loss: 0.01758733205497265, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8063/20000], Bound: 0.38915154337882996, Entropy: 140.52040100097656, Temp: 2.62322735786438, KL: 73.88487243652344, Loss: 0.017763515934348106, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8064/20000], Bound: 0.41662371158599854, Entropy: 137.65896606445312, Temp: 2.623331308364868, KL: 82.82353210449219, Loss: 0.016236277297139168, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8065/20000], Bound: 0.36571601033210754, Entropy: 142.20838928222656, Temp: 2.6234869956970215, KL: 68.38790893554688, Loss: 0.01549438200891018, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8066/20000], Bound: 0.3961349129676819, Entropy: 138.20689392089844, Temp: 2.623650312423706, KL: 76.64254760742188, Loss: 0.016394877806305885, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8067/20000], Bound: 0.38110992312431335, Entropy: 141.0376739501953, Temp: 2.6238346099853516, KL: 72.19976806640625, Loss: 0.016558583825826645, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8068/20000], Bound: 0.3621506094932556, Entropy: 140.3225555419922, Temp: 2.6240196228027344, KL: 66.67047119140625, Loss: 0.016869595274329185, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8069/20000], Bound: 0.43052178621292114, Entropy: 142.41307067871094, Temp: 2.624178886413574, KL: 85.09994506835938, Loss: 0.020000621676445007, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8070/20000], Bound: 0.3718448877334595, Entropy: 139.38491821289062, Temp: 2.6243302822113037, KL: 69.80203247070312, Loss: 0.016100328415632248, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8071/20000], Bound: 0.3654884099960327, Entropy: 139.3014678955078, Temp: 2.624484062194824, KL: 68.24653625488281, Loss: 0.015651408582925797, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8072/20000], Bound: 0.37414324283599854, Entropy: 140.84848022460938, Temp: 2.6246421337127686, KL: 70.85963439941406, Loss: 0.01533040963113308, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8073/20000], Bound: 0.3758111000061035, Entropy: 139.76083374023438, Temp: 2.624818801879883, KL: 69.55950927734375, Loss: 0.018712427467107773, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8074/20000], Bound: 0.37697118520736694, Entropy: 141.13723754882812, Temp: 2.6249501705169678, KL: 71.91427612304688, Loss: 0.014858054928481579, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8075/20000], Bound: 0.41037601232528687, Entropy: 139.5031280517578, Temp: 2.625114679336548, KL: 80.34600830078125, Loss: 0.01739642769098282, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8076/20000], Bound: 0.38577282428741455, Entropy: 141.63218688964844, Temp: 2.6252963542938232, KL: 71.85311889648438, Loss: 0.019790343940258026, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8077/20000], Bound: 0.39133888483047485, Entropy: 142.05563354492188, Temp: 2.625422477722168, KL: 74.83242797851562, Loss: 0.017192279919981956, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8078/20000], Bound: 0.4051792025566101, Entropy: 139.76889038085938, Temp: 2.6255531311035156, KL: 77.77877807617188, Loss: 0.01933741569519043, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8079/20000], Bound: 0.38122037053108215, Entropy: 138.96766662597656, Temp: 2.6256628036499023, KL: 71.13270568847656, Loss: 0.018669182434678078, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8080/20000], Bound: 0.41373956203460693, Entropy: 140.5795440673828, Temp: 2.6257402896881104, KL: 81.15826416015625, Loss: 0.017780452966690063, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8081/20000], Bound: 0.4089401960372925, Entropy: 142.15733337402344, Temp: 2.625839948654175, KL: 80.21546936035156, Loss: 0.016835302114486694, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8082/20000], Bound: 0.368953675031662, Entropy: 141.86289978027344, Temp: 2.625971794128418, KL: 69.22370910644531, Loss: 0.015661170706152916, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8083/20000], Bound: 0.41046419739723206, Entropy: 139.77218627929688, Temp: 2.626113176345825, KL: 79.61788940429688, Loss: 0.01884462870657444, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8084/20000], Bound: 0.40067291259765625, Entropy: 142.35235595703125, Temp: 2.626246929168701, KL: 75.28939819335938, Loss: 0.021542366594076157, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8085/20000], Bound: 0.39296457171440125, Entropy: 141.07379150390625, Temp: 2.6263132095336914, KL: 74.71710205078125, Loss: 0.018323658034205437, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8086/20000], Bound: 0.39775532484054565, Entropy: 142.990966796875, Temp: 2.6263701915740967, KL: 75.14051818847656, Loss: 0.020190324634313583, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8087/20000], Bound: 0.3897281587123871, Entropy: 142.7879638671875, Temp: 2.6263890266418457, KL: 74.94546508789062, Loss: 0.016095034778118134, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8088/20000], Bound: 0.3836115300655365, Entropy: 140.17111206054688, Temp: 2.6264419555664062, KL: 71.97607421875, Loss: 0.018379615619778633, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8089/20000], Bound: 0.41376131772994995, Entropy: 141.5964813232422, Temp: 2.6264755725860596, KL: 79.2530517578125, Loss: 0.02142847701907158, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8090/20000], Bound: 0.39573726058006287, Entropy: 138.35125732421875, Temp: 2.6264684200286865, KL: 76.94984436035156, Loss: 0.015618961304426193, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8091/20000], Bound: 0.4070158302783966, Entropy: 140.0826873779297, Temp: 2.6265127658843994, KL: 79.41232299804688, Loss: 0.017278578132390976, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8092/20000], Bound: 0.37453514337539673, Entropy: 140.55589294433594, Temp: 2.626584768295288, KL: 69.45918273925781, Loss: 0.01822769083082676, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8093/20000], Bound: 0.3731822967529297, Entropy: 139.5906982421875, Temp: 2.626628875732422, KL: 68.66131591796875, Loss: 0.019015390425920486, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8094/20000], Bound: 0.41083580255508423, Entropy: 140.79371643066406, Temp: 2.626631736755371, KL: 80.62994384765625, Loss: 0.01713588833808899, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8095/20000], Bound: 0.3937871754169464, Entropy: 142.127197265625, Temp: 2.6266727447509766, KL: 76.70759582519531, Loss: 0.014995705336332321, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8096/20000], Bound: 0.39212116599082947, Entropy: 142.48468017578125, Temp: 2.626770496368408, KL: 73.43692016601562, Loss: 0.020296573638916016, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8097/20000], Bound: 0.3762188255786896, Entropy: 142.82920837402344, Temp: 2.6268177032470703, KL: 70.7296142578125, Loss: 0.0167238786816597, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8098/20000], Bound: 0.4254322052001953, Entropy: 141.68881225585938, Temp: 2.6268696784973145, KL: 85.71615600585938, Loss: 0.015879420563578606, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8099/20000], Bound: 0.3910030126571655, Entropy: 141.24700927734375, Temp: 2.6269938945770264, KL: 74.3919677734375, Loss: 0.017860764637589455, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8100/20000], Bound: 0.4278094172477722, Entropy: 141.23304748535156, Temp: 2.6271095275878906, KL: 83.38581848144531, Loss: 0.02170712500810623, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8101/20000], Bound: 0.41254910826683044, Entropy: 142.11392211914062, Temp: 2.627185821533203, KL: 77.51069641113281, Loss: 0.024057745933532715, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8102/20000], Bound: 0.42752382159233093, Entropy: 139.2380828857422, Temp: 2.6271660327911377, KL: 85.21542358398438, Loss: 0.018058471381664276, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8103/20000], Bound: 0.3533169627189636, Entropy: 142.12896728515625, Temp: 2.6271872520446777, KL: 64.36137390136719, Loss: 0.016622593626379967, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8104/20000], Bound: 0.38868004083633423, Entropy: 139.2616729736328, Temp: 2.6271936893463135, KL: 73.13058471679688, Loss: 0.01897834986448288, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8105/20000], Bound: 0.3966737687587738, Entropy: 140.12030029296875, Temp: 2.6271796226501465, KL: 76.07452392578125, Loss: 0.017815789207816124, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8106/20000], Bound: 0.37993085384368896, Entropy: 140.05482482910156, Temp: 2.6271774768829346, KL: 70.60502624511719, Loss: 0.018983377143740654, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8107/20000], Bound: 0.3757818043231964, Entropy: 141.33377075195312, Temp: 2.6271462440490723, KL: 68.57142639160156, Loss: 0.020597482100129128, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8108/20000], Bound: 0.37679117918014526, Entropy: 141.81224060058594, Temp: 2.627053737640381, KL: 70.85369873046875, Loss: 0.01680058240890503, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8109/20000], Bound: 0.40037333965301514, Entropy: 139.61013793945312, Temp: 2.626978874206543, KL: 77.35285949707031, Loss: 0.017453430220484734, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8110/20000], Bound: 0.400149941444397, Entropy: 139.59742736816406, Temp: 2.6269330978393555, KL: 77.96748352050781, Loss: 0.016157565638422966, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8111/20000], Bound: 0.3754633367061615, Entropy: 140.84837341308594, Temp: 2.6269373893737793, KL: 70.02247619628906, Loss: 0.01766134239733219, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8112/20000], Bound: 0.3744157552719116, Entropy: 139.63392639160156, Temp: 2.626932382583618, KL: 71.55958557128906, Loss: 0.014168333262205124, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8113/20000], Bound: 0.3612089455127716, Entropy: 140.85867309570312, Temp: 2.6269843578338623, KL: 65.46855163574219, Loss: 0.018682265654206276, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8114/20000], Bound: 0.38031965494155884, Entropy: 140.28797912597656, Temp: 2.6269872188568115, KL: 70.49066162109375, Loss: 0.01941145770251751, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8115/20000], Bound: 0.37388724088668823, Entropy: 140.54295349121094, Temp: 2.626952648162842, KL: 69.45845031738281, Loss: 0.017881864681839943, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8116/20000], Bound: 0.4065609574317932, Entropy: 139.80723571777344, Temp: 2.626906394958496, KL: 79.44854736328125, Loss: 0.016956085339188576, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8117/20000], Bound: 0.3971383571624756, Entropy: 141.93862915039062, Temp: 2.6269023418426514, KL: 77.07852172851562, Loss: 0.01616157405078411, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8118/20000], Bound: 0.3793715238571167, Entropy: 139.70567321777344, Temp: 2.626941204071045, KL: 69.86466979980469, Loss: 0.020085534080863, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8119/20000], Bound: 0.3716050386428833, Entropy: 138.76673889160156, Temp: 2.626924991607666, KL: 68.54542541503906, Loss: 0.018387475982308388, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8120/20000], Bound: 0.37269940972328186, Entropy: 139.4604949951172, Temp: 2.6268835067749023, KL: 70.906494140625, Loss: 0.01448333915323019, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8121/20000], Bound: 0.4059085249900818, Entropy: 140.07373046875, Temp: 2.626894950866699, KL: 78.24357604980469, Loss: 0.018879752606153488, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8122/20000], Bound: 0.38564345240592957, Entropy: 140.73081970214844, Temp: 2.626906156539917, KL: 72.29548645019531, Loss: 0.018891682848334312, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8123/20000], Bound: 0.3880053758621216, Entropy: 140.57398986816406, Temp: 2.626894474029541, KL: 73.74971008300781, Loss: 0.017424656078219414, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8124/20000], Bound: 0.3865581750869751, Entropy: 140.7885284423828, Temp: 2.6268930435180664, KL: 71.48982238769531, Loss: 0.02092840149998665, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8125/20000], Bound: 0.40579116344451904, Entropy: 137.968994140625, Temp: 2.6268324851989746, KL: 78.03572082519531, Loss: 0.01920824497938156, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8126/20000], Bound: 0.3840792179107666, Entropy: 140.95590209960938, Temp: 2.626772403717041, KL: 73.29891967773438, Loss: 0.016121258959174156, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8127/20000], Bound: 0.3868449330329895, Entropy: 142.66464233398438, Temp: 2.6267478466033936, KL: 72.79791259765625, Loss: 0.018595123663544655, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8128/20000], Bound: 0.40561985969543457, Entropy: 138.37928771972656, Temp: 2.626711368560791, KL: 79.14968872070312, Loss: 0.01698947884142399, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8129/20000], Bound: 0.4112301766872406, Entropy: 139.31529235839844, Temp: 2.6267144680023193, KL: 81.17219543457031, Loss: 0.016329774633049965, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8130/20000], Bound: 0.4008432924747467, Entropy: 141.4853515625, Temp: 2.626771926879883, KL: 76.37385559082031, Loss: 0.019578842446208, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8131/20000], Bound: 0.39832401275634766, Entropy: 142.9459686279297, Temp: 2.626805305480957, KL: 76.381103515625, Loss: 0.018151551485061646, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8132/20000], Bound: 0.40422067046165466, Entropy: 141.18408203125, Temp: 2.6268420219421387, KL: 79.38493347167969, Loss: 0.015751756727695465, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8133/20000], Bound: 0.3945740759372711, Entropy: 141.28367614746094, Temp: 2.6269330978393555, KL: 76.37962341308594, Loss: 0.016061073169112206, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8134/20000], Bound: 0.43409961462020874, Entropy: 140.3189239501953, Temp: 2.62705659866333, KL: 84.87826538085938, Loss: 0.02256694808602333, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8135/20000], Bound: 0.41413596272468567, Entropy: 139.62777709960938, Temp: 2.627131223678589, KL: 80.88589477539062, Loss: 0.018542375415563583, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8136/20000], Bound: 0.38050246238708496, Entropy: 139.0983123779297, Temp: 2.6272144317626953, KL: 70.26261901855469, Loss: 0.019947199150919914, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8137/20000], Bound: 0.4029069244861603, Entropy: 140.7308349609375, Temp: 2.627241611480713, KL: 77.24440002441406, Loss: 0.01908867061138153, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8138/20000], Bound: 0.41654446721076965, Entropy: 140.35787963867188, Temp: 2.6272594928741455, KL: 81.639404296875, Loss: 0.01849340833723545, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8139/20000], Bound: 0.3871453106403351, Entropy: 141.08999633789062, Temp: 2.6272952556610107, KL: 73.33049011230469, Loss: 0.017752282321453094, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8140/20000], Bound: 0.366588294506073, Entropy: 141.26620483398438, Temp: 2.627329111099243, KL: 66.93450927734375, Loss: 0.0187627412378788, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8141/20000], Bound: 0.41837674379348755, Entropy: 139.40924072265625, Temp: 2.6273193359375, KL: 78.93998718261719, Loss: 0.02468729019165039, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8142/20000], Bound: 0.39210420846939087, Entropy: 139.97506713867188, Temp: 2.62721586227417, KL: 75.99765014648438, Loss: 0.015417803078889847, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8143/20000], Bound: 0.39046600461006165, Entropy: 139.57269287109375, Temp: 2.6271743774414062, KL: 74.49867248535156, Loss: 0.017362045124173164, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8144/20000], Bound: 0.3772001266479492, Entropy: 141.5485076904297, Temp: 2.6271495819091797, KL: 70.15797424316406, Loss: 0.01834767311811447, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8145/20000], Bound: 0.39943283796310425, Entropy: 138.9098663330078, Temp: 2.6271069049835205, KL: 75.69175720214844, Loss: 0.020088285207748413, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8146/20000], Bound: 0.37887534499168396, Entropy: 141.2756805419922, Temp: 2.6270391941070557, KL: 71.97203063964844, Loss: 0.015805132687091827, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8147/20000], Bound: 0.3780444860458374, Entropy: 140.67855834960938, Temp: 2.6270084381103516, KL: 71.58815002441406, Loss: 0.016083301976323128, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8148/20000], Bound: 0.33137232065200806, Entropy: 140.877685546875, Temp: 2.627004623413086, KL: 56.644256591796875, Loss: 0.019952034577727318, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8149/20000], Bound: 0.3733520209789276, Entropy: 139.9469757080078, Temp: 2.626899480819702, KL: 69.44560241699219, Loss: 0.01761658303439617, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8150/20000], Bound: 0.3721899688243866, Entropy: 139.9796600341797, Temp: 2.6267940998077393, KL: 68.48841857910156, Loss: 0.018810249865055084, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8151/20000], Bound: 0.38895168900489807, Entropy: 138.7812957763672, Temp: 2.626664400100708, KL: 72.92875671386719, Loss: 0.019507575780153275, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8152/20000], Bound: 0.3932304382324219, Entropy: 137.36468505859375, Temp: 2.6265182495117188, KL: 75.34452819824219, Loss: 0.017279066145420074, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8153/20000], Bound: 0.382138192653656, Entropy: 141.64942932128906, Temp: 2.626404047012329, KL: 71.84373474121094, Loss: 0.017824197188019753, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8154/20000], Bound: 0.4203154146671295, Entropy: 137.66275024414062, Temp: 2.626296281814575, KL: 83.12033081054688, Loss: 0.01783977448940277, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8155/20000], Bound: 0.3919532895088196, Entropy: 139.44041442871094, Temp: 2.6262357234954834, KL: 75.49363708496094, Loss: 0.01628275401890278, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8156/20000], Bound: 0.4058000147342682, Entropy: 139.45204162597656, Temp: 2.626216411590576, KL: 77.81373596191406, Loss: 0.01962933875620365, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8157/20000], Bound: 0.4039105772972107, Entropy: 140.77337646484375, Temp: 2.626185655593872, KL: 78.28865051269531, Loss: 0.017656022682785988, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8158/20000], Bound: 0.3533211350440979, Entropy: 141.65277099609375, Temp: 2.6261801719665527, KL: 64.51107788085938, Loss: 0.016331544145941734, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8159/20000], Bound: 0.4043239951133728, Entropy: 141.0094451904297, Temp: 2.6261682510375977, KL: 79.47761535644531, Loss: 0.015625782310962677, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8160/20000], Bound: 0.3604615032672882, Entropy: 142.62611389160156, Temp: 2.626218557357788, KL: 65.95217895507812, Loss: 0.017358673736453056, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8161/20000], Bound: 0.36802399158477783, Entropy: 141.46620178222656, Temp: 2.62624454498291, KL: 68.8927001953125, Loss: 0.015794992446899414, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8162/20000], Bound: 0.36867719888687134, Entropy: 143.09359741210938, Temp: 2.6262872219085693, KL: 67.47276306152344, Loss: 0.0188492052257061, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8163/20000], Bound: 0.387335866689682, Entropy: 141.71302795410156, Temp: 2.626286268234253, KL: 71.33317565917969, Loss: 0.02164989896118641, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8164/20000], Bound: 0.38847851753234863, Entropy: 141.74755859375, Temp: 2.6262128353118896, KL: 73.77674865722656, Loss: 0.017627492547035217, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8165/20000], Bound: 0.3738769292831421, Entropy: 141.20364379882812, Temp: 2.626152753829956, KL: 69.16232299804688, Loss: 0.018432941287755966, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8166/20000], Bound: 0.3829944431781769, Entropy: 140.62643432617188, Temp: 2.626072883605957, KL: 73.22268676757812, Loss: 0.01566438190639019, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8167/20000], Bound: 0.3735741078853607, Entropy: 140.51438903808594, Temp: 2.6260387897491455, KL: 69.79403686523438, Loss: 0.017065396532416344, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8168/20000], Bound: 0.4196770191192627, Entropy: 140.98687744140625, Temp: 2.6260087490081787, KL: 81.49017333984375, Loss: 0.020570673048496246, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8169/20000], Bound: 0.3770725727081299, Entropy: 141.794677734375, Temp: 2.6259660720825195, KL: 69.68087768554688, Loss: 0.019176164641976357, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8170/20000], Bound: 0.39459720253944397, Entropy: 140.71481323242188, Temp: 2.6258912086486816, KL: 74.37496948242188, Loss: 0.01987951621413231, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8171/20000], Bound: 0.3798651695251465, Entropy: 140.7578125, Temp: 2.625793695449829, KL: 71.75830078125, Loss: 0.01673915795981884, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8172/20000], Bound: 0.3842269480228424, Entropy: 142.98760986328125, Temp: 2.6257195472717285, KL: 71.32334899902344, Loss: 0.01995346136391163, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8173/20000], Bound: 0.3551569879055023, Entropy: 143.97654724121094, Temp: 2.6256093978881836, KL: 63.828765869140625, Loss: 0.01859218068420887, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8174/20000], Bound: 0.35429689288139343, Entropy: 144.9913787841797, Temp: 2.6254611015319824, KL: 64.04469299316406, Loss: 0.017726952210068703, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8175/20000], Bound: 0.39757055044174194, Entropy: 144.2299041748047, Temp: 2.6252946853637695, KL: 77.91950988769531, Loss: 0.014783731661736965, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8176/20000], Bound: 0.4170549213886261, Entropy: 141.8583526611328, Temp: 2.6252152919769287, KL: 80.40274047851562, Loss: 0.021118974313139915, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8177/20000], Bound: 0.3953757584095001, Entropy: 140.58396911621094, Temp: 2.625115394592285, KL: 75.15187072753906, Loss: 0.01882651448249817, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8178/20000], Bound: 0.39927971363067627, Entropy: 142.78321838378906, Temp: 2.6250159740448, KL: 77.36302185058594, Loss: 0.01679839938879013, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8179/20000], Bound: 0.3891424536705017, Entropy: 139.98207092285156, Temp: 2.6249606609344482, KL: 75.30317687988281, Loss: 0.01507438626140356, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8180/20000], Bound: 0.3974933922290802, Entropy: 142.07257080078125, Temp: 2.624967098236084, KL: 75.35456848144531, Loss: 0.019622359424829483, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8181/20000], Bound: 0.39317384362220764, Entropy: 142.35696411132812, Temp: 2.624950885772705, KL: 75.7646484375, Loss: 0.016430949792265892, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8182/20000], Bound: 0.3734515309333801, Entropy: 143.97532653808594, Temp: 2.6249704360961914, KL: 69.36395263671875, Loss: 0.0178084634244442, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8183/20000], Bound: 0.38602593541145325, Entropy: 141.20448303222656, Temp: 2.6249747276306152, KL: 72.60198974609375, Loss: 0.01850030943751335, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8184/20000], Bound: 0.381442129611969, Entropy: 141.31396484375, Temp: 2.624965190887451, KL: 70.39125061035156, Loss: 0.020196327939629555, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8185/20000], Bound: 0.3904228210449219, Entropy: 141.69468688964844, Temp: 2.624905586242676, KL: 75.76864624023438, Loss: 0.014895767904818058, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8186/20000], Bound: 0.38736286759376526, Entropy: 141.88526916503906, Temp: 2.624913215637207, KL: 74.33085632324219, Loss: 0.015943193808197975, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8187/20000], Bound: 0.3822139799594879, Entropy: 141.08984375, Temp: 2.62495756149292, KL: 71.17657470703125, Loss: 0.019122719764709473, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8188/20000], Bound: 0.40807315707206726, Entropy: 140.54884338378906, Temp: 2.6249682903289795, KL: 78.60111999511719, Loss: 0.019406739622354507, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8189/20000], Bound: 0.3938257694244385, Entropy: 140.7284393310547, Temp: 2.624971628189087, KL: 76.47444152832031, Loss: 0.01544193271547556, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8190/20000], Bound: 0.3976186215877533, Entropy: 142.70672607421875, Temp: 2.6250288486480713, KL: 76.31689453125, Loss: 0.01786011829972267, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8191/20000], Bound: 0.3684466779232025, Entropy: 141.179931640625, Temp: 2.6250922679901123, KL: 68.80130004882812, Loss: 0.016185086220502853, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8192/20000], Bound: 0.37470072507858276, Entropy: 141.70352172851562, Temp: 2.62516188621521, KL: 69.21623229980469, Loss: 0.018767468631267548, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8193/20000], Bound: 0.39322638511657715, Entropy: 142.52798461914062, Temp: 2.6251935958862305, KL: 73.87091064453125, Loss: 0.020069636404514313, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8194/20000], Bound: 0.3776760399341583, Entropy: 142.2152557373047, Temp: 2.6251864433288574, KL: 69.29556274414062, Loss: 0.020231299102306366, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8195/20000], Bound: 0.3893378674983978, Entropy: 140.83041381835938, Temp: 2.625123977661133, KL: 74.98916625976562, Loss: 0.0157823096960783, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8196/20000], Bound: 0.3920123279094696, Entropy: 140.1653289794922, Temp: 2.625110626220703, KL: 74.21646118164062, Loss: 0.018735991790890694, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8197/20000], Bound: 0.3855322301387787, Entropy: 142.3885040283203, Temp: 2.6250874996185303, KL: 72.6954345703125, Loss: 0.018051734194159508, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8198/20000], Bound: 0.3803766667842865, Entropy: 141.4403076171875, Temp: 2.625061273574829, KL: 72.31840515136719, Loss: 0.015944264829158783, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8199/20000], Bound: 0.3849491775035858, Entropy: 142.46644592285156, Temp: 2.625067949295044, KL: 70.40838623046875, Loss: 0.022087130695581436, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8200/20000], Bound: 0.4008139371871948, Entropy: 141.8365936279297, Temp: 2.624990224838257, KL: 77.44285583496094, Loss: 0.017508061602711678, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8201/20000], Bound: 0.36704540252685547, Entropy: 141.46522521972656, Temp: 2.6249420642852783, KL: 69.48239135742188, Loss: 0.014134855940937996, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8202/20000], Bound: 0.42063960433006287, Entropy: 141.48977661132812, Temp: 2.624950408935547, KL: 83.19619750976562, Loss: 0.01786664128303528, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8203/20000], Bound: 0.38687440752983093, Entropy: 139.94544982910156, Temp: 2.6249947547912598, KL: 73.29258728027344, Loss: 0.01765238679945469, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8204/20000], Bound: 0.42109909653663635, Entropy: 140.17745971679688, Temp: 2.6250391006469727, KL: 81.93974304199219, Loss: 0.020527299493551254, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8205/20000], Bound: 0.3860504627227783, Entropy: 143.66812133789062, Temp: 2.625065803527832, KL: 72.20703125, Loss: 0.01926693320274353, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8206/20000], Bound: 0.379008948802948, Entropy: 141.89842224121094, Temp: 2.6250617504119873, KL: 71.51029968261719, Loss: 0.016737472265958786, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8207/20000], Bound: 0.3712618350982666, Entropy: 140.5108184814453, Temp: 2.6250715255737305, KL: 69.07192993164062, Loss: 0.01718365214765072, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8208/20000], Bound: 0.38477709889411926, Entropy: 141.41375732421875, Temp: 2.6250762939453125, KL: 73.72596740722656, Loss: 0.015673616901040077, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8209/20000], Bound: 0.362405389547348, Entropy: 141.2541961669922, Temp: 2.6251211166381836, KL: 66.88882446289062, Loss: 0.016598766669631004, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8210/20000], Bound: 0.3958871364593506, Entropy: 142.177978515625, Temp: 2.62515926361084, KL: 76.81709289550781, Loss: 0.015940723940730095, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8211/20000], Bound: 0.4040380120277405, Entropy: 141.08197021484375, Temp: 2.6252403259277344, KL: 79.03802490234375, Loss: 0.016290301457047462, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8212/20000], Bound: 0.37621212005615234, Entropy: 139.74053955078125, Temp: 2.6253628730773926, KL: 69.37521362304688, Loss: 0.019285768270492554, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8213/20000], Bound: 0.3898589611053467, Entropy: 140.65863037109375, Temp: 2.6254332065582275, KL: 73.72149658203125, Loss: 0.01848817616701126, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8214/20000], Bound: 0.40583643317222595, Entropy: 141.59226989746094, Temp: 2.625488042831421, KL: 77.49822998046875, Loss: 0.02024322934448719, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8215/20000], Bound: 0.3935885429382324, Entropy: 137.75929260253906, Temp: 2.62551212310791, KL: 76.77706909179688, Loss: 0.014739632606506348, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8216/20000], Bound: 0.40160059928894043, Entropy: 140.20701599121094, Temp: 2.625601291656494, KL: 76.73565673828125, Loss: 0.01930413953959942, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8217/20000], Bound: 0.40409111976623535, Entropy: 141.92782592773438, Temp: 2.6256701946258545, KL: 78.10267639160156, Loss: 0.018106454983353615, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8218/20000], Bound: 0.36982449889183044, Entropy: 141.4990234375, Temp: 2.625746250152588, KL: 68.2869873046875, Loss: 0.017910761758685112, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8219/20000], Bound: 0.42222821712493896, Entropy: 142.40440368652344, Temp: 2.6257946491241455, KL: 84.33143615722656, Loss: 0.01663665659725666, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8220/20000], Bound: 0.40326187014579773, Entropy: 138.40359497070312, Temp: 2.6259002685546875, KL: 79.14923095703125, Loss: 0.01564791426062584, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8221/20000], Bound: 0.3996225893497467, Entropy: 141.8486785888672, Temp: 2.62605619430542, KL: 75.62699890136719, Loss: 0.020307747647166252, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8222/20000], Bound: 0.3932785093784332, Entropy: 142.15016174316406, Temp: 2.6261627674102783, KL: 74.49545288085938, Loss: 0.01891867071390152, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8223/20000], Bound: 0.3796396851539612, Entropy: 141.7967987060547, Temp: 2.6262450218200684, KL: 70.625732421875, Loss: 0.018776854500174522, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8224/20000], Bound: 0.41497141122817993, Entropy: 141.15982055664062, Temp: 2.6262927055358887, KL: 78.93205261230469, Loss: 0.022732015699148178, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8225/20000], Bound: 0.39073020219802856, Entropy: 140.7041015625, Temp: 2.6262731552124023, KL: 75.58554077148438, Loss: 0.015429910272359848, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8226/20000], Bound: 0.38695573806762695, Entropy: 139.81683349609375, Temp: 2.6263065338134766, KL: 72.67228698730469, Loss: 0.01889112964272499, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8227/20000], Bound: 0.4181358814239502, Entropy: 142.8600616455078, Temp: 2.6263163089752197, KL: 81.79696655273438, Loss: 0.01909969188272953, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8228/20000], Bound: 0.37875980138778687, Entropy: 139.98129272460938, Temp: 2.626335382461548, KL: 69.93382263183594, Loss: 0.019615473225712776, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8229/20000], Bound: 0.36465901136398315, Entropy: 140.37557983398438, Temp: 2.6263089179992676, KL: 67.52000427246094, Loss: 0.016608543694019318, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8230/20000], Bound: 0.3719415068626404, Entropy: 140.68167114257812, Temp: 2.6262848377227783, KL: 69.46453857421875, Loss: 0.016813533380627632, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8231/20000], Bound: 0.39675432443618774, Entropy: 141.739990234375, Temp: 2.6262669563293457, KL: 76.09933471679688, Loss: 0.017803963273763657, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8232/20000], Bound: 0.37289026379585266, Entropy: 141.67738342285156, Temp: 2.6262624263763428, KL: 67.41911315917969, Loss: 0.02121952921152115, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8233/20000], Bound: 0.38665664196014404, Entropy: 140.06993103027344, Temp: 2.626176357269287, KL: 72.78274536132812, Loss: 0.018514828756451607, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8234/20000], Bound: 0.37760359048843384, Entropy: 140.7558135986328, Temp: 2.6260857582092285, KL: 69.98509216308594, Loss: 0.018886545673012733, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8235/20000], Bound: 0.3968285620212555, Entropy: 140.98953247070312, Temp: 2.6259734630584717, KL: 76.47616577148438, Loss: 0.017124874517321587, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8236/20000], Bound: 0.4023912847042084, Entropy: 138.04891967773438, Temp: 2.6258974075317383, KL: 76.5723876953125, Loss: 0.020063504576683044, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8237/20000], Bound: 0.3897126317024231, Entropy: 143.28578186035156, Temp: 2.625802993774414, KL: 75.48518371582031, Loss: 0.01505244616419077, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8238/20000], Bound: 0.385757178068161, Entropy: 142.32986450195312, Temp: 2.6257758140563965, KL: 72.733642578125, Loss: 0.018109334632754326, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8239/20000], Bound: 0.40175390243530273, Entropy: 140.14866638183594, Temp: 2.6257452964782715, KL: 78.78530883789062, Loss: 0.015488962642848492, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8240/20000], Bound: 0.36683279275894165, Entropy: 142.92279052734375, Temp: 2.625779867172241, KL: 65.28775024414062, Loss: 0.022016797214746475, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8241/20000], Bound: 0.37307801842689514, Entropy: 142.37948608398438, Temp: 2.625706434249878, KL: 69.49320983886719, Loss: 0.017367105931043625, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8242/20000], Bound: 0.3462160527706146, Entropy: 144.20144653320312, Temp: 2.6256344318389893, KL: 62.44242858886719, Loss: 0.016550613567233086, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8243/20000], Bound: 0.3912025988101959, Entropy: 140.5522918701172, Temp: 2.6255509853363037, KL: 76.51661682128906, Loss: 0.013910706155002117, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8244/20000], Bound: 0.39751508831977844, Entropy: 142.03233337402344, Temp: 2.6255574226379395, KL: 77.72715759277344, Loss: 0.01512210350483656, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8245/20000], Bound: 0.38647061586380005, Entropy: 142.12782287597656, Temp: 2.6256279945373535, KL: 73.22099304199219, Loss: 0.017572563141584396, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8246/20000], Bound: 0.40039941668510437, Entropy: 142.8468780517578, Temp: 2.6256964206695557, KL: 75.41815185546875, Loss: 0.021138301119208336, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8247/20000], Bound: 0.370391309261322, Entropy: 141.0703582763672, Temp: 2.6257095336914062, KL: 67.81867980957031, Loss: 0.01910713128745556, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8248/20000], Bound: 0.3955075442790985, Entropy: 142.11502075195312, Temp: 2.625678062438965, KL: 76.68022155761719, Loss: 0.01599530503153801, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8249/20000], Bound: 0.38664883375167847, Entropy: 141.35386657714844, Temp: 2.6256957054138184, KL: 73.25679016113281, Loss: 0.01760323904454708, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8250/20000], Bound: 0.3873272240161896, Entropy: 140.17623901367188, Temp: 2.625715970993042, KL: 71.49969482421875, Loss: 0.02132323384284973, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8251/20000], Bound: 0.3900516927242279, Entropy: 140.98280334472656, Temp: 2.625666856765747, KL: 73.80413818359375, Loss: 0.01843976601958275, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8252/20000], Bound: 0.3952110707759857, Entropy: 142.61407470703125, Temp: 2.625614881515503, KL: 74.85833740234375, Loss: 0.01929864101111889, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8253/20000], Bound: 0.3926413357257843, Entropy: 141.75701904296875, Temp: 2.62554931640625, KL: 75.23747253417969, Loss: 0.017145315185189247, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8254/20000], Bound: 0.3892921805381775, Entropy: 140.2685089111328, Temp: 2.6255102157592773, KL: 75.28810119628906, Loss: 0.01519190426915884, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8255/20000], Bound: 0.4009493589401245, Entropy: 140.12139892578125, Temp: 2.6255300045013428, KL: 76.37812805175781, Loss: 0.01961776427924633, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8256/20000], Bound: 0.3583116829395294, Entropy: 143.3626708984375, Temp: 2.6255290508270264, KL: 66.06503295898438, Loss: 0.015998629853129387, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8257/20000], Bound: 0.3516247570514679, Entropy: 142.096435546875, Temp: 2.6255335807800293, KL: 63.864532470703125, Loss: 0.016667017713189125, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8258/20000], Bound: 0.4238750636577606, Entropy: 141.49989318847656, Temp: 2.6255221366882324, KL: 83.66888427734375, Loss: 0.018852535635232925, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8259/20000], Bound: 0.3981298804283142, Entropy: 140.7830810546875, Temp: 2.6255340576171875, KL: 75.98606872558594, Loss: 0.01878175139427185, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8260/20000], Bound: 0.3701787292957306, Entropy: 141.5301055908203, Temp: 2.6255390644073486, KL: 69.77593994140625, Loss: 0.015263962559401989, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8261/20000], Bound: 0.3880389630794525, Entropy: 144.2138671875, Temp: 2.6255764961242676, KL: 75.83049011230469, Loss: 0.01346735842525959, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8262/20000], Bound: 0.40646350383758545, Entropy: 140.36431884765625, Temp: 2.625697374343872, KL: 78.94351196289062, Loss: 0.01784859411418438, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8263/20000], Bound: 0.38287845253944397, Entropy: 142.81944274902344, Temp: 2.6258280277252197, KL: 72.20329284667969, Loss: 0.017539385706186295, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8264/20000], Bound: 0.39289507269859314, Entropy: 140.8097381591797, Temp: 2.6259474754333496, KL: 75.73060607910156, Loss: 0.016351519152522087, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8265/20000], Bound: 0.3804207742214203, Entropy: 142.96961975097656, Temp: 2.6260910034179688, KL: 71.66557312011719, Loss: 0.017221784219145775, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8266/20000], Bound: 0.39589062333106995, Entropy: 143.13088989257812, Temp: 2.626225233078003, KL: 75.973388671875, Loss: 0.01756088249385357, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8267/20000], Bound: 0.3909023404121399, Entropy: 143.28602600097656, Temp: 2.626361846923828, KL: 73.57267761230469, Loss: 0.019358331337571144, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8268/20000], Bound: 0.4144976735115051, Entropy: 141.1041717529297, Temp: 2.6264593601226807, KL: 78.7764892578125, Loss: 0.02275785245001316, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8269/20000], Bound: 0.39363494515419006, Entropy: 144.09475708007812, Temp: 2.6264827251434326, KL: 75.00868225097656, Loss: 0.018143031746149063, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8270/20000], Bound: 0.4060288965702057, Entropy: 142.44247436523438, Temp: 2.6265053749084473, KL: 78.98506164550781, Loss: 0.017532233148813248, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8271/20000], Bound: 0.3807923197746277, Entropy: 142.4573974609375, Temp: 2.6265528202056885, KL: 72.77859497070312, Loss: 0.015310263261198997, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8272/20000], Bound: 0.414156973361969, Entropy: 141.8502960205078, Temp: 2.626638889312744, KL: 80.51228332519531, Loss: 0.0192599855363369, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8273/20000], Bound: 0.38994738459587097, Entropy: 144.08921813964844, Temp: 2.6267192363739014, KL: 74.82151794433594, Loss: 0.016455769538879395, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8274/20000], Bound: 0.3785393536090851, Entropy: 142.8272247314453, Temp: 2.626821994781494, KL: 72.157958984375, Loss: 0.0152661157771945, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8275/20000], Bound: 0.3954673707485199, Entropy: 139.91961669921875, Temp: 2.626955986022949, KL: 77.03866577148438, Loss: 0.01530484203249216, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8276/20000], Bound: 0.39948534965515137, Entropy: 141.31297302246094, Temp: 2.627135753631592, KL: 77.875244140625, Loss: 0.015962352976202965, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8277/20000], Bound: 0.42014428973197937, Entropy: 139.7235107421875, Temp: 2.627347946166992, KL: 82.06263732910156, Loss: 0.019766421988606453, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8278/20000], Bound: 0.38975241780281067, Entropy: 141.68893432617188, Temp: 2.6275386810302734, KL: 73.82191467285156, Loss: 0.0182587169110775, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8279/20000], Bound: 0.39454731345176697, Entropy: 139.42088317871094, Temp: 2.6277050971984863, KL: 75.90664672851562, Loss: 0.01695462316274643, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8280/20000], Bound: 0.39858606457710266, Entropy: 140.83473205566406, Temp: 2.627880334854126, KL: 77.063232421875, Loss: 0.017011715099215508, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8281/20000], Bound: 0.37352070212364197, Entropy: 140.67385864257812, Temp: 2.6280667781829834, KL: 67.50341796875, Loss: 0.0214132871478796, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8282/20000], Bound: 0.42955419421195984, Entropy: 139.91928100585938, Temp: 2.628147840499878, KL: 87.01609802246094, Loss: 0.015834948047995567, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8283/20000], Bound: 0.3848392367362976, Entropy: 142.0196990966797, Temp: 2.6283063888549805, KL: 72.26406860351562, Loss: 0.018522709608078003, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8284/20000], Bound: 0.42150044441223145, Entropy: 140.97434997558594, Temp: 2.628432273864746, KL: 82.67431640625, Loss: 0.019400496035814285, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8285/20000], Bound: 0.3694957494735718, Entropy: 139.117431640625, Temp: 2.628553867340088, KL: 68.24468994140625, Loss: 0.017838917672634125, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8286/20000], Bound: 0.38866883516311646, Entropy: 141.8895721435547, Temp: 2.6286427974700928, KL: 74.62057495117188, Loss: 0.016151780262589455, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8287/20000], Bound: 0.37426045536994934, Entropy: 141.47703552246094, Temp: 2.6287574768066406, KL: 69.0709228515625, Loss: 0.01883683353662491, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8288/20000], Bound: 0.40795964002609253, Entropy: 141.44284057617188, Temp: 2.628826141357422, KL: 79.91154479980469, Loss: 0.016891038045287132, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8289/20000], Bound: 0.4237233102321625, Entropy: 139.73416137695312, Temp: 2.6289288997650146, KL: 83.86293029785156, Loss: 0.018436243757605553, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8290/20000], Bound: 0.3822559714317322, Entropy: 140.35189819335938, Temp: 2.6290500164031982, KL: 70.79974365234375, Loss: 0.019899191334843636, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8291/20000], Bound: 0.3826255798339844, Entropy: 139.9900360107422, Temp: 2.6291122436523438, KL: 72.83668518066406, Loss: 0.016228092834353447, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8292/20000], Bound: 0.37218621373176575, Entropy: 143.3593292236328, Temp: 2.629194736480713, KL: 70.1954345703125, Loss: 0.015582277439534664, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8293/20000], Bound: 0.3907524049282074, Entropy: 141.56488037109375, Temp: 2.629296064376831, KL: 72.84843444824219, Loss: 0.020680395886301994, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8294/20000], Bound: 0.37407824397087097, Entropy: 139.09710693359375, Temp: 2.6293349266052246, KL: 68.28839111328125, Loss: 0.02023136243224144, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8295/20000], Bound: 0.4060107171535492, Entropy: 141.7124786376953, Temp: 2.6293065547943115, KL: 78.66165161132812, Loss: 0.018168380483984947, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8296/20000], Bound: 0.3778192400932312, Entropy: 142.02284240722656, Temp: 2.6292951107025146, KL: 70.79850769042969, Loss: 0.017485078424215317, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8297/20000], Bound: 0.387644499540329, Entropy: 139.216552734375, Temp: 2.6292805671691895, KL: 74.67353820800781, Loss: 0.015492824837565422, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8298/20000], Bound: 0.38944870233535767, Entropy: 140.16049194335938, Temp: 2.6293139457702637, KL: 74.2772216796875, Loss: 0.01724243350327015, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8299/20000], Bound: 0.3707764744758606, Entropy: 139.12969970703125, Temp: 2.62935733795166, KL: 69.74128723144531, Loss: 0.015688328072428703, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8300/20000], Bound: 0.4029248356819153, Entropy: 140.32791137695312, Temp: 2.629420518875122, KL: 76.89945983886719, Loss: 0.01977737434208393, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8301/20000], Bound: 0.4017680287361145, Entropy: 138.6305694580078, Temp: 2.629456043243408, KL: 77.64761352539062, Loss: 0.01770373247563839, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8302/20000], Bound: 0.38145795464515686, Entropy: 140.88693237304688, Temp: 2.6295065879821777, KL: 72.52493286132812, Loss: 0.016186602413654327, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8303/20000], Bound: 0.4057857096195221, Entropy: 140.2038116455078, Temp: 2.62957763671875, KL: 79.74012756347656, Loss: 0.015993326902389526, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8304/20000], Bound: 0.3931417763233185, Entropy: 140.75869750976562, Temp: 2.629697799682617, KL: 75.33413696289062, Loss: 0.017282765358686447, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8305/20000], Bound: 0.36423972249031067, Entropy: 142.80404663085938, Temp: 2.6298227310180664, KL: 66.92192077636719, Loss: 0.017553135752677917, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8306/20000], Bound: 0.4014400243759155, Entropy: 140.89016723632812, Temp: 2.6299145221710205, KL: 76.81974792480469, Loss: 0.019098157063126564, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8307/20000], Bound: 0.41222134232521057, Entropy: 140.91294860839844, Temp: 2.6299872398376465, KL: 80.75630187988281, Loss: 0.01772579923272133, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8308/20000], Bound: 0.38540568947792053, Entropy: 141.26658630371094, Temp: 2.630082130432129, KL: 72.70848083496094, Loss: 0.018005477264523506, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8309/20000], Bound: 0.35737472772598267, Entropy: 140.91893005371094, Temp: 2.6301612854003906, KL: 65.80635070800781, Loss: 0.01603560335934162, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8310/20000], Bound: 0.40269845724105835, Entropy: 141.7125701904297, Temp: 2.630234718322754, KL: 77.14907836914062, Loss: 0.019183605909347534, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8311/20000], Bound: 0.4251766502857208, Entropy: 140.66053771972656, Temp: 2.6302905082702637, KL: 83.956298828125, Loss: 0.01912115141749382, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8312/20000], Bound: 0.3931058943271637, Entropy: 139.01390075683594, Temp: 2.630357503890991, KL: 75.53871154785156, Loss: 0.016880838200449944, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8313/20000], Bound: 0.41078758239746094, Entropy: 139.46470642089844, Temp: 2.6304426193237305, KL: 82.12701416015625, Loss: 0.01430733036249876, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8314/20000], Bound: 0.3650684058666229, Entropy: 140.4652557373047, Temp: 2.6306135654449463, KL: 67.40803527832031, Loss: 0.017077652737498283, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8315/20000], Bound: 0.41479113698005676, Entropy: 138.83273315429688, Temp: 2.6307568550109863, KL: 80.82096862792969, Loss: 0.019082659855484962, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8316/20000], Bound: 0.37817227840423584, Entropy: 141.47341918945312, Temp: 2.6308913230895996, KL: 70.67414855957031, Loss: 0.01792808622121811, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8317/20000], Bound: 0.4007396697998047, Entropy: 139.26766967773438, Temp: 2.6309995651245117, KL: 76.83572387695312, Loss: 0.01868543028831482, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8318/20000], Bound: 0.37855711579322815, Entropy: 139.50201416015625, Temp: 2.631093978881836, KL: 71.90914916992188, Loss: 0.01579217240214348, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8319/20000], Bound: 0.39251381158828735, Entropy: 138.2880859375, Temp: 2.631208658218384, KL: 76.50289916992188, Loss: 0.014728951267898083, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8320/20000], Bound: 0.3741655945777893, Entropy: 140.32749938964844, Temp: 2.631378412246704, KL: 70.45062255859375, Loss: 0.01618635654449463, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8321/20000], Bound: 0.37218132615089417, Entropy: 142.08058166503906, Temp: 2.6315484046936035, KL: 68.59130859375, Loss: 0.01865021511912346, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8322/20000], Bound: 0.3918455243110657, Entropy: 140.9657745361328, Temp: 2.631666660308838, KL: 75.1717529296875, Loss: 0.016892723739147186, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8323/20000], Bound: 0.400147408246994, Entropy: 138.42568969726562, Temp: 2.631795644760132, KL: 77.52653503417969, Loss: 0.017048869282007217, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8324/20000], Bound: 0.40407097339630127, Entropy: 139.57522583007812, Temp: 2.6319403648376465, KL: 76.27250671386719, Loss: 0.021639976650476456, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8325/20000], Bound: 0.39014315605163574, Entropy: 139.197265625, Temp: 2.6320130825042725, KL: 73.59654235839844, Loss: 0.018946573138237, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8326/20000], Bound: 0.37272629141807556, Entropy: 139.94686889648438, Temp: 2.6320579051971436, KL: 67.31613159179688, Loss: 0.021370580419898033, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8327/20000], Bound: 0.4172792434692383, Entropy: 141.56964111328125, Temp: 2.632009267807007, KL: 81.3121337890625, Loss: 0.01959274709224701, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8328/20000], Bound: 0.3471556603908539, Entropy: 140.61643981933594, Temp: 2.6319632530212402, KL: 61.71624755859375, Loss: 0.018467353656888008, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8329/20000], Bound: 0.3909471333026886, Entropy: 138.3650360107422, Temp: 2.631862163543701, KL: 74.9620361328125, Loss: 0.016795657575130463, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8330/20000], Bound: 0.39450952410697937, Entropy: 139.08277893066406, Temp: 2.6317946910858154, KL: 74.60569763183594, Loss: 0.01944856531918049, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8331/20000], Bound: 0.39632391929626465, Entropy: 139.45321655273438, Temp: 2.6317083835601807, KL: 76.99006652832031, Loss: 0.01592836156487465, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8332/20000], Bound: 0.40495583415031433, Entropy: 141.298583984375, Temp: 2.6316773891448975, KL: 79.93003845214844, Loss: 0.015188016928732395, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8333/20000], Bound: 0.4134863317012787, Entropy: 141.18499755859375, Temp: 2.6317203044891357, KL: 80.38691711425781, Loss: 0.01917075365781784, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8334/20000], Bound: 0.3717552125453949, Entropy: 142.514892578125, Temp: 2.631761074066162, KL: 69.18525695800781, Loss: 0.01729409210383892, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8335/20000], Bound: 0.3863780200481415, Entropy: 140.6457061767578, Temp: 2.631789445877075, KL: 73.42056274414062, Loss: 0.017203180119395256, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8336/20000], Bound: 0.37915700674057007, Entropy: 140.70994567871094, Temp: 2.6318254470825195, KL: 70.40121459960938, Loss: 0.018990544602274895, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8337/20000], Bound: 0.39288777112960815, Entropy: 142.6073760986328, Temp: 2.631823778152466, KL: 74.68658447265625, Loss: 0.01839403808116913, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8338/20000], Bound: 0.37179961800575256, Entropy: 140.65122985839844, Temp: 2.6318163871765137, KL: 68.84384155273438, Loss: 0.01796714961528778, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8339/20000], Bound: 0.37588807940483093, Entropy: 141.3702850341797, Temp: 2.6317877769470215, KL: 71.10111999511719, Loss: 0.015886077657341957, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8340/20000], Bound: 0.3975253403186798, Entropy: 141.244873046875, Temp: 2.631787061691284, KL: 76.36640930175781, Loss: 0.017784826457500458, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8341/20000], Bound: 0.4125364124774933, Entropy: 140.6927947998047, Temp: 2.6317977905273438, KL: 80.66917419433594, Loss: 0.018092138692736626, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8342/20000], Bound: 0.3703860342502594, Entropy: 139.4031219482422, Temp: 2.6318295001983643, KL: 68.16764831542969, Loss: 0.018491562455892563, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8343/20000], Bound: 0.4010305106639862, Entropy: 141.00376892089844, Temp: 2.631824016571045, KL: 76.46197509765625, Loss: 0.019567346200346947, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8344/20000], Bound: 0.38703399896621704, Entropy: 141.26043701171875, Temp: 2.63179874420166, KL: 74.50827026367188, Loss: 0.015497611835598946, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8345/20000], Bound: 0.3995906114578247, Entropy: 140.40829467773438, Temp: 2.631821393966675, KL: 76.39332580566406, Loss: 0.018889818340539932, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8346/20000], Bound: 0.4227578341960907, Entropy: 140.8321075439453, Temp: 2.631833076477051, KL: 85.09918212890625, Loss: 0.015562083572149277, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8347/20000], Bound: 0.39493778347969055, Entropy: 141.3629608154297, Temp: 2.631927013397217, KL: 75.141845703125, Loss: 0.018669607117772102, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8348/20000], Bound: 0.38650646805763245, Entropy: 140.0822296142578, Temp: 2.6320018768310547, KL: 72.61482238769531, Loss: 0.018806559965014458, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8349/20000], Bound: 0.4098057448863983, Entropy: 140.10157775878906, Temp: 2.632047653198242, KL: 78.95884704589844, Loss: 0.019787097349762917, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8350/20000], Bound: 0.4002455174922943, Entropy: 140.57481384277344, Temp: 2.6320738792419434, KL: 77.31507873535156, Loss: 0.017508607357740402, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8351/20000], Bound: 0.3631007671356201, Entropy: 142.3221893310547, Temp: 2.63211727142334, KL: 66.94332885742188, Loss: 0.016925496980547905, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8352/20000], Bound: 0.40684473514556885, Entropy: 140.40634155273438, Temp: 2.632145881652832, KL: 78.89830017089844, Loss: 0.018222007900476456, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8353/20000], Bound: 0.3840864300727844, Entropy: 140.10848999023438, Temp: 2.6321847438812256, KL: 70.92788696289062, Loss: 0.020684434100985527, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8354/20000], Bound: 0.4050499498844147, Entropy: 141.79696655273438, Temp: 2.63215708732605, KL: 78.75595092773438, Loss: 0.01747717149555683, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8355/20000], Bound: 0.4042168855667114, Entropy: 140.42898559570312, Temp: 2.632157802581787, KL: 76.81864929199219, Loss: 0.02068694494664669, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8356/20000], Bound: 0.3704977333545685, Entropy: 140.8098602294922, Temp: 2.6321191787719727, KL: 69.48382568359375, Loss: 0.016053814440965652, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8357/20000], Bound: 0.393721342086792, Entropy: 141.36297607421875, Temp: 2.6321001052856445, KL: 75.75413513183594, Loss: 0.01683173142373562, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8358/20000], Bound: 0.3695223927497864, Entropy: 141.52890014648438, Temp: 2.6321091651916504, KL: 68.52607727050781, Loss: 0.017349323257803917, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8359/20000], Bound: 0.3926645815372467, Entropy: 140.909423828125, Temp: 2.6321051120758057, KL: 76.00875854492188, Loss: 0.01576134003698826, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8360/20000], Bound: 0.4049609303474426, Entropy: 141.65689086914062, Temp: 2.632147789001465, KL: 78.06291198730469, Loss: 0.018743276596069336, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8361/20000], Bound: 0.40539443492889404, Entropy: 141.30233764648438, Temp: 2.6321866512298584, KL: 79.0875244140625, Loss: 0.01704234629869461, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8362/20000], Bound: 0.3553469181060791, Entropy: 139.5845947265625, Temp: 2.632256269454956, KL: 63.7579345703125, Loss: 0.0188762117177248, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8363/20000], Bound: 0.3839109539985657, Entropy: 141.5692596435547, Temp: 2.6322598457336426, KL: 71.42971801757812, Loss: 0.0196357574313879, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8364/20000], Bound: 0.4050886631011963, Entropy: 141.6057891845703, Temp: 2.632220983505249, KL: 76.94236755371094, Loss: 0.020944740623235703, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8365/20000], Bound: 0.4019928574562073, Entropy: 142.30873107910156, Temp: 2.6321425437927246, KL: 74.08114624023438, Loss: 0.024633999913930893, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8366/20000], Bound: 0.39974501729011536, Entropy: 142.74159240722656, Temp: 2.6319518089294434, KL: 76.65242004394531, Loss: 0.01848548650741577, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8367/20000], Bound: 0.3786005675792694, Entropy: 140.8056640625, Temp: 2.631779670715332, KL: 72.47216796875, Loss: 0.014752962626516819, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8368/20000], Bound: 0.39780232310295105, Entropy: 138.10211181640625, Temp: 2.6316754817962646, KL: 77.60116577148438, Loss: 0.015592500567436218, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8369/20000], Bound: 0.3864806890487671, Entropy: 139.670166015625, Temp: 2.6316375732421875, KL: 73.4571533203125, Loss: 0.01718856766819954, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8370/20000], Bound: 0.39087963104248047, Entropy: 141.08139038085938, Temp: 2.6316139698028564, KL: 75.54203796386719, Loss: 0.015653740614652634, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8371/20000], Bound: 0.4022754430770874, Entropy: 138.8815155029297, Temp: 2.6316394805908203, KL: 76.78596496582031, Loss: 0.01964985392987728, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8372/20000], Bound: 0.38368910551071167, Entropy: 140.3479766845703, Temp: 2.6316416263580322, KL: 74.44189453125, Loss: 0.013785758055746555, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8373/20000], Bound: 0.3790929317474365, Entropy: 142.03013610839844, Temp: 2.6317200660705566, KL: 72.52786254882812, Loss: 0.014914327301084995, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8374/20000], Bound: 0.402127742767334, Entropy: 141.78890991210938, Temp: 2.6318390369415283, KL: 76.52182006835938, Loss: 0.02007056027650833, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8375/20000], Bound: 0.42232781648635864, Entropy: 138.97312927246094, Temp: 2.6319169998168945, KL: 82.65277099609375, Loss: 0.01996130868792534, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8376/20000], Bound: 0.38303595781326294, Entropy: 140.16290283203125, Temp: 2.631983757019043, KL: 73.15618896484375, Loss: 0.015874546021223068, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8377/20000], Bound: 0.38969510793685913, Entropy: 141.73838806152344, Temp: 2.6320767402648926, KL: 74.68551635742188, Loss: 0.016630949452519417, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8378/20000], Bound: 0.3648413121700287, Entropy: 142.19589233398438, Temp: 2.6321861743927, KL: 67.22723388671875, Loss: 0.017313538119196892, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8379/20000], Bound: 0.38831251859664917, Entropy: 138.85696411132812, Temp: 2.632268190383911, KL: 75.51441955566406, Loss: 0.014295491389930248, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8380/20000], Bound: 0.39687564969062805, Entropy: 141.24131774902344, Temp: 2.632412910461426, KL: 77.17367553710938, Loss: 0.01589525118470192, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8381/20000], Bound: 0.39259400963783264, Entropy: 139.64703369140625, Temp: 2.632591485977173, KL: 74.32231140136719, Loss: 0.01893053576350212, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8382/20000], Bound: 0.3703463077545166, Entropy: 140.84970092773438, Temp: 2.6327342987060547, KL: 69.10736083984375, Loss: 0.01669316738843918, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8383/20000], Bound: 0.37074393033981323, Entropy: 139.64393615722656, Temp: 2.6328654289245605, KL: 69.33180236816406, Loss: 0.016481786966323853, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8384/20000], Bound: 0.3864199221134186, Entropy: 140.308349609375, Temp: 2.6329901218414307, KL: 75.96145629882812, Loss: 0.012412983924150467, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8385/20000], Bound: 0.38373371958732605, Entropy: 141.43844604492188, Temp: 2.633208990097046, KL: 72.50386047363281, Loss: 0.017507458105683327, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8386/20000], Bound: 0.40763071179389954, Entropy: 141.0386199951172, Temp: 2.6334068775177, KL: 79.72647094726562, Loss: 0.017108721658587456, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8387/20000], Bound: 0.3942764103412628, Entropy: 140.42593383789062, Temp: 2.6336207389831543, KL: 75.0887451171875, Loss: 0.018419422209262848, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8388/20000], Bound: 0.40195780992507935, Entropy: 140.85659790039062, Temp: 2.6338071823120117, KL: 78.86819458007812, Loss: 0.015540149062871933, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8389/20000], Bound: 0.3987296521663666, Entropy: 139.8739471435547, Temp: 2.634035587310791, KL: 76.78861999511719, Loss: 0.01767999306321144, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8390/20000], Bound: 0.4058535099029541, Entropy: 139.29843139648438, Temp: 2.6342551708221436, KL: 79.88127136230469, Loss: 0.0158185176551342, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8391/20000], Bound: 0.37892189621925354, Entropy: 140.35940551757812, Temp: 2.634511947631836, KL: 72.8232421875, Loss: 0.014289488084614277, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8392/20000], Bound: 0.4068356454372406, Entropy: 140.79067993164062, Temp: 2.63480281829834, KL: 78.40658569335938, Loss: 0.019178971648216248, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8393/20000], Bound: 0.3869248926639557, Entropy: 139.3603057861328, Temp: 2.6350579261779785, KL: 74.68644714355469, Loss: 0.015133995562791824, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8394/20000], Bound: 0.3906131982803345, Entropy: 139.1232147216797, Temp: 2.6353390216827393, KL: 75.88920593261719, Loss: 0.014887900091707706, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8395/20000], Bound: 0.3827458918094635, Entropy: 140.65945434570312, Temp: 2.635652542114258, KL: 73.02529907226562, Loss: 0.0160016231238842, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8396/20000], Bound: 0.3849722445011139, Entropy: 138.28428649902344, Temp: 2.6359639167785645, KL: 74.54475402832031, Loss: 0.01434077974408865, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8397/20000], Bound: 0.4075084626674652, Entropy: 139.09524536132812, Temp: 2.6363093852996826, KL: 79.33319091796875, Loss: 0.017818326130509377, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8398/20000], Bound: 0.4052566587924957, Entropy: 140.805908203125, Temp: 2.636640787124634, KL: 78.88395690917969, Loss: 0.01740061491727829, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8399/20000], Bound: 0.3707658052444458, Entropy: 141.3048095703125, Temp: 2.636965036392212, KL: 70.02481079101562, Loss: 0.0152168869972229, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8400/20000], Bound: 0.4199337959289551, Entropy: 139.03797912597656, Temp: 2.6372885704040527, KL: 83.27091979980469, Loss: 0.017466308549046516, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8401/20000], Bound: 0.39079296588897705, Entropy: 138.7483367919922, Temp: 2.637620449066162, KL: 74.48670959472656, Loss: 0.017670970410108566, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8402/20000], Bound: 0.40674686431884766, Entropy: 137.79722595214844, Temp: 2.6379234790802, KL: 78.26667785644531, Loss: 0.019426655024290085, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8403/20000], Bound: 0.39930427074432373, Entropy: 137.81187438964844, Temp: 2.6381826400756836, KL: 77.05949401855469, Loss: 0.017531396821141243, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8404/20000], Bound: 0.38828325271606445, Entropy: 139.3004913330078, Temp: 2.6384317874908447, KL: 74.35906982421875, Loss: 0.016537174582481384, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8405/20000], Bound: 0.3957517743110657, Entropy: 136.413818359375, Temp: 2.6386799812316895, KL: 77.15635681152344, Loss: 0.015371577814221382, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8406/20000], Bound: 0.4180183410644531, Entropy: 140.8123779296875, Temp: 2.638958692550659, KL: 81.96759033203125, Loss: 0.018852654844522476, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8407/20000], Bound: 0.3848230242729187, Entropy: 139.35691833496094, Temp: 2.6392204761505127, KL: 72.33309936523438, Loss: 0.018484245985746384, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8408/20000], Bound: 0.3979817032814026, Entropy: 137.98171997070312, Temp: 2.6394357681274414, KL: 76.18275451660156, Loss: 0.018466683104634285, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8409/20000], Bound: 0.39218348264694214, Entropy: 140.7896728515625, Temp: 2.639625072479248, KL: 75.18301391601562, Loss: 0.017140476033091545, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8410/20000], Bound: 0.3945055902004242, Entropy: 139.55531311035156, Temp: 2.639810562133789, KL: 75.35481262207031, Loss: 0.018103918060660362, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8411/20000], Bound: 0.38974034786224365, Entropy: 139.84844970703125, Temp: 2.6399757862091064, KL: 74.92189025878906, Loss: 0.016289329156279564, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8412/20000], Bound: 0.37553250789642334, Entropy: 141.0514678955078, Temp: 2.6401546001434326, KL: 70.50422668457031, Loss: 0.016905147582292557, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8413/20000], Bound: 0.38409310579299927, Entropy: 140.4227752685547, Temp: 2.6403167247772217, KL: 73.8670654296875, Loss: 0.015190155245363712, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8414/20000], Bound: 0.3960861265659332, Entropy: 139.509765625, Temp: 2.6405081748962402, KL: 77.97886657714844, Loss: 0.014020334929227829, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8415/20000], Bound: 0.3579515218734741, Entropy: 140.5962371826172, Temp: 2.640763282775879, KL: 66.29891967773438, Loss: 0.015496250241994858, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8416/20000], Bound: 0.3790445029735565, Entropy: 139.03257751464844, Temp: 2.641002893447876, KL: 71.93792724609375, Loss: 0.01609974540770054, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8417/20000], Bound: 0.3671324551105499, Entropy: 140.5424346923828, Temp: 2.641240119934082, KL: 66.30564880371094, Loss: 0.020354654639959335, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8418/20000], Bound: 0.38742804527282715, Entropy: 140.28396606445312, Temp: 2.641373634338379, KL: 75.26141357421875, Loss: 0.014388937503099442, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8419/20000], Bound: 0.3827584385871887, Entropy: 141.06507873535156, Temp: 2.64155912399292, KL: 71.84725952148438, Loss: 0.018297404050827026, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8420/20000], Bound: 0.39524877071380615, Entropy: 139.9522247314453, Temp: 2.6417064666748047, KL: 75.35591125488281, Loss: 0.018533319234848022, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8421/20000], Bound: 0.37303414940834045, Entropy: 140.97140502929688, Temp: 2.641829013824463, KL: 69.62863159179688, Loss: 0.01723133586347103, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8422/20000], Bound: 0.4047446548938751, Entropy: 140.28636169433594, Temp: 2.641930103302002, KL: 78.37602233886719, Loss: 0.018131446093320847, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8423/20000], Bound: 0.40468528866767883, Entropy: 140.5439453125, Temp: 2.6420300006866455, KL: 80.24163818359375, Loss: 0.014568375423550606, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8424/20000], Bound: 0.3785816431045532, Entropy: 142.02764892578125, Temp: 2.642200469970703, KL: 72.43727111816406, Loss: 0.014915373176336288, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8425/20000], Bound: 0.3752586245536804, Entropy: 141.2661895751953, Temp: 2.6423983573913574, KL: 69.416259765625, Loss: 0.018836557865142822, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8426/20000], Bound: 0.38710132241249084, Entropy: 139.67112731933594, Temp: 2.6425371170043945, KL: 72.90884399414062, Loss: 0.018673565238714218, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8427/20000], Bound: 0.37541481852531433, Entropy: 140.59014892578125, Temp: 2.64263916015625, KL: 69.2813720703125, Loss: 0.01917806826531887, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8428/20000], Bound: 0.39327001571655273, Entropy: 142.19248962402344, Temp: 2.6426842212677, KL: 71.40055847167969, Loss: 0.02492917701601982, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8429/20000], Bound: 0.3763323724269867, Entropy: 142.27513122558594, Temp: 2.6425821781158447, KL: 72.00967407226562, Loss: 0.014510883949697018, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8430/20000], Bound: 0.38405025005340576, Entropy: 141.75437927246094, Temp: 2.642540693283081, KL: 71.46209716796875, Loss: 0.01974022202193737, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8431/20000], Bound: 0.42861390113830566, Entropy: 140.87557983398438, Temp: 2.6424551010131836, KL: 86.03263854980469, Loss: 0.017337925732135773, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8432/20000], Bound: 0.4028400778770447, Entropy: 141.03598022460938, Temp: 2.642429828643799, KL: 78.54867553710938, Loss: 0.016738824546337128, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8433/20000], Bound: 0.3789690136909485, Entropy: 141.28823852539062, Temp: 2.642441511154175, KL: 71.24929809570312, Loss: 0.01737573742866516, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8434/20000], Bound: 0.3897000551223755, Entropy: 140.14599609375, Temp: 2.642446756362915, KL: 74.36813354492188, Loss: 0.017340395599603653, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8435/20000], Bound: 0.40064194798469543, Entropy: 141.02589416503906, Temp: 2.64245867729187, KL: 77.05503845214844, Loss: 0.018332824110984802, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8436/20000], Bound: 0.3880375623703003, Entropy: 143.2216796875, Temp: 2.6424691677093506, KL: 74.14553833007812, Loss: 0.016847072169184685, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8437/20000], Bound: 0.41600465774536133, Entropy: 139.0486602783203, Temp: 2.6424942016601562, KL: 81.19464111328125, Loss: 0.01919989287853241, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8438/20000], Bound: 0.37867411971092224, Entropy: 139.2428436279297, Temp: 2.642516613006592, KL: 72.21548461914062, Loss: 0.015388366766273975, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8439/20000], Bound: 0.3968287706375122, Entropy: 142.3751983642578, Temp: 2.6425719261169434, KL: 74.15855407714844, Loss: 0.02168581262230873, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8440/20000], Bound: 0.3881691098213196, Entropy: 139.3986053466797, Temp: 2.6425487995147705, KL: 73.918701171875, Loss: 0.01734936609864235, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8441/20000], Bound: 0.41381385922431946, Entropy: 142.7733154296875, Temp: 2.642533540725708, KL: 81.38700866699219, Loss: 0.017583444714546204, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8442/20000], Bound: 0.37626171112060547, Entropy: 141.2115936279297, Temp: 2.642549753189087, KL: 69.82177734375, Loss: 0.018612122163176537, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8443/20000], Bound: 0.4036213159561157, Entropy: 141.089599609375, Temp: 2.6425302028656006, KL: 78.83035278320312, Loss: 0.01664598286151886, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8444/20000], Bound: 0.40400853753089905, Entropy: 140.763671875, Temp: 2.642549991607666, KL: 78.66229248046875, Loss: 0.017182011157274246, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8445/20000], Bound: 0.37912479043006897, Entropy: 142.25885009765625, Temp: 2.642594575881958, KL: 72.45559692382812, Loss: 0.015179229900240898, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8446/20000], Bound: 0.3931063413619995, Entropy: 139.9027862548828, Temp: 2.642674446105957, KL: 75.7884521484375, Loss: 0.016536468639969826, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8447/20000], Bound: 0.3854691684246063, Entropy: 141.8786163330078, Temp: 2.6427743434906006, KL: 72.07025146484375, Loss: 0.01936769112944603, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8448/20000], Bound: 0.3756452202796936, Entropy: 141.71707153320312, Temp: 2.64282488822937, KL: 70.66616821289062, Loss: 0.016684042289853096, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8449/20000], Bound: 0.3744945526123047, Entropy: 142.0165252685547, Temp: 2.6428754329681396, KL: 69.26643371582031, Loss: 0.01871199533343315, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8450/20000], Bound: 0.3818405866622925, Entropy: 140.87632751464844, Temp: 2.6428825855255127, KL: 73.59410095214844, Loss: 0.014504391700029373, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8451/20000], Bound: 0.3756021559238434, Entropy: 140.812744140625, Temp: 2.6429455280303955, KL: 69.91709899902344, Loss: 0.01807902380824089, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8452/20000], Bound: 0.3730858862400055, Entropy: 140.7145233154297, Temp: 2.6429784297943115, KL: 68.32334899902344, Loss: 0.019738618284463882, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8453/20000], Bound: 0.3772737681865692, Entropy: 141.7010498046875, Temp: 2.642946720123291, KL: 70.05570983886719, Loss: 0.018720131367444992, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8454/20000], Bound: 0.3964105546474457, Entropy: 141.88694763183594, Temp: 2.642883062362671, KL: 76.63973999023438, Loss: 0.016761720180511475, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8455/20000], Bound: 0.3938628137111664, Entropy: 141.251708984375, Temp: 2.642852306365967, KL: 74.94230651855469, Loss: 0.01855817809700966, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8456/20000], Bound: 0.3820512294769287, Entropy: 140.8683319091797, Temp: 2.6428115367889404, KL: 71.55096435546875, Loss: 0.018483826890587807, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8457/20000], Bound: 0.374115526676178, Entropy: 142.46836853027344, Temp: 2.6427502632141113, KL: 70.46675109863281, Loss: 0.016235793009400368, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8458/20000], Bound: 0.3520589768886566, Entropy: 142.04751586914062, Temp: 2.642707586288452, KL: 64.23486328125, Loss: 0.016328228637576103, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8459/20000], Bound: 0.41036736965179443, Entropy: 140.2153778076172, Temp: 2.6426546573638916, KL: 79.58683776855469, Loss: 0.01902831718325615, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8460/20000], Bound: 0.3945165276527405, Entropy: 141.3304901123047, Temp: 2.642603635787964, KL: 75.14427185058594, Loss: 0.018536103889346123, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8461/20000], Bound: 0.3818618059158325, Entropy: 140.041748046875, Temp: 2.6425464153289795, KL: 72.21539306640625, Loss: 0.017121095210313797, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8462/20000], Bound: 0.39014336466789246, Entropy: 140.466064453125, Temp: 2.6424975395202637, KL: 73.75506591796875, Loss: 0.018745245411992073, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8463/20000], Bound: 0.38878148794174194, Entropy: 141.46151733398438, Temp: 2.642432928085327, KL: 73.22421264648438, Loss: 0.01899908483028412, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8464/20000], Bound: 0.3831149637699127, Entropy: 141.42759704589844, Temp: 2.6423470973968506, KL: 72.88667297363281, Loss: 0.016532212495803833, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8465/20000], Bound: 0.3971032202243805, Entropy: 142.7751007080078, Temp: 2.6422863006591797, KL: 77.1614990234375, Loss: 0.016153670847415924, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8466/20000], Bound: 0.39927130937576294, Entropy: 139.8035125732422, Temp: 2.6422722339630127, KL: 76.51124572753906, Loss: 0.01859336905181408, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8467/20000], Bound: 0.41096562147140503, Entropy: 139.70347595214844, Temp: 2.642252206802368, KL: 80.673583984375, Loss: 0.01730748824775219, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8468/20000], Bound: 0.39047253131866455, Entropy: 142.623779296875, Temp: 2.6422669887542725, KL: 75.40304565429688, Loss: 0.015806078910827637, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8469/20000], Bound: 0.3958468437194824, Entropy: 142.35037231445312, Temp: 2.642320156097412, KL: 74.01466369628906, Loss: 0.021409621462225914, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8470/20000], Bound: 0.37211889028549194, Entropy: 143.48191833496094, Temp: 2.6422994136810303, KL: 69.73448181152344, Loss: 0.016543177887797356, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8471/20000], Bound: 0.40652260184288025, Entropy: 142.14674377441406, Temp: 2.642284631729126, KL: 80.50978088378906, Loss: 0.015100291930139065, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8472/20000], Bound: 0.38700777292251587, Entropy: 141.59048461914062, Temp: 2.642343759536743, KL: 72.3671875, Loss: 0.019645391032099724, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8473/20000], Bound: 0.404843807220459, Entropy: 140.7154998779297, Temp: 2.6423537731170654, KL: 79.34184265136719, Loss: 0.016364216804504395, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8474/20000], Bound: 0.38310253620147705, Entropy: 140.12191772460938, Temp: 2.6424078941345215, KL: 73.3033447265625, Loss: 0.01573758013546467, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8475/20000], Bound: 0.3759720027446747, Entropy: 141.47442626953125, Temp: 2.6424896717071533, KL: 71.74992370605469, Loss: 0.014806770719587803, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8476/20000], Bound: 0.3846278786659241, Entropy: 141.8428192138672, Temp: 2.6426074504852295, KL: 71.86976623535156, Loss: 0.019285205751657486, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8477/20000], Bound: 0.4369460344314575, Entropy: 140.68927001953125, Temp: 2.6426749229431152, KL: 88.49951171875, Loss: 0.017570434138178825, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8478/20000], Bound: 0.3871605098247528, Entropy: 142.1460723876953, Temp: 2.6427927017211914, KL: 74.01719665527344, Loss: 0.016611449420452118, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8479/20000], Bound: 0.4103669226169586, Entropy: 140.76031494140625, Temp: 2.642918348312378, KL: 80.09742736816406, Loss: 0.018064891919493675, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8480/20000], Bound: 0.39498355984687805, Entropy: 141.00543212890625, Temp: 2.64304780960083, KL: 75.53013610839844, Loss: 0.018069611862301826, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8481/20000], Bound: 0.4039927124977112, Entropy: 141.30645751953125, Temp: 2.643162965774536, KL: 76.86346435546875, Loss: 0.020582597702741623, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8482/20000], Bound: 0.37689051032066345, Entropy: 140.81912231445312, Temp: 2.643223762512207, KL: 70.78594970703125, Loss: 0.017133867368102074, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8483/20000], Bound: 0.36818474531173706, Entropy: 143.17762756347656, Temp: 2.643275260925293, KL: 66.92445373535156, Loss: 0.019760552793741226, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8484/20000], Bound: 0.34952813386917114, Entropy: 140.37232971191406, Temp: 2.643253803253174, KL: 63.39228820800781, Loss: 0.016607118770480156, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8485/20000], Bound: 0.4002828001976013, Entropy: 141.06651306152344, Temp: 2.6432106494903564, KL: 76.92625427246094, Loss: 0.018383191898465157, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8486/20000], Bound: 0.3751744329929352, Entropy: 141.8730010986328, Temp: 2.64316987991333, KL: 70.25157165527344, Loss: 0.01721745729446411, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8487/20000], Bound: 0.3797268569469452, Entropy: 140.3913116455078, Temp: 2.6431267261505127, KL: 69.88372802734375, Loss: 0.020376527681946754, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8488/20000], Bound: 0.39007940888404846, Entropy: 141.65731811523438, Temp: 2.6430201530456543, KL: 72.77195739746094, Loss: 0.020574722439050674, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8489/20000], Bound: 0.40136420726776123, Entropy: 138.7731475830078, Temp: 2.6428651809692383, KL: 78.207275390625, Loss: 0.016561605036258698, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8490/20000], Bound: 0.4051739275455475, Entropy: 141.54624938964844, Temp: 2.6427626609802246, KL: 79.0379638671875, Loss: 0.01712973043322563, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8491/20000], Bound: 0.3901274502277374, Entropy: 140.871826171875, Temp: 2.642699718475342, KL: 74.53082275390625, Loss: 0.017270617187023163, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8492/20000], Bound: 0.4082750678062439, Entropy: 140.1098175048828, Temp: 2.642652750015259, KL: 80.10037231445312, Loss: 0.016870182007551193, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8493/20000], Bound: 0.38859614729881287, Entropy: 142.00950622558594, Temp: 2.642648696899414, KL: 74.29478454589844, Loss: 0.016873575747013092, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8494/20000], Bound: 0.3616289794445038, Entropy: 141.73373413085938, Temp: 2.6426608562469482, KL: 65.62947082519531, Loss: 0.018720442429184914, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8495/20000], Bound: 0.3839302062988281, Entropy: 140.03684997558594, Temp: 2.64261794090271, KL: 72.98553466796875, Loss: 0.016792897135019302, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8496/20000], Bound: 0.3886253833770752, Entropy: 140.71490478515625, Temp: 2.6425914764404297, KL: 74.98176574707031, Loss: 0.015589248389005661, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8497/20000], Bound: 0.37822988629341125, Entropy: 140.81541442871094, Temp: 2.6426103115081787, KL: 72.6907958984375, Loss: 0.014249216765165329, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8498/20000], Bound: 0.401204913854599, Entropy: 142.2307586669922, Temp: 2.642685890197754, KL: 74.88623046875, Loss: 0.022753881290555, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8499/20000], Bound: 0.4125279486179352, Entropy: 142.8192596435547, Temp: 2.6426632404327393, KL: 78.8671875, Loss: 0.021619105711579323, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8500/20000], Bound: 0.37310782074928284, Entropy: 140.19296264648438, Temp: 2.642589569091797, KL: 70.39956665039062, Loss: 0.015819011256098747, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8501/20000], Bound: 0.3887431025505066, Entropy: 141.06239318847656, Temp: 2.642543315887451, KL: 73.3603515625, Loss: 0.01872142031788826, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8502/20000], Bound: 0.40297725796699524, Entropy: 140.86917114257812, Temp: 2.642479419708252, KL: 79.45545959472656, Loss: 0.015100632794201374, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8503/20000], Bound: 0.3967815935611725, Entropy: 140.2699737548828, Temp: 2.642490863800049, KL: 77.72653198242188, Loss: 0.01490766927599907, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8504/20000], Bound: 0.4221302270889282, Entropy: 140.78709411621094, Temp: 2.642566680908203, KL: 84.35198974609375, Loss: 0.01675184816122055, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8505/20000], Bound: 0.3961877226829529, Entropy: 139.46820068359375, Temp: 2.6426920890808105, KL: 76.92489624023438, Loss: 0.01609620824456215, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8506/20000], Bound: 0.38727742433547974, Entropy: 141.13426208496094, Temp: 2.642845392227173, KL: 74.11103820800781, Loss: 0.016498612239956856, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8507/20000], Bound: 0.40209707617759705, Entropy: 140.5986328125, Temp: 2.64300537109375, KL: 78.28561401367188, Loss: 0.016825856640934944, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8508/20000], Bound: 0.43024349212646484, Entropy: 140.25550842285156, Temp: 2.643181800842285, KL: 84.17120361328125, Loss: 0.021820954978466034, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8509/20000], Bound: 0.3820595443248749, Entropy: 140.87045288085938, Temp: 2.6433041095733643, KL: 70.07196044921875, Loss: 0.02129039168357849, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8510/20000], Bound: 0.37137356400489807, Entropy: 140.77041625976562, Temp: 2.6433303356170654, KL: 70.69944763183594, Loss: 0.014327163808047771, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8511/20000], Bound: 0.38263005018234253, Entropy: 140.74574279785156, Temp: 2.6434030532836914, KL: 72.55819702148438, Loss: 0.016899384558200836, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8512/20000], Bound: 0.39761513471603394, Entropy: 140.6081085205078, Temp: 2.643476963043213, KL: 76.52169799804688, Loss: 0.017661819234490395, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8513/20000], Bound: 0.41875341534614563, Entropy: 140.6806640625, Temp: 2.6435530185699463, KL: 83.36599731445312, Loss: 0.016682080924510956, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8514/20000], Bound: 0.3911881446838379, Entropy: 139.687255859375, Temp: 2.6436755657196045, KL: 75.86882019042969, Loss: 0.015334736555814743, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8515/20000], Bound: 0.4056074619293213, Entropy: 141.20834350585938, Temp: 2.64383602142334, KL: 78.94410705566406, Loss: 0.017563363537192345, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8516/20000], Bound: 0.3871653079986572, Entropy: 141.3583984375, Temp: 2.6440017223358154, KL: 74.33261108398438, Loss: 0.01602969877421856, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8517/20000], Bound: 0.3794143497943878, Entropy: 139.0810089111328, Temp: 2.644181728363037, KL: 70.28611755371094, Loss: 0.019454576075077057, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8518/20000], Bound: 0.37705740332603455, Entropy: 139.94798278808594, Temp: 2.6442949771881104, KL: 70.93879699707031, Loss: 0.016944846138358116, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8519/20000], Bound: 0.384995698928833, Entropy: 140.18218994140625, Temp: 2.644397497177124, KL: 72.28701782226562, Loss: 0.018713101744651794, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8520/20000], Bound: 0.3784567713737488, Entropy: 140.8715362548828, Temp: 2.6444625854492188, KL: 71.64250183105469, Loss: 0.01637323573231697, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8521/20000], Bound: 0.35876092314720154, Entropy: 139.70655822753906, Temp: 2.6445353031158447, KL: 66.49911499023438, Loss: 0.015576287172734737, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8522/20000], Bound: 0.39858680963516235, Entropy: 138.88861083984375, Temp: 2.644608974456787, KL: 75.52626037597656, Loss: 0.02009708061814308, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8523/20000], Bound: 0.39737755060195923, Entropy: 140.0179901123047, Temp: 2.6446354389190674, KL: 77.01438903808594, Loss: 0.016609901562333107, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8524/20000], Bound: 0.3931097984313965, Entropy: 141.26222229003906, Temp: 2.6446897983551025, KL: 76.06004333496094, Loss: 0.016045862808823586, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8525/20000], Bound: 0.3775491714477539, Entropy: 140.96365356445312, Temp: 2.6447765827178955, KL: 71.11993408203125, Loss: 0.016872771084308624, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8526/20000], Bound: 0.38107773661613464, Entropy: 140.51487731933594, Temp: 2.6448569297790527, KL: 70.69175720214844, Loss: 0.019596733152866364, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8527/20000], Bound: 0.3882884383201599, Entropy: 139.1435546875, Temp: 2.64487886428833, KL: 75.43272399902344, Loss: 0.014575514011085033, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8528/20000], Bound: 0.3783520758152008, Entropy: 139.70706176757812, Temp: 2.644961357116699, KL: 70.54499816894531, Loss: 0.018395941704511642, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8529/20000], Bound: 0.376740038394928, Entropy: 139.09315490722656, Temp: 2.6450068950653076, KL: 70.93939208984375, Loss: 0.016778714954853058, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8530/20000], Bound: 0.3846036195755005, Entropy: 141.23025512695312, Temp: 2.6450514793395996, KL: 74.10020446777344, Loss: 0.015077163465321064, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8531/20000], Bound: 0.38776981830596924, Entropy: 140.05210876464844, Temp: 2.645139217376709, KL: 73.16072082519531, Loss: 0.018588168546557426, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8532/20000], Bound: 0.3887511193752289, Entropy: 141.525634765625, Temp: 2.6451969146728516, KL: 74.6094970703125, Loss: 0.016389280557632446, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8533/20000], Bound: 0.4038620591163635, Entropy: 139.3662109375, Temp: 2.6452739238739014, KL: 78.58702087402344, Loss: 0.017271563410758972, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8534/20000], Bound: 0.39111077785491943, Entropy: 142.6616973876953, Temp: 2.6453678607940674, KL: 74.53915405273438, Loss: 0.0178233552724123, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8535/20000], Bound: 0.37982380390167236, Entropy: 140.61415100097656, Temp: 2.6454505920410156, KL: 71.67842102050781, Loss: 0.017055779695510864, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8536/20000], Bound: 0.3626164197921753, Entropy: 140.12940979003906, Temp: 2.6455259323120117, KL: 67.7906494140625, Loss: 0.015180069953203201, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8537/20000], Bound: 0.3787112832069397, Entropy: 142.9632568359375, Temp: 2.6456143856048584, KL: 72.52781677246094, Loss: 0.014848819933831692, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8538/20000], Bound: 0.3732718527317047, Entropy: 141.74551391601562, Temp: 2.645740270614624, KL: 69.84596252441406, Loss: 0.016982877627015114, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8539/20000], Bound: 0.3985450267791748, Entropy: 140.07481384277344, Temp: 2.645848274230957, KL: 77.42378234863281, Loss: 0.01649956963956356, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8540/20000], Bound: 0.38402068614959717, Entropy: 141.76513671875, Temp: 2.645979881286621, KL: 71.60591125488281, Loss: 0.019481824710965157, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8541/20000], Bound: 0.41196969151496887, Entropy: 141.6195526123047, Temp: 2.6460530757904053, KL: 80.74485778808594, Loss: 0.01778651401400566, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8542/20000], Bound: 0.39534837007522583, Entropy: 140.51321411132812, Temp: 2.646141767501831, KL: 75.19294738769531, Loss: 0.01893986016511917, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8543/20000], Bound: 0.36101236939430237, Entropy: 141.6085968017578, Temp: 2.6462011337280273, KL: 66.66728210449219, Loss: 0.016460340470075607, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8544/20000], Bound: 0.37565529346466064, Entropy: 142.9331512451172, Temp: 2.6462461948394775, KL: 70.38043212890625, Loss: 0.01726078987121582, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8545/20000], Bound: 0.39146387577056885, Entropy: 142.3363494873047, Temp: 2.6462783813476562, KL: 74.62649536132812, Loss: 0.017861956730484962, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8546/20000], Bound: 0.3736644685268402, Entropy: 143.881103515625, Temp: 2.6463043689727783, KL: 68.85592651367188, Loss: 0.019069597125053406, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8547/20000], Bound: 0.3943118751049042, Entropy: 141.08673095703125, Temp: 2.6462788581848145, KL: 76.32440185546875, Loss: 0.016228627413511276, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8548/20000], Bound: 0.4078806936740875, Entropy: 142.88436889648438, Temp: 2.6462907791137695, KL: 78.63410949707031, Loss: 0.019458621740341187, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8549/20000], Bound: 0.391139954328537, Entropy: 141.97509765625, Temp: 2.646285057067871, KL: 73.422119140625, Loss: 0.01995895616710186, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8550/20000], Bound: 0.3906088173389435, Entropy: 143.26580810546875, Temp: 2.6462326049804688, KL: 76.53352355957031, Loss: 0.013786711730062962, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8551/20000], Bound: 0.38865190744400024, Entropy: 141.65386962890625, Temp: 2.6462671756744385, KL: 74.73690795898438, Loss: 0.016104822978377342, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8552/20000], Bound: 0.41633182764053345, Entropy: 142.65899658203125, Temp: 2.646329164505005, KL: 82.29862976074219, Loss: 0.017343241721391678, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8553/20000], Bound: 0.3714451491832733, Entropy: 142.5037384033203, Temp: 2.6464223861694336, KL: 70.43399047851562, Loss: 0.014897417277097702, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8554/20000], Bound: 0.42675474286079407, Entropy: 142.41087341308594, Temp: 2.64654278755188, KL: 86.06845092773438, Loss: 0.016238557174801826, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8555/20000], Bound: 0.3923285901546478, Entropy: 140.49012756347656, Temp: 2.6467230319976807, KL: 76.61514282226562, Loss: 0.014586837962269783, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8556/20000], Bound: 0.38512876629829407, Entropy: 143.14683532714844, Temp: 2.646951913833618, KL: 73.85539245605469, Loss: 0.0158462543040514, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8557/20000], Bound: 0.39091256260871887, Entropy: 141.045654296875, Temp: 2.6471900939941406, KL: 73.92890930175781, Loss: 0.01888442225754261, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8558/20000], Bound: 0.40839624404907227, Entropy: 141.0723419189453, Temp: 2.647379159927368, KL: 77.41998291015625, Loss: 0.022054297849535942, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8559/20000], Bound: 0.39651525020599365, Entropy: 139.36944580078125, Temp: 2.647479295730591, KL: 75.84687805175781, Loss: 0.018365543335676193, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8560/20000], Bound: 0.42678597569465637, Entropy: 140.57139587402344, Temp: 2.64756178855896, KL: 86.05020141601562, Loss: 0.016304224729537964, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8561/20000], Bound: 0.40050849318504333, Entropy: 140.6731414794922, Temp: 2.6477060317993164, KL: 76.29806518554688, Loss: 0.01974116452038288, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8562/20000], Bound: 0.3887006938457489, Entropy: 141.9857940673828, Temp: 2.647804021835327, KL: 74.92327880859375, Loss: 0.015795398503541946, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8563/20000], Bound: 0.39233535528182983, Entropy: 141.02992248535156, Temp: 2.6479291915893555, KL: 74.28843688964844, Loss: 0.01899731159210205, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8564/20000], Bound: 0.3924292027950287, Entropy: 140.80918884277344, Temp: 2.6480157375335693, KL: 73.703857421875, Loss: 0.020153721794486046, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8565/20000], Bound: 0.36351799964904785, Entropy: 138.34373474121094, Temp: 2.648043394088745, KL: 66.74473571777344, Loss: 0.017654819414019585, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8566/20000], Bound: 0.38685476779937744, Entropy: 141.04090881347656, Temp: 2.648036479949951, KL: 73.50430297851562, Loss: 0.01746448129415512, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8567/20000], Bound: 0.37096431851387024, Entropy: 139.38919067382812, Temp: 2.6480300426483154, KL: 70.06694030761719, Loss: 0.015348274260759354, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8568/20000], Bound: 0.40078771114349365, Entropy: 140.75271606445312, Temp: 2.648050308227539, KL: 75.86146545410156, Loss: 0.020724959671497345, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8569/20000], Bound: 0.3754878044128418, Entropy: 140.4020538330078, Temp: 2.6480162143707275, KL: 70.57206726074219, Loss: 0.016824446618556976, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8570/20000], Bound: 0.420853853225708, Entropy: 140.06976318359375, Temp: 2.6479854583740234, KL: 81.66493225097656, Loss: 0.02115597203373909, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8571/20000], Bound: 0.4162416458129883, Entropy: 137.76156616210938, Temp: 2.647920608520508, KL: 81.91854858398438, Loss: 0.01802782341837883, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8572/20000], Bound: 0.3845447599887848, Entropy: 141.7376708984375, Temp: 2.6478848457336426, KL: 73.77154541015625, Loss: 0.01569494977593422, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8573/20000], Bound: 0.397841215133667, Entropy: 139.74664306640625, Temp: 2.6478867530822754, KL: 76.13771057128906, Loss: 0.01855781301856041, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8574/20000], Bound: 0.42172831296920776, Entropy: 138.77200317382812, Temp: 2.647878408432007, KL: 84.29463195800781, Loss: 0.01669347658753395, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8575/20000], Bound: 0.3653332591056824, Entropy: 140.15565490722656, Temp: 2.6479272842407227, KL: 67.47880554199219, Loss: 0.017230940982699394, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8576/20000], Bound: 0.41205498576164246, Entropy: 140.3776092529297, Temp: 2.6479504108428955, KL: 81.57948303222656, Loss: 0.016280166804790497, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8577/20000], Bound: 0.38270360231399536, Entropy: 139.19033813476562, Temp: 2.6480250358581543, KL: 72.70700073242188, Loss: 0.016702480614185333, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8578/20000], Bound: 0.4076640009880066, Entropy: 140.09815979003906, Temp: 2.6481032371520996, KL: 81.48893737792969, Loss: 0.013964393176138401, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8579/20000], Bound: 0.38707634806632996, Entropy: 140.24034118652344, Temp: 2.648270606994629, KL: 72.55900573730469, Loss: 0.01937284879386425, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8580/20000], Bound: 0.36380961537361145, Entropy: 139.83363342285156, Temp: 2.6483802795410156, KL: 67.25886535644531, Loss: 0.01684139482676983, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8581/20000], Bound: 0.3908386528491974, Entropy: 140.83787536621094, Temp: 2.648465156555176, KL: 73.47799682617188, Loss: 0.019706809893250465, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8582/20000], Bound: 0.4064868986606598, Entropy: 139.31863403320312, Temp: 2.64849853515625, KL: 78.02626037597656, Loss: 0.019842049106955528, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8583/20000], Bound: 0.4064924418926239, Entropy: 140.3654327392578, Temp: 2.64850115776062, KL: 79.864013671875, Loss: 0.01637578383088112, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8584/20000], Bound: 0.40833836793899536, Entropy: 140.2023162841797, Temp: 2.6485488414764404, KL: 79.10929870605469, Loss: 0.01884329319000244, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8585/20000], Bound: 0.41895920038223267, Entropy: 138.4953155517578, Temp: 2.648587465286255, KL: 82.82807922363281, Loss: 0.017876461148262024, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8586/20000], Bound: 0.3763366639614105, Entropy: 140.87791442871094, Temp: 2.648650884628296, KL: 70.39450073242188, Loss: 0.01762309856712818, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8587/20000], Bound: 0.40025943517684937, Entropy: 137.73095703125, Temp: 2.648691415786743, KL: 76.94921875, Loss: 0.018382303416728973, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8588/20000], Bound: 0.3753064274787903, Entropy: 139.42713928222656, Temp: 2.648723840713501, KL: 69.40109252929688, Loss: 0.018943604081869125, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8589/20000], Bound: 0.4079848825931549, Entropy: 140.22857666015625, Temp: 2.648707151412964, KL: 77.74006652832031, Loss: 0.02122988924384117, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8590/20000], Bound: 0.37147414684295654, Entropy: 142.0167694091797, Temp: 2.648637533187866, KL: 68.19537353515625, Loss: 0.0191600751131773, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8591/20000], Bound: 0.4105284512042999, Entropy: 139.16473388671875, Temp: 2.6485202312469482, KL: 79.34722900390625, Loss: 0.019634081050753593, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8592/20000], Bound: 0.40991219878196716, Entropy: 139.59413146972656, Temp: 2.6483964920043945, KL: 79.69964599609375, Loss: 0.01861809752881527, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8593/20000], Bound: 0.3921406865119934, Entropy: 141.04087829589844, Temp: 2.648287296295166, KL: 74.36715698242188, Loss: 0.018744545057415962, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8594/20000], Bound: 0.412751168012619, Entropy: 140.23451232910156, Temp: 2.648167848587036, KL: 80.83074951171875, Loss: 0.01809251308441162, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8595/20000], Bound: 0.3779009282588959, Entropy: 139.59017944335938, Temp: 2.6480770111083984, KL: 70.54716491699219, Loss: 0.018174802884459496, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8596/20000], Bound: 0.37863487005233765, Entropy: 142.64114379882812, Temp: 2.6479694843292236, KL: 72.02923583984375, Loss: 0.0157724991440773, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8597/20000], Bound: 0.37604206800460815, Entropy: 140.13751220703125, Temp: 2.6478984355926514, KL: 70.73089599609375, Loss: 0.016822315752506256, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8598/20000], Bound: 0.3965526521205902, Entropy: 140.02239990234375, Temp: 2.6478352546691895, KL: 76.64759826660156, Loss: 0.016877781599760056, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8599/20000], Bound: 0.39064788818359375, Entropy: 140.24600219726562, Temp: 2.647801637649536, KL: 72.43093872070312, Loss: 0.02157297544181347, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8600/20000], Bound: 0.38573601841926575, Entropy: 138.52757263183594, Temp: 2.647688865661621, KL: 73.41180419921875, Loss: 0.017023375257849693, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8601/20000], Bound: 0.39934042096138, Entropy: 139.90927124023438, Temp: 2.647594690322876, KL: 77.61224365234375, Loss: 0.016605950891971588, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8602/20000], Bound: 0.4219902455806732, Entropy: 138.87001037597656, Temp: 2.647542715072632, KL: 84.27528381347656, Loss: 0.016877073794603348, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8603/20000], Bound: 0.4012238383293152, Entropy: 141.1257781982422, Temp: 2.6475493907928467, KL: 77.94100952148438, Loss: 0.01703697443008423, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8604/20000], Bound: 0.38145655393600464, Entropy: 140.57382202148438, Temp: 2.647581100463867, KL: 70.42138671875, Loss: 0.0203360877931118, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8605/20000], Bound: 0.3985533118247986, Entropy: 139.3238983154297, Temp: 2.6475417613983154, KL: 76.91850280761719, Loss: 0.01747654192149639, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8606/20000], Bound: 0.3860439658164978, Entropy: 140.96957397460938, Temp: 2.647519826889038, KL: 72.54287719726562, Loss: 0.018831249326467514, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8607/20000], Bound: 0.3867720663547516, Entropy: 141.13400268554688, Temp: 2.6474697589874268, KL: 73.98675537109375, Loss: 0.016502568498253822, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8608/20000], Bound: 0.4004257321357727, Entropy: 139.093017578125, Temp: 2.6474449634552, KL: 77.31881713867188, Loss: 0.017764640972018242, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8609/20000], Bound: 0.38702377676963806, Entropy: 141.2067413330078, Temp: 2.647432327270508, KL: 72.058837890625, Loss: 0.020281221717596054, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8610/20000], Bound: 0.3642432391643524, Entropy: 141.1029510498047, Temp: 2.6473608016967773, KL: 67.94548034667969, Loss: 0.015766100957989693, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8611/20000], Bound: 0.39818623661994934, Entropy: 139.86447143554688, Temp: 2.647306203842163, KL: 78.16213989257812, Loss: 0.01492069661617279, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8612/20000], Bound: 0.3770864009857178, Entropy: 139.8843536376953, Temp: 2.6473240852355957, KL: 70.54507446289062, Loss: 0.01773184724152088, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8613/20000], Bound: 0.38851475715637207, Entropy: 139.64146423339844, Temp: 2.647322654724121, KL: 74.06965637207031, Loss: 0.01730051264166832, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8614/20000], Bound: 0.3579328954219818, Entropy: 142.36778259277344, Temp: 2.647326946258545, KL: 65.00201416015625, Loss: 0.017991993576288223, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8615/20000], Bound: 0.37561681866645813, Entropy: 141.2799835205078, Temp: 2.647285223007202, KL: 70.45875549316406, Loss: 0.017101382836699486, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8616/20000], Bound: 0.3758339285850525, Entropy: 138.9562225341797, Temp: 2.6472415924072266, KL: 69.08509826660156, Loss: 0.0198125708848238, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8617/20000], Bound: 0.39417752623558044, Entropy: 140.0192108154297, Temp: 2.647139072418213, KL: 75.77223205566406, Loss: 0.017206208780407906, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8618/20000], Bound: 0.3856194019317627, Entropy: 139.91293334960938, Temp: 2.6470608711242676, KL: 73.41188049316406, Loss: 0.016953354701399803, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8619/20000], Bound: 0.3719038665294647, Entropy: 139.21792602539062, Temp: 2.6470000743865967, KL: 70.62814331054688, Loss: 0.014782152138650417, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8620/20000], Bound: 0.39956995844841003, Entropy: 138.875732421875, Temp: 2.6469852924346924, KL: 76.34835815429688, Loss: 0.01911490224301815, Learning Rate: 0.0005931980228999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8621/20000], Bound: 0.39339151978492737, Entropy: 139.7533416748047, Temp: 2.646951913833618, KL: 73.95086669921875, Loss: 0.02020968869328499, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8622/20000], Bound: 0.40615227818489075, Entropy: 139.25662231445312, Temp: 2.646871328353882, KL: 77.63836669921875, Loss: 0.020369820296764374, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8623/20000], Bound: 0.4224476218223572, Entropy: 143.5446014404297, Temp: 2.6467602252960205, KL: 82.83818054199219, Loss: 0.019846562296152115, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8624/20000], Bound: 0.40549513697624207, Entropy: 140.150634765625, Temp: 2.646652936935425, KL: 78.283203125, Loss: 0.018779154866933823, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8625/20000], Bound: 0.37811407446861267, Entropy: 144.3740234375, Temp: 2.6465506553649902, KL: 70.05143737792969, Loss: 0.0192133616656065, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8626/20000], Bound: 0.3739579916000366, Entropy: 142.32302856445312, Temp: 2.6464107036590576, KL: 69.2025146484375, Loss: 0.018573516979813576, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8627/20000], Bound: 0.3992427885532379, Entropy: 141.02867126464844, Temp: 2.6462459564208984, KL: 76.03724670410156, Loss: 0.01951286569237709, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8628/20000], Bound: 0.400404691696167, Entropy: 139.63333129882812, Temp: 2.646069049835205, KL: 76.92298889160156, Loss: 0.01848657988011837, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8629/20000], Bound: 0.3959936201572418, Entropy: 140.20115661621094, Temp: 2.645904302597046, KL: 74.76287841796875, Loss: 0.020108548924326897, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8630/20000], Bound: 0.3845594525337219, Entropy: 140.787109375, Temp: 2.6457111835479736, KL: 73.40464782714844, Loss: 0.016374383121728897, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8631/20000], Bound: 0.38215285539627075, Entropy: 139.90106201171875, Temp: 2.6455585956573486, KL: 73.21656799316406, Loss: 0.015415757894515991, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8632/20000], Bound: 0.38824591040611267, Entropy: 139.19728088378906, Temp: 2.6454601287841797, KL: 73.54598999023438, Loss: 0.018124403432011604, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8633/20000], Bound: 0.4181697368621826, Entropy: 140.56405639648438, Temp: 2.645359516143799, KL: 80.78480529785156, Loss: 0.021247277036309242, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8634/20000], Bound: 0.393520712852478, Entropy: 139.93605041503906, Temp: 2.6452271938323975, KL: 75.17112731933594, Loss: 0.017959188669919968, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8635/20000], Bound: 0.3649318218231201, Entropy: 142.5146484375, Temp: 2.645106315612793, KL: 67.87901306152344, Loss: 0.016237584874033928, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8636/20000], Bound: 0.37907975912094116, Entropy: 140.93728637695312, Temp: 2.644998788833618, KL: 71.27102661132812, Loss: 0.017418112605810165, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8637/20000], Bound: 0.3937101662158966, Entropy: 140.59410095214844, Temp: 2.6448941230773926, KL: 73.65017700195312, Loss: 0.020936064422130585, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8638/20000], Bound: 0.3839046359062195, Entropy: 141.25291442871094, Temp: 2.6447348594665527, KL: 73.04916381835938, Loss: 0.01667914353311062, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8639/20000], Bound: 0.39339765906333923, Entropy: 140.38485717773438, Temp: 2.6446056365966797, KL: 75.36968994140625, Loss: 0.01750953681766987, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8640/20000], Bound: 0.41863974928855896, Entropy: 139.99029541015625, Temp: 2.6444971561431885, KL: 81.6279296875, Loss: 0.019914323464035988, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8641/20000], Bound: 0.4010360836982727, Entropy: 140.4922332763672, Temp: 2.6443867683410645, KL: 75.30374145507812, Loss: 0.021884502843022346, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8642/20000], Bound: 0.385277658700943, Entropy: 140.78677368164062, Temp: 2.644211530685425, KL: 73.7523193359375, Loss: 0.016094902530312538, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8643/20000], Bound: 0.38009798526763916, Entropy: 142.10952758789062, Temp: 2.6440823078155518, KL: 71.43815612792969, Loss: 0.017646266147494316, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8644/20000], Bound: 0.41209089756011963, Entropy: 140.42532348632812, Temp: 2.6439547538757324, KL: 81.40263366699219, Loss: 0.01658809557557106, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8645/20000], Bound: 0.38995519280433655, Entropy: 142.8408203125, Temp: 2.643890142440796, KL: 74.63926696777344, Loss: 0.016982445493340492, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8646/20000], Bound: 0.3781406879425049, Entropy: 139.63821411132812, Temp: 2.6438472270965576, KL: 70.52497863769531, Loss: 0.018309572711586952, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8647/20000], Bound: 0.3675231337547302, Entropy: 140.79490661621094, Temp: 2.6437809467315674, KL: 68.0640869140625, Loss: 0.01725597307085991, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8648/20000], Bound: 0.38418325781822205, Entropy: 142.01519775390625, Temp: 2.6437041759490967, KL: 74.28721618652344, Loss: 0.014479799196124077, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8649/20000], Bound: 0.3638649582862854, Entropy: 144.30490112304688, Temp: 2.6436972618103027, KL: 69.1746826171875, Loss: 0.013208193704485893, Learning Rate: 0.0005931980228999996\n",
      "Epoch [8650/20000], Bound: 0.38674071431159973, Entropy: 139.37315368652344, Temp: 2.6437394618988037, KL: 75.6884765625, Loss: 0.01322980597615242, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8651/20000], Bound: 0.37690332531929016, Entropy: 141.3675537109375, Temp: 2.6438422203063965, KL: 69.51786804199219, Loss: 0.019544610753655434, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8652/20000], Bound: 0.3993980586528778, Entropy: 139.80227661132812, Temp: 2.6438956260681152, KL: 75.91082763671875, Loss: 0.019815953448414803, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8653/20000], Bound: 0.40537339448928833, Entropy: 137.45803833007812, Temp: 2.6439194679260254, KL: 79.04544067382812, Loss: 0.01724071241915226, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8654/20000], Bound: 0.4056841731071472, Entropy: 140.74945068359375, Temp: 2.643960952758789, KL: 79.24346923828125, Loss: 0.01704183593392372, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8655/20000], Bound: 0.37350305914878845, Entropy: 141.3766326904297, Temp: 2.6440212726593018, KL: 69.92864990234375, Loss: 0.016935523599386215, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8656/20000], Bound: 0.39624613523483276, Entropy: 140.27354431152344, Temp: 2.644073009490967, KL: 74.81288146972656, Loss: 0.020137447863817215, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8657/20000], Bound: 0.38380327820777893, Entropy: 142.4691619873047, Temp: 2.6440882682800293, KL: 73.02766418457031, Loss: 0.016658170148730278, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8658/20000], Bound: 0.3791598379611969, Entropy: 140.5325164794922, Temp: 2.6441125869750977, KL: 71.82766723632812, Loss: 0.01640084758400917, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8659/20000], Bound: 0.3714299499988556, Entropy: 139.6871795654297, Temp: 2.6441447734832764, KL: 69.35269165039062, Loss: 0.01691211573779583, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8660/20000], Bound: 0.38401782512664795, Entropy: 139.41949462890625, Temp: 2.644169807434082, KL: 71.54403686523438, Loss: 0.01958158053457737, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8661/20000], Bound: 0.3671509921550751, Entropy: 141.4075469970703, Temp: 2.6441590785980225, KL: 67.30802917480469, Loss: 0.018490390852093697, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8662/20000], Bound: 0.37955841422080994, Entropy: 140.59690856933594, Temp: 2.644117832183838, KL: 71.88972473144531, Loss: 0.016499768942594528, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8663/20000], Bound: 0.3815377354621887, Entropy: 141.28501892089844, Temp: 2.644089937210083, KL: 72.23884582519531, Loss: 0.016914941370487213, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8664/20000], Bound: 0.3726707994937897, Entropy: 139.15823364257812, Temp: 2.6440696716308594, KL: 71.20620727539062, Loss: 0.014072615653276443, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8665/20000], Bound: 0.3772498071193695, Entropy: 140.30189514160156, Temp: 2.6440916061401367, KL: 70.71177673339844, Loss: 0.017476333305239677, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8666/20000], Bound: 0.38304662704467773, Entropy: 141.64605712890625, Temp: 2.644104242324829, KL: 70.87019348144531, Loss: 0.02032521180808544, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8667/20000], Bound: 0.368890643119812, Entropy: 142.08651733398438, Temp: 2.6440696716308594, KL: 68.29452514648438, Loss: 0.017552834004163742, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8668/20000], Bound: 0.37639814615249634, Entropy: 140.09896850585938, Temp: 2.6440229415893555, KL: 70.30355834960938, Loss: 0.017787309363484383, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8669/20000], Bound: 0.35463282465934753, Entropy: 141.28640747070312, Temp: 2.643967866897583, KL: 64.6629638671875, Loss: 0.016874704509973526, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8670/20000], Bound: 0.4188036620616913, Entropy: 140.96194458007812, Temp: 2.6439013481140137, KL: 81.66461181640625, Loss: 0.01993275061249733, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8671/20000], Bound: 0.4396573603153229, Entropy: 139.4661102294922, Temp: 2.6438324451446533, KL: 89.40126037597656, Loss: 0.01748707704246044, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8672/20000], Bound: 0.3741157054901123, Entropy: 142.37730407714844, Temp: 2.643815279006958, KL: 71.44155883789062, Loss: 0.01440221257507801, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8673/20000], Bound: 0.39564260840415955, Entropy: 141.47874450683594, Temp: 2.643836736679077, KL: 76.03652954101562, Loss: 0.017485694959759712, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8674/20000], Bound: 0.37825390696525574, Entropy: 140.8771209716797, Temp: 2.643864154815674, KL: 70.41114807128906, Loss: 0.018586307764053345, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8675/20000], Bound: 0.38588762283325195, Entropy: 141.4210968017578, Temp: 2.6438651084899902, KL: 72.64498901367188, Loss: 0.018519481644034386, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8676/20000], Bound: 0.37423115968704224, Entropy: 142.40220642089844, Temp: 2.643850088119507, KL: 69.54348754882812, Loss: 0.018054351210594177, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8677/20000], Bound: 0.38597187399864197, Entropy: 142.161376953125, Temp: 2.643817663192749, KL: 73.36857604980469, Loss: 0.01719675585627556, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8678/20000], Bound: 0.40143701434135437, Entropy: 140.17648315429688, Temp: 2.6437928676605225, KL: 76.23562622070312, Loss: 0.020341387018561363, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8679/20000], Bound: 0.3831726610660553, Entropy: 140.1162109375, Temp: 2.64374041557312, KL: 72.31564331054688, Loss: 0.017657237127423286, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8680/20000], Bound: 0.3813663423061371, Entropy: 140.94346618652344, Temp: 2.6436879634857178, KL: 71.76077270507812, Loss: 0.01772201806306839, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8681/20000], Bound: 0.37083280086517334, Entropy: 141.0700225830078, Temp: 2.6436328887939453, KL: 70.40040588378906, Loss: 0.01460577454417944, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8682/20000], Bound: 0.409345805644989, Entropy: 141.22097778320312, Temp: 2.6436150074005127, KL: 81.646484375, Loss: 0.014563369564712048, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8683/20000], Bound: 0.43583425879478455, Entropy: 142.67190551757812, Temp: 2.6436619758605957, KL: 88.73524475097656, Loss: 0.016480613499879837, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8684/20000], Bound: 0.3773745894432068, Entropy: 142.03082275390625, Temp: 2.643760919570923, KL: 69.64970397949219, Loss: 0.019549475982785225, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8685/20000], Bound: 0.38156887888908386, Entropy: 142.72471618652344, Temp: 2.643810749053955, KL: 72.63473510742188, Loss: 0.016180509701371193, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8686/20000], Bound: 0.39405423402786255, Entropy: 139.63308715820312, Temp: 2.643872022628784, KL: 76.1820068359375, Loss: 0.016329636797308922, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8687/20000], Bound: 0.38515108823776245, Entropy: 141.8782958984375, Temp: 2.643951177597046, KL: 71.96923828125, Loss: 0.019395049661397934, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8688/20000], Bound: 0.3922131359577179, Entropy: 141.34571838378906, Temp: 2.6439924240112305, KL: 75.91325378417969, Loss: 0.015820171684026718, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8689/20000], Bound: 0.4127342402935028, Entropy: 142.2462615966797, Temp: 2.644059658050537, KL: 82.79632568359375, Loss: 0.014320207759737968, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8690/20000], Bound: 0.3707895576953888, Entropy: 143.10626220703125, Temp: 2.6441895961761475, KL: 69.464599609375, Loss: 0.01635756716132164, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8691/20000], Bound: 0.36427411437034607, Entropy: 140.57461547851562, Temp: 2.64431095123291, KL: 68.14694213867188, Loss: 0.015374693088233471, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8692/20000], Bound: 0.3986349403858185, Entropy: 141.22122192382812, Temp: 2.6444339752197266, KL: 77.9132080078125, Loss: 0.01560913398861885, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8693/20000], Bound: 0.3799488842487335, Entropy: 140.2200927734375, Temp: 2.644583225250244, KL: 70.33328247070312, Loss: 0.01965881697833538, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8694/20000], Bound: 0.37210869789123535, Entropy: 139.59730529785156, Temp: 2.644678831100464, KL: 69.16661071777344, Loss: 0.017632851377129555, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8695/20000], Bound: 0.41480377316474915, Entropy: 140.51791381835938, Temp: 2.6447505950927734, KL: 80.16668701171875, Loss: 0.020481137558817863, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8696/20000], Bound: 0.3765917718410492, Entropy: 140.9476318359375, Temp: 2.644794464111328, KL: 70.44606018066406, Loss: 0.017629293724894524, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8697/20000], Bound: 0.35751593112945557, Entropy: 141.8253631591797, Temp: 2.6448233127593994, KL: 67.203857421875, Loss: 0.013591151684522629, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8698/20000], Bound: 0.3635334372520447, Entropy: 142.2092742919922, Temp: 2.6448850631713867, KL: 68.70069885253906, Loss: 0.013939964585006237, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8699/20000], Bound: 0.38644108176231384, Entropy: 141.1441650390625, Temp: 2.6449756622314453, KL: 72.98204040527344, Loss: 0.018195662647485733, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8700/20000], Bound: 0.4103221595287323, Entropy: 142.14334106445312, Temp: 2.6450462341308594, KL: 79.71974182128906, Loss: 0.018776819109916687, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8701/20000], Bound: 0.4025663733482361, Entropy: 141.9290313720703, Temp: 2.645110845565796, KL: 77.16709899902344, Loss: 0.01922624744474888, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8702/20000], Bound: 0.3899112343788147, Entropy: 140.1546173095703, Temp: 2.645155906677246, KL: 75.61299133300781, Loss: 0.01513035036623478, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8703/20000], Bound: 0.39202427864074707, Entropy: 141.5281982421875, Temp: 2.645235300064087, KL: 76.04103088378906, Loss: 0.015487462282180786, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8704/20000], Bound: 0.37266460061073303, Entropy: 141.7128143310547, Temp: 2.645341396331787, KL: 68.14555358886719, Loss: 0.019866937771439552, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8705/20000], Bound: 0.4121149182319641, Entropy: 141.8474578857422, Temp: 2.645388603210449, KL: 81.62199401855469, Loss: 0.016203880310058594, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8706/20000], Bound: 0.39822709560394287, Entropy: 142.1779022216797, Temp: 2.6454715728759766, KL: 76.89508056640625, Loss: 0.0173175148665905, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8707/20000], Bound: 0.39646512269973755, Entropy: 141.91537475585938, Temp: 2.6455585956573486, KL: 76.68112182617188, Loss: 0.01674199104309082, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8708/20000], Bound: 0.38626065850257874, Entropy: 142.37405395507812, Temp: 2.6456565856933594, KL: 73.193603515625, Loss: 0.017703307792544365, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8709/20000], Bound: 0.37287142872810364, Entropy: 141.57823181152344, Temp: 2.6457409858703613, KL: 69.78657531738281, Loss: 0.016879897564649582, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8710/20000], Bound: 0.4074510335922241, Entropy: 139.160400390625, Temp: 2.6458141803741455, KL: 79.67697143554688, Loss: 0.017240097746253014, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8711/20000], Bound: 0.36955833435058594, Entropy: 141.49476623535156, Temp: 2.6459014415740967, KL: 68.02554321289062, Loss: 0.018433433026075363, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8712/20000], Bound: 0.39822810888290405, Entropy: 140.35279846191406, Temp: 2.6459505558013916, KL: 78.61894226074219, Loss: 0.01406554039567709, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8713/20000], Bound: 0.38741379976272583, Entropy: 142.58349609375, Temp: 2.6460564136505127, KL: 73.81365966796875, Loss: 0.017167476937174797, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8714/20000], Bound: 0.4006150960922241, Entropy: 143.18643188476562, Temp: 2.6461567878723145, KL: 76.05491638183594, Loss: 0.02024533972144127, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8715/20000], Bound: 0.39596864581108093, Entropy: 140.9776153564453, Temp: 2.646216869354248, KL: 75.88185119628906, Loss: 0.017983242869377136, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8716/20000], Bound: 0.4021734893321991, Entropy: 143.20945739746094, Temp: 2.646271228790283, KL: 74.76045227050781, Loss: 0.023564783856272697, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8717/20000], Bound: 0.3590184450149536, Entropy: 142.08935546875, Temp: 2.6462411880493164, KL: 66.5076904296875, Loss: 0.015710312873125076, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8718/20000], Bound: 0.40005621314048767, Entropy: 142.3381805419922, Temp: 2.6462178230285645, KL: 77.94178771972656, Loss: 0.016368253156542778, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8719/20000], Bound: 0.4010855555534363, Entropy: 143.17982482910156, Temp: 2.6462247371673584, KL: 75.68470764160156, Loss: 0.021208684891462326, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8720/20000], Bound: 0.40024811029434204, Entropy: 140.47369384765625, Temp: 2.646186590194702, KL: 77.1383056640625, Loss: 0.01799333468079567, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8721/20000], Bound: 0.395601749420166, Entropy: 142.0194091796875, Temp: 2.646155834197998, KL: 77.49624633789062, Loss: 0.014728459529578686, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8722/20000], Bound: 0.3925151824951172, Entropy: 142.76283264160156, Temp: 2.646177291870117, KL: 76.1737060546875, Loss: 0.015518042258918285, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8723/20000], Bound: 0.4040113091468811, Entropy: 142.29306030273438, Temp: 2.646231174468994, KL: 79.8031005859375, Loss: 0.015068050473928452, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8724/20000], Bound: 0.37113088369369507, Entropy: 141.13406372070312, Temp: 2.6463303565979004, KL: 68.05136108398438, Loss: 0.019229859113693237, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8725/20000], Bound: 0.38573503494262695, Entropy: 141.4130096435547, Temp: 2.6463794708251953, KL: 71.52665710449219, Loss: 0.020571907982230186, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8726/20000], Bound: 0.3918263018131256, Entropy: 140.42213439941406, Temp: 2.6463754177093506, KL: 74.30708312988281, Loss: 0.018666401505470276, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8727/20000], Bound: 0.37705352902412415, Entropy: 142.88926696777344, Temp: 2.6463582515716553, KL: 72.08955383300781, Loss: 0.014787433668971062, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8728/20000], Bound: 0.3719918429851532, Entropy: 137.9937744140625, Temp: 2.6463754177093506, KL: 68.5733642578125, Loss: 0.01870560087263584, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8729/20000], Bound: 0.4077293276786804, Entropy: 142.9940643310547, Temp: 2.64635968208313, KL: 78.49069213867188, Loss: 0.01964469626545906, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8730/20000], Bound: 0.4304529130458832, Entropy: 141.23150634765625, Temp: 2.6463303565979004, KL: 86.31614685058594, Loss: 0.017925404012203217, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8731/20000], Bound: 0.40518903732299805, Entropy: 139.95001220703125, Temp: 2.646334171295166, KL: 78.83795166015625, Loss: 0.01755530945956707, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8732/20000], Bound: 0.3638869524002075, Entropy: 142.98878479003906, Temp: 2.6463518142700195, KL: 65.6920166015625, Loss: 0.019825858995318413, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8733/20000], Bound: 0.3621859848499298, Entropy: 141.8487548828125, Temp: 2.6463122367858887, KL: 65.78042602539062, Loss: 0.018757397308945656, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8734/20000], Bound: 0.38986721634864807, Entropy: 139.45806884765625, Temp: 2.646235704421997, KL: 75.76022338867188, Loss: 0.014839417301118374, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8735/20000], Bound: 0.4154108762741089, Entropy: 140.22251892089844, Temp: 2.646209716796875, KL: 82.28131103515625, Loss: 0.016847670078277588, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8736/20000], Bound: 0.3882423937320709, Entropy: 139.69976806640625, Temp: 2.6462197303771973, KL: 72.59368896484375, Loss: 0.019928988069295883, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8737/20000], Bound: 0.39400073885917664, Entropy: 138.95481872558594, Temp: 2.646192789077759, KL: 74.56076049804688, Loss: 0.01938776858150959, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8738/20000], Bound: 0.3765994608402252, Entropy: 140.617431640625, Temp: 2.6461455821990967, KL: 70.18817138671875, Loss: 0.01813274808228016, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8739/20000], Bound: 0.3799836337566376, Entropy: 140.95346069335938, Temp: 2.6460845470428467, KL: 72.29489135742188, Loss: 0.01598353497684002, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8740/20000], Bound: 0.388174831867218, Entropy: 139.23272705078125, Temp: 2.6460464000701904, KL: 73.41717529296875, Loss: 0.01833432912826538, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8741/20000], Bound: 0.35748231410980225, Entropy: 140.44371032714844, Temp: 2.6460001468658447, KL: 66.75273132324219, Loss: 0.01443678792566061, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8742/20000], Bound: 0.3797492980957031, Entropy: 142.64859008789062, Temp: 2.6459813117980957, KL: 70.8985595703125, Loss: 0.018493976444005966, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8743/20000], Bound: 0.40432944893836975, Entropy: 140.3317108154297, Temp: 2.645942449569702, KL: 79.03422546386719, Loss: 0.016696486622095108, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8744/20000], Bound: 0.38291850686073303, Entropy: 140.64463806152344, Temp: 2.6459343433380127, KL: 71.66157531738281, Loss: 0.018775053322315216, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8745/20000], Bound: 0.4099906086921692, Entropy: 139.3237762451172, Temp: 2.6459035873413086, KL: 81.64141845703125, Loss: 0.014966515824198723, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8746/20000], Bound: 0.40979623794555664, Entropy: 141.76657104492188, Temp: 2.6459338665008545, KL: 80.46316528320312, Loss: 0.017083173617720604, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8747/20000], Bound: 0.3656887114048004, Entropy: 144.0673370361328, Temp: 2.645986318588257, KL: 67.20162963867188, Loss: 0.017927536740899086, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8748/20000], Bound: 0.3674084544181824, Entropy: 141.1303253173828, Temp: 2.6460089683532715, KL: 67.97132873535156, Loss: 0.01738893985748291, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8749/20000], Bound: 0.41140395402908325, Entropy: 141.0600128173828, Temp: 2.6460142135620117, KL: 79.64836120605469, Loss: 0.019536444917321205, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8750/20000], Bound: 0.37576913833618164, Entropy: 140.02810668945312, Temp: 2.6460089683532715, KL: 69.94261169433594, Loss: 0.018147366121411324, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8751/20000], Bound: 0.3816053569316864, Entropy: 139.58790588378906, Temp: 2.645984411239624, KL: 72.75846862792969, Loss: 0.015987740829586983, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8752/20000], Bound: 0.3494599163532257, Entropy: 140.88363647460938, Temp: 2.6459808349609375, KL: 62.8717041015625, Loss: 0.01757615990936756, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8753/20000], Bound: 0.3876255750656128, Entropy: 138.56101989746094, Temp: 2.645944118499756, KL: 74.57577514648438, Loss: 0.015842445194721222, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8754/20000], Bound: 0.40054699778556824, Entropy: 141.15386962890625, Temp: 2.645937204360962, KL: 77.61233520507812, Loss: 0.017262142151594162, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8755/20000], Bound: 0.3484991788864136, Entropy: 141.72947692871094, Temp: 2.6459457874298096, KL: 63.37217712402344, Loss: 0.0161309614777565, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8756/20000], Bound: 0.3780425488948822, Entropy: 141.04954528808594, Temp: 2.645941972732544, KL: 70.74697875976562, Loss: 0.017855221405625343, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8757/20000], Bound: 0.396544873714447, Entropy: 140.07342529296875, Temp: 2.645925521850586, KL: 75.80711364746094, Loss: 0.01844177208840847, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8758/20000], Bound: 0.3776164948940277, Entropy: 143.01614379882812, Temp: 2.645904064178467, KL: 71.48382568359375, Loss: 0.016231907531619072, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8759/20000], Bound: 0.37172630429267883, Entropy: 141.4856719970703, Temp: 2.6458964347839355, KL: 69.5816650390625, Loss: 0.01665378175675869, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8760/20000], Bound: 0.3711395859718323, Entropy: 140.44224548339844, Temp: 2.6458895206451416, KL: 69.14306640625, Loss: 0.017168015241622925, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8761/20000], Bound: 0.4037737548351288, Entropy: 141.2058563232422, Temp: 2.6458747386932373, KL: 77.26246643066406, Loss: 0.01973152533173561, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8762/20000], Bound: 0.37442678213119507, Entropy: 143.02574157714844, Temp: 2.6458418369293213, KL: 69.94143676757812, Loss: 0.01742468774318695, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8763/20000], Bound: 0.4297925531864166, Entropy: 141.75010681152344, Temp: 2.6458022594451904, KL: 85.11697387695312, Loss: 0.019798792898654938, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8764/20000], Bound: 0.38697171211242676, Entropy: 142.43507385253906, Temp: 2.645768880844116, KL: 73.59347534179688, Loss: 0.0173383429646492, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8765/20000], Bound: 0.3905922770500183, Entropy: 141.251708984375, Temp: 2.6457412242889404, KL: 74.8272705078125, Loss: 0.016996625810861588, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8766/20000], Bound: 0.3865012228488922, Entropy: 143.02737426757812, Temp: 2.6457269191741943, KL: 73.62232971191406, Loss: 0.01702553778886795, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8767/20000], Bound: 0.4031227231025696, Entropy: 141.9947052001953, Temp: 2.645720958709717, KL: 76.96064758300781, Loss: 0.019934719428420067, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8768/20000], Bound: 0.3919508457183838, Entropy: 143.09585571289062, Temp: 2.6456921100616455, KL: 74.65518188476562, Loss: 0.018070852383971214, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8769/20000], Bound: 0.3673066794872284, Entropy: 142.1384735107422, Temp: 2.6456615924835205, KL: 68.8741455078125, Loss: 0.01562558114528656, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8770/20000], Bound: 0.4088561534881592, Entropy: 142.2423553466797, Temp: 2.645646333694458, KL: 79.88177490234375, Loss: 0.01764599420130253, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8771/20000], Bound: 0.3598766326904297, Entropy: 143.90699768066406, Temp: 2.645648956298828, KL: 66.05119323730469, Loss: 0.017020434141159058, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8772/20000], Bound: 0.4183655083179474, Entropy: 139.9768829345703, Temp: 2.6456356048583984, KL: 82.59371948242188, Loss: 0.01794389635324478, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8773/20000], Bound: 0.3917922079563141, Entropy: 139.9433135986328, Temp: 2.645643949508667, KL: 74.97483825683594, Loss: 0.017378682270646095, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8774/20000], Bound: 0.36891433596611023, Entropy: 143.3417510986328, Temp: 2.6456573009490967, KL: 69.15164184570312, Loss: 0.01595902070403099, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8775/20000], Bound: 0.37751537561416626, Entropy: 141.72901916503906, Temp: 2.6456775665283203, KL: 70.89144897460938, Loss: 0.01729460246860981, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8776/20000], Bound: 0.3631278872489929, Entropy: 140.74368286132812, Temp: 2.64569091796875, KL: 68.14579772949219, Loss: 0.014781302772462368, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8777/20000], Bound: 0.3938765525817871, Entropy: 142.39952087402344, Temp: 2.6457250118255615, KL: 75.240478515625, Loss: 0.01803004927933216, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8778/20000], Bound: 0.3545021712779999, Entropy: 143.9734344482422, Temp: 2.6457533836364746, KL: 64.42047119140625, Loss: 0.017278499901294708, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8779/20000], Bound: 0.37333521246910095, Entropy: 143.28733825683594, Temp: 2.645754337310791, KL: 70.77906799316406, Loss: 0.015253674238920212, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8780/20000], Bound: 0.40198901295661926, Entropy: 142.78134155273438, Temp: 2.6457786560058594, KL: 78.7366943359375, Loss: 0.015942975878715515, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8781/20000], Bound: 0.40960565209388733, Entropy: 141.4329071044922, Temp: 2.6458370685577393, KL: 78.50961303710938, Loss: 0.02066575549542904, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8782/20000], Bound: 0.38640695810317993, Entropy: 142.1876983642578, Temp: 2.645860433578491, KL: 72.65536499023438, Loss: 0.018802523612976074, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8783/20000], Bound: 0.38210397958755493, Entropy: 141.8900909423828, Temp: 2.6458609104156494, KL: 71.31446838378906, Loss: 0.01898667775094509, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8784/20000], Bound: 0.3947629928588867, Entropy: 142.2777099609375, Temp: 2.645833969116211, KL: 75.74360656738281, Loss: 0.01757153309881687, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8785/20000], Bound: 0.387097030878067, Entropy: 143.13186645507812, Temp: 2.645815134048462, KL: 72.57261657714844, Loss: 0.019336670637130737, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8786/20000], Bound: 0.3823436200618744, Entropy: 140.72178649902344, Temp: 2.6457698345184326, KL: 71.57328796386719, Loss: 0.018627261742949486, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8787/20000], Bound: 0.3698441684246063, Entropy: 140.81591796875, Temp: 2.645707130432129, KL: 68.79849243164062, Loss: 0.01712397113442421, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8788/20000], Bound: 0.38617223501205444, Entropy: 141.7841339111328, Temp: 2.645641803741455, KL: 73.74728393554688, Loss: 0.016608335077762604, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8789/20000], Bound: 0.4005441963672638, Entropy: 140.79090881347656, Temp: 2.6455962657928467, KL: 77.81816101074219, Loss: 0.016867950558662415, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8790/20000], Bound: 0.3893599510192871, Entropy: 142.43154907226562, Temp: 2.6455767154693604, KL: 74.47044372558594, Loss: 0.016990745440125465, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8791/20000], Bound: 0.3816514313220978, Entropy: 143.1870880126953, Temp: 2.645568609237671, KL: 72.42585754394531, Loss: 0.016637345775961876, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8792/20000], Bound: 0.3957754671573639, Entropy: 143.4054718017578, Temp: 2.6455702781677246, KL: 75.79971313476562, Loss: 0.018024737015366554, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8793/20000], Bound: 0.3999594748020172, Entropy: 142.36419677734375, Temp: 2.645571231842041, KL: 78.50991821289062, Loss: 0.015233405865728855, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8794/20000], Bound: 0.4035632908344269, Entropy: 141.74740600585938, Temp: 2.645618200302124, KL: 76.57341003417969, Loss: 0.02091297134757042, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8795/20000], Bound: 0.38541319966316223, Entropy: 141.83383178710938, Temp: 2.6456222534179688, KL: 74.46559143066406, Loss: 0.014835145324468613, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8796/20000], Bound: 0.44243231415748596, Entropy: 141.76075744628906, Temp: 2.6456658840179443, KL: 90.00953674316406, Loss: 0.01801285892724991, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8797/20000], Bound: 0.3720557391643524, Entropy: 141.58499145507812, Temp: 2.645745038986206, KL: 69.12567138671875, Loss: 0.01769096404314041, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8798/20000], Bound: 0.38245075941085815, Entropy: 142.3672637939453, Temp: 2.645800828933716, KL: 72.81874084472656, Loss: 0.016332225874066353, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8799/20000], Bound: 0.400728315114975, Entropy: 140.88819885253906, Temp: 2.645864963531494, KL: 77.54570007324219, Loss: 0.017488710582256317, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8800/20000], Bound: 0.384184330701828, Entropy: 142.01512145996094, Temp: 2.645934581756592, KL: 71.66835021972656, Loss: 0.0194527767598629, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8801/20000], Bound: 0.3857799172401428, Entropy: 142.19992065429688, Temp: 2.6459643840789795, KL: 72.50015258789062, Loss: 0.018753398209810257, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8802/20000], Bound: 0.4005238711833954, Entropy: 138.68284606933594, Temp: 2.645970344543457, KL: 77.60929870605469, Loss: 0.017255280166864395, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8803/20000], Bound: 0.3953619599342346, Entropy: 142.35964965820312, Temp: 2.645991086959839, KL: 76.03866577148438, Loss: 0.017347820103168488, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8804/20000], Bound: 0.4101921617984772, Entropy: 139.7230682373047, Temp: 2.646019458770752, KL: 80.86984252929688, Loss: 0.01654019020497799, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8805/20000], Bound: 0.41855931282043457, Entropy: 140.90565490722656, Temp: 2.6460793018341064, KL: 83.70295715332031, Loss: 0.015964336693286896, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8806/20000], Bound: 0.40230318903923035, Entropy: 141.07919311523438, Temp: 2.646183967590332, KL: 78.3101806640625, Loss: 0.01692950539290905, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8807/20000], Bound: 0.4331373870372772, Entropy: 138.71441650390625, Temp: 2.6462998390197754, KL: 87.12667846679688, Loss: 0.017967451363801956, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8808/20000], Bound: 0.39779961109161377, Entropy: 141.73104858398438, Temp: 2.646437168121338, KL: 76.29444885253906, Loss: 0.018224192783236504, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8809/20000], Bound: 0.37654829025268555, Entropy: 141.49717712402344, Temp: 2.6465585231781006, KL: 72.60377502441406, Loss: 0.01354500837624073, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8810/20000], Bound: 0.3633113503456116, Entropy: 141.4033203125, Temp: 2.6467201709747314, KL: 66.20933532714844, Loss: 0.01854611746966839, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8811/20000], Bound: 0.3837764859199524, Entropy: 140.22439575195312, Temp: 2.6468286514282227, KL: 73.49906921386719, Loss: 0.015779606997966766, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8812/20000], Bound: 0.38775041699409485, Entropy: 142.18521118164062, Temp: 2.6469497680664062, KL: 73.04580688476562, Loss: 0.018811294808983803, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8813/20000], Bound: 0.3936857283115387, Entropy: 140.7591552734375, Temp: 2.647038459777832, KL: 75.08308410644531, Loss: 0.018234651535749435, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8814/20000], Bound: 0.3989558815956116, Entropy: 140.4196014404297, Temp: 2.6471126079559326, KL: 77.12254333496094, Loss: 0.017311202362179756, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8815/20000], Bound: 0.4177643954753876, Entropy: 139.90780639648438, Temp: 2.6471917629241943, KL: 82.26412963867188, Loss: 0.018239421769976616, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8816/20000], Bound: 0.4341464340686798, Entropy: 141.6185760498047, Temp: 2.647278308868408, KL: 86.79583740234375, Loss: 0.019197873771190643, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8817/20000], Bound: 0.4031963050365448, Entropy: 139.81674194335938, Temp: 2.6473705768585205, KL: 76.92251586914062, Loss: 0.020064059644937515, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8818/20000], Bound: 0.3576759994029999, Entropy: 141.42166137695312, Temp: 2.647427558898926, KL: 64.65789794921875, Loss: 0.018507611006498337, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8819/20000], Bound: 0.40267637372016907, Entropy: 142.62828063964844, Temp: 2.6474368572235107, KL: 79.09538269042969, Loss: 0.015669340267777443, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8820/20000], Bound: 0.39578989148139954, Entropy: 141.83006286621094, Temp: 2.647486448287964, KL: 76.65412902832031, Loss: 0.01643817126750946, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8821/20000], Bound: 0.4022771716117859, Entropy: 138.86581420898438, Temp: 2.647554636001587, KL: 79.19068908691406, Loss: 0.015266855247318745, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8822/20000], Bound: 0.417609840631485, Entropy: 140.02796936035156, Temp: 2.6476635932922363, KL: 82.003173828125, Loss: 0.018648959696292877, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8823/20000], Bound: 0.4013518691062927, Entropy: 141.7884521484375, Temp: 2.6477696895599365, KL: 78.46218872070312, Loss: 0.016126815229654312, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8824/20000], Bound: 0.3833078145980835, Entropy: 141.1420440673828, Temp: 2.6478981971740723, KL: 72.3739013671875, Loss: 0.017659446224570274, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8825/20000], Bound: 0.384696900844574, Entropy: 139.5560302734375, Temp: 2.648007869720459, KL: 72.35322570800781, Loss: 0.018457340076565742, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8826/20000], Bound: 0.39315998554229736, Entropy: 139.62750244140625, Temp: 2.6480886936187744, KL: 76.53178405761719, Loss: 0.015218681655824184, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8827/20000], Bound: 0.40596264600753784, Entropy: 139.39559936523438, Temp: 2.6482017040252686, KL: 78.37074279785156, Loss: 0.01889316737651825, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8828/20000], Bound: 0.41295287013053894, Entropy: 139.97903442382812, Temp: 2.6482973098754883, KL: 81.21607971191406, Loss: 0.017481258139014244, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8829/20000], Bound: 0.37826210260391235, Entropy: 139.66470336914062, Temp: 2.6484055519104004, KL: 71.0352783203125, Loss: 0.01745150238275528, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8830/20000], Bound: 0.3549479842185974, Entropy: 138.9560089111328, Temp: 2.648495674133301, KL: 66.24525451660156, Loss: 0.014087800867855549, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8831/20000], Bound: 0.38654011487960815, Entropy: 139.39581298828125, Temp: 2.64860200881958, KL: 72.91970825195312, Loss: 0.018401099368929863, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8832/20000], Bound: 0.36615103483200073, Entropy: 140.28411865234375, Temp: 2.6486828327178955, KL: 67.99415588378906, Loss: 0.016698965802788734, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8833/20000], Bound: 0.39190438389778137, Entropy: 139.2764129638672, Temp: 2.6487491130828857, KL: 75.2833251953125, Loss: 0.01688903383910656, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8834/20000], Bound: 0.3695432245731354, Entropy: 140.2926483154297, Temp: 2.6488218307495117, KL: 68.95399475097656, Loss: 0.01669643074274063, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8835/20000], Bound: 0.3954548239707947, Entropy: 139.60272216796875, Temp: 2.648883819580078, KL: 76.86888122558594, Loss: 0.0158616341650486, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8836/20000], Bound: 0.3542649745941162, Entropy: 139.8068389892578, Temp: 2.6489715576171875, KL: 65.53567504882812, Loss: 0.015074014663696289, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8837/20000], Bound: 0.3785850703716278, Entropy: 142.6888885498047, Temp: 2.649060010910034, KL: 72.14360046386719, Loss: 0.015540226362645626, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8838/20000], Bound: 0.35652032494544983, Entropy: 139.70445251464844, Temp: 2.6491620540618896, KL: 64.23587036132812, Loss: 0.018710097298026085, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8839/20000], Bound: 0.3656889796257019, Entropy: 140.03204345703125, Temp: 2.6492068767547607, KL: 67.696044921875, Loss: 0.017020465806126595, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8840/20000], Bound: 0.373546838760376, Entropy: 140.28038024902344, Temp: 2.649235725402832, KL: 69.81092834472656, Loss: 0.017227720469236374, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8841/20000], Bound: 0.38691893219947815, Entropy: 139.92318725585938, Temp: 2.6492528915405273, KL: 74.14540100097656, Loss: 0.016301244497299194, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8842/20000], Bound: 0.3997490406036377, Entropy: 140.304931640625, Temp: 2.6492860317230225, KL: 75.97158813476562, Loss: 0.019948409870266914, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8843/20000], Bound: 0.3784388601779938, Entropy: 141.1963653564453, Temp: 2.6492879390716553, KL: 69.84210205078125, Loss: 0.019806908443570137, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8844/20000], Bound: 0.3946024477481842, Entropy: 139.2186737060547, Temp: 2.649244546890259, KL: 75.14007568359375, Loss: 0.018655924126505852, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8845/20000], Bound: 0.3842604458332062, Entropy: 141.79872131347656, Temp: 2.6491928100585938, KL: 72.220703125, Loss: 0.01847992278635502, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8846/20000], Bound: 0.37788334488868713, Entropy: 142.5874481201172, Temp: 2.64912748336792, KL: 71.3050537109375, Loss: 0.01674397476017475, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8847/20000], Bound: 0.40731921792030334, Entropy: 140.7500762939453, Temp: 2.64907169342041, KL: 79.06407165527344, Loss: 0.018358413130044937, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8848/20000], Bound: 0.3630618453025818, Entropy: 141.03379821777344, Temp: 2.649024724960327, KL: 66.22756958007812, Loss: 0.018397219479084015, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8849/20000], Bound: 0.38056132197380066, Entropy: 140.8959503173828, Temp: 2.6489460468292236, KL: 72.87263488769531, Loss: 0.01523411925882101, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8850/20000], Bound: 0.36475691199302673, Entropy: 141.67977905273438, Temp: 2.64890456199646, KL: 68.38307189941406, Loss: 0.015226132236421108, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8851/20000], Bound: 0.3841719925403595, Entropy: 138.93106079101562, Temp: 2.648883104324341, KL: 74.92276000976562, Loss: 0.013328520581126213, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8852/20000], Bound: 0.4008468687534332, Entropy: 139.40042114257812, Temp: 2.648926258087158, KL: 78.69293212890625, Loss: 0.015421557240188122, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8853/20000], Bound: 0.3933632969856262, Entropy: 142.3638458251953, Temp: 2.6490087509155273, KL: 75.60786437988281, Loss: 0.017084933817386627, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8854/20000], Bound: 0.3729502558708191, Entropy: 142.27386474609375, Temp: 2.6490941047668457, KL: 69.11177062988281, Loss: 0.018225673586130142, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8855/20000], Bound: 0.36592328548431396, Entropy: 140.9420623779297, Temp: 2.6491458415985107, KL: 65.46406555175781, Loss: 0.021357111632823944, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8856/20000], Bound: 0.387145459651947, Entropy: 141.54721069335938, Temp: 2.649111747741699, KL: 74.73466491699219, Loss: 0.015311753377318382, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8857/20000], Bound: 0.39506852626800537, Entropy: 140.3694305419922, Temp: 2.6491143703460693, KL: 75.67727661132812, Loss: 0.017898958176374435, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8858/20000], Bound: 0.39353662729263306, Entropy: 139.53916931152344, Temp: 2.6491165161132812, KL: 76.17611694335938, Loss: 0.01610933430492878, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8859/20000], Bound: 0.36975961923599243, Entropy: 140.79461669921875, Temp: 2.6491451263427734, KL: 69.48323059082031, Loss: 0.015815993770956993, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8860/20000], Bound: 0.41228246688842773, Entropy: 140.66590881347656, Temp: 2.649181604385376, KL: 82.1754150390625, Loss: 0.015299186110496521, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8861/20000], Bound: 0.36828112602233887, Entropy: 143.38966369628906, Temp: 2.6492695808410645, KL: 68.7564697265625, Loss: 0.016399472951889038, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8862/20000], Bound: 0.39015763998031616, Entropy: 140.704833984375, Temp: 2.649348735809326, KL: 74.47914123535156, Loss: 0.017450345680117607, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8863/20000], Bound: 0.3991639316082001, Entropy: 141.59828186035156, Temp: 2.6494226455688477, KL: 76.40751647949219, Loss: 0.01880061626434326, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8864/20000], Bound: 0.39266377687454224, Entropy: 141.2786102294922, Temp: 2.6494781970977783, KL: 76.03752136230469, Loss: 0.01589229889214039, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8865/20000], Bound: 0.38777828216552734, Entropy: 141.52297973632812, Temp: 2.64955735206604, KL: 74.59234619140625, Loss: 0.01593177393078804, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8866/20000], Bound: 0.39027899503707886, Entropy: 142.16848754882812, Temp: 2.6496527194976807, KL: 75.16885375976562, Loss: 0.016218578442931175, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8867/20000], Bound: 0.3993186950683594, Entropy: 138.84719848632812, Temp: 2.6497607231140137, KL: 77.77182006835938, Loss: 0.016315851360559464, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8868/20000], Bound: 0.38464266061782837, Entropy: 140.16368103027344, Temp: 2.649885892868042, KL: 73.40362548828125, Loss: 0.016462702304124832, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8869/20000], Bound: 0.412576287984848, Entropy: 142.50399780273438, Temp: 2.6500117778778076, KL: 79.31382751464844, Loss: 0.02087545394897461, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8870/20000], Bound: 0.38656872510910034, Entropy: 140.35411071777344, Temp: 2.6500935554504395, KL: 73.23809814453125, Loss: 0.01782970316708088, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8871/20000], Bound: 0.40651601552963257, Entropy: 141.4020233154297, Temp: 2.650160312652588, KL: 78.02616882324219, Loss: 0.01987510547041893, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8872/20000], Bound: 0.37414243817329407, Entropy: 142.18272399902344, Temp: 2.6501989364624023, KL: 70.90415954589844, Loss: 0.01549363974481821, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8873/20000], Bound: 0.41799628734588623, Entropy: 140.39891052246094, Temp: 2.6502532958984375, KL: 81.11354064941406, Loss: 0.02057781256735325, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8874/20000], Bound: 0.40363991260528564, Entropy: 141.59039306640625, Temp: 2.650279998779297, KL: 76.5152587890625, Loss: 0.021109312772750854, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8875/20000], Bound: 0.39768558740615845, Entropy: 142.33090209960938, Temp: 2.6502606868743896, KL: 75.31065368652344, Loss: 0.020054945722222328, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8876/20000], Bound: 0.3775053024291992, Entropy: 141.16552734375, Temp: 2.6502113342285156, KL: 72.00462341308594, Loss: 0.015229818411171436, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8877/20000], Bound: 0.39513644576072693, Entropy: 139.58702087402344, Temp: 2.650193452835083, KL: 75.45755004882812, Loss: 0.018361827358603477, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8878/20000], Bound: 0.3766080439090729, Entropy: 141.95188903808594, Temp: 2.650169610977173, KL: 70.79087829589844, Loss: 0.01703498512506485, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8879/20000], Bound: 0.41991207003593445, Entropy: 141.96055603027344, Temp: 2.6501448154449463, KL: 82.55557250976562, Loss: 0.018956219777464867, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8880/20000], Bound: 0.36780112981796265, Entropy: 140.584228515625, Temp: 2.650127649307251, KL: 67.07357788085938, Loss: 0.01932625100016594, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8881/20000], Bound: 0.39044514298439026, Entropy: 139.83602905273438, Temp: 2.650064706802368, KL: 74.78166198730469, Loss: 0.017044756561517715, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8882/20000], Bound: 0.35565853118896484, Entropy: 140.54396057128906, Temp: 2.6500167846679688, KL: 63.66552734375, Loss: 0.01934060826897621, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8883/20000], Bound: 0.37841325998306274, Entropy: 141.058349609375, Temp: 2.6499154567718506, KL: 71.38288879394531, Loss: 0.01689089834690094, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8884/20000], Bound: 0.3983617424964905, Entropy: 138.9735870361328, Temp: 2.649825096130371, KL: 77.43028259277344, Loss: 0.016427695751190186, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8885/20000], Bound: 0.3508187532424927, Entropy: 140.06700134277344, Temp: 2.6497690677642822, KL: 63.62220764160156, Loss: 0.01689370721578598, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8886/20000], Bound: 0.3754904568195343, Entropy: 143.66094970703125, Temp: 2.649695873260498, KL: 69.55522155761719, Loss: 0.018759876489639282, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8887/20000], Bound: 0.41806870698928833, Entropy: 139.56475830078125, Temp: 2.6495983600616455, KL: 82.94636535644531, Loss: 0.0171537846326828, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8888/20000], Bound: 0.3918795585632324, Entropy: 141.4712677001953, Temp: 2.6495420932769775, KL: 75.33970642089844, Loss: 0.016776926815509796, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8889/20000], Bound: 0.3818508982658386, Entropy: 140.64016723632812, Temp: 2.649505853652954, KL: 70.61097717285156, Loss: 0.020208248868584633, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8890/20000], Bound: 0.36618879437446594, Entropy: 141.61936950683594, Temp: 2.6494245529174805, KL: 66.13163757324219, Loss: 0.020240291953086853, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8891/20000], Bound: 0.3705032765865326, Entropy: 142.16290283203125, Temp: 2.6492881774902344, KL: 70.43826293945312, Loss: 0.014412458054721355, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8892/20000], Bound: 0.3775477111339569, Entropy: 141.9337615966797, Temp: 2.6491992473602295, KL: 72.16191101074219, Loss: 0.014946033246815205, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8893/20000], Bound: 0.41384249925613403, Entropy: 139.577392578125, Temp: 2.6491506099700928, KL: 81.85684204101562, Loss: 0.016788259148597717, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8894/20000], Bound: 0.3997982442378998, Entropy: 140.6591796875, Temp: 2.6491403579711914, KL: 74.38714599609375, Loss: 0.022964967414736748, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8895/20000], Bound: 0.3947862684726715, Entropy: 141.07785034179688, Temp: 2.6490554809570312, KL: 76.30091857910156, Loss: 0.016564859077334404, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8896/20000], Bound: 0.40552690625190735, Entropy: 141.19439697265625, Temp: 2.6489996910095215, KL: 78.80274963378906, Loss: 0.0178405549377203, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8897/20000], Bound: 0.3984794616699219, Entropy: 140.5199737548828, Temp: 2.6489593982696533, KL: 77.54031372070312, Loss: 0.01627630926668644, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8898/20000], Bound: 0.39130356907844543, Entropy: 139.88218688964844, Temp: 2.648951292037964, KL: 75.35679626464844, Loss: 0.016421087086200714, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8899/20000], Bound: 0.3856659233570099, Entropy: 142.36593627929688, Temp: 2.6489639282226562, KL: 73.83987426757812, Loss: 0.016189292073249817, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8900/20000], Bound: 0.37868407368659973, Entropy: 142.28843688964844, Temp: 2.648994207382202, KL: 70.65817260742188, Loss: 0.018396904692053795, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8901/20000], Bound: 0.3967967927455902, Entropy: 141.7785186767578, Temp: 2.648998498916626, KL: 77.2440185546875, Loss: 0.01589982397854328, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8902/20000], Bound: 0.38579654693603516, Entropy: 141.88768005371094, Temp: 2.6490352153778076, KL: 73.7501220703125, Loss: 0.016430873423814774, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8903/20000], Bound: 0.388371080160141, Entropy: 143.77549743652344, Temp: 2.649083375930786, KL: 74.83535766601562, Loss: 0.01579352840781212, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8904/20000], Bound: 0.40739792585372925, Entropy: 141.2405548095703, Temp: 2.6491539478302, KL: 79.15838623046875, Loss: 0.018225695937871933, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8905/20000], Bound: 0.38065701723098755, Entropy: 140.88729858398438, Temp: 2.6492228507995605, KL: 73.61537170410156, Loss: 0.013887018896639347, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8906/20000], Bound: 0.41589468717575073, Entropy: 140.3017578125, Temp: 2.6493358612060547, KL: 81.49464416503906, Loss: 0.018645456060767174, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8907/20000], Bound: 0.3731738030910492, Entropy: 140.7670440673828, Temp: 2.6494438648223877, KL: 69.82569885253906, Loss: 0.01700129546225071, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8908/20000], Bound: 0.3979490399360657, Entropy: 139.89486694335938, Temp: 2.649535894393921, KL: 77.61770629882812, Loss: 0.01584116369485855, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8909/20000], Bound: 0.3985876142978668, Entropy: 142.0648651123047, Temp: 2.649653673171997, KL: 77.54576110839844, Loss: 0.016333797946572304, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8910/20000], Bound: 0.4096834361553192, Entropy: 140.93399047851562, Temp: 2.649786949157715, KL: 76.97776794433594, Loss: 0.02363932505249977, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8911/20000], Bound: 0.3765375316143036, Entropy: 141.03627014160156, Temp: 2.649829149246216, KL: 71.74815368652344, Loss: 0.015187566168606281, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8912/20000], Bound: 0.4060400128364563, Entropy: 138.7060546875, Temp: 2.6498937606811523, KL: 78.09312438964844, Loss: 0.019477948546409607, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8913/20000], Bound: 0.40637585520744324, Entropy: 141.38829040527344, Temp: 2.6499364376068115, KL: 78.36909484863281, Loss: 0.019146863371133804, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8914/20000], Bound: 0.407172828912735, Entropy: 141.06175231933594, Temp: 2.6499648094177246, KL: 78.94717407226562, Loss: 0.018505815416574478, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8915/20000], Bound: 0.37815114855766296, Entropy: 140.3941192626953, Temp: 2.6499907970428467, KL: 72.67259216308594, Loss: 0.014316427521407604, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8916/20000], Bound: 0.3903978765010834, Entropy: 140.10211181640625, Temp: 2.6500563621520996, KL: 75.45452880859375, Loss: 0.01574913039803505, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8917/20000], Bound: 0.36914095282554626, Entropy: 141.14871215820312, Temp: 2.650144577026367, KL: 70.50035095214844, Loss: 0.013575668446719646, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8918/20000], Bound: 0.4090333580970764, Entropy: 140.63336181640625, Temp: 2.6502699851989746, KL: 78.19569396972656, Loss: 0.020977839827537537, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8919/20000], Bound: 0.3997250199317932, Entropy: 141.94224548339844, Temp: 2.650346040725708, KL: 76.08146667480469, Loss: 0.019737711176276207, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8920/20000], Bound: 0.3688749372959137, Entropy: 140.9400177001953, Temp: 2.6503891944885254, KL: 68.07118225097656, Loss: 0.018018782138824463, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8921/20000], Bound: 0.3913625180721283, Entropy: 141.13568115234375, Temp: 2.650402545928955, KL: 73.7996826171875, Loss: 0.01940583437681198, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8922/20000], Bound: 0.35599285364151, Entropy: 140.6018524169922, Temp: 2.6503865718841553, KL: 65.44810485839844, Loss: 0.01615544781088829, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8923/20000], Bound: 0.3930668234825134, Entropy: 140.61532592773438, Temp: 2.6503653526306152, KL: 75.28262329101562, Loss: 0.0175483301281929, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8924/20000], Bound: 0.3951888680458069, Entropy: 140.8970184326172, Temp: 2.6503493785858154, KL: 75.28089904785156, Loss: 0.018725646659731865, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8925/20000], Bound: 0.4125535190105438, Entropy: 140.74473571777344, Temp: 2.6503214836120605, KL: 79.97805786132812, Loss: 0.019612479954957962, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8926/20000], Bound: 0.39045530557632446, Entropy: 141.293701171875, Temp: 2.6502842903137207, KL: 74.51467895507812, Loss: 0.017556237056851387, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8927/20000], Bound: 0.38891783356666565, Entropy: 138.49346923828125, Temp: 2.6502513885498047, KL: 73.99601745605469, Loss: 0.017689142376184464, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8928/20000], Bound: 0.37110477685928345, Entropy: 140.8294219970703, Temp: 2.650219440460205, KL: 69.27743530273438, Loss: 0.016933344304561615, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8929/20000], Bound: 0.3772018849849701, Entropy: 141.7970428466797, Temp: 2.650184154510498, KL: 72.662109375, Loss: 0.013825254514813423, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8930/20000], Bound: 0.42855149507522583, Entropy: 140.71275329589844, Temp: 2.6502013206481934, KL: 83.10549926757812, Loss: 0.022921008989214897, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8931/20000], Bound: 0.3762980103492737, Entropy: 142.14535522460938, Temp: 2.650167942047119, KL: 68.99977111816406, Loss: 0.020246950909495354, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8932/20000], Bound: 0.38399815559387207, Entropy: 141.61244201660156, Temp: 2.650083065032959, KL: 72.60423278808594, Loss: 0.017621250823140144, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8933/20000], Bound: 0.38316795229911804, Entropy: 140.0822296142578, Temp: 2.65000057220459, KL: 72.17062377929688, Loss: 0.01798619143664837, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8934/20000], Bound: 0.3811032474040985, Entropy: 139.9220733642578, Temp: 2.649914026260376, KL: 70.97958374023438, Loss: 0.01910984143614769, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8935/20000], Bound: 0.42903873324394226, Entropy: 141.356201171875, Temp: 2.649803638458252, KL: 85.2042236328125, Loss: 0.01924082078039646, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8936/20000], Bound: 0.4141041040420532, Entropy: 142.64654541015625, Temp: 2.6497132778167725, KL: 82.32614135742188, Loss: 0.01605832576751709, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8937/20000], Bound: 0.3708297908306122, Entropy: 138.96145629882812, Temp: 2.6496775150299072, KL: 70.02899169921875, Loss: 0.015363221056759357, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8938/20000], Bound: 0.38100335001945496, Entropy: 140.4977569580078, Temp: 2.6496639251708984, KL: 72.53536987304688, Loss: 0.01611766591668129, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8939/20000], Bound: 0.3944644033908844, Entropy: 140.2529754638672, Temp: 2.649667501449585, KL: 76.14295959472656, Loss: 0.016691014170646667, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8940/20000], Bound: 0.3945544958114624, Entropy: 139.614501953125, Temp: 2.649688959121704, KL: 76.45085144042969, Loss: 0.016160139814019203, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8941/20000], Bound: 0.369198203086853, Entropy: 140.46051025390625, Temp: 2.6497349739074707, KL: 68.62089538574219, Loss: 0.01714867167174816, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8942/20000], Bound: 0.37749066948890686, Entropy: 139.47486877441406, Temp: 2.6497652530670166, KL: 71.23783874511719, Loss: 0.01666448824107647, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8943/20000], Bound: 0.39016154408454895, Entropy: 140.47413635253906, Temp: 2.6497962474823, KL: 73.47314453125, Loss: 0.019355133175849915, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8944/20000], Bound: 0.3792078495025635, Entropy: 141.02394104003906, Temp: 2.6497960090637207, KL: 69.15455627441406, Loss: 0.021524658426642418, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8945/20000], Bound: 0.38910263776779175, Entropy: 140.74200439453125, Temp: 2.6497230529785156, KL: 74.88627624511719, Loss: 0.016105694696307182, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8946/20000], Bound: 0.38421350717544556, Entropy: 141.38189697265625, Temp: 2.6496803760528564, KL: 72.55136108398438, Loss: 0.017834747210144997, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8947/20000], Bound: 0.3933769464492798, Entropy: 141.17347717285156, Temp: 2.6496329307556152, KL: 76.3740234375, Loss: 0.01565297320485115, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8948/20000], Bound: 0.3880258798599243, Entropy: 143.455810546875, Temp: 2.6496241092681885, KL: 74.7237548828125, Loss: 0.01582026295363903, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8949/20000], Bound: 0.3987911343574524, Entropy: 138.01707458496094, Temp: 2.6496427059173584, KL: 74.24021911621094, Loss: 0.022684771567583084, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8950/20000], Bound: 0.4199310541152954, Entropy: 140.5330047607422, Temp: 2.6495864391326904, KL: 81.61721801757812, Loss: 0.020731624215841293, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8951/20000], Bound: 0.3775665760040283, Entropy: 141.45419311523438, Temp: 2.649513006210327, KL: 70.74774169921875, Loss: 0.017628055065870285, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8952/20000], Bound: 0.385321706533432, Entropy: 140.65296936035156, Temp: 2.649435520172119, KL: 72.10079956054688, Loss: 0.019287850707769394, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8953/20000], Bound: 0.38173171877861023, Entropy: 141.52886962890625, Temp: 2.64933443069458, KL: 72.65335083007812, Loss: 0.0162875484675169, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8954/20000], Bound: 0.4056207537651062, Entropy: 139.89230346679688, Temp: 2.649257183074951, KL: 78.12603759765625, Loss: 0.019173290580511093, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8955/20000], Bound: 0.3825691044330597, Entropy: 140.08799743652344, Temp: 2.6491763591766357, KL: 73.19282531738281, Loss: 0.015723327174782753, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8956/20000], Bound: 0.41601842641830444, Entropy: 140.66708374023438, Temp: 2.649127244949341, KL: 80.82904052734375, Loss: 0.019970178604125977, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8957/20000], Bound: 0.3725779354572296, Entropy: 141.78982543945312, Temp: 2.649068832397461, KL: 70.78591918945312, Loss: 0.014865756034851074, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8958/20000], Bound: 0.38862475752830505, Entropy: 140.24159240722656, Temp: 2.6490447521209717, KL: 72.63922119140625, Loss: 0.020077530294656754, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8959/20000], Bound: 0.3748713433742523, Entropy: 142.54721069335938, Temp: 2.648982524871826, KL: 70.22731018066406, Loss: 0.01715194247663021, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8960/20000], Bound: 0.3768552243709564, Entropy: 141.5325927734375, Temp: 2.6489202976226807, KL: 70.28877258300781, Loss: 0.018104886636137962, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8961/20000], Bound: 0.3861351013183594, Entropy: 139.58770751953125, Temp: 2.6488444805145264, KL: 73.31668090820312, Loss: 0.017432361841201782, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8962/20000], Bound: 0.3586731553077698, Entropy: 143.530517578125, Temp: 2.648775577545166, KL: 65.14581298828125, Loss: 0.01812080852687359, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8963/20000], Bound: 0.3741433322429657, Entropy: 142.78335571289062, Temp: 2.648677349090576, KL: 68.16311645507812, Loss: 0.02065409906208515, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8964/20000], Bound: 0.38594767451286316, Entropy: 141.85633850097656, Temp: 2.6485257148742676, KL: 75.13951110839844, Loss: 0.013885564170777798, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8965/20000], Bound: 0.37798020243644714, Entropy: 142.27825927734375, Temp: 2.6484456062316895, KL: 70.58636474609375, Loss: 0.018146859481930733, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8966/20000], Bound: 0.39534127712249756, Entropy: 141.6868896484375, Temp: 2.6483540534973145, KL: 76.29145812988281, Loss: 0.0168831255286932, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8967/20000], Bound: 0.38660579919815063, Entropy: 141.95803833007812, Temp: 2.6482880115509033, KL: 73.65232849121094, Loss: 0.017051002010703087, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8968/20000], Bound: 0.40338602662086487, Entropy: 142.14212036132812, Temp: 2.6482346057891846, KL: 79.270263671875, Loss: 0.015746211633086205, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8969/20000], Bound: 0.37447866797447205, Entropy: 141.06146240234375, Temp: 2.648228168487549, KL: 69.69255065917969, Loss: 0.017943495884537697, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8970/20000], Bound: 0.3969743251800537, Entropy: 140.83718872070312, Temp: 2.6482033729553223, KL: 76.60261535644531, Loss: 0.01720091514289379, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8971/20000], Bound: 0.39576292037963867, Entropy: 142.73118591308594, Temp: 2.648193836212158, KL: 75.74488830566406, Loss: 0.01814734749495983, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8972/20000], Bound: 0.3714253306388855, Entropy: 140.0187530517578, Temp: 2.648181915283203, KL: 70.10491943359375, Loss: 0.015524933114647865, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8973/20000], Bound: 0.4054758846759796, Entropy: 141.84042358398438, Temp: 2.648188591003418, KL: 79.31582641601562, Loss: 0.016834473237395287, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8974/20000], Bound: 0.37479332089424133, Entropy: 142.6513671875, Temp: 2.6482207775115967, KL: 72.08758544921875, Loss: 0.013590830378234386, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8975/20000], Bound: 0.4064216613769531, Entropy: 141.46572875976562, Temp: 2.648301124572754, KL: 78.10255432128906, Loss: 0.019659215584397316, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8976/20000], Bound: 0.37595269083976746, Entropy: 140.50228881835938, Temp: 2.648355722427368, KL: 71.0919189453125, Loss: 0.016096677631139755, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8977/20000], Bound: 0.3631550371646881, Entropy: 142.31744384765625, Temp: 2.6484172344207764, KL: 66.75465393066406, Loss: 0.01744680106639862, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8978/20000], Bound: 0.38271865248680115, Entropy: 142.6392059326172, Temp: 2.648451089859009, KL: 71.85905456542969, Loss: 0.018315574154257774, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8979/20000], Bound: 0.3990137279033661, Entropy: 142.20904541015625, Temp: 2.648463726043701, KL: 77.652587890625, Loss: 0.01635683886706829, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8980/20000], Bound: 0.37166574597358704, Entropy: 143.01834106445312, Temp: 2.648503303527832, KL: 68.30619812011719, Loss: 0.01905248314142227, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8981/20000], Bound: 0.37731611728668213, Entropy: 143.26565551757812, Temp: 2.6484992504119873, KL: 70.78564453125, Loss: 0.01741213910281658, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8982/20000], Bound: 0.3911850154399872, Entropy: 141.1465606689453, Temp: 2.6484873294830322, KL: 76.03507995605469, Loss: 0.015070423483848572, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8983/20000], Bound: 0.3855940103530884, Entropy: 141.78196716308594, Temp: 2.6485185623168945, KL: 73.59349060058594, Loss: 0.016610702499747276, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8984/20000], Bound: 0.38316500186920166, Entropy: 143.35101318359375, Temp: 2.648559093475342, KL: 71.71122741699219, Loss: 0.01883872039616108, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8985/20000], Bound: 0.38704895973205566, Entropy: 143.23953247070312, Temp: 2.648569345474243, KL: 74.08880615234375, Loss: 0.01647249050438404, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8986/20000], Bound: 0.3731949031352997, Entropy: 141.62554931640625, Temp: 2.648594379425049, KL: 69.938720703125, Loss: 0.016791773959994316, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8987/20000], Bound: 0.36877334117889404, Entropy: 141.62326049804688, Temp: 2.6486153602600098, KL: 70.09837341308594, Loss: 0.014123107306659222, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8988/20000], Bound: 0.41234010457992554, Entropy: 141.33363342285156, Temp: 2.6486716270446777, KL: 82.15396118164062, Loss: 0.015366281382739544, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8989/20000], Bound: 0.37980780005455017, Entropy: 141.95562744140625, Temp: 2.648777961730957, KL: 70.08131408691406, Loss: 0.020092763006687164, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8990/20000], Bound: 0.3928847312927246, Entropy: 142.75567626953125, Temp: 2.648824453353882, KL: 75.34696960449219, Loss: 0.017310958355665207, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8991/20000], Bound: 0.37993481755256653, Entropy: 141.26353454589844, Temp: 2.648873805999756, KL: 72.19404602050781, Loss: 0.016174446791410446, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8992/20000], Bound: 0.40537044405937195, Entropy: 142.1858673095703, Temp: 2.6489319801330566, KL: 78.94667053222656, Loss: 0.01748010888695717, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8993/20000], Bound: 0.3950687348842621, Entropy: 142.2667999267578, Temp: 2.6490001678466797, KL: 73.78077697753906, Loss: 0.021477604284882545, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8994/20000], Bound: 0.37940678000450134, Entropy: 142.78128051757812, Temp: 2.6490039825439453, KL: 71.47160339355469, Loss: 0.01725306175649166, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8995/20000], Bound: 0.4152999818325043, Entropy: 143.6541748046875, Temp: 2.649003744125366, KL: 80.93833923339844, Loss: 0.019351936876773834, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8996/20000], Bound: 0.37271392345428467, Entropy: 142.28184509277344, Temp: 2.648998260498047, KL: 68.31391906738281, Loss: 0.019603971391916275, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8997/20000], Bound: 0.41545024514198303, Entropy: 140.26107788085938, Temp: 2.6489453315734863, KL: 81.8948974609375, Loss: 0.017631608992815018, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8998/20000], Bound: 0.41035059094429016, Entropy: 141.24505615234375, Temp: 2.6489200592041016, KL: 80.21743774414062, Loss: 0.01789477840065956, Learning Rate: 0.0004152386160299997\n",
      "Epoch [8999/20000], Bound: 0.37837404012680054, Entropy: 142.42356872558594, Temp: 2.6489109992980957, KL: 70.96876525878906, Loss: 0.017642132937908173, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9000/20000], Bound: 0.38138821721076965, Entropy: 140.27102661132812, Temp: 2.6488916873931885, KL: 72.65156555175781, Loss: 0.016099942848086357, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9001/20000], Bound: 0.37942269444465637, Entropy: 144.12850952148438, Temp: 2.648890972137451, KL: 72.46949768066406, Loss: 0.015377016738057137, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9002/20000], Bound: 0.35689547657966614, Entropy: 141.56619262695312, Temp: 2.6489171981811523, KL: 65.66485595703125, Loss: 0.01620791107416153, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9003/20000], Bound: 0.3956620991230011, Entropy: 141.66925048828125, Temp: 2.6489341259002686, KL: 76.4185791015625, Loss: 0.016827119514346123, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9004/20000], Bound: 0.38945889472961426, Entropy: 143.0959930419922, Temp: 2.6489665508270264, KL: 74.96513366699219, Loss: 0.016144953668117523, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9005/20000], Bound: 0.35863253474235535, Entropy: 141.4852752685547, Temp: 2.649019241333008, KL: 65.81887817382812, Loss: 0.01683085225522518, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9006/20000], Bound: 0.4074172079563141, Entropy: 141.39483642578125, Temp: 2.6490509510040283, KL: 79.23616027832031, Loss: 0.018088705837726593, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9007/20000], Bound: 0.36199891567230225, Entropy: 141.81321716308594, Temp: 2.649087429046631, KL: 67.04107666015625, Loss: 0.01629992015659809, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9008/20000], Bound: 0.3775419294834137, Entropy: 143.4056396484375, Temp: 2.6491165161132812, KL: 70.94654846191406, Loss: 0.0172360111027956, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9009/20000], Bound: 0.3799879550933838, Entropy: 141.58071899414062, Temp: 2.6491377353668213, KL: 72.08169555664062, Loss: 0.01641779951751232, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9010/20000], Bound: 0.40014946460723877, Entropy: 140.4020233154297, Temp: 2.6491665840148926, KL: 77.44715881347656, Loss: 0.01738584041595459, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9011/20000], Bound: 0.3837850093841553, Entropy: 141.1200408935547, Temp: 2.649205446243286, KL: 72.51908874511719, Loss: 0.017657548189163208, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9012/20000], Bound: 0.38365697860717773, Entropy: 142.47996520996094, Temp: 2.649233818054199, KL: 72.54127502441406, Loss: 0.017546171322464943, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9013/20000], Bound: 0.3955710232257843, Entropy: 142.87557983398438, Temp: 2.649254322052002, KL: 75.46839904785156, Loss: 0.01857318915426731, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9014/20000], Bound: 0.3975987732410431, Entropy: 140.9801483154297, Temp: 2.6492624282836914, KL: 76.41159057617188, Loss: 0.01791958697140217, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9015/20000], Bound: 0.3988955020904541, Entropy: 142.9616241455078, Temp: 2.649271249771118, KL: 75.22247314453125, Loss: 0.02088598720729351, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9016/20000], Bound: 0.3953942656517029, Entropy: 141.78990173339844, Temp: 2.6492342948913574, KL: 75.86892700195312, Loss: 0.017719034105539322, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9017/20000], Bound: 0.3830175995826721, Entropy: 141.43740844726562, Temp: 2.6492037773132324, KL: 73.53587341308594, Loss: 0.015320328995585442, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9018/20000], Bound: 0.40303483605384827, Entropy: 139.2046661376953, Temp: 2.6492068767547607, KL: 78.93373107910156, Loss: 0.016195347532629967, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9019/20000], Bound: 0.3828819692134857, Entropy: 140.57843017578125, Temp: 2.649244546890259, KL: 72.76531982421875, Loss: 0.01670115441083908, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9020/20000], Bound: 0.3594304323196411, Entropy: 139.94442749023438, Temp: 2.6492862701416016, KL: 67.66877746582031, Loss: 0.01376170851290226, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9021/20000], Bound: 0.4065048396587372, Entropy: 139.2192840576172, Temp: 2.649359941482544, KL: 78.60235595703125, Loss: 0.018773440271615982, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9022/20000], Bound: 0.37722453474998474, Entropy: 141.13507080078125, Temp: 2.6494221687316895, KL: 71.89718627929688, Loss: 0.015273196622729301, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9023/20000], Bound: 0.39744389057159424, Entropy: 140.6835479736328, Temp: 2.6495046615600586, KL: 76.13044738769531, Loss: 0.018366415053606033, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9024/20000], Bound: 0.41104793548583984, Entropy: 140.14395141601562, Temp: 2.6495730876922607, KL: 80.2032470703125, Loss: 0.018324250355362892, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9025/20000], Bound: 0.39411503076553345, Entropy: 138.24913024902344, Temp: 2.649641990661621, KL: 76.17594909667969, Loss: 0.01643509790301323, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9026/20000], Bound: 0.3781943917274475, Entropy: 142.86328125, Temp: 2.649726629257202, KL: 69.25480651855469, Loss: 0.020786484703421593, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9027/20000], Bound: 0.3985871374607086, Entropy: 141.64950561523438, Temp: 2.6497397422790527, KL: 77.11767578125, Loss: 0.01714220829308033, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9028/20000], Bound: 0.3816696107387543, Entropy: 142.25340270996094, Temp: 2.649766445159912, KL: 72.71556091308594, Loss: 0.01614055037498474, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9029/20000], Bound: 0.3855571746826172, Entropy: 140.92955017089844, Temp: 2.6498067378997803, KL: 73.78645324707031, Loss: 0.016239071264863014, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9030/20000], Bound: 0.40162578225135803, Entropy: 139.26170349121094, Temp: 2.6498610973358154, KL: 78.53024291992188, Loss: 0.016174662858247757, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9031/20000], Bound: 0.39527764916419983, Entropy: 139.22933959960938, Temp: 2.6499433517456055, KL: 75.95452880859375, Loss: 0.017499949783086777, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9032/20000], Bound: 0.37171056866645813, Entropy: 141.24465942382812, Temp: 2.6500234603881836, KL: 68.59068298339844, Loss: 0.018551817163825035, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9033/20000], Bound: 0.36962267756462097, Entropy: 140.59161376953125, Temp: 2.6500632762908936, KL: 67.12445068359375, Loss: 0.02020157314836979, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9034/20000], Bound: 0.3929620683193207, Entropy: 139.85052490234375, Temp: 2.6500372886657715, KL: 74.10664367675781, Loss: 0.019706010818481445, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9035/20000], Bound: 0.3842179477214813, Entropy: 140.80653381347656, Temp: 2.649982213973999, KL: 72.66563415527344, Loss: 0.01762431673705578, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9036/20000], Bound: 0.39150190353393555, Entropy: 140.01324462890625, Temp: 2.6499269008636475, KL: 75.9842529296875, Loss: 0.015356442891061306, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9037/20000], Bound: 0.37278884649276733, Entropy: 141.82508850097656, Temp: 2.6499147415161133, KL: 69.88336181640625, Loss: 0.016690008342266083, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9038/20000], Bound: 0.3752292990684509, Entropy: 141.1807403564453, Temp: 2.649902820587158, KL: 69.78103637695312, Loss: 0.01819486916065216, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9039/20000], Bound: 0.4112774431705475, Entropy: 141.46896362304688, Temp: 2.6498684883117676, KL: 81.28704833984375, Loss: 0.016412755474448204, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9040/20000], Bound: 0.39692503213882446, Entropy: 141.45896911621094, Temp: 2.649876117706299, KL: 77.49317932128906, Loss: 0.015510321594774723, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9041/20000], Bound: 0.3846210241317749, Entropy: 142.48179626464844, Temp: 2.6499228477478027, KL: 70.83631896972656, Loss: 0.02129535563290119, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9042/20000], Bound: 0.38316184282302856, Entropy: 141.95907592773438, Temp: 2.649899482727051, KL: 73.91708374023438, Loss: 0.014686640352010727, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9043/20000], Bound: 0.3691730797290802, Entropy: 142.09127807617188, Temp: 2.6499195098876953, KL: 69.55906677246094, Loss: 0.015366609208285809, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9044/20000], Bound: 0.38031065464019775, Entropy: 141.35206604003906, Temp: 2.6499555110931396, KL: 72.01832580566406, Loss: 0.016720116138458252, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9045/20000], Bound: 0.3907555937767029, Entropy: 140.8251190185547, Temp: 2.6499931812286377, KL: 74.40536499023438, Loss: 0.01792498491704464, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9046/20000], Bound: 0.38232091069221497, Entropy: 141.5379638671875, Temp: 2.650022268295288, KL: 74.23973083496094, Loss: 0.013621394522488117, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9047/20000], Bound: 0.3688325881958008, Entropy: 143.05618286132812, Temp: 2.650106430053711, KL: 67.78997802734375, Loss: 0.018524399027228355, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9048/20000], Bound: 0.40662112832069397, Entropy: 137.99295043945312, Temp: 2.6501474380493164, KL: 80.20718383789062, Loss: 0.015819352120161057, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9049/20000], Bound: 0.3852092921733856, Entropy: 141.29380798339844, Temp: 2.6502277851104736, KL: 72.530517578125, Loss: 0.018422644585371017, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9050/20000], Bound: 0.3838023841381073, Entropy: 141.37188720703125, Temp: 2.650282144546509, KL: 71.47021484375, Loss: 0.01965579390525818, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9051/20000], Bound: 0.38877809047698975, Entropy: 140.1537322998047, Temp: 2.6502914428710938, KL: 73.02798461914062, Loss: 0.019439084455370903, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9052/20000], Bound: 0.4320489466190338, Entropy: 141.98590087890625, Temp: 2.650268316268921, KL: 86.92295837402344, Loss: 0.017763439565896988, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9053/20000], Bound: 0.39991462230682373, Entropy: 140.50469970703125, Temp: 2.650282859802246, KL: 76.40475463867188, Loss: 0.019233006983995438, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9054/20000], Bound: 0.38511180877685547, Entropy: 140.84152221679688, Temp: 2.650277853012085, KL: 71.58351135253906, Loss: 0.02015647105872631, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9055/20000], Bound: 0.3869440257549286, Entropy: 142.56039428710938, Temp: 2.6502268314361572, KL: 73.74131774902344, Loss: 0.017087023705244064, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9056/20000], Bound: 0.397875040769577, Entropy: 140.1385498046875, Temp: 2.65018630027771, KL: 77.08969116210938, Loss: 0.016803201287984848, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9057/20000], Bound: 0.3767329752445221, Entropy: 140.52699279785156, Temp: 2.650169610977173, KL: 70.96499633789062, Loss: 0.01677389070391655, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9058/20000], Bound: 0.3804774582386017, Entropy: 142.01097106933594, Temp: 2.650155782699585, KL: 71.58949279785156, Loss: 0.01762154884636402, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9059/20000], Bound: 0.4109899401664734, Entropy: 140.58990478515625, Temp: 2.6501340866088867, KL: 79.44729614257812, Loss: 0.0197236817330122, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9060/20000], Bound: 0.3869266211986542, Entropy: 141.24563598632812, Temp: 2.650099039077759, KL: 73.05592346191406, Loss: 0.01836942508816719, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9061/20000], Bound: 0.36410629749298096, Entropy: 140.81275939941406, Temp: 2.6500518321990967, KL: 64.14109802246094, Loss: 0.02289499156177044, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9062/20000], Bound: 0.38862982392311096, Entropy: 140.37193298339844, Temp: 2.6498982906341553, KL: 74.99034118652344, Loss: 0.015651462599635124, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9063/20000], Bound: 0.3982327878475189, Entropy: 140.54066467285156, Temp: 2.6497905254364014, KL: 77.86589050292969, Loss: 0.0155335683375597, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9064/20000], Bound: 0.40305987000465393, Entropy: 141.6310272216797, Temp: 2.6497342586517334, KL: 78.11335754394531, Loss: 0.017763236537575722, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9065/20000], Bound: 0.3886908292770386, Entropy: 140.55673217773438, Temp: 2.6496925354003906, KL: 71.62208557128906, Loss: 0.022038768976926804, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9066/20000], Bound: 0.4032345712184906, Entropy: 138.84988403320312, Temp: 2.6495814323425293, KL: 77.10276794433594, Loss: 0.01976669766008854, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9067/20000], Bound: 0.39508625864982605, Entropy: 141.15042114257812, Temp: 2.649458408355713, KL: 77.57887268066406, Loss: 0.014323551207780838, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9068/20000], Bound: 0.37264248728752136, Entropy: 140.683837890625, Temp: 2.649405002593994, KL: 71.40299987792969, Loss: 0.013739028014242649, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9069/20000], Bound: 0.4115104079246521, Entropy: 141.24197387695312, Temp: 2.649404764175415, KL: 81.45368957519531, Loss: 0.01622517965734005, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9070/20000], Bound: 0.37949812412261963, Entropy: 140.22525024414062, Temp: 2.6494460105895996, KL: 72.07968139648438, Loss: 0.016159018501639366, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9071/20000], Bound: 0.3482922911643982, Entropy: 140.4808807373047, Temp: 2.6494972705841064, KL: 63.400146484375, Loss: 0.01599820889532566, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9072/20000], Bound: 0.36694762110710144, Entropy: 140.75477600097656, Temp: 2.6495323181152344, KL: 68.51408386230469, Loss: 0.0161487627774477, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9073/20000], Bound: 0.37989506125450134, Entropy: 141.88644409179688, Temp: 2.649566888809204, KL: 71.55416870117188, Loss: 0.01736699976027012, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9074/20000], Bound: 0.36670422554016113, Entropy: 139.97767639160156, Temp: 2.649592638015747, KL: 67.65138244628906, Loss: 0.01764773204922676, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9075/20000], Bound: 0.4235231280326843, Entropy: 139.52972412109375, Temp: 2.6495935916900635, KL: 82.49783325195312, Loss: 0.021141814067959785, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9076/20000], Bound: 0.40486496686935425, Entropy: 140.0442352294922, Temp: 2.6495683193206787, KL: 79.28016662597656, Loss: 0.01657332293689251, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9077/20000], Bound: 0.40468984842300415, Entropy: 140.6043701171875, Temp: 2.6495754718780518, KL: 78.79881286621094, Loss: 0.017383256927132607, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9078/20000], Bound: 0.38618040084838867, Entropy: 139.4922637939453, Temp: 2.6495985984802246, KL: 73.73597717285156, Loss: 0.01667306013405323, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9079/20000], Bound: 0.38827893137931824, Entropy: 140.67042541503906, Temp: 2.6496307849884033, KL: 74.82368469238281, Loss: 0.01577058993279934, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9080/20000], Bound: 0.3984522223472595, Entropy: 140.6654815673828, Temp: 2.6496880054473877, KL: 77.91258239746094, Loss: 0.015566514804959297, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9081/20000], Bound: 0.4020572006702423, Entropy: 140.84619140625, Temp: 2.649780035018921, KL: 78.23876953125, Loss: 0.016965219751000404, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9082/20000], Bound: 0.3765622675418854, Entropy: 141.3920440673828, Temp: 2.649883985519409, KL: 72.16796875, Loss: 0.014409312978386879, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9083/20000], Bound: 0.37180399894714355, Entropy: 140.43655395507812, Temp: 2.650017499923706, KL: 69.35797119140625, Loss: 0.01715412735939026, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9084/20000], Bound: 0.3812263011932373, Entropy: 141.08680725097656, Temp: 2.6501283645629883, KL: 72.30717468261719, Loss: 0.01667374186217785, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9085/20000], Bound: 0.4124554395675659, Entropy: 138.930419921875, Temp: 2.650235176086426, KL: 80.66412353515625, Loss: 0.018261462450027466, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9086/20000], Bound: 0.4076765179634094, Entropy: 140.4140167236328, Temp: 2.6503405570983887, KL: 79.87629699707031, Loss: 0.017041170969605446, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9087/20000], Bound: 0.35928213596343994, Entropy: 141.57594299316406, Temp: 2.6504602432250977, KL: 66.33937072753906, Loss: 0.0162022914737463, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9088/20000], Bound: 0.37723636627197266, Entropy: 141.24266052246094, Temp: 2.6505630016326904, KL: 70.89228820800781, Loss: 0.017186351120471954, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9089/20000], Bound: 0.4041154384613037, Entropy: 139.01513671875, Temp: 2.6506505012512207, KL: 78.35247802734375, Loss: 0.01791403256356716, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9090/20000], Bound: 0.37618520855903625, Entropy: 139.42820739746094, Temp: 2.6507365703582764, KL: 70.39886474609375, Loss: 0.017551518976688385, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9091/20000], Bound: 0.38648873567581177, Entropy: 140.92210388183594, Temp: 2.650801658630371, KL: 73.8004150390625, Loss: 0.016731930896639824, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9092/20000], Bound: 0.4055975675582886, Entropy: 143.39306640625, Temp: 2.6508710384368896, KL: 78.65634155273438, Loss: 0.018176330253481865, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9093/20000], Bound: 0.3802899718284607, Entropy: 138.06748962402344, Temp: 2.650937795639038, KL: 73.49075317382812, Loss: 0.013940887525677681, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9094/20000], Bound: 0.37128907442092896, Entropy: 139.76316833496094, Temp: 2.6510486602783203, KL: 69.24038696289062, Loss: 0.017109069973230362, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9095/20000], Bound: 0.3841147720813751, Entropy: 142.2909698486328, Temp: 2.6511390209198, KL: 72.47471618652344, Loss: 0.017938880249857903, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9096/20000], Bound: 0.37765708565711975, Entropy: 141.62318420410156, Temp: 2.6512091159820557, KL: 71.89048767089844, Loss: 0.015536817722022533, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9097/20000], Bound: 0.3935072720050812, Entropy: 142.07423400878906, Temp: 2.651294469833374, KL: 76.44482421875, Loss: 0.015609120950102806, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9098/20000], Bound: 0.3900448977947235, Entropy: 139.92190551757812, Temp: 2.6514062881469727, KL: 74.31492614746094, Loss: 0.017718052491545677, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9099/20000], Bound: 0.39210033416748047, Entropy: 139.4847869873047, Temp: 2.6515045166015625, KL: 75.72474670410156, Loss: 0.01619238592684269, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9100/20000], Bound: 0.39034920930862427, Entropy: 140.07342529296875, Temp: 2.6516172885894775, KL: 75.63174438476562, Loss: 0.015404383651912212, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9101/20000], Bound: 0.3889137804508209, Entropy: 142.70335388183594, Temp: 2.651754140853882, KL: 75.90864562988281, Loss: 0.014094917103648186, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9102/20000], Bound: 0.4041116535663605, Entropy: 140.16482543945312, Temp: 2.651933193206787, KL: 80.66500854492188, Loss: 0.013565265573561192, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9103/20000], Bound: 0.3826484680175781, Entropy: 139.2732391357422, Temp: 2.6521716117858887, KL: 72.13719177246094, Loss: 0.017786210402846336, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9104/20000], Bound: 0.3965758681297302, Entropy: 138.54103088378906, Temp: 2.6523756980895996, KL: 77.27371215820312, Loss: 0.015757441520690918, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9105/20000], Bound: 0.3843472898006439, Entropy: 141.12725830078125, Temp: 2.6525943279266357, KL: 74.74015808105469, Loss: 0.013808655552566051, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9106/20000], Bound: 0.37641119956970215, Entropy: 140.02590942382812, Temp: 2.6528472900390625, KL: 70.42498779296875, Loss: 0.017642557621002197, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9107/20000], Bound: 0.36557185649871826, Entropy: 141.31285095214844, Temp: 2.6530609130859375, KL: 68.85389709472656, Loss: 0.01480826549232006, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9108/20000], Bound: 0.3820023536682129, Entropy: 140.15582275390625, Temp: 2.6532764434814453, KL: 72.7508544921875, Loss: 0.016288619488477707, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9109/20000], Bound: 0.38095173239707947, Entropy: 141.27835083007812, Temp: 2.6534833908081055, KL: 72.7261962890625, Loss: 0.01576678268611431, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9110/20000], Bound: 0.40017712116241455, Entropy: 140.24005126953125, Temp: 2.6536905765533447, KL: 76.40190124511719, Loss: 0.019417723640799522, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9111/20000], Bound: 0.4077115058898926, Entropy: 140.32675170898438, Temp: 2.653855562210083, KL: 80.46273803710938, Loss: 0.01599474623799324, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9112/20000], Bound: 0.38415923714637756, Entropy: 142.0986328125, Temp: 2.6540446281433105, KL: 74.79270935058594, Loss: 0.013622652739286423, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9113/20000], Bound: 0.3996571898460388, Entropy: 141.53985595703125, Temp: 2.654273271560669, KL: 77.56867980957031, Loss: 0.01693546585738659, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9114/20000], Bound: 0.3993968367576599, Entropy: 140.5244903564453, Temp: 2.6544973850250244, KL: 76.63221740722656, Loss: 0.01855669729411602, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9115/20000], Bound: 0.4049266576766968, Entropy: 139.91697692871094, Temp: 2.6546905040740967, KL: 77.44866943359375, Loss: 0.020113816484808922, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9116/20000], Bound: 0.3644227981567383, Entropy: 141.80027770996094, Temp: 2.6548352241516113, KL: 66.97573852539062, Loss: 0.017752578482031822, Learning Rate: 0.0004152386160299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9117/20000], Bound: 0.3818199336528778, Entropy: 141.2899932861328, Temp: 2.654937982559204, KL: 70.88008117675781, Loss: 0.019728654995560646, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9118/20000], Bound: 0.39949560165405273, Entropy: 139.81765747070312, Temp: 2.6549863815307617, KL: 76.18679809570312, Loss: 0.019455386325716972, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9119/20000], Bound: 0.3778984844684601, Entropy: 141.48558044433594, Temp: 2.6550064086914062, KL: 70.5849609375, Loss: 0.01816212385892868, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9120/20000], Bound: 0.3872453272342682, Entropy: 140.51087951660156, Temp: 2.6550025939941406, KL: 73.86003112792969, Loss: 0.01707414910197258, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9121/20000], Bound: 0.41824015974998474, Entropy: 140.39149475097656, Temp: 2.655003547668457, KL: 83.27935791015625, Loss: 0.016688119620084763, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9122/20000], Bound: 0.36330446600914, Entropy: 139.947509765625, Temp: 2.655042886734009, KL: 66.72917175292969, Loss: 0.017626754939556122, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9123/20000], Bound: 0.380271315574646, Entropy: 140.97528076171875, Temp: 2.6550519466400146, KL: 72.121826171875, Loss: 0.016551369801163673, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9124/20000], Bound: 0.4201309084892273, Entropy: 138.71197509765625, Temp: 2.655066967010498, KL: 82.69430541992188, Loss: 0.0188754815608263, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9125/20000], Bound: 0.38987886905670166, Entropy: 140.3892364501953, Temp: 2.655085325241089, KL: 75.19673156738281, Loss: 0.016001397743821144, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9126/20000], Bound: 0.3882431089878082, Entropy: 140.60572814941406, Temp: 2.6551265716552734, KL: 74.59747314453125, Loss: 0.01623295247554779, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9127/20000], Bound: 0.3640642464160919, Entropy: 142.40200805664062, Temp: 2.6551828384399414, KL: 68.44635009765625, Loss: 0.014796185307204723, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9128/20000], Bound: 0.4030093848705292, Entropy: 141.5331268310547, Temp: 2.655255079269409, KL: 75.92599487304688, Loss: 0.02191142924129963, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9129/20000], Bound: 0.37661227583885193, Entropy: 142.6815948486328, Temp: 2.655259370803833, KL: 71.52432250976562, Loss: 0.01570175215601921, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9130/20000], Bound: 0.37407606840133667, Entropy: 141.59202575683594, Temp: 2.6552810668945312, KL: 69.87582397460938, Loss: 0.017442144453525543, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9131/20000], Bound: 0.40143248438835144, Entropy: 140.55471801757812, Temp: 2.6552867889404297, KL: 78.25718688964844, Loss: 0.01664002425968647, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9132/20000], Bound: 0.38420718908309937, Entropy: 142.40321350097656, Temp: 2.655316114425659, KL: 72.18965148925781, Loss: 0.01856396719813347, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9133/20000], Bound: 0.3978962004184723, Entropy: 140.8258819580078, Temp: 2.6553194522857666, KL: 75.42770385742188, Loss: 0.019998040050268173, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9134/20000], Bound: 0.3744278848171234, Entropy: 142.05935668945312, Temp: 2.6552886962890625, KL: 70.37625122070312, Loss: 0.016688814386725426, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9135/20000], Bound: 0.36016082763671875, Entropy: 140.84429931640625, Temp: 2.6552600860595703, KL: 66.31086730957031, Loss: 0.016757657751441002, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9136/20000], Bound: 0.3638511002063751, Entropy: 141.12416076660156, Temp: 2.655219316482544, KL: 66.16207885742188, Loss: 0.018985167145729065, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9137/20000], Bound: 0.3664851486682892, Entropy: 139.4815673828125, Temp: 2.6551337242126465, KL: 69.21607971191406, Loss: 0.014629836194217205, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9138/20000], Bound: 0.41355085372924805, Entropy: 142.19284057617188, Temp: 2.655083179473877, KL: 80.2462158203125, Loss: 0.01972382888197899, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9139/20000], Bound: 0.35334858298301697, Entropy: 143.201171875, Temp: 2.6550228595733643, KL: 64.53620910644531, Loss: 0.016528530046343803, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9140/20000], Bound: 0.3963835835456848, Entropy: 140.2069091796875, Temp: 2.654951572418213, KL: 77.30307006835938, Loss: 0.015623094514012337, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9141/20000], Bound: 0.3890343904495239, Entropy: 140.31094360351562, Temp: 2.654923915863037, KL: 71.88816833496094, Loss: 0.021767206490039825, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9142/20000], Bound: 0.37515708804130554, Entropy: 142.66070556640625, Temp: 2.65482759475708, KL: 70.69131469726562, Loss: 0.01648324728012085, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9143/20000], Bound: 0.3668711185455322, Entropy: 141.60409545898438, Temp: 2.6547443866729736, KL: 68.21826171875, Loss: 0.016710620373487473, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9144/20000], Bound: 0.41117992997169495, Entropy: 143.20582580566406, Temp: 2.6546614170074463, KL: 80.54153442382812, Loss: 0.017816687002778053, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9145/20000], Bound: 0.3923109769821167, Entropy: 140.55477905273438, Temp: 2.6546008586883545, KL: 73.95199584960938, Loss: 0.019679319113492966, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9146/20000], Bound: 0.3735092580318451, Entropy: 140.72079467773438, Temp: 2.654512643814087, KL: 69.69776916503906, Loss: 0.017466677352786064, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9147/20000], Bound: 0.398904412984848, Entropy: 139.21551513671875, Temp: 2.6544189453125, KL: 76.54487609863281, Loss: 0.01844623312354088, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9148/20000], Bound: 0.36564281582832336, Entropy: 140.41392517089844, Temp: 2.654327154159546, KL: 67.51708984375, Loss: 0.017375579103827477, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9149/20000], Bound: 0.39449751377105713, Entropy: 140.51547241210938, Temp: 2.654224395751953, KL: 74.72331237792969, Loss: 0.019430430606007576, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9150/20000], Bound: 0.4001235365867615, Entropy: 139.91404724121094, Temp: 2.654104232788086, KL: 76.37518310546875, Loss: 0.019442101940512657, Learning Rate: 0.0004152386160299997\n",
      "Epoch [9151/20000], Bound: 0.3902149498462677, Entropy: 142.344482421875, Temp: 2.65401291847229, KL: 75.55221557617188, Loss: 0.01550544984638691, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9152/20000], Bound: 0.376926451921463, Entropy: 141.159423828125, Temp: 2.653953790664673, KL: 71.49600219726562, Loss: 0.015912167727947235, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9153/20000], Bound: 0.3698108196258545, Entropy: 141.37440490722656, Temp: 2.6539111137390137, KL: 69.884521484375, Loss: 0.015130260027945042, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9154/20000], Bound: 0.396483451128006, Entropy: 140.21340942382812, Temp: 2.6538877487182617, KL: 76.48675537109375, Loss: 0.01720500737428665, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9155/20000], Bound: 0.3800579607486725, Entropy: 142.66184997558594, Temp: 2.653874397277832, KL: 72.57026672363281, Loss: 0.015579906292259693, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9156/20000], Bound: 0.3876565396785736, Entropy: 140.22430419921875, Temp: 2.6538784503936768, KL: 74.7615966796875, Loss: 0.015589899383485317, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9157/20000], Bound: 0.39067378640174866, Entropy: 142.85824584960938, Temp: 2.6539032459259033, KL: 74.51397705078125, Loss: 0.017712688073515892, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9158/20000], Bound: 0.3781673312187195, Entropy: 142.03675842285156, Temp: 2.653923511505127, KL: 71.93743896484375, Loss: 0.015749918296933174, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9159/20000], Bound: 0.389702707529068, Entropy: 141.82086181640625, Temp: 2.6539547443389893, KL: 74.35595703125, Loss: 0.01747710257768631, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9160/20000], Bound: 0.3697023093700409, Entropy: 141.1479949951172, Temp: 2.6539831161499023, KL: 68.73146057128906, Loss: 0.017245305702090263, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9161/20000], Bound: 0.3523850440979004, Entropy: 142.947998046875, Temp: 2.653998851776123, KL: 64.17630004882812, Loss: 0.016696421429514885, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9162/20000], Bound: 0.37078243494033813, Entropy: 143.57083129882812, Temp: 2.653998374938965, KL: 67.98345947265625, Loss: 0.019231555983424187, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9163/20000], Bound: 0.3863688111305237, Entropy: 141.10179138183594, Temp: 2.653965711593628, KL: 73.41476440429688, Loss: 0.01742364838719368, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9164/20000], Bound: 0.36971598863601685, Entropy: 139.57748413085938, Temp: 2.6539347171783447, KL: 68.62956237792969, Loss: 0.01744415983557701, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9165/20000], Bound: 0.37883156538009644, Entropy: 141.37710571289062, Temp: 2.6538946628570557, KL: 69.91799926757812, Loss: 0.019913312047719955, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9166/20000], Bound: 0.3857325613498688, Entropy: 142.92323303222656, Temp: 2.6538233757019043, KL: 73.92298889160156, Loss: 0.016117140650749207, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9167/20000], Bound: 0.3921903669834137, Entropy: 141.2349395751953, Temp: 2.6537728309631348, KL: 75.3651123046875, Loss: 0.016942938789725304, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9168/20000], Bound: 0.39270469546318054, Entropy: 142.61819458007812, Temp: 2.653735399246216, KL: 75.46176147460938, Loss: 0.01704414188861847, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9169/20000], Bound: 0.40082985162734985, Entropy: 143.08969116210938, Temp: 2.6537086963653564, KL: 77.12850952148438, Loss: 0.01841312274336815, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9170/20000], Bound: 0.3988736867904663, Entropy: 141.6069793701172, Temp: 2.653681516647339, KL: 76.72145080566406, Loss: 0.018089139834046364, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9171/20000], Bound: 0.39130306243896484, Entropy: 141.5912322998047, Temp: 2.653656005859375, KL: 74.33064270019531, Loss: 0.01840204745531082, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9172/20000], Bound: 0.3702617287635803, Entropy: 141.664794921875, Temp: 2.653623580932617, KL: 67.46929931640625, Loss: 0.019919171929359436, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9173/20000], Bound: 0.40777477622032166, Entropy: 142.52870178222656, Temp: 2.6535534858703613, KL: 80.49697875976562, Loss: 0.01596248522400856, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9174/20000], Bound: 0.3523573577404022, Entropy: 142.79478454589844, Temp: 2.653519868850708, KL: 63.79551696777344, Loss: 0.017395880073308945, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9175/20000], Bound: 0.3748667240142822, Entropy: 142.69261169433594, Temp: 2.6534669399261475, KL: 70.69879150390625, Loss: 0.016300616785883904, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9176/20000], Bound: 0.383312851190567, Entropy: 141.57009887695312, Temp: 2.65342378616333, KL: 73.34968566894531, Loss: 0.015874231234192848, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9177/20000], Bound: 0.4130271077156067, Entropy: 142.48619079589844, Temp: 2.65339994430542, KL: 80.34388732910156, Loss: 0.01922435872256756, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9178/20000], Bound: 0.3979083299636841, Entropy: 141.89849853515625, Temp: 2.6533737182617188, KL: 75.88142395019531, Loss: 0.019131910055875778, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9179/20000], Bound: 0.38277921080589294, Entropy: 141.88726806640625, Temp: 2.653336524963379, KL: 72.74383544921875, Loss: 0.01672467030584812, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9180/20000], Bound: 0.3777940273284912, Entropy: 139.99783325195312, Temp: 2.6533076763153076, KL: 70.38627624511719, Loss: 0.018465492874383926, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9181/20000], Bound: 0.36719220876693726, Entropy: 143.46827697753906, Temp: 2.6532628536224365, KL: 69.25874328613281, Loss: 0.01490808092057705, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9182/20000], Bound: 0.378803014755249, Entropy: 141.20199584960938, Temp: 2.6532387733459473, KL: 72.1551513671875, Loss: 0.0156767126172781, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9183/20000], Bound: 0.38966992497444153, Entropy: 142.7440948486328, Temp: 2.653231620788574, KL: 75.06941223144531, Loss: 0.01610766164958477, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9184/20000], Bound: 0.39331355690956116, Entropy: 141.72630310058594, Temp: 2.6532416343688965, KL: 77.23884582519531, Loss: 0.014026409946382046, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9185/20000], Bound: 0.38165438175201416, Entropy: 139.974853515625, Temp: 2.6532933712005615, KL: 72.854248046875, Loss: 0.015904944390058517, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9186/20000], Bound: 0.4109337627887726, Entropy: 142.35452270507812, Temp: 2.653353691101074, KL: 81.37612915039062, Loss: 0.01609010063111782, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9187/20000], Bound: 0.39879727363586426, Entropy: 141.6591796875, Temp: 2.653438091278076, KL: 79.56803894042969, Loss: 0.012680216692388058, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9188/20000], Bound: 0.3970932960510254, Entropy: 141.19810485839844, Temp: 2.653575897216797, KL: 76.5501708984375, Loss: 0.017420830205082893, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9189/20000], Bound: 0.35653334856033325, Entropy: 143.32522583007812, Temp: 2.653705358505249, KL: 64.00042724609375, Loss: 0.01919291727244854, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9190/20000], Bound: 0.3863399922847748, Entropy: 140.6840362548828, Temp: 2.653780460357666, KL: 74.46080017089844, Loss: 0.015435304492712021, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9191/20000], Bound: 0.39926570653915405, Entropy: 141.1698455810547, Temp: 2.653870105743408, KL: 77.21302795410156, Loss: 0.017383204773068428, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9192/20000], Bound: 0.3881373703479767, Entropy: 141.91775512695312, Temp: 2.653958320617676, KL: 74.57461547851562, Loss: 0.016206417232751846, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9193/20000], Bound: 0.4078810513019562, Entropy: 142.86251831054688, Temp: 2.6540520191192627, KL: 79.57110595703125, Loss: 0.01777241751551628, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9194/20000], Bound: 0.39257070422172546, Entropy: 141.3065643310547, Temp: 2.654144525527954, KL: 74.85264587402344, Loss: 0.018121786415576935, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9195/20000], Bound: 0.41808822751045227, Entropy: 141.49819946289062, Temp: 2.6542224884033203, KL: 81.61837768554688, Loss: 0.019720757380127907, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9196/20000], Bound: 0.35991179943084717, Entropy: 143.7133331298828, Temp: 2.654285430908203, KL: 65.53240966796875, Loss: 0.018085157498717308, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9197/20000], Bound: 0.3948066830635071, Entropy: 142.650634765625, Temp: 2.654315710067749, KL: 75.48333740234375, Loss: 0.01817059889435768, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9198/20000], Bound: 0.39101898670196533, Entropy: 141.3404083251953, Temp: 2.6543385982513428, KL: 73.49143981933594, Loss: 0.01983293890953064, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9199/20000], Bound: 0.3749629557132721, Entropy: 142.74661254882812, Temp: 2.6543326377868652, KL: 70.84127807617188, Loss: 0.016091831028461456, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9200/20000], Bound: 0.4018864631652832, Entropy: 140.86363220214844, Temp: 2.65433406829834, KL: 78.05874633789062, Loss: 0.01725742407143116, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9201/20000], Bound: 0.390431672334671, Entropy: 141.7180938720703, Temp: 2.654345989227295, KL: 74.79362487792969, Loss: 0.0170570220798254, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9202/20000], Bound: 0.38105109333992004, Entropy: 140.548583984375, Temp: 2.654362201690674, KL: 72.59724426269531, Loss: 0.01607207953929901, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9203/20000], Bound: 0.39872148633003235, Entropy: 140.2976531982422, Temp: 2.654387950897217, KL: 77.33982849121094, Loss: 0.01684667356312275, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9204/20000], Bound: 0.3917311429977417, Entropy: 141.5033416748047, Temp: 2.6544244289398193, KL: 75.27104187011719, Loss: 0.016873611137270927, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9205/20000], Bound: 0.38045069575309753, Entropy: 142.06204223632812, Temp: 2.654465913772583, KL: 73.67088317871094, Loss: 0.01372525654733181, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9206/20000], Bound: 0.4031559228897095, Entropy: 140.70594787597656, Temp: 2.654541492462158, KL: 78.43753051757812, Loss: 0.01725655607879162, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9207/20000], Bound: 0.39735960960388184, Entropy: 140.93966674804688, Temp: 2.654620885848999, KL: 76.95887756347656, Loss: 0.01680952124297619, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9208/20000], Bound: 0.39030104875564575, Entropy: 141.106201171875, Temp: 2.654705286026001, KL: 74.27299499511719, Loss: 0.01796933077275753, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9209/20000], Bound: 0.3803172707557678, Entropy: 142.4876708984375, Temp: 2.654776096343994, KL: 71.20977783203125, Loss: 0.018291443586349487, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9210/20000], Bound: 0.41584834456443787, Entropy: 141.18746948242188, Temp: 2.6548240184783936, KL: 82.94932556152344, Loss: 0.01593960076570511, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9211/20000], Bound: 0.39748191833496094, Entropy: 141.35406494140625, Temp: 2.6549017429351807, KL: 77.8590087890625, Loss: 0.015185140073299408, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9212/20000], Bound: 0.39249274134635925, Entropy: 141.48373413085938, Temp: 2.655003786087036, KL: 76.7115478515625, Loss: 0.014586241915822029, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9213/20000], Bound: 0.4261731207370758, Entropy: 141.63780212402344, Temp: 2.6551308631896973, KL: 86.49699401855469, Loss: 0.015203099697828293, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9214/20000], Bound: 0.4243508577346802, Entropy: 139.90435791015625, Temp: 2.655294895172119, KL: 84.5550537109375, Loss: 0.017806624993681908, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9215/20000], Bound: 0.3909513056278229, Entropy: 141.2391357421875, Temp: 2.655461072921753, KL: 74.06636047363281, Loss: 0.01872306503355503, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9216/20000], Bound: 0.3754798471927643, Entropy: 142.42431640625, Temp: 2.655596971511841, KL: 71.3631591796875, Loss: 0.015398797579109669, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9217/20000], Bound: 0.38317185640335083, Entropy: 139.58349609375, Temp: 2.6557347774505615, KL: 71.79959106445312, Loss: 0.018738532438874245, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9218/20000], Bound: 0.3956383168697357, Entropy: 141.73670959472656, Temp: 2.655839443206787, KL: 76.43524169921875, Loss: 0.01685352250933647, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9219/20000], Bound: 0.3792980909347534, Entropy: 139.74322509765625, Temp: 2.655944347381592, KL: 71.37350463867188, Loss: 0.017441799864172935, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9220/20000], Bound: 0.398423969745636, Entropy: 140.87843322753906, Temp: 2.6560325622558594, KL: 77.41558837890625, Loss: 0.01655571348965168, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9221/20000], Bound: 0.3851269483566284, Entropy: 141.2429656982422, Temp: 2.656128168106079, KL: 74.05764770507812, Loss: 0.01555594615638256, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9222/20000], Bound: 0.3615497648715973, Entropy: 140.10487365722656, Temp: 2.656233787536621, KL: 67.47441101074219, Loss: 0.015306933782994747, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9223/20000], Bound: 0.39576297998428345, Entropy: 139.87864685058594, Temp: 2.6563360691070557, KL: 75.71168518066406, Loss: 0.01828954927623272, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9224/20000], Bound: 0.3760107457637787, Entropy: 141.6275177001953, Temp: 2.6564226150512695, KL: 70.2630615234375, Loss: 0.01776285655796528, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9225/20000], Bound: 0.3993958830833435, Entropy: 140.12049865722656, Temp: 2.6564877033233643, KL: 77.5694580078125, Loss: 0.01681165210902691, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9226/20000], Bound: 0.3886265158653259, Entropy: 142.26817321777344, Temp: 2.656560182571411, KL: 73.78497314453125, Loss: 0.017986509948968887, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9227/20000], Bound: 0.39280128479003906, Entropy: 142.13812255859375, Temp: 2.656618595123291, KL: 76.66786193847656, Loss: 0.014856049790978432, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9228/20000], Bound: 0.37136444449424744, Entropy: 141.9982147216797, Temp: 2.656703472137451, KL: 70.323974609375, Loss: 0.015158545225858688, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9229/20000], Bound: 0.40360045433044434, Entropy: 142.26878356933594, Temp: 2.6567952632904053, KL: 79.16519165039062, Loss: 0.0161599088460207, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9230/20000], Bound: 0.39224231243133545, Entropy: 139.42373657226562, Temp: 2.6569018363952637, KL: 74.583251953125, Loss: 0.018474072217941284, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9231/20000], Bound: 0.41436558961868286, Entropy: 140.0446319580078, Temp: 2.656987428665161, KL: 81.26681518554688, Loss: 0.018286479637026787, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9232/20000], Bound: 0.3551372289657593, Entropy: 140.54380798339844, Temp: 2.6570706367492676, KL: 63.97724914550781, Loss: 0.018529484048485756, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9233/20000], Bound: 0.3880043029785156, Entropy: 142.2234344482422, Temp: 2.6571099758148193, KL: 73.90843200683594, Loss: 0.0174185112118721, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9234/20000], Bound: 0.38122689723968506, Entropy: 141.66064453125, Temp: 2.657144546508789, KL: 72.46159362792969, Loss: 0.016449181362986565, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9235/20000], Bound: 0.3825032114982605, Entropy: 140.7501220703125, Temp: 2.657181978225708, KL: 73.52423095703125, Loss: 0.015142532996833324, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9236/20000], Bound: 0.3947618901729584, Entropy: 139.89447021484375, Temp: 2.657238006591797, KL: 77.6673583984375, Loss: 0.014064486138522625, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9237/20000], Bound: 0.39007189869880676, Entropy: 139.6005859375, Temp: 2.6573314666748047, KL: 76.05416870117188, Loss: 0.01451681088656187, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9238/20000], Bound: 0.416934609413147, Entropy: 141.04833984375, Temp: 2.6574501991271973, KL: 82.57138061523438, Loss: 0.01730223558843136, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9239/20000], Bound: 0.3936728537082672, Entropy: 141.77346801757812, Temp: 2.657576084136963, KL: 76.08976745605469, Loss: 0.016434762626886368, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9240/20000], Bound: 0.41063573956489563, Entropy: 139.5875701904297, Temp: 2.6577038764953613, KL: 80.39445495605469, Loss: 0.017818354070186615, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9241/20000], Bound: 0.359241783618927, Entropy: 141.3886260986328, Temp: 2.657827854156494, KL: 66.79222106933594, Loss: 0.015389503911137581, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9242/20000], Bound: 0.37993165850639343, Entropy: 139.27210998535156, Temp: 2.6579442024230957, KL: 70.21209716796875, Loss: 0.019987007603049278, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9243/20000], Bound: 0.3799196183681488, Entropy: 141.71051025390625, Temp: 2.6580123901367188, KL: 72.17852783203125, Loss: 0.0162819791585207, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9244/20000], Bound: 0.39265143871307373, Entropy: 138.56800842285156, Temp: 2.658080816268921, KL: 75.9017333984375, Loss: 0.016230342909693718, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9245/20000], Bound: 0.3902323246002197, Entropy: 140.415771484375, Temp: 2.658158540725708, KL: 75.375244140625, Loss: 0.01589079760015011, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9246/20000], Bound: 0.3849180340766907, Entropy: 143.59046936035156, Temp: 2.6582467555999756, KL: 71.54862976074219, Loss: 0.020182574167847633, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9247/20000], Bound: 0.3761155903339386, Entropy: 139.2595672607422, Temp: 2.658290386199951, KL: 72.61468505859375, Loss: 0.013412033207714558, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9248/20000], Bound: 0.3950284421443939, Entropy: 141.7925567626953, Temp: 2.6583683490753174, KL: 77.27447509765625, Loss: 0.014963367022573948, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9249/20000], Bound: 0.3447774648666382, Entropy: 140.9694366455078, Temp: 2.658470869064331, KL: 62.11470031738281, Loss: 0.016670046374201775, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9250/20000], Bound: 0.37576791644096375, Entropy: 141.68431091308594, Temp: 2.6585428714752197, KL: 72.55825805664062, Loss: 0.013333768583834171, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9251/20000], Bound: 0.39304277300834656, Entropy: 140.72222900390625, Temp: 2.658647298812866, KL: 74.35464477539062, Loss: 0.01936132088303566, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9252/20000], Bound: 0.37294963002204895, Entropy: 141.90724182128906, Temp: 2.6587204933166504, KL: 70.46548461914062, Loss: 0.01575900800526142, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9253/20000], Bound: 0.3804361820220947, Entropy: 139.311767578125, Temp: 2.658795118331909, KL: 71.6051025390625, Loss: 0.017647145316004753, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9254/20000], Bound: 0.3780560791492462, Entropy: 139.5244598388672, Temp: 2.658853769302368, KL: 72.66436767578125, Loss: 0.014369623735547066, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9255/20000], Bound: 0.3801345229148865, Entropy: 141.14413452148438, Temp: 2.6589348316192627, KL: 70.20855712890625, Loss: 0.02011127956211567, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9256/20000], Bound: 0.3749047517776489, Entropy: 141.68209838867188, Temp: 2.6589696407318115, KL: 70.00051879882812, Loss: 0.017683997750282288, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9257/20000], Bound: 0.37873926758766174, Entropy: 140.71888732910156, Temp: 2.6589879989624023, KL: 72.239990234375, Loss: 0.015537657774984837, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9258/20000], Bound: 0.3810794949531555, Entropy: 142.42176818847656, Temp: 2.659019708633423, KL: 72.74131774902344, Loss: 0.01586093381047249, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9259/20000], Bound: 0.4130958020687103, Entropy: 140.8193359375, Temp: 2.6590611934661865, KL: 81.48361206054688, Loss: 0.01717955805361271, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9260/20000], Bound: 0.3983594477176666, Entropy: 138.73812866210938, Temp: 2.65911602973938, KL: 77.93159484863281, Loss: 0.015581882558763027, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9261/20000], Bound: 0.383415549993515, Entropy: 141.3826904296875, Temp: 2.6591925621032715, KL: 70.99343872070312, Loss: 0.020416684448719025, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9262/20000], Bound: 0.3790511190891266, Entropy: 142.48680114746094, Temp: 2.659221649169922, KL: 72.460693359375, Loss: 0.015293335542082787, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9263/20000], Bound: 0.3899971842765808, Entropy: 140.98440551757812, Temp: 2.6592659950256348, KL: 75.65602111816406, Loss: 0.01524504367262125, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9264/20000], Bound: 0.4132745862007141, Entropy: 139.159423828125, Temp: 2.6593313217163086, KL: 83.27935791015625, Loss: 0.013907825574278831, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9265/20000], Bound: 0.3805365562438965, Entropy: 139.82640075683594, Temp: 2.6594460010528564, KL: 71.90016174316406, Loss: 0.01715247891843319, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9266/20000], Bound: 0.4121187925338745, Entropy: 138.97711181640625, Temp: 2.659546136856079, KL: 81.97120666503906, Loss: 0.01571391150355339, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9267/20000], Bound: 0.36855989694595337, Entropy: 142.2074432373047, Temp: 2.659670352935791, KL: 69.48655700683594, Loss: 0.015265128575265408, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9268/20000], Bound: 0.39657413959503174, Entropy: 139.1058349609375, Temp: 2.6597938537597656, KL: 77.7242431640625, Loss: 0.014988633804023266, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9269/20000], Bound: 0.40482771396636963, Entropy: 140.48362731933594, Temp: 2.659937620162964, KL: 78.90434265136719, Loss: 0.017372215166687965, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9270/20000], Bound: 0.3871135115623474, Entropy: 141.70640563964844, Temp: 2.6600770950317383, KL: 73.4697265625, Loss: 0.017784154042601585, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9271/20000], Bound: 0.3742276132106781, Entropy: 141.3562469482422, Temp: 2.660196304321289, KL: 70.09541320800781, Loss: 0.017152704298496246, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9272/20000], Bound: 0.3678652048110962, Entropy: 141.50648498535156, Temp: 2.6602964401245117, KL: 69.38249206542969, Loss: 0.015097014605998993, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9273/20000], Bound: 0.40274932980537415, Entropy: 141.032470703125, Temp: 2.6603996753692627, KL: 78.75166320800781, Loss: 0.01650041528046131, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9274/20000], Bound: 0.3891516327857971, Entropy: 141.78729248046875, Temp: 2.6605114936828613, KL: 73.89047241210938, Loss: 0.01811245083808899, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9275/20000], Bound: 0.419612318277359, Entropy: 140.2732391357422, Temp: 2.6606030464172363, KL: 84.57215881347656, Loss: 0.01511034369468689, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9276/20000], Bound: 0.3915994465351105, Entropy: 142.86898803710938, Temp: 2.6607308387756348, KL: 75.7652587890625, Loss: 0.015934862196445465, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9277/20000], Bound: 0.3660537004470825, Entropy: 142.36614990234375, Temp: 2.6608643531799316, KL: 68.10659790039062, Loss: 0.01653829962015152, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9278/20000], Bound: 0.3973621428012848, Entropy: 141.53286743164062, Temp: 2.6609792709350586, KL: 76.10337829589844, Loss: 0.018483808264136314, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9279/20000], Bound: 0.3954331874847412, Entropy: 141.3339080810547, Temp: 2.661074638366699, KL: 75.27253723144531, Loss: 0.01897781901061535, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9280/20000], Bound: 0.39041653275489807, Entropy: 141.5559539794922, Temp: 2.6611452102661133, KL: 75.30892944335938, Loss: 0.016146738082170486, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9281/20000], Bound: 0.39193618297576904, Entropy: 141.9225311279297, Temp: 2.6612236499786377, KL: 75.42164611816406, Loss: 0.016770659014582634, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9282/20000], Bound: 0.39708954095840454, Entropy: 141.8506622314453, Temp: 2.6613028049468994, KL: 76.17996215820312, Loss: 0.018191948533058167, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9283/20000], Bound: 0.38949212431907654, Entropy: 140.9547882080078, Temp: 2.6613693237304688, KL: 74.53971862792969, Loss: 0.017087150365114212, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9284/20000], Bound: 0.42767268419265747, Entropy: 141.78245544433594, Temp: 2.6614320278167725, KL: 86.14350891113281, Loss: 0.016817612573504448, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9285/20000], Bound: 0.4053387939929962, Entropy: 141.44102478027344, Temp: 2.6615195274353027, KL: 78.54190063476562, Loss: 0.01835632510483265, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9286/20000], Bound: 0.36570048332214355, Entropy: 141.9446258544922, Temp: 2.6615967750549316, KL: 66.34632873535156, Loss: 0.019664090126752853, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9287/20000], Bound: 0.3701300323009491, Entropy: 143.2228240966797, Temp: 2.661623239517212, KL: 67.70423889160156, Loss: 0.019466770812869072, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9288/20000], Bound: 0.3916429281234741, Entropy: 140.45237731933594, Temp: 2.661609172821045, KL: 75.21238708496094, Loss: 0.017006319016218185, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9289/20000], Bound: 0.3865813612937927, Entropy: 138.91085815429688, Temp: 2.6616017818450928, KL: 74.1962890625, Loss: 0.01614278368651867, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9290/20000], Bound: 0.3973568081855774, Entropy: 142.11119079589844, Temp: 2.661607503890991, KL: 77.82701110839844, Loss: 0.015248925425112247, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9291/20000], Bound: 0.3582519590854645, Entropy: 141.32089233398438, Temp: 2.661642551422119, KL: 65.28338623046875, Loss: 0.01773681677877903, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9292/20000], Bound: 0.3951357901096344, Entropy: 139.48240661621094, Temp: 2.661649227142334, KL: 74.5889892578125, Loss: 0.020102867856621742, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9293/20000], Bound: 0.38702523708343506, Entropy: 139.94378662109375, Temp: 2.6616263389587402, KL: 74.31825256347656, Loss: 0.016156228259205818, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9294/20000], Bound: 0.41970252990722656, Entropy: 141.5922393798828, Temp: 2.661618232727051, KL: 79.36207580566406, Loss: 0.0249620508402586, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9295/20000], Bound: 0.3982173502445221, Entropy: 137.4911651611328, Temp: 2.6615421772003174, KL: 78.32551574707031, Loss: 0.014789115637540817, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9296/20000], Bound: 0.38834476470947266, Entropy: 140.14710998535156, Temp: 2.6615095138549805, KL: 72.85401916503906, Loss: 0.019627058878540993, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9297/20000], Bound: 0.3952191472053528, Entropy: 140.69065856933594, Temp: 2.661452531814575, KL: 75.0821533203125, Loss: 0.019220704212784767, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9298/20000], Bound: 0.39010941982269287, Entropy: 139.9512176513672, Temp: 2.6613829135894775, KL: 74.98548889160156, Loss: 0.0165882408618927, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9299/20000], Bound: 0.3879643976688385, Entropy: 139.94168090820312, Temp: 2.661329746246338, KL: 73.71537780761719, Loss: 0.017799146473407745, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9300/20000], Bound: 0.3841646611690521, Entropy: 141.3396759033203, Temp: 2.661275625228882, KL: 73.68891906738281, Loss: 0.01577620394527912, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9301/20000], Bound: 0.38293662667274475, Entropy: 140.8375701904297, Temp: 2.6612422466278076, KL: 73.26679992675781, Loss: 0.015901701524853706, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9302/20000], Bound: 0.39301565289497375, Entropy: 140.80169677734375, Temp: 2.661224842071533, KL: 75.57232666015625, Loss: 0.017081815749406815, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9303/20000], Bound: 0.3899230659008026, Entropy: 140.24974060058594, Temp: 2.661214828491211, KL: 75.27951049804688, Loss: 0.015931950882077217, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9304/20000], Bound: 0.3840750455856323, Entropy: 142.59271240234375, Temp: 2.6612229347229004, KL: 71.46923828125, Loss: 0.01989740878343582, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9305/20000], Bound: 0.40377476811408997, Entropy: 140.21177673339844, Temp: 2.661196708679199, KL: 78.64553833007812, Loss: 0.017281923443078995, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9306/20000], Bound: 0.3727433383464813, Entropy: 140.9342803955078, Temp: 2.6611831188201904, KL: 69.79988098144531, Loss: 0.016921577975153923, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9307/20000], Bound: 0.3785122334957123, Entropy: 141.13357543945312, Temp: 2.661165237426758, KL: 70.35121154785156, Loss: 0.018984729424118996, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9308/20000], Bound: 0.3818228542804718, Entropy: 142.23777770996094, Temp: 2.6611225605010986, KL: 72.43328857421875, Loss: 0.01686246506869793, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9309/20000], Bound: 0.38620510697364807, Entropy: 139.86880493164062, Temp: 2.6610851287841797, KL: 73.82643127441406, Loss: 0.016627375036478043, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9310/20000], Bound: 0.3859128952026367, Entropy: 139.0492401123047, Temp: 2.661057949066162, KL: 75.05354309082031, Loss: 0.014162048697471619, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9311/20000], Bound: 0.3749111294746399, Entropy: 140.54942321777344, Temp: 2.661068916320801, KL: 70.56721496582031, Loss: 0.016640426591038704, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9312/20000], Bound: 0.3985042870044708, Entropy: 141.37037658691406, Temp: 2.6610779762268066, KL: 78.30758666992188, Loss: 0.01497697550803423, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9313/20000], Bound: 0.3831700384616852, Entropy: 141.2335662841797, Temp: 2.6611201763153076, KL: 72.761962890625, Loss: 0.01697584055364132, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9314/20000], Bound: 0.37532147765159607, Entropy: 139.570068359375, Temp: 2.661158561706543, KL: 70.97589111328125, Loss: 0.016093654558062553, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9315/20000], Bound: 0.40718454122543335, Entropy: 140.3597869873047, Temp: 2.661198854446411, KL: 80.45640563964844, Loss: 0.015792978927493095, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9316/20000], Bound: 0.3778865337371826, Entropy: 138.25433349609375, Temp: 2.6612648963928223, KL: 70.47508239746094, Loss: 0.018415428698062897, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9317/20000], Bound: 0.3699130117893219, Entropy: 141.2932586669922, Temp: 2.661304473876953, KL: 68.05982971191406, Loss: 0.018680648878216743, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9318/20000], Bound: 0.39444389939308167, Entropy: 141.20559692382812, Temp: 2.661311388015747, KL: 74.50837707519531, Loss: 0.01986919902265072, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9319/20000], Bound: 0.37785208225250244, Entropy: 137.65951538085938, Temp: 2.6612908840179443, KL: 70.55979919433594, Loss: 0.018237929791212082, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9320/20000], Bound: 0.3908661901950836, Entropy: 143.06781005859375, Temp: 2.661254405975342, KL: 75.39884948730469, Loss: 0.016225753352046013, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9321/20000], Bound: 0.4172176420688629, Entropy: 139.432373046875, Temp: 2.661235809326172, KL: 83.653564453125, Loss: 0.015473809093236923, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9322/20000], Bound: 0.39247632026672363, Entropy: 139.40576171875, Temp: 2.661259174346924, KL: 75.13841247558594, Loss: 0.017600327730178833, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9323/20000], Bound: 0.4081767797470093, Entropy: 140.57179260253906, Temp: 2.6612792015075684, KL: 80.46351623535156, Loss: 0.016339173540472984, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9324/20000], Bound: 0.37430140376091003, Entropy: 142.53294372558594, Temp: 2.6613214015960693, KL: 70.46818542480469, Loss: 0.0165016558021307, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9325/20000], Bound: 0.4328484833240509, Entropy: 136.99024963378906, Temp: 2.6613593101501465, KL: 87.12550354003906, Loss: 0.017987718805670738, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9326/20000], Bound: 0.3953666388988495, Entropy: 140.65682983398438, Temp: 2.661414623260498, KL: 75.9241943359375, Loss: 0.017719902098178864, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9327/20000], Bound: 0.39228489995002747, Entropy: 140.3686065673828, Temp: 2.661463975906372, KL: 76.37120056152344, Loss: 0.015180971473455429, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9328/20000], Bound: 0.37756872177124023, Entropy: 140.19261169433594, Temp: 2.6615357398986816, KL: 71.34910583496094, Loss: 0.016604522243142128, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9329/20000], Bound: 0.38617515563964844, Entropy: 140.2503662109375, Temp: 2.6616015434265137, KL: 74.34642028808594, Loss: 0.01563914678990841, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9330/20000], Bound: 0.40540602803230286, Entropy: 138.49053955078125, Temp: 2.6616790294647217, KL: 79.47654724121094, Loss: 0.016639942303299904, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9331/20000], Bound: 0.3922961950302124, Entropy: 142.00421142578125, Temp: 2.6617674827575684, KL: 75.03028869628906, Loss: 0.017709216102957726, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9332/20000], Bound: 0.37310099601745605, Entropy: 142.22970581054688, Temp: 2.661844491958618, KL: 67.65513610839844, Loss: 0.021147310733795166, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9333/20000], Bound: 0.40925586223602295, Entropy: 139.59178161621094, Temp: 2.661857843399048, KL: 81.27638244628906, Loss: 0.015427091158926487, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9334/20000], Bound: 0.38815897703170776, Entropy: 138.25448608398438, Temp: 2.661904811859131, KL: 74.75914001464844, Loss: 0.015950292348861694, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9335/20000], Bound: 0.383798748254776, Entropy: 141.82080078125, Temp: 2.6619627475738525, KL: 72.13682556152344, Loss: 0.018499333411455154, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9336/20000], Bound: 0.387741357088089, Entropy: 142.38916015625, Temp: 2.6619975566864014, KL: 74.91642761230469, Loss: 0.015427498146891594, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9337/20000], Bound: 0.38792991638183594, Entropy: 141.2699737548828, Temp: 2.662050724029541, KL: 75.24287414550781, Loss: 0.014917942695319653, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9338/20000], Bound: 0.37056243419647217, Entropy: 139.38218688964844, Temp: 2.662126064300537, KL: 70.34895324707031, Loss: 0.01473372895270586, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9339/20000], Bound: 0.38417112827301025, Entropy: 142.0585479736328, Temp: 2.662213087081909, KL: 72.94918823242188, Loss: 0.017178216949105263, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9340/20000], Bound: 0.3600618243217468, Entropy: 141.58511352539062, Temp: 2.66228985786438, KL: 66.94749450683594, Loss: 0.015565912239253521, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9341/20000], Bound: 0.3770875632762909, Entropy: 138.59786987304688, Temp: 2.662360906600952, KL: 70.15547180175781, Loss: 0.01859458163380623, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9342/20000], Bound: 0.37411442399024963, Entropy: 141.2843780517578, Temp: 2.6624019145965576, KL: 69.94889831542969, Loss: 0.01738627813756466, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9343/20000], Bound: 0.41681987047195435, Entropy: 141.89956665039062, Temp: 2.662428140640259, KL: 83.0113525390625, Loss: 0.016467217355966568, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9344/20000], Bound: 0.36815565824508667, Entropy: 141.36575317382812, Temp: 2.662479877471924, KL: 69.58369445800781, Loss: 0.014893114566802979, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9345/20000], Bound: 0.40862905979156494, Entropy: 140.84744262695312, Temp: 2.662541627883911, KL: 79.46839904785156, Loss: 0.018476760014891624, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9346/20000], Bound: 0.39097312092781067, Entropy: 140.836181640625, Temp: 2.6625964641571045, KL: 75.54763793945312, Loss: 0.016018467023968697, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9347/20000], Bound: 0.4026833772659302, Entropy: 140.3333740234375, Temp: 2.6626625061035156, KL: 79.18528747558594, Loss: 0.015673520043492317, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9348/20000], Bound: 0.3848203122615814, Entropy: 139.92787170410156, Temp: 2.662749767303467, KL: 73.06040954589844, Loss: 0.017327412962913513, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9349/20000], Bound: 0.40798383951187134, Entropy: 142.04403686523438, Temp: 2.662825345993042, KL: 75.73141479492188, Loss: 0.025133172050118446, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9350/20000], Bound: 0.36643093824386597, Entropy: 139.74232482910156, Temp: 2.662813663482666, KL: 69.349853515625, Loss: 0.014419985935091972, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9351/20000], Bound: 0.3730325698852539, Entropy: 141.73207092285156, Temp: 2.66282320022583, KL: 70.70721435546875, Loss: 0.015386688522994518, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9352/20000], Bound: 0.39714139699935913, Entropy: 141.2685089111328, Temp: 2.662844181060791, KL: 75.529296875, Loss: 0.01945730671286583, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9353/20000], Bound: 0.3693760335445404, Entropy: 139.3125, Temp: 2.6628429889678955, KL: 68.23182678222656, Loss: 0.018083732575178146, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9354/20000], Bound: 0.40603578090667725, Entropy: 141.74276733398438, Temp: 2.6628196239471436, KL: 79.9854736328125, Loss: 0.016050033271312714, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9355/20000], Bound: 0.3963751196861267, Entropy: 141.5229034423828, Temp: 2.6628243923187256, KL: 76.95896911621094, Loss: 0.016348326578736305, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9356/20000], Bound: 0.3730035126209259, Entropy: 141.65647888183594, Temp: 2.6628448963165283, KL: 70.19119262695312, Loss: 0.016340306028723717, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9357/20000], Bound: 0.37330111861228943, Entropy: 141.0012664794922, Temp: 2.6628644466400146, KL: 68.80282592773438, Loss: 0.019106604158878326, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9358/20000], Bound: 0.38243672251701355, Entropy: 140.92166137695312, Temp: 2.6628499031066895, KL: 71.06497192382812, Loss: 0.01978037878870964, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9359/20000], Bound: 0.3693825304508209, Entropy: 142.1435089111328, Temp: 2.6628031730651855, KL: 68.01976013183594, Loss: 0.018485095351934433, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9360/20000], Bound: 0.398027241230011, Entropy: 141.0038604736328, Temp: 2.662734031677246, KL: 78.59347534179688, Loss: 0.01419361587613821, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9361/20000], Bound: 0.4185355305671692, Entropy: 140.97300720214844, Temp: 2.6627144813537598, KL: 84.00804138183594, Loss: 0.015579228289425373, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9362/20000], Bound: 0.4083489775657654, Entropy: 140.94320678710938, Temp: 2.662736177444458, KL: 79.417724609375, Loss: 0.01841611973941326, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9363/20000], Bound: 0.3821609318256378, Entropy: 140.82183837890625, Temp: 2.662755250930786, KL: 73.54647827148438, Loss: 0.01497043576091528, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9364/20000], Bound: 0.3803039491176605, Entropy: 140.43850708007812, Temp: 2.6627955436706543, KL: 73.12026977539062, Loss: 0.014765873551368713, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9365/20000], Bound: 0.37118837237358093, Entropy: 141.99502563476562, Temp: 2.6628565788269043, KL: 69.87379455566406, Loss: 0.015966564416885376, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9366/20000], Bound: 0.40944769978523254, Entropy: 140.24925231933594, Temp: 2.6629161834716797, KL: 77.31617736816406, Loss: 0.02298339083790779, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9367/20000], Bound: 0.4007168114185333, Entropy: 141.70538330078125, Temp: 2.6629161834716797, KL: 76.69252014160156, Loss: 0.019259946420788765, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9368/20000], Bound: 0.4018910527229309, Entropy: 139.2660369873047, Temp: 2.662900447845459, KL: 79.41470336914062, Loss: 0.01480304915457964, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9369/20000], Bound: 0.4007527232170105, Entropy: 140.67880249023438, Temp: 2.662924289703369, KL: 78.6630859375, Loss: 0.015580026432871819, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9370/20000], Bound: 0.3993087410926819, Entropy: 139.6508331298828, Temp: 2.662973403930664, KL: 78.76583862304688, Loss: 0.014584200456738472, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9371/20000], Bound: 0.37514781951904297, Entropy: 140.88514709472656, Temp: 2.6630563735961914, KL: 72.03079223632812, Loss: 0.014037040993571281, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9372/20000], Bound: 0.3915070593357086, Entropy: 142.3291015625, Temp: 2.663161277770996, KL: 74.76443481445312, Loss: 0.017787855118513107, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9373/20000], Bound: 0.3896724283695221, Entropy: 139.16629028320312, Temp: 2.6632511615753174, KL: 73.21702575683594, Loss: 0.01968717947602272, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9374/20000], Bound: 0.3867073655128479, Entropy: 140.15045166015625, Temp: 2.663304328918457, KL: 72.32481384277344, Loss: 0.019741570577025414, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9375/20000], Bound: 0.37981733679771423, Entropy: 142.1297607421875, Temp: 2.6633212566375732, KL: 71.00521850585938, Loss: 0.018478823825716972, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9376/20000], Bound: 0.39109429717063904, Entropy: 140.88795471191406, Temp: 2.6633167266845703, KL: 74.18037414550781, Loss: 0.01865907572209835, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9377/20000], Bound: 0.4073377251625061, Entropy: 140.73902893066406, Temp: 2.663297653198242, KL: 79.83436584472656, Loss: 0.017070546746253967, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9378/20000], Bound: 0.3943285048007965, Entropy: 140.19830322265625, Temp: 2.663294792175293, KL: 75.90597534179688, Loss: 0.017199311405420303, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9379/20000], Bound: 0.409498393535614, Entropy: 138.6099853515625, Temp: 2.663297176361084, KL: 79.42547607421875, Loss: 0.01905536651611328, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9380/20000], Bound: 0.4004046618938446, Entropy: 139.49398803710938, Temp: 2.6632916927337646, KL: 76.53701782226562, Loss: 0.019381670281291008, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9381/20000], Bound: 0.3779796361923218, Entropy: 140.63204956054688, Temp: 2.663269519805908, KL: 71.33908081054688, Loss: 0.016860250383615494, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9382/20000], Bound: 0.39853018522262573, Entropy: 142.1629638671875, Temp: 2.663247585296631, KL: 77.15074157714844, Loss: 0.017187071964144707, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9383/20000], Bound: 0.37954914569854736, Entropy: 140.66909790039062, Temp: 2.6632354259490967, KL: 69.90339660644531, Loss: 0.02040184661746025, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9384/20000], Bound: 0.39228054881095886, Entropy: 143.03369140625, Temp: 2.6631810665130615, KL: 76.72463989257812, Loss: 0.014533022418618202, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9385/20000], Bound: 0.38942843675613403, Entropy: 139.24143981933594, Temp: 2.6631672382354736, KL: 75.08836364746094, Loss: 0.01603943482041359, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9386/20000], Bound: 0.3716062009334564, Entropy: 140.46571350097656, Temp: 2.6631696224212646, KL: 70.1221923828125, Loss: 0.015725988894701004, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9387/20000], Bound: 0.37190794944763184, Entropy: 140.00914001464844, Temp: 2.663179397583008, KL: 70.58474731445312, Loss: 0.015018770471215248, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9388/20000], Bound: 0.3922712802886963, Entropy: 140.2353515625, Temp: 2.6632044315338135, KL: 75.99365234375, Loss: 0.015900565311312675, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9389/20000], Bound: 0.38686925172805786, Entropy: 139.67881774902344, Temp: 2.663245677947998, KL: 74.55661010742188, Loss: 0.015639401972293854, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9390/20000], Bound: 0.41014376282691956, Entropy: 139.9143524169922, Temp: 2.6633012294769287, KL: 79.58309936523438, Loss: 0.019123779609799385, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9391/20000], Bound: 0.3983306586742401, Entropy: 139.40191650390625, Temp: 2.6633431911468506, KL: 77.3408203125, Loss: 0.016720427200198174, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9392/20000], Bound: 0.3992401659488678, Entropy: 140.77528381347656, Temp: 2.663393974304199, KL: 75.62107849121094, Loss: 0.02045445330440998, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9393/20000], Bound: 0.38040173053741455, Entropy: 141.58880615234375, Temp: 2.6634089946746826, KL: 71.467041015625, Loss: 0.01792842335999012, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9394/20000], Bound: 0.40597471594810486, Entropy: 140.8950958251953, Temp: 2.6634092330932617, KL: 80.03517150878906, Loss: 0.01592898741364479, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9395/20000], Bound: 0.3972899317741394, Entropy: 139.18948364257812, Temp: 2.6634364128112793, KL: 76.74990844726562, Loss: 0.01725362055003643, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9396/20000], Bound: 0.38312283158302307, Entropy: 140.9331512451172, Temp: 2.6634669303894043, KL: 71.68992614746094, Loss: 0.018984341993927956, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9397/20000], Bound: 0.3748921751976013, Entropy: 140.5766143798828, Temp: 2.6634697914123535, KL: 70.48316955566406, Loss: 0.016809236258268356, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9398/20000], Bound: 0.3909359872341156, Entropy: 139.11277770996094, Temp: 2.6634695529937744, KL: 74.93878173828125, Loss: 0.01714986562728882, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9399/20000], Bound: 0.4022575318813324, Entropy: 138.6171112060547, Temp: 2.6634719371795654, KL: 79.00106811523438, Loss: 0.01579049974679947, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9400/20000], Bound: 0.3993288576602936, Entropy: 141.5521240234375, Temp: 2.6635005474090576, KL: 76.93728637695312, Loss: 0.018033871427178383, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9401/20000], Bound: 0.3894350826740265, Entropy: 140.35940551757812, Temp: 2.6635241508483887, KL: 74.70751953125, Loss: 0.01676156558096409, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9402/20000], Bound: 0.39632025361061096, Entropy: 142.39111328125, Temp: 2.6635518074035645, KL: 76.86770629882812, Loss: 0.016496798023581505, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9403/20000], Bound: 0.4208301603794098, Entropy: 139.6902313232422, Temp: 2.663590908050537, KL: 82.76116943359375, Loss: 0.019245179370045662, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9404/20000], Bound: 0.37246590852737427, Entropy: 140.07135009765625, Temp: 2.6636240482330322, KL: 70.1192626953125, Loss: 0.016194716095924377, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9405/20000], Bound: 0.38177937269210815, Entropy: 141.70252990722656, Temp: 2.663656234741211, KL: 73.18093872070312, Loss: 0.015458755195140839, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9406/20000], Bound: 0.35484835505485535, Entropy: 141.29476928710938, Temp: 2.6637022495269775, KL: 64.9366455078125, Loss: 0.01662418060004711, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9407/20000], Bound: 0.4033282697200775, Entropy: 139.433837890625, Temp: 2.663729667663574, KL: 79.81221008300781, Loss: 0.014868767000734806, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9408/20000], Bound: 0.4073508381843567, Entropy: 141.57669067382812, Temp: 2.663792133331299, KL: 81.20501708984375, Loss: 0.014510488137602806, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9409/20000], Bound: 0.3810669779777527, Entropy: 140.88333129882812, Temp: 2.663893222808838, KL: 73.19007873535156, Loss: 0.015058311633765697, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9410/20000], Bound: 0.4062192142009735, Entropy: 139.837158203125, Temp: 2.6640055179595947, KL: 79.11711120605469, Loss: 0.017795901745557785, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9411/20000], Bound: 0.38777172565460205, Entropy: 139.8896026611328, Temp: 2.664111852645874, KL: 73.09449768066406, Loss: 0.01888478919863701, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9412/20000], Bound: 0.4064338207244873, Entropy: 139.9413299560547, Temp: 2.664187431335449, KL: 78.91050720214844, Loss: 0.01830601505935192, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9413/20000], Bound: 0.37856414914131165, Entropy: 139.70668029785156, Temp: 2.664254903793335, KL: 72.11956787109375, Loss: 0.015719281509518623, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9414/20000], Bound: 0.39719510078430176, Entropy: 139.19358825683594, Temp: 2.664327383041382, KL: 77.78285217285156, Loss: 0.015271520242094994, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9415/20000], Bound: 0.3765544891357422, Entropy: 140.64706420898438, Temp: 2.664421796798706, KL: 72.18531799316406, Loss: 0.014515751041471958, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9416/20000], Bound: 0.3984866142272949, Entropy: 141.205078125, Temp: 2.664531946182251, KL: 76.91636657714844, Loss: 0.01761566661298275, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9417/20000], Bound: 0.3959999978542328, Entropy: 141.77029418945312, Temp: 2.66463303565979, KL: 77.09999084472656, Loss: 0.015894873067736626, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9418/20000], Bound: 0.40013357996940613, Entropy: 137.98388671875, Temp: 2.6647450923919678, KL: 80.476806640625, Loss: 0.011852067895233631, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9419/20000], Bound: 0.39175304770469666, Entropy: 139.81967163085938, Temp: 2.6649169921875, KL: 75.77520751953125, Loss: 0.016043027862906456, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9420/20000], Bound: 0.39177608489990234, Entropy: 141.15792846679688, Temp: 2.665088176727295, KL: 76.22483825683594, Loss: 0.015213844366371632, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9421/20000], Bound: 0.37865516543388367, Entropy: 138.08042907714844, Temp: 2.6652684211730957, KL: 71.01741027832031, Loss: 0.01784544438123703, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9422/20000], Bound: 0.3711434304714203, Entropy: 141.3868865966797, Temp: 2.665416717529297, KL: 70.30943298339844, Loss: 0.015148015692830086, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9423/20000], Bound: 0.3833719789981842, Entropy: 141.55184936523438, Temp: 2.6655642986297607, KL: 72.95292663574219, Loss: 0.016768064349889755, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9424/20000], Bound: 0.3868285119533539, Entropy: 141.29920959472656, Temp: 2.6656992435455322, KL: 74.77815246582031, Loss: 0.01522600743919611, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9425/20000], Bound: 0.39970430731773376, Entropy: 139.3074188232422, Temp: 2.6658432483673096, KL: 76.52435302734375, Loss: 0.019039971753954887, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9426/20000], Bound: 0.3581313490867615, Entropy: 142.12950134277344, Temp: 2.665958881378174, KL: 65.65457153320312, Loss: 0.01700938493013382, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9427/20000], Bound: 0.3702768087387085, Entropy: 141.65777587890625, Temp: 2.666045665740967, KL: 69.11091613769531, Loss: 0.01693972758948803, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9428/20000], Bound: 0.3876848518848419, Entropy: 142.27220153808594, Temp: 2.6661150455474854, KL: 75.87556457519531, Loss: 0.013639329001307487, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9429/20000], Bound: 0.36718738079071045, Entropy: 141.69178771972656, Temp: 2.66621994972229, KL: 67.54547119140625, Loss: 0.01823556236922741, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9430/20000], Bound: 0.4048715829849243, Entropy: 139.7340545654297, Temp: 2.666287899017334, KL: 80.09794616699219, Loss: 0.015224933624267578, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9431/20000], Bound: 0.3944109380245209, Entropy: 141.11009216308594, Temp: 2.6663832664489746, KL: 75.56759643554688, Loss: 0.017909573391079903, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9432/20000], Bound: 0.39346960186958313, Entropy: 138.96368408203125, Temp: 2.6664648056030273, KL: 75.39265441894531, Loss: 0.01771978661417961, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9433/20000], Bound: 0.35353779792785645, Entropy: 142.7920379638672, Temp: 2.6665351390838623, KL: 65.71711730957031, Loss: 0.014500077813863754, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9434/20000], Bound: 0.3745568096637726, Entropy: 140.76463317871094, Temp: 2.6666088104248047, KL: 70.3404541015625, Loss: 0.016924571245908737, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9435/20000], Bound: 0.4129772484302521, Entropy: 140.76873779296875, Temp: 2.6666693687438965, KL: 81.1634521484375, Loss: 0.01779717020690441, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9436/20000], Bound: 0.41973957419395447, Entropy: 140.45089721679688, Temp: 2.6667330265045166, KL: 82.61672973632812, Loss: 0.018925055861473083, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9437/20000], Bound: 0.38897547125816345, Entropy: 141.5521697998047, Temp: 2.66679048538208, KL: 74.43339538574219, Loss: 0.01705562137067318, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9438/20000], Bound: 0.4073636829853058, Entropy: 142.9032745361328, Temp: 2.666844129562378, KL: 79.66014099121094, Loss: 0.01744978502392769, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9439/20000], Bound: 0.3785274624824524, Entropy: 142.62815856933594, Temp: 2.6669020652770996, KL: 70.82284545898438, Loss: 0.018155407160520554, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9440/20000], Bound: 0.3756278157234192, Entropy: 142.13900756835938, Temp: 2.666935920715332, KL: 69.88470458984375, Loss: 0.01835589110851288, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9441/20000], Bound: 0.3776799440383911, Entropy: 142.01954650878906, Temp: 2.6669442653656006, KL: 70.72979736328125, Loss: 0.017873967066407204, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9442/20000], Bound: 0.4525335729122162, Entropy: 140.05421447753906, Temp: 2.6669363975524902, KL: 94.44697570800781, Loss: 0.01601007953286171, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9443/20000], Bound: 0.40910935401916504, Entropy: 142.46310424804688, Temp: 2.666985034942627, KL: 79.43276977539062, Loss: 0.01885976269841194, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9444/20000], Bound: 0.38161858916282654, Entropy: 140.1748046875, Temp: 2.667022466659546, KL: 72.23274230957031, Loss: 0.01718185842037201, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9445/20000], Bound: 0.37320294976234436, Entropy: 143.0281524658203, Temp: 2.6670520305633545, KL: 69.80946350097656, Loss: 0.017199458554387093, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9446/20000], Bound: 0.37678518891334534, Entropy: 141.29014587402344, Temp: 2.6670684814453125, KL: 71.39888000488281, Loss: 0.016139646992087364, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9447/20000], Bound: 0.3944116532802582, Entropy: 140.354736328125, Temp: 2.667088270187378, KL: 74.76104736328125, Loss: 0.01942867785692215, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9448/20000], Bound: 0.367549866437912, Entropy: 143.07791137695312, Temp: 2.667083501815796, KL: 68.27818298339844, Loss: 0.017060767859220505, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9449/20000], Bound: 0.38128846883773804, Entropy: 141.19418334960938, Temp: 2.667067050933838, KL: 71.94807434082031, Loss: 0.017537392675876617, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9450/20000], Bound: 0.3923860490322113, Entropy: 140.78134155273438, Temp: 2.6670432090759277, KL: 74.7132568359375, Loss: 0.01840301789343357, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9451/20000], Bound: 0.38010963797569275, Entropy: 137.38966369628906, Temp: 2.6670100688934326, KL: 72.00563049316406, Loss: 0.01679203286767006, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9452/20000], Bound: 0.3916502594947815, Entropy: 142.0392608642578, Temp: 2.6669797897338867, KL: 76.39901733398438, Loss: 0.014837849885225296, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9453/20000], Bound: 0.3791911005973816, Entropy: 140.2154541015625, Temp: 2.666982889175415, KL: 71.40029907226562, Loss: 0.017431097105145454, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9454/20000], Bound: 0.37515735626220703, Entropy: 142.67147827148438, Temp: 2.6669766902923584, KL: 71.66258239746094, Loss: 0.014770790934562683, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9455/20000], Bound: 0.3559141159057617, Entropy: 143.47210693359375, Temp: 2.666991710662842, KL: 65.77284240722656, Loss: 0.01563706435263157, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9456/20000], Bound: 0.40191349387168884, Entropy: 141.68072509765625, Temp: 2.6670029163360596, KL: 77.83415222167969, Loss: 0.01782466098666191, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9457/20000], Bound: 0.3538564145565033, Entropy: 140.86387634277344, Temp: 2.6670145988464355, KL: 64.65087890625, Loss: 0.01666872389614582, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9458/20000], Bound: 0.3764101266860962, Entropy: 141.5836944580078, Temp: 2.667008638381958, KL: 68.818603515625, Loss: 0.02077503688633442, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9459/20000], Bound: 0.38259246945381165, Entropy: 144.2054901123047, Temp: 2.666952133178711, KL: 72.70292663574219, Loss: 0.0168270505964756, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9460/20000], Bound: 0.38648757338523865, Entropy: 142.77413940429688, Temp: 2.6669020652770996, KL: 75.1444091796875, Loss: 0.01436559297144413, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9461/20000], Bound: 0.4002446234226227, Entropy: 140.23663330078125, Temp: 2.6668896675109863, KL: 78.06402587890625, Loss: 0.016463540494441986, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9462/20000], Bound: 0.3969733417034149, Entropy: 143.369384765625, Temp: 2.6668949127197266, KL: 75.54548645019531, Loss: 0.019370999187231064, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9463/20000], Bound: 0.4016207158565521, Entropy: 140.46478271484375, Temp: 2.666879415512085, KL: 76.65631103515625, Loss: 0.019868548959493637, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9464/20000], Bound: 0.39265012741088867, Entropy: 142.49392700195312, Temp: 2.666842222213745, KL: 76.13368225097656, Loss: 0.015883224084973335, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9465/20000], Bound: 0.37627264857292175, Entropy: 140.35902404785156, Temp: 2.6668269634246826, KL: 71.77308654785156, Loss: 0.015160530805587769, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9466/20000], Bound: 0.4064556658267975, Entropy: 140.24855041503906, Temp: 2.66683030128479, KL: 80.46170043945312, Loss: 0.015436874702572823, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9467/20000], Bound: 0.38109731674194336, Entropy: 142.66709899902344, Temp: 2.6668660640716553, KL: 71.95547485351562, Loss: 0.017418431118130684, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9468/20000], Bound: 0.38612017035484314, Entropy: 140.88299560546875, Temp: 2.6668906211853027, KL: 74.54176330566406, Loss: 0.015295212157070637, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9469/20000], Bound: 0.387902170419693, Entropy: 140.23231506347656, Temp: 2.6669342517852783, KL: 74.16139221191406, Loss: 0.016980454325675964, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9470/20000], Bound: 0.3801746964454651, Entropy: 141.68923950195312, Temp: 2.666975975036621, KL: 72.16441345214844, Loss: 0.016529209911823273, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9471/20000], Bound: 0.38957929611206055, Entropy: 141.67529296875, Temp: 2.66701602935791, KL: 75.94342041015625, Loss: 0.014557154849171638, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9472/20000], Bound: 0.35420650243759155, Entropy: 141.18272399902344, Temp: 2.6670846939086914, KL: 64.85992431640625, Loss: 0.01645948365330696, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9473/20000], Bound: 0.3534638583660126, Entropy: 141.68296813964844, Temp: 2.667133092880249, KL: 62.25120544433594, Loss: 0.020964011549949646, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9474/20000], Bound: 0.3744654059410095, Entropy: 139.10411071777344, Temp: 2.667107105255127, KL: 71.52928161621094, Loss: 0.014651205390691757, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9475/20000], Bound: 0.3897317945957184, Entropy: 138.146240234375, Temp: 2.6671054363250732, KL: 74.90750122070312, Loss: 0.01658358797430992, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9476/20000], Bound: 0.39322641491889954, Entropy: 141.8550567626953, Temp: 2.667112112045288, KL: 76.42593383789062, Loss: 0.01565505377948284, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9477/20000], Bound: 0.3867419362068176, Entropy: 144.66822814941406, Temp: 2.667139768600464, KL: 74.42362976074219, Loss: 0.015857871621847153, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9478/20000], Bound: 0.4219188690185547, Entropy: 141.20272827148438, Temp: 2.667179822921753, KL: 84.2156982421875, Loss: 0.017182154580950737, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9479/20000], Bound: 0.40727439522743225, Entropy: 141.1190185546875, Temp: 2.6672379970550537, KL: 79.79302978515625, Loss: 0.017154671251773834, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9480/20000], Bound: 0.3757324814796448, Entropy: 140.70123291015625, Temp: 2.6673028469085693, KL: 71.17086791992188, Loss: 0.01600402593612671, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9481/20000], Bound: 0.3911570608615875, Entropy: 141.9013671875, Temp: 2.667367458343506, KL: 75.60214233398438, Loss: 0.016065049916505814, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9482/20000], Bound: 0.39912065863609314, Entropy: 142.36801147460938, Temp: 2.667440891265869, KL: 76.2037353515625, Loss: 0.01933198794722557, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9483/20000], Bound: 0.3686683773994446, Entropy: 142.08468627929688, Temp: 2.6674883365631104, KL: 67.44338989257812, Loss: 0.019222311675548553, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9484/20000], Bound: 0.40504321455955505, Entropy: 141.19412231445312, Temp: 2.6674933433532715, KL: 78.28431701660156, Loss: 0.018734002485871315, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9485/20000], Bound: 0.4064105451107025, Entropy: 140.44781494140625, Temp: 2.667490243911743, KL: 79.57041931152344, Loss: 0.01708959974348545, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9486/20000], Bound: 0.3977186381816864, Entropy: 141.41830444335938, Temp: 2.6675004959106445, KL: 77.03004455566406, Loss: 0.01700643263757229, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9487/20000], Bound: 0.34353771805763245, Entropy: 141.6781005859375, Temp: 2.66751766204834, KL: 62.96080017089844, Loss: 0.01451137661933899, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9488/20000], Bound: 0.40360233187675476, Entropy: 142.41537475585938, Temp: 2.667536973953247, KL: 77.69557189941406, Loss: 0.019032131880521774, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9489/20000], Bound: 0.3658613860607147, Entropy: 142.45785522460938, Temp: 2.6675422191619873, KL: 67.79119873046875, Loss: 0.01708318665623665, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9490/20000], Bound: 0.37322625517845154, Entropy: 141.79017639160156, Temp: 2.6675333976745605, KL: 67.19964599609375, Loss: 0.022107809782028198, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9491/20000], Bound: 0.38480448722839355, Entropy: 142.0361328125, Temp: 2.6674556732177734, KL: 74.16517639160156, Loss: 0.015290976502001286, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9492/20000], Bound: 0.3883674442768097, Entropy: 142.51719665527344, Temp: 2.6674060821533203, KL: 74.45114135742188, Loss: 0.01669587753713131, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9493/20000], Bound: 0.4205842614173889, Entropy: 143.1225128173828, Temp: 2.6673672199249268, KL: 83.28414916992188, Loss: 0.018164735287427902, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9494/20000], Bound: 0.36468201875686646, Entropy: 141.90023803710938, Temp: 2.667341947555542, KL: 67.47750854492188, Loss: 0.0170462466776371, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9495/20000], Bound: 0.38628089427948, Entropy: 142.19705200195312, Temp: 2.6673054695129395, KL: 73.60743713378906, Loss: 0.017138337716460228, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9496/20000], Bound: 0.40763387084007263, Entropy: 142.2785186767578, Temp: 2.667271614074707, KL: 81.08592224121094, Loss: 0.01493340265005827, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9497/20000], Bound: 0.3683246076107025, Entropy: 141.40025329589844, Temp: 2.667280673980713, KL: 67.61666870117188, Loss: 0.018713461235165596, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9498/20000], Bound: 0.3679273724555969, Entropy: 142.43093872070312, Temp: 2.667257308959961, KL: 69.87034606933594, Loss: 0.014277766458690166, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9499/20000], Bound: 0.38381412625312805, Entropy: 142.88002014160156, Temp: 2.6672580242156982, KL: 71.65605163574219, Loss: 0.019454728811979294, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9500/20000], Bound: 0.37174922227859497, Entropy: 143.0840301513672, Temp: 2.6672284603118896, KL: 68.98037719726562, Loss: 0.017978915944695473, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9501/20000], Bound: 0.3897729814052582, Entropy: 142.40696716308594, Temp: 2.6671810150146484, KL: 73.06788635253906, Loss: 0.020055506378412247, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9502/20000], Bound: 0.37675318121910095, Entropy: 141.1986083984375, Temp: 2.667104959487915, KL: 72.2188720703125, Loss: 0.014585516415536404, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9503/20000], Bound: 0.4070793688297272, Entropy: 141.24046325683594, Temp: 2.667060375213623, KL: 80.13491821289062, Loss: 0.016402235254645348, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9504/20000], Bound: 0.39767736196517944, Entropy: 140.95860290527344, Temp: 2.6670420169830322, KL: 77.84991455078125, Loss: 0.015441901050508022, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9505/20000], Bound: 0.384756863117218, Entropy: 143.3605499267578, Temp: 2.667052745819092, KL: 72.78886413574219, Loss: 0.017841309309005737, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9506/20000], Bound: 0.3880468010902405, Entropy: 141.52313232421875, Temp: 2.6670520305633545, KL: 73.79147338867188, Loss: 0.017754025757312775, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9507/20000], Bound: 0.39136001467704773, Entropy: 138.8436279296875, Temp: 2.667044162750244, KL: 76.92326354980469, Loss: 0.013696386478841305, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9508/20000], Bound: 0.37776023149490356, Entropy: 140.65496826171875, Temp: 2.667081594467163, KL: 72.00212097167969, Loss: 0.015533091500401497, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9509/20000], Bound: 0.3753184378147125, Entropy: 142.99478149414062, Temp: 2.667128086090088, KL: 71.052734375, Loss: 0.016001855954527855, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9510/20000], Bound: 0.39533230662345886, Entropy: 143.4486846923828, Temp: 2.6671760082244873, KL: 74.66494750976562, Loss: 0.020117439329624176, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9511/20000], Bound: 0.40523332357406616, Entropy: 139.7023468017578, Temp: 2.6671884059906006, KL: 79.5189208984375, Loss: 0.01652299240231514, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9512/20000], Bound: 0.39613473415374756, Entropy: 140.4848175048828, Temp: 2.6672186851501465, KL: 74.17070007324219, Loss: 0.02148747816681862, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9513/20000], Bound: 0.39456605911254883, Entropy: 142.39935302734375, Temp: 2.6671993732452393, KL: 75.50505065917969, Loss: 0.01812008209526539, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9514/20000], Bound: 0.38376712799072266, Entropy: 141.579833984375, Temp: 2.6671745777130127, KL: 71.28816223144531, Loss: 0.02011820301413536, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9515/20000], Bound: 0.3975614011287689, Entropy: 141.4196319580078, Temp: 2.6671142578125, KL: 75.89094543457031, Loss: 0.01905088685452938, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9516/20000], Bound: 0.3685452938079834, Entropy: 143.90675354003906, Temp: 2.6670432090759277, KL: 69.0185546875, Loss: 0.016200629994273186, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9517/20000], Bound: 0.3652838170528412, Entropy: 141.1665802001953, Temp: 2.666978597640991, KL: 68.951416015625, Loss: 0.014598052948713303, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9518/20000], Bound: 0.40643584728240967, Entropy: 143.40869140625, Temp: 2.6669368743896484, KL: 78.90399169921875, Loss: 0.01834738254547119, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9519/20000], Bound: 0.374729186296463, Entropy: 140.9220733642578, Temp: 2.6668972969055176, KL: 69.40646362304688, Loss: 0.018770450726151466, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9520/20000], Bound: 0.4000124931335449, Entropy: 142.8502655029297, Temp: 2.6668336391448975, KL: 77.13519287109375, Loss: 0.018075348809361458, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9521/20000], Bound: 0.3707457482814789, Entropy: 142.8006134033203, Temp: 2.6667733192443848, KL: 68.65731811523438, Loss: 0.018046025186777115, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9522/20000], Bound: 0.38024958968162537, Entropy: 142.22999572753906, Temp: 2.6666970252990723, KL: 71.76669311523438, Loss: 0.01731279119849205, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9523/20000], Bound: 0.3826254904270172, Entropy: 141.81521606445312, Temp: 2.666621685028076, KL: 71.68878173828125, Loss: 0.018743447959423065, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9524/20000], Bound: 0.36295369267463684, Entropy: 140.52561950683594, Temp: 2.6665310859680176, KL: 67.8638916015625, Loss: 0.015403449535369873, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9525/20000], Bound: 0.4174051582813263, Entropy: 139.0382843017578, Temp: 2.666455030441284, KL: 81.96719360351562, Loss: 0.018805690109729767, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9526/20000], Bound: 0.3836125135421753, Entropy: 140.95864868164062, Temp: 2.666386604309082, KL: 72.54769897460938, Loss: 0.01766609214246273, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9527/20000], Bound: 0.3683200776576996, Entropy: 142.23995971679688, Temp: 2.666316032409668, KL: 69.38978576660156, Loss: 0.015378684736788273, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9528/20000], Bound: 0.4282855689525604, Entropy: 143.92926025390625, Temp: 2.666261672973633, KL: 87.68669128417969, Loss: 0.014338281005620956, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9529/20000], Bound: 0.3724224865436554, Entropy: 142.3131103515625, Temp: 2.666273355484009, KL: 68.76699829101562, Loss: 0.018730733543634415, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9530/20000], Bound: 0.360117644071579, Entropy: 140.82403564453125, Temp: 2.6662545204162598, KL: 66.19453430175781, Loss: 0.017040103673934937, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9531/20000], Bound: 0.3947153389453888, Entropy: 141.9328155517578, Temp: 2.6662209033966064, KL: 76.54837036132812, Loss: 0.01623661071062088, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9532/20000], Bound: 0.4006992280483246, Entropy: 142.11117553710938, Temp: 2.6662065982818604, KL: 78.10598754882812, Loss: 0.016630591824650764, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9533/20000], Bound: 0.37495332956314087, Entropy: 141.52328491210938, Temp: 2.6662087440490723, KL: 70.45240783691406, Loss: 0.016923587769269943, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9534/20000], Bound: 0.387712687253952, Entropy: 143.01455688476562, Temp: 2.6662051677703857, KL: 75.0919189453125, Loss: 0.015125093050301075, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9535/20000], Bound: 0.40147829055786133, Entropy: 141.80238342285156, Temp: 2.666226625442505, KL: 77.15206909179688, Loss: 0.018853450194001198, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9536/20000], Bound: 0.38590148091316223, Entropy: 140.72006225585938, Temp: 2.6662349700927734, KL: 72.91844177246094, Loss: 0.018213804811239243, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9537/20000], Bound: 0.36332908272743225, Entropy: 142.03829956054688, Temp: 2.6662285327911377, KL: 67.5643310546875, Loss: 0.016160501167178154, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9538/20000], Bound: 0.3881371021270752, Entropy: 141.10714721679688, Temp: 2.6662189960479736, KL: 72.79425048828125, Loss: 0.01966586336493492, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9539/20000], Bound: 0.37851962447166443, Entropy: 139.89901733398438, Temp: 2.666180372238159, KL: 71.75779724121094, Loss: 0.016391724348068237, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9540/20000], Bound: 0.37119215726852417, Entropy: 140.73216247558594, Temp: 2.666149139404297, KL: 70.77540588378906, Loss: 0.014306783676147461, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9541/20000], Bound: 0.38202574849128723, Entropy: 140.36474609375, Temp: 2.666145086288452, KL: 73.09132385253906, Loss: 0.015784284099936485, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9542/20000], Bound: 0.3547845482826233, Entropy: 142.47674560546875, Temp: 2.666154623031616, KL: 64.13468933105469, Loss: 0.0181135181337595, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9543/20000], Bound: 0.4257831275463104, Entropy: 140.49327087402344, Temp: 2.6661298274993896, KL: 83.60183715820312, Loss: 0.02054797299206257, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9544/20000], Bound: 0.4064149558544159, Entropy: 141.33169555664062, Temp: 2.6660923957824707, KL: 80.05177307128906, Loss: 0.016174523159861565, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9545/20000], Bound: 0.3641420006752014, Entropy: 142.2552032470703, Temp: 2.666083335876465, KL: 66.51841735839844, Loss: 0.018549710512161255, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9546/20000], Bound: 0.36810481548309326, Entropy: 141.5260772705078, Temp: 2.6660425662994385, KL: 69.88951110839844, Loss: 0.01432478241622448, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9547/20000], Bound: 0.38416340947151184, Entropy: 139.69081115722656, Temp: 2.6660280227661133, KL: 73.69129943847656, Loss: 0.01581723801791668, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9548/20000], Bound: 0.4058432877063751, Entropy: 139.46743774414062, Temp: 2.6660287380218506, KL: 80.66778564453125, Loss: 0.014697804115712643, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9549/20000], Bound: 0.3969135284423828, Entropy: 139.4015655517578, Temp: 2.6660714149475098, KL: 77.20851135253906, Loss: 0.016211526468396187, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9550/20000], Bound: 0.4053506851196289, Entropy: 140.440673828125, Temp: 2.6661272048950195, KL: 79.49407958984375, Loss: 0.016623955219984055, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9551/20000], Bound: 0.3963506817817688, Entropy: 140.5320587158203, Temp: 2.666195869445801, KL: 76.83140563964844, Loss: 0.016608674079179764, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9552/20000], Bound: 0.35223737359046936, Entropy: 141.68612670898438, Temp: 2.6662702560424805, KL: 65.06558227539062, Loss: 0.01504400186240673, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9553/20000], Bound: 0.39150702953338623, Entropy: 141.26739501953125, Temp: 2.6663401126861572, KL: 75.26339721679688, Loss: 0.016882039606571198, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9554/20000], Bound: 0.3790771961212158, Entropy: 144.41758728027344, Temp: 2.6664087772369385, KL: 71.02334594726562, Loss: 0.01807156763970852, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9555/20000], Bound: 0.38544365763664246, Entropy: 141.6707763671875, Temp: 2.6664540767669678, KL: 73.12847900390625, Loss: 0.01757265441119671, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9556/20000], Bound: 0.3695797026157379, Entropy: 142.8783721923828, Temp: 2.6664881706237793, KL: 70.0087890625, Loss: 0.014888832345604897, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9557/20000], Bound: 0.3928614854812622, Entropy: 141.0819549560547, Temp: 2.6665351390838623, KL: 76.25306701660156, Loss: 0.015772495418787003, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9558/20000], Bound: 0.41039544343948364, Entropy: 141.0741729736328, Temp: 2.666597366333008, KL: 81.30503845214844, Loss: 0.016070552170276642, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9559/20000], Bound: 0.38290420174598694, Entropy: 140.88092041015625, Temp: 2.6666817665100098, KL: 73.50241088867188, Loss: 0.01549450121819973, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9560/20000], Bound: 0.3691118061542511, Entropy: 142.9917755126953, Temp: 2.6667749881744385, KL: 68.70283508300781, Loss: 0.017091281712055206, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9561/20000], Bound: 0.395189493894577, Entropy: 143.5416717529297, Temp: 2.6668472290039062, KL: 75.52113342285156, Loss: 0.01843051239848137, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9562/20000], Bound: 0.39480337500572205, Entropy: 140.82264709472656, Temp: 2.6669018268585205, KL: 76.95594787597656, Loss: 0.015527951531112194, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9563/20000], Bound: 0.3644227087497711, Entropy: 140.9221954345703, Temp: 2.666975498199463, KL: 66.88919067382812, Loss: 0.01800934597849846, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9564/20000], Bound: 0.4100472331047058, Entropy: 140.5116424560547, Temp: 2.667015790939331, KL: 80.47886657714844, Loss: 0.017427613958716393, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9565/20000], Bound: 0.39691057801246643, Entropy: 144.19131469726562, Temp: 2.6670637130737305, KL: 76.5030517578125, Loss: 0.017542660236358643, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9566/20000], Bound: 0.3715130090713501, Entropy: 140.50238037109375, Temp: 2.6671082973480225, KL: 68.40829467773438, Loss: 0.01892445608973503, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9567/20000], Bound: 0.3907235860824585, Entropy: 140.877685546875, Temp: 2.6671159267425537, KL: 74.95611572265625, Loss: 0.017035899683833122, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9568/20000], Bound: 0.3914377987384796, Entropy: 141.39239501953125, Temp: 2.667126178741455, KL: 75.577392578125, Loss: 0.01626306213438511, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9569/20000], Bound: 0.4027836322784424, Entropy: 142.75811767578125, Temp: 2.6671485900878906, KL: 79.33671569824219, Loss: 0.015494611114263535, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9570/20000], Bound: 0.39520132541656494, Entropy: 142.77989196777344, Temp: 2.667198896408081, KL: 74.63829040527344, Loss: 0.02009533904492855, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9571/20000], Bound: 0.3936040997505188, Entropy: 141.75399780273438, Temp: 2.667213201522827, KL: 75.35276794433594, Loss: 0.017875751480460167, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9572/20000], Bound: 0.3654005825519562, Entropy: 140.8782958984375, Temp: 2.6672215461730957, KL: 66.04693603515625, Loss: 0.020106690004467964, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9573/20000], Bound: 0.36055710911750793, Entropy: 143.45494079589844, Temp: 2.667177677154541, KL: 67.77365112304688, Loss: 0.014317664317786694, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9574/20000], Bound: 0.3831092417240143, Entropy: 142.3671875, Temp: 2.6671550273895264, KL: 73.58811950683594, Loss: 0.015449551865458488, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9575/20000], Bound: 0.3796686828136444, Entropy: 141.8459930419922, Temp: 2.6671526432037354, KL: 73.10322570800781, Loss: 0.014497753232717514, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9576/20000], Bound: 0.4037066102027893, Entropy: 143.98179626464844, Temp: 2.66717791557312, KL: 79.61781311035156, Loss: 0.015483417548239231, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9577/20000], Bound: 0.38223791122436523, Entropy: 141.9661102294922, Temp: 2.6672310829162598, KL: 71.70286560058594, Loss: 0.018512295559048653, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9578/20000], Bound: 0.39075255393981934, Entropy: 142.29429626464844, Temp: 2.6672587394714355, KL: 75.20887756347656, Loss: 0.01657932437956333, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9579/20000], Bound: 0.3960082232952118, Entropy: 141.81222534179688, Temp: 2.667292594909668, KL: 75.83984375, Loss: 0.018289266154170036, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9580/20000], Bound: 0.3792850077152252, Entropy: 141.42977905273438, Temp: 2.6673150062561035, KL: 73.56472778320312, Loss: 0.01342732086777687, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9581/20000], Bound: 0.4018518626689911, Entropy: 141.63905334472656, Temp: 2.667375087738037, KL: 77.51042175292969, Loss: 0.018400875851511955, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9582/20000], Bound: 0.41362321376800537, Entropy: 142.6561737060547, Temp: 2.6674234867095947, KL: 81.50357055664062, Loss: 0.017533954232931137, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9583/20000], Bound: 0.3863168954849243, Entropy: 142.3290557861328, Temp: 2.6674795150756836, KL: 73.75968933105469, Loss: 0.01687413454055786, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9584/20000], Bound: 0.3863890767097473, Entropy: 140.85848999023438, Temp: 2.667532444000244, KL: 74.82818603515625, Loss: 0.01491116639226675, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9585/20000], Bound: 0.41420355439186096, Entropy: 140.80836486816406, Temp: 2.6676063537597656, KL: 83.59202575683594, Loss: 0.013950798660516739, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9586/20000], Bound: 0.37106186151504517, Entropy: 140.3529510498047, Temp: 2.667728900909424, KL: 70.83872985839844, Loss: 0.014133508317172527, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9587/20000], Bound: 0.36728572845458984, Entropy: 142.4288787841797, Temp: 2.66786527633667, KL: 68.57548522949219, Loss: 0.016369933262467384, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9588/20000], Bound: 0.3790709972381592, Entropy: 141.94558715820312, Temp: 2.6679837703704834, KL: 71.06707763671875, Loss: 0.017999574542045593, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9589/20000], Bound: 0.39637696743011475, Entropy: 138.33343505859375, Temp: 2.668074369430542, KL: 77.04681396484375, Loss: 0.016238529235124588, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9590/20000], Bound: 0.379612535238266, Entropy: 142.59434509277344, Temp: 2.6681723594665527, KL: 72.01069641113281, Loss: 0.016524827107787132, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9591/20000], Bound: 0.4115864038467407, Entropy: 142.9268035888672, Temp: 2.6682629585266113, KL: 82.37318420410156, Loss: 0.01476037222892046, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9592/20000], Bound: 0.3565329909324646, Entropy: 142.67965698242188, Temp: 2.668388843536377, KL: 65.16705322265625, Loss: 0.01710622012615204, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9593/20000], Bound: 0.37028491497039795, Entropy: 143.39187622070312, Temp: 2.6684818267822266, KL: 70.06991577148438, Loss: 0.01516750268638134, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9594/20000], Bound: 0.3551538586616516, Entropy: 140.6866455078125, Temp: 2.6685779094696045, KL: 65.59893798828125, Loss: 0.015579311177134514, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9595/20000], Bound: 0.38346555829048157, Entropy: 141.2387237548828, Temp: 2.6686623096466064, KL: 73.10565185546875, Loss: 0.016561267897486687, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9596/20000], Bound: 0.3815903961658478, Entropy: 140.5215606689453, Temp: 2.6687424182891846, KL: 71.94483947753906, Loss: 0.017721446231007576, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9597/20000], Bound: 0.38105645775794983, Entropy: 140.69163513183594, Temp: 2.6688032150268555, KL: 71.66633605957031, Loss: 0.017955131828784943, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9598/20000], Bound: 0.41368624567985535, Entropy: 140.69447326660156, Temp: 2.6688432693481445, KL: 81.21763610839844, Loss: 0.018120912835001945, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9599/20000], Bound: 0.3788283169269562, Entropy: 141.36668395996094, Temp: 2.668884515762329, KL: 69.62783813476562, Loss: 0.020572815090417862, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9600/20000], Bound: 0.40325987339019775, Entropy: 141.58993530273438, Temp: 2.6688730716705322, KL: 78.67951965332031, Loss: 0.017010433599352837, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9601/20000], Bound: 0.3731227219104767, Entropy: 141.3208770751953, Temp: 2.668874740600586, KL: 69.89564514160156, Loss: 0.017010459676384926, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9602/20000], Bound: 0.38467937707901, Entropy: 141.64312744140625, Temp: 2.668867826461792, KL: 73.40234375, Loss: 0.016666052863001823, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9603/20000], Bound: 0.4139152467250824, Entropy: 141.85618591308594, Temp: 2.668865203857422, KL: 83.08697509765625, Loss: 0.01474885642528534, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9604/20000], Bound: 0.4042825400829315, Entropy: 141.3801727294922, Temp: 2.6689088344573975, KL: 78.60868835449219, Loss: 0.01771482080221176, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9605/20000], Bound: 0.40299707651138306, Entropy: 141.2763671875, Temp: 2.668952226638794, KL: 80.63380432128906, Loss: 0.013203437440097332, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9606/20000], Bound: 0.3754638135433197, Entropy: 139.79017639160156, Temp: 2.669048309326172, KL: 71.18898010253906, Loss: 0.015841782093048096, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9607/20000], Bound: 0.38672274351119995, Entropy: 142.2845916748047, Temp: 2.669142723083496, KL: 75.45697021484375, Loss: 0.013931255787611008, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9608/20000], Bound: 0.3950118124485016, Entropy: 140.15902709960938, Temp: 2.6692659854888916, KL: 77.97329711914062, Loss: 0.013761754147708416, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9609/20000], Bound: 0.3792659342288971, Entropy: 141.13453674316406, Temp: 2.6694223880767822, KL: 71.53155517578125, Loss: 0.017246808856725693, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9610/20000], Bound: 0.38721928000450134, Entropy: 140.3095703125, Temp: 2.669555902481079, KL: 73.28860473632812, Loss: 0.018267402425408363, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9611/20000], Bound: 0.3680287003517151, Entropy: 140.8458709716797, Temp: 2.6696617603302, KL: 68.51185607910156, Loss: 0.016897885128855705, Learning Rate: 0.0002906670312209998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9612/20000], Bound: 0.3904028832912445, Entropy: 141.3782196044922, Temp: 2.6697466373443604, KL: 75.20603942871094, Loss: 0.016417356207966805, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9613/20000], Bound: 0.38599374890327454, Entropy: 140.81631469726562, Temp: 2.669832944869995, KL: 73.25285339355469, Loss: 0.01766948588192463, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9614/20000], Bound: 0.41956642270088196, Entropy: 140.90829467773438, Temp: 2.669903039932251, KL: 83.70201110839844, Loss: 0.016827836632728577, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9615/20000], Bound: 0.3917258083820343, Entropy: 142.94845581054688, Temp: 2.6699905395507812, KL: 74.18769836425781, Loss: 0.019051847979426384, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9616/20000], Bound: 0.37345731258392334, Entropy: 140.1078643798828, Temp: 2.6700477600097656, KL: 70.28489685058594, Loss: 0.01647021248936653, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9617/20000], Bound: 0.38958585262298584, Entropy: 139.5424346923828, Temp: 2.670098066329956, KL: 75.40744018554688, Loss: 0.015596519224345684, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9618/20000], Bound: 0.39367038011550903, Entropy: 139.7569580078125, Temp: 2.6701629161834717, KL: 76.46900939941406, Loss: 0.015849851071834564, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9619/20000], Bound: 0.40895119309425354, Entropy: 141.22377014160156, Temp: 2.6702404022216797, KL: 80.75323486328125, Loss: 0.016331033781170845, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9620/20000], Bound: 0.39866510033607483, Entropy: 139.86378479003906, Temp: 2.6703333854675293, KL: 78.55000305175781, Loss: 0.014712984673678875, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9621/20000], Bound: 0.3901962637901306, Entropy: 142.1646270751953, Temp: 2.6704535484313965, KL: 76.0770263671875, Loss: 0.01468036137521267, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9622/20000], Bound: 0.37305644154548645, Entropy: 141.26739501953125, Temp: 2.670592784881592, KL: 69.99685668945312, Loss: 0.01680011861026287, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9623/20000], Bound: 0.3916179835796356, Entropy: 141.0858154296875, Temp: 2.6707117557525635, KL: 73.66867065429688, Loss: 0.019970728084445, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9624/20000], Bound: 0.3962838053703308, Entropy: 139.0741729736328, Temp: 2.6707866191864014, KL: 75.14094543457031, Loss: 0.019782772287726402, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9625/20000], Bound: 0.4027325510978699, Entropy: 139.3584747314453, Temp: 2.6708269119262695, KL: 78.24444580078125, Loss: 0.01755111664533615, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9626/20000], Bound: 0.4083903133869171, Entropy: 139.6231231689453, Temp: 2.6708672046661377, KL: 80.41893005371094, Loss: 0.016648299992084503, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9627/20000], Bound: 0.37472960352897644, Entropy: 140.19126892089844, Temp: 2.6709225177764893, KL: 70.2596435546875, Loss: 0.01720535010099411, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9628/20000], Bound: 0.39313188195228577, Entropy: 140.4716796875, Temp: 2.6709625720977783, KL: 75.82237243652344, Loss: 0.016772370785474777, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9629/20000], Bound: 0.3930625915527344, Entropy: 139.9012908935547, Temp: 2.671005964279175, KL: 75.10307312011719, Loss: 0.018081221729516983, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9630/20000], Bound: 0.4100627899169922, Entropy: 140.2666015625, Temp: 2.6710362434387207, KL: 78.58816528320312, Loss: 0.02101861499249935, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9631/20000], Bound: 0.40210869908332825, Entropy: 140.29173278808594, Temp: 2.6710309982299805, KL: 77.03652954101562, Loss: 0.0194667000323534, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9632/20000], Bound: 0.4111812114715576, Entropy: 139.86740112304688, Temp: 2.671006679534912, KL: 81.92158508300781, Loss: 0.015409211628139019, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9633/20000], Bound: 0.4136402904987335, Entropy: 139.145751953125, Temp: 2.671020746231079, KL: 81.00979614257812, Loss: 0.01850711554288864, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9634/20000], Bound: 0.39923834800720215, Entropy: 140.29043579101562, Temp: 2.6710333824157715, KL: 78.75979614257812, Loss: 0.014645636081695557, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9635/20000], Bound: 0.37017756700515747, Entropy: 139.93312072753906, Temp: 2.6710822582244873, KL: 69.07780456542969, Loss: 0.01699088141322136, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9636/20000], Bound: 0.3776509165763855, Entropy: 140.69981384277344, Temp: 2.6711158752441406, KL: 73.18443298339844, Loss: 0.01329869031906128, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9637/20000], Bound: 0.39603811502456665, Entropy: 140.91566467285156, Temp: 2.6711859703063965, KL: 78.48876953125, Loss: 0.013384143821895123, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9638/20000], Bound: 0.39111217856407166, Entropy: 141.97010803222656, Temp: 2.671299457550049, KL: 76.00114440917969, Loss: 0.015332682989537716, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9639/20000], Bound: 0.41177603602409363, Entropy: 140.7741241455078, Temp: 2.6714253425598145, KL: 81.46871948242188, Loss: 0.016597596928477287, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9640/20000], Bound: 0.3867523968219757, Entropy: 142.1612548828125, Temp: 2.671560287475586, KL: 74.66146850585938, Loss: 0.015461505390703678, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9641/20000], Bound: 0.3945481479167938, Entropy: 139.73257446289062, Temp: 2.671700954437256, KL: 76.12396240234375, Loss: 0.016994213685393333, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9642/20000], Bound: 0.34755396842956543, Entropy: 142.1964874267578, Temp: 2.6718332767486572, KL: 63.0887451171875, Loss: 0.016367800533771515, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9643/20000], Bound: 0.37101465463638306, Entropy: 142.29241943359375, Temp: 2.6719343662261963, KL: 67.7069091796875, Loss: 0.020008517429232597, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9644/20000], Bound: 0.38946354389190674, Entropy: 141.9654083251953, Temp: 2.671977996826172, KL: 74.24429321289062, Loss: 0.0177250187844038, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9645/20000], Bound: 0.38390639424324036, Entropy: 140.54232788085938, Temp: 2.6720104217529297, KL: 73.60997009277344, Loss: 0.015887439250946045, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9646/20000], Bound: 0.3489724397659302, Entropy: 142.1847686767578, Temp: 2.672051429748535, KL: 64.00657653808594, Loss: 0.015383154153823853, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9647/20000], Bound: 0.3748348355293274, Entropy: 140.51063537597656, Temp: 2.672083854675293, KL: 71.58157348632812, Loss: 0.01479787565767765, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9648/20000], Bound: 0.3851505219936371, Entropy: 141.14242553710938, Temp: 2.6721324920654297, KL: 71.90946960449219, Loss: 0.019745569676160812, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9649/20000], Bound: 0.3666004538536072, Entropy: 143.379150390625, Temp: 2.6721415519714355, KL: 66.55863952636719, Loss: 0.019816821441054344, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9650/20000], Bound: 0.4087814390659332, Entropy: 141.243408203125, Temp: 2.6721017360687256, KL: 79.08619689941406, Loss: 0.019375311210751534, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9651/20000], Bound: 0.3926162123680115, Entropy: 140.81626892089844, Temp: 2.6720516681671143, KL: 75.68095397949219, Loss: 0.01676439493894577, Learning Rate: 0.0002906670312209998\n",
      "Epoch [9652/20000], Bound: 0.37695905566215515, Entropy: 141.50730895996094, Temp: 2.672024965286255, KL: 71.13960266113281, Loss: 0.016762737184762955, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9653/20000], Bound: 0.3970499336719513, Entropy: 143.35406494140625, Temp: 2.6719987392425537, KL: 75.17094421386719, Loss: 0.02016051672399044, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9654/20000], Bound: 0.33748355507850647, Entropy: 141.39393615722656, Temp: 2.671952962875366, KL: 60.45832824707031, Loss: 0.016140121966600418, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9655/20000], Bound: 0.4069947302341461, Entropy: 141.2930145263672, Temp: 2.6718969345092773, KL: 79.26057434082031, Loss: 0.01804346591234207, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9656/20000], Bound: 0.3864673972129822, Entropy: 138.45408630371094, Temp: 2.671847105026245, KL: 74.70755004882812, Loss: 0.01522296667098999, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9657/20000], Bound: 0.38461923599243164, Entropy: 140.99053955078125, Temp: 2.6718173027038574, KL: 73.95425415039062, Loss: 0.015627983957529068, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9658/20000], Bound: 0.3861367702484131, Entropy: 140.48118591308594, Temp: 2.6718015670776367, KL: 74.14132690429688, Loss: 0.016102271154522896, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9659/20000], Bound: 0.42347100377082825, Entropy: 139.45150756835938, Temp: 2.671794891357422, KL: 84.38374328613281, Loss: 0.017812706530094147, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9660/20000], Bound: 0.37555208802223206, Entropy: 143.3622283935547, Temp: 2.671799421310425, KL: 70.85639953613281, Loss: 0.016536297276616096, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9661/20000], Bound: 0.3872946798801422, Entropy: 141.47740173339844, Temp: 2.671802282333374, KL: 74.13658142089844, Loss: 0.01674148254096508, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9662/20000], Bound: 0.37328848242759705, Entropy: 139.84011840820312, Temp: 2.6718077659606934, KL: 68.65625, Loss: 0.01944311521947384, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9663/20000], Bound: 0.34511440992355347, Entropy: 140.73880004882812, Temp: 2.671785354614258, KL: 63.169708251953125, Loss: 0.014961580745875835, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9664/20000], Bound: 0.35955655574798584, Entropy: 141.2724151611328, Temp: 2.6717638969421387, KL: 67.13117980957031, Loss: 0.015035630203783512, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9665/20000], Bound: 0.4025877118110657, Entropy: 139.39869689941406, Temp: 2.6717491149902344, KL: 78.76298522949219, Loss: 0.01650932803750038, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9666/20000], Bound: 0.3863772451877594, Entropy: 139.89846801757812, Temp: 2.6717476844787598, KL: 71.84573364257812, Loss: 0.020528621971607208, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9667/20000], Bound: 0.406375527381897, Entropy: 141.92379760742188, Temp: 2.6717159748077393, KL: 80.15322875976562, Loss: 0.016023864969611168, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9668/20000], Bound: 0.38296088576316833, Entropy: 140.90451049804688, Temp: 2.6717052459716797, KL: 73.73031616210938, Loss: 0.015147079713642597, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9669/20000], Bound: 0.4105472266674042, Entropy: 142.83633422851562, Temp: 2.6717097759246826, KL: 80.62643432617188, Loss: 0.017483435571193695, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9670/20000], Bound: 0.39617919921875, Entropy: 139.596435546875, Temp: 2.6717212200164795, KL: 76.70402526855469, Loss: 0.01680804416537285, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9671/20000], Bound: 0.420664519071579, Entropy: 138.66993713378906, Temp: 2.6717376708984375, KL: 82.66455078125, Loss: 0.019418852403759956, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9672/20000], Bound: 0.39021414518356323, Entropy: 141.65245056152344, Temp: 2.671748161315918, KL: 74.96487426757812, Loss: 0.01678488217294216, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9673/20000], Bound: 0.37524721026420593, Entropy: 142.2089080810547, Temp: 2.6717610359191895, KL: 70.4832763671875, Loss: 0.01707097329199314, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9674/20000], Bound: 0.4056324362754822, Entropy: 142.130126953125, Temp: 2.671766757965088, KL: 78.67646789550781, Loss: 0.018371863290667534, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9675/20000], Bound: 0.3790380656719208, Entropy: 141.04086303710938, Temp: 2.671769142150879, KL: 70.6651611328125, Loss: 0.01876598596572876, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9676/20000], Bound: 0.4004027247428894, Entropy: 142.17234802246094, Temp: 2.671752691268921, KL: 78.53948974609375, Loss: 0.01571202278137207, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9677/20000], Bound: 0.3679676651954651, Entropy: 140.54367065429688, Temp: 2.671755313873291, KL: 68.87289428710938, Loss: 0.016207003965973854, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9678/20000], Bound: 0.416498064994812, Entropy: 140.32579040527344, Temp: 2.6717562675476074, KL: 83.058837890625, Loss: 0.01630285009741783, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9679/20000], Bound: 0.3918377161026001, Entropy: 141.8854217529297, Temp: 2.67177677154541, KL: 74.77536010742188, Loss: 0.018029209226369858, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9680/20000], Bound: 0.3971923887729645, Entropy: 140.451416015625, Temp: 2.6717894077301025, KL: 75.48171997070312, Loss: 0.01965586096048355, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9681/20000], Bound: 0.3781355023384094, Entropy: 142.40188598632812, Temp: 2.6717827320098877, KL: 72.26498413085938, Loss: 0.015286574140191078, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9682/20000], Bound: 0.37555399537086487, Entropy: 141.43360900878906, Temp: 2.671787738800049, KL: 71.21501159667969, Loss: 0.01586613617837429, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9683/20000], Bound: 0.4158746302127838, Entropy: 141.68898010253906, Temp: 2.671797037124634, KL: 82.26910400390625, Loss: 0.017426589503884315, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9684/20000], Bound: 0.40537229180336, Entropy: 141.80247497558594, Temp: 2.6718156337738037, KL: 78.35011291503906, Loss: 0.018837541341781616, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9685/20000], Bound: 0.3864125609397888, Entropy: 140.857421875, Temp: 2.671825647354126, KL: 75.55882263183594, Loss: 0.013599870726466179, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9686/20000], Bound: 0.40506231784820557, Entropy: 141.7848358154297, Temp: 2.6718640327453613, KL: 76.47207641601562, Loss: 0.022179050371050835, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9687/20000], Bound: 0.36590471863746643, Entropy: 141.94052124023438, Temp: 2.671863079071045, KL: 68.7750244140625, Loss: 0.01529945619404316, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9688/20000], Bound: 0.3900701403617859, Entropy: 142.8600616455078, Temp: 2.6718671321868896, KL: 73.30316162109375, Loss: 0.01981690153479576, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9689/20000], Bound: 0.364902526140213, Entropy: 141.5791778564453, Temp: 2.6718485355377197, KL: 66.05586242675781, Loss: 0.01985873468220234, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9690/20000], Bound: 0.39132073521614075, Entropy: 142.75125122070312, Temp: 2.6717965602874756, KL: 74.44647216796875, Loss: 0.018361467868089676, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9691/20000], Bound: 0.3627944588661194, Entropy: 141.2682647705078, Temp: 2.6717405319213867, KL: 68.16191101074219, Loss: 0.014805978164076805, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9692/20000], Bound: 0.3908422887325287, Entropy: 142.5677947998047, Temp: 2.6716980934143066, KL: 74.81387329101562, Loss: 0.017410865053534508, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9693/20000], Bound: 0.37731853127479553, Entropy: 142.76263427734375, Temp: 2.671658515930176, KL: 73.03790283203125, Loss: 0.01339989434927702, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9694/20000], Bound: 0.3640539050102234, Entropy: 139.44285583496094, Temp: 2.6716501712799072, KL: 68.54866027832031, Loss: 0.014744586311280727, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9695/20000], Bound: 0.36632776260375977, Entropy: 140.93128967285156, Temp: 2.671651840209961, KL: 68.66644287109375, Loss: 0.015724461525678635, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9696/20000], Bound: 0.36716943979263306, Entropy: 141.6486358642578, Temp: 2.6716551780700684, KL: 68.85508728027344, Loss: 0.01581672392785549, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9697/20000], Bound: 0.3857339918613434, Entropy: 143.08998107910156, Temp: 2.6716597080230713, KL: 73.15234375, Loss: 0.01773276925086975, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9698/20000], Bound: 0.4021453261375427, Entropy: 141.50518798828125, Temp: 2.671657085418701, KL: 77.77836608886719, Loss: 0.018104631453752518, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9699/20000], Bound: 0.37829428911209106, Entropy: 140.4084930419922, Temp: 2.6716525554656982, KL: 71.61415100097656, Loss: 0.016588807106018066, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9700/20000], Bound: 0.4015320837497711, Entropy: 141.1365509033203, Temp: 2.6716485023498535, KL: 76.90306091308594, Loss: 0.019401321187615395, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9701/20000], Bound: 0.37640833854675293, Entropy: 141.92388916015625, Temp: 2.671631097793579, KL: 71.06314086914062, Loss: 0.016606904566287994, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9702/20000], Bound: 0.4083312451839447, Entropy: 138.9397735595703, Temp: 2.671614170074463, KL: 78.85018920898438, Loss: 0.019559098407626152, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9703/20000], Bound: 0.3815097510814667, Entropy: 141.29556274414062, Temp: 2.6715874671936035, KL: 72.64834594726562, Loss: 0.016385983675718307, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9704/20000], Bound: 0.38119032979011536, Entropy: 141.23068237304688, Temp: 2.6715664863586426, KL: 72.25984191894531, Loss: 0.016940375789999962, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9705/20000], Bound: 0.39049726724624634, Entropy: 141.1853485107422, Temp: 2.6715455055236816, KL: 74.1416015625, Loss: 0.01847873255610466, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9706/20000], Bound: 0.38644641637802124, Entropy: 140.35487365722656, Temp: 2.671516180038452, KL: 74.51609802246094, Loss: 0.015566572546958923, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9707/20000], Bound: 0.3600466251373291, Entropy: 142.12387084960938, Temp: 2.671502113342285, KL: 66.1260986328125, Loss: 0.017171286046504974, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9708/20000], Bound: 0.3983364999294281, Entropy: 142.50303649902344, Temp: 2.671475648880005, KL: 78.32917785644531, Loss: 0.014956695958971977, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9709/20000], Bound: 0.4041542708873749, Entropy: 142.04129028320312, Temp: 2.6714751720428467, KL: 78.83999633789062, Loss: 0.017236320301890373, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9710/20000], Bound: 0.429311603307724, Entropy: 140.73037719726562, Temp: 2.671480894088745, KL: 86.8658447265625, Loss: 0.016538050025701523, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9711/20000], Bound: 0.412858784198761, Entropy: 142.3600616455078, Temp: 2.6715102195739746, KL: 78.96501159667969, Loss: 0.021896716207265854, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9712/20000], Bound: 0.3806193172931671, Entropy: 142.30825805664062, Temp: 2.6715073585510254, KL: 73.1741943359375, Loss: 0.01492026261985302, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9713/20000], Bound: 0.4000512659549713, Entropy: 142.40736389160156, Temp: 2.6715199947357178, KL: 77.95732116699219, Loss: 0.01660393923521042, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9714/20000], Bound: 0.35944512486457825, Entropy: 143.0108642578125, Temp: 2.671541452407837, KL: 66.84037780761719, Loss: 0.015519662760198116, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9715/20000], Bound: 0.38480788469314575, Entropy: 141.76466369628906, Temp: 2.6715612411499023, KL: 73.47189331054688, Loss: 0.016630692407488823, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9716/20000], Bound: 0.3708130717277527, Entropy: 142.27023315429688, Temp: 2.671581268310547, KL: 69.14706420898438, Loss: 0.017203375697135925, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9717/20000], Bound: 0.4055669903755188, Entropy: 140.41159057617188, Temp: 2.671590566635132, KL: 78.45610046386719, Loss: 0.018745912238955498, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9718/20000], Bound: 0.3962539732456207, Entropy: 141.21612548828125, Temp: 2.6715927124023438, KL: 76.56062316894531, Loss: 0.01711646467447281, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9719/20000], Bound: 0.389298677444458, Entropy: 138.80795288085938, Temp: 2.671598434448242, KL: 73.42362976074219, Loss: 0.019167354330420494, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9720/20000], Bound: 0.3799026608467102, Entropy: 140.77708435058594, Temp: 2.671586275100708, KL: 71.46168518066406, Loss: 0.01773955672979355, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9721/20000], Bound: 0.38635504245758057, Entropy: 139.72622680664062, Temp: 2.671565532684326, KL: 75.12010192871094, Loss: 0.014386901631951332, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9722/20000], Bound: 0.3699262738227844, Entropy: 139.68362426757812, Temp: 2.67156982421875, KL: 68.43603515625, Loss: 0.01806247979402542, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9723/20000], Bound: 0.4134443998336792, Entropy: 142.21372985839844, Temp: 2.6715567111968994, KL: 81.44227600097656, Loss: 0.01759233884513378, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9724/20000], Bound: 0.38890594244003296, Entropy: 139.93724060058594, Temp: 2.6715524196624756, KL: 72.73220825195312, Loss: 0.020246446132659912, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9725/20000], Bound: 0.38340744376182556, Entropy: 141.07080078125, Temp: 2.6715216636657715, KL: 74.76792907714844, Loss: 0.01344510167837143, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9726/20000], Bound: 0.41219303011894226, Entropy: 140.18487548828125, Temp: 2.6715235710144043, KL: 80.60362243652344, Loss: 0.018453450873494148, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9727/20000], Bound: 0.38252779841423035, Entropy: 141.4151611328125, Temp: 2.671525239944458, KL: 72.39213562011719, Loss: 0.017415381968021393, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9728/20000], Bound: 0.40512263774871826, Entropy: 140.00840759277344, Temp: 2.6715211868286133, KL: 78.04142761230469, Loss: 0.019272707402706146, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9729/20000], Bound: 0.37946218252182007, Entropy: 140.51991271972656, Temp: 2.671506881713867, KL: 73.437744140625, Loss: 0.013803086243569851, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9730/20000], Bound: 0.38626986742019653, Entropy: 141.72055053710938, Temp: 2.671518564224243, KL: 73.96380615234375, Loss: 0.016504209488630295, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9731/20000], Bound: 0.39514219760894775, Entropy: 140.5908660888672, Temp: 2.6715333461761475, KL: 76.94772338867188, Loss: 0.01577814854681492, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9732/20000], Bound: 0.38352686166763306, Entropy: 142.3732452392578, Temp: 2.6715617179870605, KL: 72.5257568359375, Loss: 0.01770663447678089, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9733/20000], Bound: 0.3879983425140381, Entropy: 142.24745178222656, Temp: 2.671579599380493, KL: 73.6983642578125, Loss: 0.017943130806088448, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9734/20000], Bound: 0.36026427149772644, Entropy: 141.89561462402344, Temp: 2.671588182449341, KL: 65.56434631347656, Loss: 0.018337344750761986, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9735/20000], Bound: 0.3940839171409607, Entropy: 142.2893524169922, Temp: 2.671571969985962, KL: 76.77519226074219, Loss: 0.015518592670559883, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9736/20000], Bound: 0.38155093789100647, Entropy: 143.55958557128906, Temp: 2.6715738773345947, KL: 71.8558349609375, Loss: 0.017891347408294678, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9737/20000], Bound: 0.4157579839229584, Entropy: 141.6117706298828, Temp: 2.671565294265747, KL: 84.10557556152344, Loss: 0.013920709490776062, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9738/20000], Bound: 0.3869517743587494, Entropy: 140.75222778320312, Temp: 2.671597957611084, KL: 72.95571899414062, Loss: 0.018762804567813873, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9739/20000], Bound: 0.3660893738269806, Entropy: 140.7084197998047, Temp: 2.671612501144409, KL: 69.67192077636719, Loss: 0.013716349378228188, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9740/20000], Bound: 0.37171313166618347, Entropy: 140.43017578125, Temp: 2.671644926071167, KL: 68.73068237304688, Loss: 0.018462419509887695, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9741/20000], Bound: 0.3886514902114868, Entropy: 141.8104248046875, Temp: 2.671654224395752, KL: 73.85574340820312, Loss: 0.018005667254328728, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9742/20000], Bound: 0.3706945478916168, Entropy: 142.44549560546875, Temp: 2.67165470123291, KL: 69.27157592773438, Loss: 0.01690789870917797, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9743/20000], Bound: 0.37275558710098267, Entropy: 140.11474609375, Temp: 2.6716489791870117, KL: 71.14370727539062, Loss: 0.01450224407017231, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9744/20000], Bound: 0.3900483250617981, Entropy: 139.99667358398438, Temp: 2.671659231185913, KL: 74.21067810058594, Loss: 0.018104810267686844, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9745/20000], Bound: 0.37286925315856934, Entropy: 142.50131225585938, Temp: 2.6716606616973877, KL: 69.99751281738281, Loss: 0.016708072274923325, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9746/20000], Bound: 0.3747243583202362, Entropy: 144.68199157714844, Temp: 2.6716582775115967, KL: 71.2274169921875, Loss: 0.015397588722407818, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9747/20000], Bound: 0.3792666792869568, Entropy: 141.8688507080078, Temp: 2.671664237976074, KL: 72.15440368652344, Loss: 0.01610112190246582, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9748/20000], Bound: 0.4259111285209656, Entropy: 140.66416931152344, Temp: 2.6716744899749756, KL: 85.38919067382812, Loss: 0.017335696145892143, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9749/20000], Bound: 0.4306136965751648, Entropy: 141.07046508789062, Temp: 2.6716997623443604, KL: 87.46585083007812, Loss: 0.01617426425218582, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9750/20000], Bound: 0.38906678557395935, Entropy: 142.05941772460938, Temp: 2.671750068664551, KL: 74.47991943359375, Loss: 0.017065180465579033, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9751/20000], Bound: 0.3989923894405365, Entropy: 140.961669921875, Temp: 2.6717958450317383, KL: 77.59645080566406, Loss: 0.01669473387300968, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9752/20000], Bound: 0.37982138991355896, Entropy: 142.723876953125, Temp: 2.6718459129333496, KL: 70.58187866210938, Loss: 0.019344398751854897, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9753/20000], Bound: 0.4031926691532135, Entropy: 140.53253173828125, Temp: 2.6718671321868896, KL: 79.22293090820312, Loss: 0.015987074002623558, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9754/20000], Bound: 0.3797258138656616, Entropy: 139.74749755859375, Temp: 2.671903133392334, KL: 72.21336364746094, Loss: 0.01624031364917755, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9755/20000], Bound: 0.3742971122264862, Entropy: 142.8728485107422, Temp: 2.671938896179199, KL: 70.75225830078125, Loss: 0.016060803085565567, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9756/20000], Bound: 0.38080641627311707, Entropy: 141.24148559570312, Temp: 2.671973705291748, KL: 73.24925231933594, Loss: 0.014885294251143932, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9757/20000], Bound: 0.38482990860939026, Entropy: 141.9860076904297, Temp: 2.67202091217041, KL: 74.25563049316406, Loss: 0.015180341899394989, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9758/20000], Bound: 0.40556222200393677, Entropy: 140.74591064453125, Temp: 2.6720786094665527, KL: 78.63037109375, Loss: 0.01842196099460125, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9759/20000], Bound: 0.4037274122238159, Entropy: 139.52169799804688, Temp: 2.6721272468566895, KL: 79.47880554199219, Loss: 0.015809407457709312, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9760/20000], Bound: 0.40023547410964966, Entropy: 141.33998107910156, Temp: 2.672189474105835, KL: 77.13522338867188, Loss: 0.018251366913318634, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9761/20000], Bound: 0.376757949590683, Entropy: 142.0569305419922, Temp: 2.6722412109375, KL: 70.44906616210938, Loss: 0.01794878952205181, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9762/20000], Bound: 0.39985090494155884, Entropy: 140.04934692382812, Temp: 2.672274589538574, KL: 76.61659240722656, Loss: 0.019009100273251534, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9763/20000], Bound: 0.37508511543273926, Entropy: 141.41941833496094, Temp: 2.6722936630249023, KL: 72.51406860351562, Loss: 0.013189027085900307, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9764/20000], Bound: 0.38669320940971375, Entropy: 141.4504852294922, Temp: 2.6723389625549316, KL: 74.19642639160156, Loss: 0.01630704663693905, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9765/20000], Bound: 0.39343833923339844, Entropy: 141.10189819335938, Temp: 2.6723859310150146, KL: 76.55825805664062, Loss: 0.015577823854982853, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9766/20000], Bound: 0.4085347354412079, Entropy: 141.67364501953125, Temp: 2.6724436283111572, KL: 79.05287170410156, Loss: 0.01930231973528862, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9767/20000], Bound: 0.38090717792510986, Entropy: 140.064208984375, Temp: 2.6724860668182373, KL: 73.03887939453125, Loss: 0.015338205732405186, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9768/20000], Bound: 0.39815831184387207, Entropy: 141.39195251464844, Temp: 2.6725363731384277, KL: 75.63368225097656, Loss: 0.01991245709359646, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9769/20000], Bound: 0.4064740836620331, Entropy: 138.56092834472656, Temp: 2.6725616455078125, KL: 79.63163757324219, Loss: 0.017064161598682404, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9770/20000], Bound: 0.38084256649017334, Entropy: 140.85362243652344, Temp: 2.672593116760254, KL: 72.76365661621094, Loss: 0.015819258987903595, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9771/20000], Bound: 0.3907615542411804, Entropy: 140.9827423095703, Temp: 2.6726293563842773, KL: 75.6202392578125, Loss: 0.015866795554757118, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9772/20000], Bound: 0.36464744806289673, Entropy: 141.1331329345703, Temp: 2.6726737022399902, KL: 65.58491516113281, Loss: 0.020610906183719635, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9773/20000], Bound: 0.37225067615509033, Entropy: 140.76133728027344, Temp: 2.67267107963562, KL: 70.44436645507812, Loss: 0.01555086113512516, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9774/20000], Bound: 0.413818895816803, Entropy: 140.41627502441406, Temp: 2.6726746559143066, KL: 81.09886169433594, Loss: 0.018459005281329155, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9775/20000], Bound: 0.4003463387489319, Entropy: 139.7073516845703, Temp: 2.672678232192993, KL: 76.04045104980469, Loss: 0.02036568522453308, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9776/20000], Bound: 0.3707743287086487, Entropy: 140.53993225097656, Temp: 2.672658681869507, KL: 70.29379272460938, Loss: 0.015046311542391777, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9777/20000], Bound: 0.38219112157821655, Entropy: 142.3732147216797, Temp: 2.6726508140563965, KL: 71.48155212402344, Loss: 0.018946785479784012, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9778/20000], Bound: 0.38494372367858887, Entropy: 140.76596069335938, Temp: 2.6726245880126953, KL: 72.43167114257812, Loss: 0.018660347908735275, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9779/20000], Bound: 0.38707235455513, Entropy: 141.7114715576172, Temp: 2.6725857257843018, KL: 73.75242614746094, Loss: 0.0173464547842741, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9780/20000], Bound: 0.41872671246528625, Entropy: 138.84786987304688, Temp: 2.6725475788116455, KL: 81.38832092285156, Loss: 0.020707275718450546, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9781/20000], Bound: 0.37753984332084656, Entropy: 140.02059936523438, Temp: 2.672496795654297, KL: 72.69685363769531, Loss: 0.014165211468935013, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9782/20000], Bound: 0.38621917366981506, Entropy: 141.04794311523438, Temp: 2.672471761703491, KL: 73.57209777832031, Loss: 0.01721850037574768, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9783/20000], Bound: 0.39218267798423767, Entropy: 140.00938415527344, Temp: 2.6724472045898438, KL: 76.68641662597656, Loss: 0.014649154618382454, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9784/20000], Bound: 0.3650726079940796, Entropy: 142.3209991455078, Temp: 2.672447919845581, KL: 68.38360595703125, Loss: 0.015597493387758732, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9785/20000], Bound: 0.3867742419242859, Entropy: 139.50100708007812, Temp: 2.6724510192871094, KL: 73.5987548828125, Loss: 0.01747041381895542, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9786/20000], Bound: 0.39323168992996216, Entropy: 141.00665283203125, Temp: 2.672449827194214, KL: 76.4659423828125, Loss: 0.01563761569559574, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9787/20000], Bound: 0.38470783829689026, Entropy: 139.2105712890625, Temp: 2.672463893890381, KL: 73.02851867675781, Loss: 0.017414305359125137, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9788/20000], Bound: 0.3920052945613861, Entropy: 141.20297241210938, Temp: 2.672471761703491, KL: 76.06124877929688, Loss: 0.015721719712018967, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9789/20000], Bound: 0.36651918292045593, Entropy: 142.2534942626953, Temp: 2.672492742538452, KL: 67.17544555664062, Loss: 0.01862233877182007, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9790/20000], Bound: 0.4061800539493561, Entropy: 140.2935791015625, Temp: 2.672487497329712, KL: 79.92901611328125, Loss: 0.01634230464696884, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9791/20000], Bound: 0.3821706771850586, Entropy: 141.0045928955078, Temp: 2.6724979877471924, KL: 72.03875732421875, Loss: 0.017892008647322655, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9792/20000], Bound: 0.39514994621276855, Entropy: 141.70068359375, Temp: 2.672497272491455, KL: 76.69696044921875, Loss: 0.016261419281363487, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9793/20000], Bound: 0.34770384430885315, Entropy: 141.43902587890625, Temp: 2.6725072860717773, KL: 63.05787658691406, Loss: 0.016507530584931374, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9794/20000], Bound: 0.39231520891189575, Entropy: 140.02391052246094, Temp: 2.672502279281616, KL: 74.96578979492188, Loss: 0.017941541969776154, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9795/20000], Bound: 0.3619621694087982, Entropy: 143.1942901611328, Temp: 2.672492265701294, KL: 66.66519165039062, Loss: 0.01717519760131836, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9796/20000], Bound: 0.38566410541534424, Entropy: 141.4400177001953, Temp: 2.6724698543548584, KL: 73.31918334960938, Loss: 0.017389854416251183, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9797/20000], Bound: 0.4079394042491913, Entropy: 140.60498046875, Temp: 2.672445774078369, KL: 81.00614929199219, Loss: 0.015313367359340191, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9798/20000], Bound: 0.3877168893814087, Entropy: 140.71734619140625, Temp: 2.6724488735198975, KL: 74.076416015625, Loss: 0.01709018275141716, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9799/20000], Bound: 0.35604971647262573, Entropy: 140.65093994140625, Temp: 2.6724514961242676, KL: 63.63404846191406, Loss: 0.019752616062760353, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9800/20000], Bound: 0.4195440113544464, Entropy: 139.95994567871094, Temp: 2.672414779663086, KL: 82.03506469726562, Loss: 0.01996278017759323, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9801/20000], Bound: 0.40381816029548645, Entropy: 140.82388305664062, Temp: 2.6723716259002686, KL: 78.85391235351562, Loss: 0.017031889408826828, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9802/20000], Bound: 0.3815890848636627, Entropy: 140.5275115966797, Temp: 2.6723408699035645, KL: 73.06968688964844, Loss: 0.01564742811024189, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9803/20000], Bound: 0.3738711476325989, Entropy: 142.1002197265625, Temp: 2.6723227500915527, KL: 71.27728271484375, Loss: 0.014854175969958305, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9804/20000], Bound: 0.37259745597839355, Entropy: 138.79400634765625, Temp: 2.672319173812866, KL: 69.82913208007812, Loss: 0.016883766278624535, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9805/20000], Bound: 0.39839139580726624, Entropy: 140.60604858398438, Temp: 2.6723105907440186, KL: 78.09442138671875, Loss: 0.015435327775776386, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9806/20000], Bound: 0.3956787884235382, Entropy: 138.69143676757812, Temp: 2.6723222732543945, KL: 76.61083984375, Loss: 0.016712337732315063, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9807/20000], Bound: 0.3847835659980774, Entropy: 140.7207794189453, Temp: 2.67233943939209, KL: 73.3411865234375, Loss: 0.016869263723492622, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9808/20000], Bound: 0.4130387306213379, Entropy: 139.85650634765625, Temp: 2.6723551750183105, KL: 82.09111022949219, Loss: 0.016157329082489014, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9809/20000], Bound: 0.379382461309433, Entropy: 141.5021209716797, Temp: 2.672389507293701, KL: 71.89436340332031, Loss: 0.016656603664159775, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9810/20000], Bound: 0.3620087504386902, Entropy: 141.9749298095703, Temp: 2.672419786453247, KL: 65.66021728515625, Loss: 0.019079383462667465, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9811/20000], Bound: 0.38623446226119995, Entropy: 143.0108184814453, Temp: 2.672416925430298, KL: 73.54972839355469, Loss: 0.017268158495426178, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9812/20000], Bound: 0.40381550788879395, Entropy: 141.1637420654297, Temp: 2.6724116802215576, KL: 78.61859130859375, Loss: 0.017471078783273697, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9813/20000], Bound: 0.36808279156684875, Entropy: 142.409423828125, Temp: 2.6724112033843994, KL: 69.80838012695312, Loss: 0.014523281715810299, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9814/20000], Bound: 0.38971033692359924, Entropy: 142.58090209960938, Temp: 2.672423839569092, KL: 74.38276672363281, Loss: 0.017604947090148926, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9815/20000], Bound: 0.3852674663066864, Entropy: 139.69589233398438, Temp: 2.672431468963623, KL: 74.15507507324219, Loss: 0.015610108152031898, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9816/20000], Bound: 0.382781445980072, Entropy: 141.18238830566406, Temp: 2.672449827194214, KL: 72.44303894042969, Loss: 0.017465585842728615, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9817/20000], Bound: 0.36290183663368225, Entropy: 141.4967498779297, Temp: 2.6724603176116943, KL: 67.19015502929688, Loss: 0.01668678969144821, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9818/20000], Bound: 0.3707694411277771, Entropy: 141.0408935546875, Temp: 2.67246150970459, KL: 68.54396057128906, Loss: 0.018315769731998444, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9819/20000], Bound: 0.3816062808036804, Entropy: 140.09185791015625, Temp: 2.672443389892578, KL: 73.02052307128906, Loss: 0.015749648213386536, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9820/20000], Bound: 0.3862592577934265, Entropy: 140.40147399902344, Temp: 2.6724355220794678, KL: 73.51071166992188, Loss: 0.017354816198349, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9821/20000], Bound: 0.37235525250434875, Entropy: 142.2472686767578, Temp: 2.6724252700805664, KL: 69.4761962890625, Loss: 0.017415812239050865, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9822/20000], Bound: 0.40555089712142944, Entropy: 141.58203125, Temp: 2.67240571975708, KL: 79.22686767578125, Loss: 0.017302852123975754, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9823/20000], Bound: 0.40295419096946716, Entropy: 141.33843994140625, Temp: 2.672394275665283, KL: 77.72598266601562, Loss: 0.018660522997379303, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9824/20000], Bound: 0.40373358130455017, Entropy: 141.525390625, Temp: 2.672377586364746, KL: 77.68728637695312, Loss: 0.019167523831129074, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9825/20000], Bound: 0.37476518750190735, Entropy: 139.92042541503906, Temp: 2.672351598739624, KL: 69.81866455078125, Loss: 0.018061548471450806, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9826/20000], Bound: 0.3813818395137787, Entropy: 141.56382751464844, Temp: 2.6723129749298096, KL: 73.13519287109375, Loss: 0.015412621200084686, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9827/20000], Bound: 0.39234673976898193, Entropy: 142.91339111328125, Temp: 2.6722898483276367, KL: 76.19320678710938, Loss: 0.015660323202610016, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9828/20000], Bound: 0.36786019802093506, Entropy: 140.17478942871094, Temp: 2.67228364944458, KL: 69.54998779296875, Loss: 0.014887628145515919, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9829/20000], Bound: 0.3810546398162842, Entropy: 142.65135192871094, Temp: 2.672287940979004, KL: 74.45869445800781, Loss: 0.012759359553456306, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9830/20000], Bound: 0.4021534323692322, Entropy: 139.59725952148438, Temp: 2.6723268032073975, KL: 79.28591918945312, Loss: 0.015295056626200676, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9831/20000], Bound: 0.36879175901412964, Entropy: 143.2831268310547, Temp: 2.672384262084961, KL: 70.41667175292969, Loss: 0.013760809786617756, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9832/20000], Bound: 0.36990270018577576, Entropy: 142.7655029296875, Temp: 2.6724565029144287, KL: 68.94358825683594, Loss: 0.0171072855591774, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9833/20000], Bound: 0.3815547525882721, Entropy: 142.23907470703125, Temp: 2.6725125312805176, KL: 71.42227172851562, Loss: 0.018712632358074188, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9834/20000], Bound: 0.37986427545547485, Entropy: 142.0890350341797, Temp: 2.6725451946258545, KL: 71.59088134765625, Loss: 0.017485346645116806, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9835/20000], Bound: 0.4104244112968445, Entropy: 142.3800811767578, Temp: 2.672567129135132, KL: 81.5025634765625, Loss: 0.015784190967679024, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9836/20000], Bound: 0.36975330114364624, Entropy: 140.97357177734375, Temp: 2.6726088523864746, KL: 68.55545043945312, Loss: 0.017755288630723953, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9837/20000], Bound: 0.3858659863471985, Entropy: 140.12844848632812, Temp: 2.6726315021514893, KL: 73.89126586914062, Loss: 0.01643075980246067, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9838/20000], Bound: 0.39540916681289673, Entropy: 142.34356689453125, Temp: 2.672656536102295, KL: 77.41825866699219, Loss: 0.015056486241519451, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9839/20000], Bound: 0.399669885635376, Entropy: 142.80435180664062, Temp: 2.6727006435394287, KL: 78.71037292480469, Loss: 0.014995669946074486, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9840/20000], Bound: 0.3675529658794403, Entropy: 142.9734344482422, Temp: 2.6727638244628906, KL: 68.25077819824219, Loss: 0.017159640789031982, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9841/20000], Bound: 0.4058622419834137, Entropy: 140.9268035888672, Temp: 2.6728103160858154, KL: 79.6571044921875, Loss: 0.016676468774676323, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9842/20000], Bound: 0.39510875940322876, Entropy: 140.77159118652344, Temp: 2.6728639602661133, KL: 77.57449340820312, Loss: 0.014600872993469238, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9843/20000], Bound: 0.3986988961696625, Entropy: 141.86935424804688, Temp: 2.6729373931884766, KL: 76.62088012695312, Loss: 0.018368655815720558, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9844/20000], Bound: 0.36986127495765686, Entropy: 142.09898376464844, Temp: 2.672997236251831, KL: 70.46853637695312, Loss: 0.014237187802791595, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9845/20000], Bound: 0.3685539662837982, Entropy: 140.155029296875, Temp: 2.673067808151245, KL: 68.90231323242188, Loss: 0.01647370122373104, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9846/20000], Bound: 0.40058788657188416, Entropy: 140.9829559326172, Temp: 2.6731271743774414, KL: 78.85563659667969, Loss: 0.01523809414356947, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9847/20000], Bound: 0.40929827094078064, Entropy: 138.99867248535156, Temp: 2.6732027530670166, KL: 80.76152038574219, Loss: 0.01654326356947422, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9848/20000], Bound: 0.3895608186721802, Entropy: 140.4013214111328, Temp: 2.673285484313965, KL: 74.96316528320312, Loss: 0.016445623710751534, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9849/20000], Bound: 0.38320693373680115, Entropy: 141.10902404785156, Temp: 2.673366069793701, KL: 74.02186584472656, Loss: 0.014751091599464417, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9850/20000], Bound: 0.3638615608215332, Entropy: 139.3429412841797, Temp: 2.673456907272339, KL: 67.49714660644531, Loss: 0.01662560924887657, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9851/20000], Bound: 0.3808524012565613, Entropy: 141.31776428222656, Temp: 2.6735310554504395, KL: 72.89395141601562, Loss: 0.015589642338454723, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9852/20000], Bound: 0.3901173770427704, Entropy: 140.49114990234375, Temp: 2.673607349395752, KL: 74.31962585449219, Loss: 0.017956407740712166, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9853/20000], Bound: 0.386620432138443, Entropy: 140.23536682128906, Temp: 2.6736690998077393, KL: 75.11238098144531, Loss: 0.014567180536687374, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9854/20000], Bound: 0.35835179686546326, Entropy: 140.623291015625, Temp: 2.673746109008789, KL: 64.96293640136719, Loss: 0.018476802855730057, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9855/20000], Bound: 0.3756644129753113, Entropy: 140.83448791503906, Temp: 2.67378830909729, KL: 70.84454345703125, Loss: 0.01663595624268055, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9856/20000], Bound: 0.39000874757766724, Entropy: 141.2605438232422, Temp: 2.673823833465576, KL: 75.61383056640625, Loss: 0.015478849411010742, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9857/20000], Bound: 0.3876352310180664, Entropy: 139.65541076660156, Temp: 2.6738710403442383, KL: 75.68505859375, Loss: 0.014050782658159733, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9858/20000], Bound: 0.38150277733802795, Entropy: 142.53158569335938, Temp: 2.6739397048950195, KL: 71.3023681640625, Loss: 0.018920592963695526, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9859/20000], Bound: 0.36902251839637756, Entropy: 141.31982421875, Temp: 2.6739821434020996, KL: 67.76522827148438, Loss: 0.018855996429920197, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9860/20000], Bound: 0.37706270813941956, Entropy: 140.56112670898438, Temp: 2.673994541168213, KL: 71.84820556640625, Loss: 0.015510551631450653, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9861/20000], Bound: 0.40309444069862366, Entropy: 142.62063598632812, Temp: 2.6740145683288574, KL: 78.01716613769531, Loss: 0.018209880217909813, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9862/20000], Bound: 0.3822365999221802, Entropy: 139.92633056640625, Temp: 2.674029588699341, KL: 72.97972106933594, Loss: 0.01618139259517193, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9863/20000], Bound: 0.3716791570186615, Entropy: 140.7621307373047, Temp: 2.6740479469299316, KL: 69.49098205566406, Loss: 0.017041346058249474, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9864/20000], Bound: 0.37314605712890625, Entropy: 142.2957000732422, Temp: 2.6740567684173584, KL: 71.4598388671875, Loss: 0.014141845516860485, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9865/20000], Bound: 0.38818785548210144, Entropy: 142.12924194335938, Temp: 2.674083709716797, KL: 73.47543334960938, Loss: 0.01848572865128517, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9866/20000], Bound: 0.4032309055328369, Entropy: 141.20127868652344, Temp: 2.6740951538085938, KL: 78.48310852050781, Loss: 0.01741553656756878, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9867/20000], Bound: 0.3897281289100647, Entropy: 141.57029724121094, Temp: 2.674109697341919, KL: 73.58500671386719, Loss: 0.019121821969747543, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9868/20000], Bound: 0.38340798020362854, Entropy: 141.41539001464844, Temp: 2.674104928970337, KL: 73.203369140625, Loss: 0.016397595405578613, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9869/20000], Bound: 0.36313170194625854, Entropy: 140.3854217529297, Temp: 2.6741042137145996, KL: 67.18692016601562, Loss: 0.016826720908284187, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9870/20000], Bound: 0.3873717188835144, Entropy: 141.0966796875, Temp: 2.6740939617156982, KL: 73.83416748046875, Loss: 0.017370378598570824, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9871/20000], Bound: 0.4011365473270416, Entropy: 140.59671020507812, Temp: 2.6740810871124268, KL: 78.55831909179688, Loss: 0.016109056770801544, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9872/20000], Bound: 0.4335998296737671, Entropy: 139.87913513183594, Temp: 2.674084424972534, KL: 89.54379272460938, Loss: 0.014058234170079231, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9873/20000], Bound: 0.39904388785362244, Entropy: 138.21261596679688, Temp: 2.6741349697113037, KL: 77.60951232910156, Loss: 0.01672264002263546, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9874/20000], Bound: 0.38086995482444763, Entropy: 139.0284423828125, Temp: 2.6741886138916016, KL: 70.48698425292969, Loss: 0.020105643197894096, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9875/20000], Bound: 0.38457581400871277, Entropy: 142.28298950195312, Temp: 2.674206256866455, KL: 72.82478332519531, Loss: 0.01773924008011818, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9876/20000], Bound: 0.37211427092552185, Entropy: 140.40858459472656, Temp: 2.6742141246795654, KL: 70.37258911132812, Loss: 0.01562611758708954, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9877/20000], Bound: 0.3962971568107605, Entropy: 139.58203125, Temp: 2.6742262840270996, KL: 77.6727294921875, Loss: 0.015086811035871506, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9878/20000], Bound: 0.3801689147949219, Entropy: 141.54757690429688, Temp: 2.6742587089538574, KL: 71.72714233398438, Loss: 0.017409490421414375, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9879/20000], Bound: 0.41311973333358765, Entropy: 140.46165466308594, Temp: 2.6742806434631348, KL: 81.17759704589844, Loss: 0.017932796850800514, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9880/20000], Bound: 0.3758143484592438, Entropy: 139.155029296875, Temp: 2.674304962158203, KL: 72.08534240722656, Loss: 0.014400901272892952, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9881/20000], Bound: 0.36807331442832947, Entropy: 143.0280303955078, Temp: 2.674344539642334, KL: 69.1151123046875, Loss: 0.015831805765628815, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9882/20000], Bound: 0.405172735452652, Entropy: 143.09979248046875, Temp: 2.6743814945220947, KL: 78.20234680175781, Loss: 0.019027283415198326, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9883/20000], Bound: 0.34837934374809265, Entropy: 143.5302734375, Temp: 2.674405336380005, KL: 63.169219970703125, Loss: 0.016660768538713455, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9884/20000], Bound: 0.42132341861724854, Entropy: 141.68125915527344, Temp: 2.6744112968444824, KL: 84.43283081054688, Loss: 0.01651851087808609, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9885/20000], Bound: 0.39415138959884644, Entropy: 140.65635681152344, Temp: 2.6744372844696045, KL: 75.767822265625, Loss: 0.01746843382716179, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9886/20000], Bound: 0.3815322518348694, Entropy: 140.478515625, Temp: 2.674459934234619, KL: 72.94476318359375, Loss: 0.01587025262415409, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9887/20000], Bound: 0.35671475529670715, Entropy: 141.09457397460938, Temp: 2.674487590789795, KL: 65.02799987792969, Loss: 0.017506029456853867, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9888/20000], Bound: 0.4007072150707245, Entropy: 140.40301513671875, Temp: 2.6744933128356934, KL: 77.7550048828125, Loss: 0.01737673208117485, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9889/20000], Bound: 0.37901249527931213, Entropy: 139.80860900878906, Temp: 2.674501419067383, KL: 71.95240783691406, Loss: 0.016367845237255096, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9890/20000], Bound: 0.3782467246055603, Entropy: 140.580322265625, Temp: 2.6745104789733887, KL: 71.65676879882812, Loss: 0.016508890315890312, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9891/20000], Bound: 0.3891460597515106, Entropy: 141.38076782226562, Temp: 2.6745188236236572, KL: 73.83415222167969, Loss: 0.018341638147830963, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9892/20000], Bound: 0.39163243770599365, Entropy: 142.97068786621094, Temp: 2.6745152473449707, KL: 75.5235595703125, Loss: 0.01654309406876564, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9893/20000], Bound: 0.3883773684501648, Entropy: 141.59800720214844, Temp: 2.67451810836792, KL: 74.19309997558594, Loss: 0.01725117117166519, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9894/20000], Bound: 0.40690505504608154, Entropy: 142.91152954101562, Temp: 2.6745190620422363, KL: 80.24105834960938, Loss: 0.016186876222491264, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9895/20000], Bound: 0.391889363527298, Entropy: 141.09719848632812, Temp: 2.6745364665985107, KL: 76.20443725585938, Loss: 0.015411226078867912, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9896/20000], Bound: 0.36198124289512634, Entropy: 141.68508911132812, Temp: 2.6745684146881104, KL: 66.13008117675781, Loss: 0.018201487138867378, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9897/20000], Bound: 0.3917490839958191, Entropy: 141.6520538330078, Temp: 2.674574136734009, KL: 74.4576416015625, Loss: 0.01860031485557556, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9898/20000], Bound: 0.3865832984447479, Entropy: 142.641357421875, Temp: 2.674567222595215, KL: 74.419189453125, Loss: 0.015851959586143494, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9899/20000], Bound: 0.3959464430809021, Entropy: 140.93850708007812, Temp: 2.6745710372924805, KL: 76.38392639160156, Loss: 0.01730639860033989, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9900/20000], Bound: 0.41048505902290344, Entropy: 143.1407928466797, Temp: 2.6745760440826416, KL: 81.14555358886719, Loss: 0.01650836504995823, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9901/20000], Bound: 0.3812684416770935, Entropy: 141.1869659423828, Temp: 2.674595832824707, KL: 72.05650329589844, Loss: 0.017389662563800812, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9902/20000], Bound: 0.3929884731769562, Entropy: 141.05625915527344, Temp: 2.674607276916504, KL: 77.25994873046875, Loss: 0.014041670598089695, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9903/20000], Bound: 0.37991559505462646, Entropy: 142.7997589111328, Temp: 2.6746466159820557, KL: 72.56602478027344, Loss: 0.01570820063352585, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9904/20000], Bound: 0.38142889738082886, Entropy: 142.9456329345703, Temp: 2.674690008163452, KL: 72.06486511230469, Loss: 0.017461469396948814, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9905/20000], Bound: 0.3761809170246124, Entropy: 142.66024780273438, Temp: 2.674722194671631, KL: 72.22271728515625, Loss: 0.014344432391226292, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9906/20000], Bound: 0.40349793434143066, Entropy: 139.75929260253906, Temp: 2.674769878387451, KL: 77.78782653808594, Loss: 0.018870903179049492, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9907/20000], Bound: 0.4082091450691223, Entropy: 141.52391052246094, Temp: 2.6748037338256836, KL: 77.94178771972656, Loss: 0.02121950127184391, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9908/20000], Bound: 0.409784734249115, Entropy: 140.89247131347656, Temp: 2.674807071685791, KL: 79.37030029296875, Loss: 0.019434984773397446, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9909/20000], Bound: 0.3817410171031952, Entropy: 139.35031127929688, Temp: 2.674799680709839, KL: 72.00933837890625, Loss: 0.017734713852405548, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9910/20000], Bound: 0.3923869729042053, Entropy: 141.65576171875, Temp: 2.674783229827881, KL: 76.77560424804688, Loss: 0.01461886242032051, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9911/20000], Bound: 0.3902873396873474, Entropy: 141.614013671875, Temp: 2.6747922897338867, KL: 75.16949462890625, Loss: 0.016471439972519875, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9912/20000], Bound: 0.38113605976104736, Entropy: 140.4838104248047, Temp: 2.674806594848633, KL: 73.14572143554688, Loss: 0.015283986926078796, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9913/20000], Bound: 0.3866232931613922, Entropy: 140.45606994628906, Temp: 2.6748318672180176, KL: 74.60400390625, Loss: 0.015530817210674286, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9914/20000], Bound: 0.41385069489479065, Entropy: 140.60169982910156, Temp: 2.674867630004883, KL: 80.99281311035156, Loss: 0.018698159605264664, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9915/20000], Bound: 0.3903767764568329, Entropy: 141.19378662109375, Temp: 2.6748971939086914, KL: 74.85514831542969, Loss: 0.017108934000134468, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9916/20000], Bound: 0.3751436471939087, Entropy: 141.5265655517578, Temp: 2.674924612045288, KL: 70.80546569824219, Loss: 0.01644022762775421, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9917/20000], Bound: 0.396014004945755, Entropy: 139.61618041992188, Temp: 2.674948215484619, KL: 78.12545776367188, Loss: 0.01409203838557005, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9918/20000], Bound: 0.3468506336212158, Entropy: 141.20974731445312, Temp: 2.674999475479126, KL: 64.48008728027344, Loss: 0.013428008183836937, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9919/20000], Bound: 0.38726943731307983, Entropy: 140.8668670654297, Temp: 2.6750590801239014, KL: 74.04542541503906, Loss: 0.01692862994968891, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9920/20000], Bound: 0.4013945162296295, Entropy: 139.66783142089844, Temp: 2.6751129627227783, KL: 79.28317260742188, Loss: 0.01490843016654253, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9921/20000], Bound: 0.3698304295539856, Entropy: 141.55789184570312, Temp: 2.6751868724823, KL: 70.01199340820312, Loss: 0.015094282105565071, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9922/20000], Bound: 0.3940262496471405, Entropy: 140.85264587402344, Temp: 2.675262212753296, KL: 75.53353881835938, Loss: 0.017845366150140762, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9923/20000], Bound: 0.3651723563671112, Entropy: 141.50668334960938, Temp: 2.675325393676758, KL: 68.74803161621094, Loss: 0.014993264339864254, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9924/20000], Bound: 0.39655056595802307, Entropy: 143.68104553222656, Temp: 2.6753897666931152, KL: 76.02783203125, Loss: 0.018313003703951836, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9925/20000], Bound: 0.39970335364341736, Entropy: 141.498046875, Temp: 2.675440549850464, KL: 78.54725646972656, Loss: 0.015348685905337334, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9926/20000], Bound: 0.3733319938182831, Entropy: 141.2367706298828, Temp: 2.675506591796875, KL: 68.77940368652344, Loss: 0.01926391012966633, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9927/20000], Bound: 0.38197600841522217, Entropy: 142.05377197265625, Temp: 2.6755387783050537, KL: 70.770751953125, Loss: 0.020182635635137558, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9928/20000], Bound: 0.3851478099822998, Entropy: 142.23789978027344, Temp: 2.6755363941192627, KL: 73.22279357910156, Loss: 0.01731739565730095, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9929/20000], Bound: 0.41695860028266907, Entropy: 142.87274169921875, Temp: 2.675529956817627, KL: 81.63034057617188, Loss: 0.01927749440073967, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9930/20000], Bound: 0.3977850377559662, Entropy: 141.75047302246094, Temp: 2.675518751144409, KL: 76.92495727539062, Loss: 0.01731937564909458, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9931/20000], Bound: 0.37190723419189453, Entropy: 140.98681640625, Temp: 2.6755106449127197, KL: 71.12110900878906, Loss: 0.014128463342785835, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9932/20000], Bound: 0.3637668490409851, Entropy: 141.5483856201172, Temp: 2.6755218505859375, KL: 66.19944763183594, Loss: 0.019017335027456284, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9933/20000], Bound: 0.36897164583206177, Entropy: 141.86184692382812, Temp: 2.675501585006714, KL: 69.4586181640625, Loss: 0.01567569375038147, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9934/20000], Bound: 0.4014461636543274, Entropy: 140.6109161376953, Temp: 2.6754868030548096, KL: 78.92880249023438, Loss: 0.015603484585881233, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9935/20000], Bound: 0.3670773208141327, Entropy: 141.7580108642578, Temp: 2.675492525100708, KL: 67.4437255859375, Loss: 0.01843811571598053, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9936/20000], Bound: 0.3630519211292267, Entropy: 140.3768768310547, Temp: 2.6754748821258545, KL: 67.44923400878906, Loss: 0.016305312514305115, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9937/20000], Bound: 0.3935944736003876, Entropy: 140.79876708984375, Temp: 2.6754531860351562, KL: 72.98123168945312, Loss: 0.02237975038588047, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9938/20000], Bound: 0.3928379714488983, Entropy: 143.05821228027344, Temp: 2.675388813018799, KL: 74.45106506347656, Loss: 0.019216928631067276, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9939/20000], Bound: 0.37125512957572937, Entropy: 141.60716247558594, Temp: 2.675313711166382, KL: 69.76277160644531, Loss: 0.01631826162338257, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9940/20000], Bound: 0.4093692898750305, Entropy: 140.21885681152344, Temp: 2.675244092941284, KL: 81.49067687988281, Loss: 0.015242578461766243, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9941/20000], Bound: 0.363870769739151, Entropy: 141.3991241455078, Temp: 2.6752078533172607, KL: 69.03031921386719, Loss: 0.013778879307210445, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9942/20000], Bound: 0.3659895062446594, Entropy: 141.53692626953125, Temp: 2.6751928329467773, KL: 68.19741821289062, Loss: 0.016452418640255928, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9943/20000], Bound: 0.39442208409309387, Entropy: 140.52255249023438, Temp: 2.6751739978790283, KL: 75.98812866210938, Loss: 0.017212551087141037, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9944/20000], Bound: 0.39226439595222473, Entropy: 142.38998413085938, Temp: 2.6751585006713867, KL: 75.07966613769531, Loss: 0.01772533357143402, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9945/20000], Bound: 0.4058057367801666, Entropy: 143.15377807617188, Temp: 2.675140380859375, KL: 79.66085815429688, Loss: 0.016662538051605225, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9946/20000], Bound: 0.39657190442085266, Entropy: 140.5817108154297, Temp: 2.675135850906372, KL: 76.23262023925781, Loss: 0.01793963834643364, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9947/20000], Bound: 0.3702831566333771, Entropy: 142.17735290527344, Temp: 2.6751277446746826, KL: 70.61019897460938, Loss: 0.014216112904250622, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9948/20000], Bound: 0.38211026787757874, Entropy: 140.49813842773438, Temp: 2.675137519836426, KL: 71.4986572265625, Loss: 0.018891582265496254, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9949/20000], Bound: 0.38166603446006775, Entropy: 141.18423461914062, Temp: 2.675126791000366, KL: 71.6712646484375, Loss: 0.018328947946429253, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9950/20000], Bound: 0.38775748014450073, Entropy: 142.58953857421875, Temp: 2.6751019954681396, KL: 74.15560913085938, Loss: 0.016988839954137802, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9951/20000], Bound: 0.34602928161621094, Entropy: 143.05484008789062, Temp: 2.6750802993774414, KL: 64.16181945800781, Loss: 0.013601524755358696, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9952/20000], Bound: 0.38318100571632385, Entropy: 142.40293884277344, Temp: 2.675071954727173, KL: 73.05738830566406, Loss: 0.016556527465581894, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9953/20000], Bound: 0.37039193511009216, Entropy: 139.64974975585938, Temp: 2.6750662326812744, KL: 69.46328735351562, Loss: 0.016417047008872032, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9954/20000], Bound: 0.373359739780426, Entropy: 141.94248962402344, Temp: 2.675057888031006, KL: 70.65623474121094, Loss: 0.0157672967761755, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9955/20000], Bound: 0.3868055045604706, Entropy: 140.57431030273438, Temp: 2.6750550270080566, KL: 74.42074584960938, Loss: 0.01597462221980095, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9956/20000], Bound: 0.36010947823524475, Entropy: 140.9231414794922, Temp: 2.6750612258911133, KL: 66.52427673339844, Loss: 0.01648685708642006, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9957/20000], Bound: 0.3668759763240814, Entropy: 143.18362426757812, Temp: 2.675058364868164, KL: 68.01422119140625, Loss: 0.01726210117340088, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9958/20000], Bound: 0.3701933026313782, Entropy: 141.1808624267578, Temp: 2.6750433444976807, KL: 68.31086730957031, Loss: 0.018465347588062286, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9959/20000], Bound: 0.37541863322257996, Entropy: 140.11643981933594, Temp: 2.6750080585479736, KL: 70.26652526855469, Loss: 0.01759544014930725, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9960/20000], Bound: 0.38981229066848755, Entropy: 139.86668395996094, Temp: 2.6749651432037354, KL: 73.40768432617188, Loss: 0.019506605342030525, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9961/20000], Bound: 0.3833872377872467, Entropy: 140.5078887939453, Temp: 2.674905300140381, KL: 72.76530456542969, Loss: 0.017212603241205215, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9962/20000], Bound: 0.38933664560317993, Entropy: 141.90072631835938, Temp: 2.674847364425659, KL: 76.54539489746094, Loss: 0.013380603864789009, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9963/20000], Bound: 0.3996661305427551, Entropy: 140.62872314453125, Temp: 2.67482852935791, KL: 77.47981262207031, Loss: 0.017316890880465508, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9964/20000], Bound: 0.36841481924057007, Entropy: 142.38461303710938, Temp: 2.6748151779174805, KL: 66.9293212890625, Loss: 0.020102588459849358, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9965/20000], Bound: 0.3752044141292572, Entropy: 141.5841827392578, Temp: 2.6747655868530273, KL: 71.27166748046875, Loss: 0.015599865466356277, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9966/20000], Bound: 0.3939563035964966, Entropy: 141.21995544433594, Temp: 2.6747279167175293, KL: 75.75901794433594, Loss: 0.017380403354763985, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9967/20000], Bound: 0.3812420666217804, Entropy: 140.4837646484375, Temp: 2.6746935844421387, KL: 72.52635192871094, Loss: 0.016497943550348282, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9968/20000], Bound: 0.38299447298049927, Entropy: 142.12286376953125, Temp: 2.674664258956909, KL: 73.55235290527344, Loss: 0.01552659459412098, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9969/20000], Bound: 0.39062660932540894, Entropy: 141.3788604736328, Temp: 2.674649238586426, KL: 75.2265625, Loss: 0.01654895208775997, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9970/20000], Bound: 0.35798582434654236, Entropy: 142.5911102294922, Temp: 2.6746411323547363, KL: 64.72993469238281, Loss: 0.018727414309978485, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9971/20000], Bound: 0.4063434302806854, Entropy: 139.7562713623047, Temp: 2.6746039390563965, KL: 78.42449951171875, Loss: 0.019269075244665146, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9972/20000], Bound: 0.39654675126075745, Entropy: 140.50857543945312, Temp: 2.6745595932006836, KL: 78.46157836914062, Loss: 0.013753343373537064, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9973/20000], Bound: 0.3999762237071991, Entropy: 141.2125701904297, Temp: 2.674553155899048, KL: 75.77445983886719, Loss: 0.0206742063164711, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9974/20000], Bound: 0.41113755106925964, Entropy: 140.55831909179688, Temp: 2.67452073097229, KL: 81.23828125, Loss: 0.016702255234122276, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9975/20000], Bound: 0.38670748472213745, Entropy: 141.94741821289062, Temp: 2.6745059490203857, KL: 73.62571716308594, Loss: 0.017402319237589836, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9976/20000], Bound: 0.3732462227344513, Entropy: 140.71337890625, Temp: 2.6744885444641113, KL: 69.86030578613281, Loss: 0.017189694568514824, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9977/20000], Bound: 0.4032835066318512, Entropy: 140.4286651611328, Temp: 2.674464464187622, KL: 80.1806640625, Loss: 0.014274944551289082, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9978/20000], Bound: 0.38213157653808594, Entropy: 143.18260192871094, Temp: 2.6744744777679443, KL: 71.16703796386719, Loss: 0.019517596811056137, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9979/20000], Bound: 0.4108884334564209, Entropy: 141.60755920410156, Temp: 2.6744585037231445, KL: 80.61415100097656, Loss: 0.017727898433804512, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9980/20000], Bound: 0.401678204536438, Entropy: 138.72657775878906, Temp: 2.6744492053985596, KL: 78.04087829589844, Loss: 0.017381420359015465, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9981/20000], Bound: 0.39807793498039246, Entropy: 141.74496459960938, Temp: 2.6744446754455566, KL: 76.4505615234375, Loss: 0.018357757478952408, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9982/20000], Bound: 0.39982858300209045, Entropy: 139.39669799804688, Temp: 2.674433469772339, KL: 79.20088195800781, Loss: 0.014185412786900997, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9983/20000], Bound: 0.35247620940208435, Entropy: 141.2581787109375, Temp: 2.674454689025879, KL: 63.990234375, Loss: 0.017243117094039917, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9984/20000], Bound: 0.36421358585357666, Entropy: 141.7835693359375, Temp: 2.674454689025879, KL: 67.35635375976562, Loss: 0.017082147300243378, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9985/20000], Bound: 0.3607335388660431, Entropy: 140.63900756835938, Temp: 2.6744425296783447, KL: 67.51921081542969, Loss: 0.014948961324989796, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9986/20000], Bound: 0.3820871114730835, Entropy: 141.86236572265625, Temp: 2.6744377613067627, KL: 73.18963623046875, Loss: 0.015711922198534012, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9987/20000], Bound: 0.3764384388923645, Entropy: 140.31028747558594, Temp: 2.6744422912597656, KL: 71.44451904296875, Loss: 0.015934620052576065, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9988/20000], Bound: 0.38718727231025696, Entropy: 141.6354522705078, Temp: 2.6744511127471924, KL: 74.81419372558594, Loss: 0.015441021881997585, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9989/20000], Bound: 0.3859213590621948, Entropy: 140.7063446044922, Temp: 2.6744728088378906, KL: 73.26341247558594, Loss: 0.017651990056037903, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9990/20000], Bound: 0.4084046483039856, Entropy: 141.5194854736328, Temp: 2.674485921859741, KL: 80.09574890136719, Loss: 0.017299573868513107, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9991/20000], Bound: 0.39349809288978577, Entropy: 141.75291442871094, Temp: 2.6745052337646484, KL: 75.56864929199219, Loss: 0.0174823310226202, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9992/20000], Bound: 0.3794878423213959, Entropy: 142.07772827148438, Temp: 2.6745214462280273, KL: 71.7606201171875, Loss: 0.016982408240437508, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9993/20000], Bound: 0.39865121245384216, Entropy: 140.3581085205078, Temp: 2.674532175064087, KL: 77.68948364257812, Loss: 0.016359664499759674, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9994/20000], Bound: 0.3904443681240082, Entropy: 140.5452117919922, Temp: 2.674553394317627, KL: 75.25291442871094, Loss: 0.0163990780711174, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9995/20000], Bound: 0.3808113932609558, Entropy: 140.98590087890625, Temp: 2.674579381942749, KL: 71.53126525878906, Loss: 0.018124839290976524, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9996/20000], Bound: 0.3898940086364746, Entropy: 141.37899780273438, Temp: 2.6745893955230713, KL: 75.91622924804688, Loss: 0.014858479611575603, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9997/20000], Bound: 0.41952386498451233, Entropy: 140.7272491455078, Temp: 2.67461895942688, KL: 83.47029113769531, Loss: 0.017290940508246422, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9998/20000], Bound: 0.4179946184158325, Entropy: 140.80238342285156, Temp: 2.6746585369110107, KL: 83.09284973144531, Loss: 0.017124418169260025, Learning Rate: 0.00020346692185469983\n",
      "Epoch [9999/20000], Bound: 0.38460952043533325, Entropy: 140.6015167236328, Temp: 2.674708127975464, KL: 73.04776000976562, Loss: 0.017345096915960312, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10000/20000], Bound: 0.4157180190086365, Entropy: 138.8168182373047, Temp: 2.674748182296753, KL: 82.61978149414062, Loss: 0.01671457849442959, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10001/20000], Bound: 0.39204564690589905, Entropy: 140.17178344726562, Temp: 2.6748008728027344, KL: 75.86239624023438, Loss: 0.016138946637511253, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10002/20000], Bound: 0.371786504983902, Entropy: 141.44847106933594, Temp: 2.6748580932617188, KL: 70.03607177734375, Loss: 0.016086263582110405, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10003/20000], Bound: 0.3866261839866638, Entropy: 141.07302856445312, Temp: 2.674910545349121, KL: 75.03572082519531, Loss: 0.014726151712238789, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10004/20000], Bound: 0.38130849599838257, Entropy: 143.20741271972656, Temp: 2.6749777793884277, KL: 72.94633483886719, Loss: 0.015751352533698082, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10005/20000], Bound: 0.4105345606803894, Entropy: 141.67466735839844, Temp: 2.675046920776367, KL: 80.42050170898438, Loss: 0.017896613106131554, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10006/20000], Bound: 0.3820141851902008, Entropy: 139.11001586914062, Temp: 2.675112009048462, KL: 72.39215087890625, Loss: 0.017169473692774773, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10007/20000], Bound: 0.39618128538131714, Entropy: 141.55967712402344, Temp: 2.675166606903076, KL: 75.05903625488281, Loss: 0.0199179295450449, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10008/20000], Bound: 0.40416795015335083, Entropy: 143.43577575683594, Temp: 2.6751937866210938, KL: 79.03843688964844, Loss: 0.016911262646317482, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10009/20000], Bound: 0.3717556595802307, Entropy: 143.2783203125, Temp: 2.675227165222168, KL: 69.57388305664062, Loss: 0.0169368926435709, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10010/20000], Bound: 0.3910052478313446, Entropy: 141.84945678710938, Temp: 2.6752500534057617, KL: 75.01319885253906, Loss: 0.017160695046186447, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10011/20000], Bound: 0.3796283006668091, Entropy: 141.13284301757812, Temp: 2.6752707958221436, KL: 71.53192138671875, Loss: 0.01749197579920292, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10012/20000], Bound: 0.4065309762954712, Entropy: 140.59886169433594, Temp: 2.675281286239624, KL: 80.91853332519531, Loss: 0.014719386585056782, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10013/20000], Bound: 0.40005218982696533, Entropy: 141.1342010498047, Temp: 2.6753203868865967, KL: 78.41490173339844, Loss: 0.015788190066814423, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10014/20000], Bound: 0.3782104253768921, Entropy: 141.0800323486328, Temp: 2.67537260055542, KL: 72.28387451171875, Loss: 0.015325045213103294, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10015/20000], Bound: 0.37756794691085815, Entropy: 141.9144744873047, Temp: 2.6754302978515625, KL: 71.73907470703125, Loss: 0.015998760238289833, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10016/20000], Bound: 0.34605711698532104, Entropy: 142.43881225585938, Temp: 2.6754863262176514, KL: 63.25923156738281, Loss: 0.015305827371776104, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10017/20000], Bound: 0.3925810754299164, Entropy: 140.82730102539062, Temp: 2.675532341003418, KL: 76.03074645996094, Loss: 0.016125133261084557, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10018/20000], Bound: 0.40040186047554016, Entropy: 140.13270568847656, Temp: 2.675584077835083, KL: 78.42318725585938, Loss: 0.01596943661570549, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10019/20000], Bound: 0.37596890330314636, Entropy: 141.8378143310547, Temp: 2.6756463050842285, KL: 71.27201843261719, Loss: 0.01601628214120865, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10020/20000], Bound: 0.41566041111946106, Entropy: 140.14329528808594, Temp: 2.6757054328918457, KL: 80.79208374023438, Loss: 0.020107952877879143, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10021/20000], Bound: 0.40666189789772034, Entropy: 140.5038299560547, Temp: 2.6757447719573975, KL: 79.40290832519531, Loss: 0.01763012632727623, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10022/20000], Bound: 0.36845314502716064, Entropy: 141.54579162597656, Temp: 2.67578387260437, KL: 66.68119812011719, Loss: 0.020593255758285522, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10023/20000], Bound: 0.37563881278038025, Entropy: 141.32766723632812, Temp: 2.675776481628418, KL: 71.7457275390625, Loss: 0.014955572783946991, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10024/20000], Bound: 0.38683998584747314, Entropy: 140.694580078125, Temp: 2.6757826805114746, KL: 72.99568176269531, Loss: 0.01866324059665203, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10025/20000], Bound: 0.3821004331111908, Entropy: 140.31796264648438, Temp: 2.6757729053497314, KL: 73.51498413085938, Loss: 0.01512374822050333, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10026/20000], Bound: 0.4063265323638916, Entropy: 140.3193359375, Temp: 2.675778388977051, KL: 79.88053894042969, Loss: 0.01655017025768757, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10027/20000], Bound: 0.3807380497455597, Entropy: 140.98472595214844, Temp: 2.6757965087890625, KL: 72.13929748535156, Loss: 0.016959356144070625, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10028/20000], Bound: 0.39094263315200806, Entropy: 139.95726013183594, Temp: 2.675809860229492, KL: 76.33229064941406, Loss: 0.01466683205217123, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10029/20000], Bound: 0.38971325755119324, Entropy: 140.1547393798828, Temp: 2.675844430923462, KL: 74.85716247558594, Loss: 0.01675150729715824, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10030/20000], Bound: 0.40229925513267517, Entropy: 137.36093139648438, Temp: 2.6758787631988525, KL: 78.21806335449219, Loss: 0.017410069704055786, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10031/20000], Bound: 0.3900584578514099, Entropy: 140.5383758544922, Temp: 2.6759133338928223, KL: 74.94049072265625, Loss: 0.0167850274592638, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10032/20000], Bound: 0.4193786084651947, Entropy: 140.02098083496094, Temp: 2.675947427749634, KL: 80.44853210449219, Loss: 0.022869080305099487, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10033/20000], Bound: 0.37756335735321045, Entropy: 140.7962188720703, Temp: 2.6759414672851562, KL: 72.24258422851562, Loss: 0.015060076490044594, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10034/20000], Bound: 0.3769477903842926, Entropy: 140.68128967285156, Temp: 2.6759488582611084, KL: 71.38681030273438, Loss: 0.01632893830537796, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10035/20000], Bound: 0.41162416338920593, Entropy: 138.6811981201172, Temp: 2.6759560108184814, KL: 81.53668212890625, Loss: 0.016434792429208755, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10036/20000], Bound: 0.3800857961177826, Entropy: 139.115966796875, Temp: 2.6759793758392334, KL: 72.83049011230469, Loss: 0.015318045392632484, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10037/20000], Bound: 0.4214162528514862, Entropy: 139.30160522460938, Temp: 2.6760120391845703, KL: 85.16542053222656, Loss: 0.015221557579934597, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10038/20000], Bound: 0.40024542808532715, Entropy: 140.2774658203125, Temp: 2.6760737895965576, KL: 78.63710021972656, Loss: 0.015488124452531338, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10039/20000], Bound: 0.38634753227233887, Entropy: 140.7725067138672, Temp: 2.6761491298675537, KL: 74.53175354003906, Loss: 0.015528715215623379, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10040/20000], Bound: 0.3920741677284241, Entropy: 140.380615234375, Temp: 2.676229476928711, KL: 75.98214721679688, Loss: 0.01594494841992855, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10041/20000], Bound: 0.3940291404724121, Entropy: 141.5023651123047, Temp: 2.6763134002685547, KL: 75.48768615722656, Loss: 0.01794244907796383, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10042/20000], Bound: 0.37641894817352295, Entropy: 139.23802185058594, Temp: 2.6763834953308105, KL: 71.23246765136719, Loss: 0.016337713226675987, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10043/20000], Bound: 0.377679705619812, Entropy: 141.5180206298828, Temp: 2.6764469146728516, KL: 72.14189147949219, Loss: 0.015315329656004906, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10044/20000], Bound: 0.3968319296836853, Entropy: 140.16558837890625, Temp: 2.6765146255493164, KL: 78.03886413574219, Loss: 0.014721948653459549, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10045/20000], Bound: 0.4078412652015686, Entropy: 139.33311462402344, Temp: 2.676600217819214, KL: 79.70700073242188, Loss: 0.01773180067539215, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10046/20000], Bound: 0.37817975878715515, Entropy: 142.87599182128906, Temp: 2.676680564880371, KL: 72.09861755371094, Loss: 0.015666784718632698, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10047/20000], Bound: 0.3743636906147003, Entropy: 139.09568786621094, Temp: 2.676760196685791, KL: 71.3663330078125, Loss: 0.014991738833487034, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10048/20000], Bound: 0.37635403871536255, Entropy: 140.1328125, Temp: 2.6768436431884766, KL: 73.12069702148438, Loss: 0.012780060060322285, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10049/20000], Bound: 0.3787219822406769, Entropy: 138.60818481445312, Temp: 2.6769516468048096, KL: 71.75994873046875, Loss: 0.016593098640441895, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10050/20000], Bound: 0.3838895857334137, Entropy: 139.69796752929688, Temp: 2.6770479679107666, KL: 73.86265563964844, Loss: 0.015454000793397427, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10051/20000], Bound: 0.39010927081108093, Entropy: 139.54502868652344, Temp: 2.6771469116210938, KL: 76.30543518066406, Loss: 0.01427521463483572, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10052/20000], Bound: 0.39644214510917664, Entropy: 141.31198120117188, Temp: 2.6772613525390625, KL: 77.51963806152344, Loss: 0.015484601259231567, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10053/20000], Bound: 0.4179971218109131, Entropy: 140.4078826904297, Temp: 2.677382230758667, KL: 82.98936462402344, Loss: 0.01734958216547966, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10054/20000], Bound: 0.38743939995765686, Entropy: 141.80178833007812, Temp: 2.6775026321411133, KL: 73.85267639160156, Loss: 0.017403630539774895, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10055/20000], Bound: 0.3522114157676697, Entropy: 140.4689178466797, Temp: 2.6776068210601807, KL: 65.89517211914062, Loss: 0.013571090064942837, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10056/20000], Bound: 0.3668299615383148, Entropy: 138.35678100585938, Temp: 2.6777150630950928, KL: 69.64939880371094, Loss: 0.014205346815288067, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10057/20000], Bound: 0.40177178382873535, Entropy: 138.06126403808594, Temp: 2.6778273582458496, KL: 77.34686279296875, Loss: 0.018763037398457527, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10058/20000], Bound: 0.39877504110336304, Entropy: 140.61534118652344, Temp: 2.677919387817383, KL: 76.61721801757812, Loss: 0.0184647124260664, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10059/20000], Bound: 0.3843315839767456, Entropy: 143.62155151367188, Temp: 2.6779940128326416, KL: 73.96200561523438, Loss: 0.015516879968345165, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10060/20000], Bound: 0.3882218301296234, Entropy: 139.92535400390625, Temp: 2.678072690963745, KL: 75.14639282226562, Loss: 0.01541934721171856, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10061/20000], Bound: 0.37778663635253906, Entropy: 139.4257049560547, Temp: 2.6781580448150635, KL: 71.72087097167969, Loss: 0.016174541786313057, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10062/20000], Bound: 0.3869423270225525, Entropy: 140.24853515625, Temp: 2.678236961364746, KL: 74.898681640625, Loss: 0.015187176875770092, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10063/20000], Bound: 0.37289074063301086, Entropy: 140.96139526367188, Temp: 2.678323745727539, KL: 70.58808898925781, Loss: 0.01567336916923523, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10064/20000], Bound: 0.395702987909317, Entropy: 139.3698272705078, Temp: 2.6784067153930664, KL: 75.56773376464844, Loss: 0.018732966855168343, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10065/20000], Bound: 0.3800753951072693, Entropy: 139.7415313720703, Temp: 2.678469181060791, KL: 71.2255859375, Loss: 0.018331773579120636, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10066/20000], Bound: 0.4044608771800995, Entropy: 139.4196014404297, Temp: 2.678508758544922, KL: 79.17892456054688, Loss: 0.01684662140905857, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10067/20000], Bound: 0.3696039617061615, Entropy: 141.3289031982422, Temp: 2.678553819656372, KL: 67.00518798828125, Loss: 0.020616736263036728, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10068/20000], Bound: 0.3610038161277771, Entropy: 141.43455505371094, Temp: 2.67855167388916, KL: 65.81813049316406, Loss: 0.0183002520352602, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10069/20000], Bound: 0.38376909494400024, Entropy: 139.70277404785156, Temp: 2.6785240173339844, KL: 74.93296813964844, Loss: 0.013404984958469868, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10070/20000], Bound: 0.3985852301120758, Entropy: 141.2085723876953, Temp: 2.678529977798462, KL: 78.74061584472656, Loss: 0.014401726424694061, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10071/20000], Bound: 0.3776780366897583, Entropy: 140.0955047607422, Temp: 2.6785635948181152, KL: 71.01148986816406, Loss: 0.017444055527448654, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10072/20000], Bound: 0.3873402774333954, Entropy: 138.12515258789062, Temp: 2.678584098815918, KL: 75.43051147460938, Loss: 0.01441422663629055, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10073/20000], Bound: 0.39755991101264954, Entropy: 141.53807067871094, Temp: 2.6786255836486816, KL: 76.06620788574219, Loss: 0.018828369677066803, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10074/20000], Bound: 0.41996699571609497, Entropy: 139.7317352294922, Temp: 2.6786508560180664, KL: 83.88804626464844, Loss: 0.016809361055493355, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10075/20000], Bound: 0.3890173137187958, Entropy: 139.221923828125, Temp: 2.6786906719207764, KL: 74.77218627929688, Loss: 0.0165573637932539, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10076/20000], Bound: 0.41018521785736084, Entropy: 140.06459045410156, Temp: 2.6787304878234863, KL: 81.40328979492188, Loss: 0.015903817489743233, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10077/20000], Bound: 0.4027717113494873, Entropy: 140.09446716308594, Temp: 2.6787869930267334, KL: 78.41983032226562, Loss: 0.017325375229120255, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10078/20000], Bound: 0.3892768919467926, Entropy: 140.737060546875, Temp: 2.6788418292999268, KL: 74.57127380371094, Loss: 0.01707536354660988, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10079/20000], Bound: 0.38647139072418213, Entropy: 142.92088317871094, Temp: 2.6788907051086426, KL: 73.47880554199219, Loss: 0.017587805166840553, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10080/20000], Bound: 0.37957409024238586, Entropy: 138.55882263183594, Temp: 2.6789281368255615, KL: 71.99847412109375, Loss: 0.01662331633269787, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10081/20000], Bound: 0.39824819564819336, Entropy: 143.96389770507812, Temp: 2.6789610385894775, KL: 77.11802673339844, Loss: 0.01724846474826336, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10082/20000], Bound: 0.3846875727176666, Entropy: 141.23193359375, Temp: 2.678992986679077, KL: 73.09725952148438, Loss: 0.017333148047327995, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10083/20000], Bound: 0.37581315636634827, Entropy: 141.2029266357422, Temp: 2.6790168285369873, KL: 69.59983825683594, Loss: 0.019083712249994278, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10084/20000], Bound: 0.3812669515609741, Entropy: 141.9174346923828, Temp: 2.6790130138397217, KL: 71.25541687011719, Loss: 0.018922315910458565, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10085/20000], Bound: 0.40592139959335327, Entropy: 138.38519287109375, Temp: 2.678987979888916, KL: 80.0380859375, Loss: 0.016063569113612175, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10086/20000], Bound: 0.3691733479499817, Entropy: 141.9866943359375, Temp: 2.6789824962615967, KL: 69.1761474609375, Loss: 0.016339564695954323, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10087/20000], Bound: 0.3924802541732788, Entropy: 138.6510467529297, Temp: 2.678974151611328, KL: 76.1177978515625, Loss: 0.015941385179758072, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10088/20000], Bound: 0.35283318161964417, Entropy: 142.2946014404297, Temp: 2.6789777278900146, KL: 65.05224609375, Loss: 0.015477645210921764, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10089/20000], Bound: 0.3720172941684723, Entropy: 142.41265869140625, Temp: 2.6789777278900146, KL: 70.27742004394531, Loss: 0.015794022008776665, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10090/20000], Bound: 0.3936586380004883, Entropy: 141.5930633544922, Temp: 2.678980588912964, KL: 76.86703491210938, Loss: 0.015189321711659431, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10091/20000], Bound: 0.40008988976478577, Entropy: 141.08045959472656, Temp: 2.679002285003662, KL: 76.96133422851562, Loss: 0.018560510128736496, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10092/20000], Bound: 0.40292009711265564, Entropy: 142.30836486816406, Temp: 2.6790130138397217, KL: 80.69847106933594, Loss: 0.01315741054713726, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10093/20000], Bound: 0.38351747393608093, Entropy: 139.12841796875, Temp: 2.6790645122528076, KL: 73.80595397949219, Loss: 0.015377798117697239, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10094/20000], Bound: 0.39438360929489136, Entropy: 141.84242248535156, Temp: 2.6791231632232666, KL: 75.92698669433594, Loss: 0.017343340441584587, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10095/20000], Bound: 0.3974786698818207, Entropy: 140.14923095703125, Temp: 2.679175615310669, KL: 77.3187255859375, Loss: 0.016451044008135796, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10096/20000], Bound: 0.3638537526130676, Entropy: 141.35760498046875, Temp: 2.679232120513916, KL: 69.29852294921875, Loss: 0.013305546715855598, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10097/20000], Bound: 0.38255730271339417, Entropy: 142.87901306152344, Temp: 2.679304599761963, KL: 72.269775390625, Loss: 0.017727922648191452, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10098/20000], Bound: 0.4019983410835266, Entropy: 139.85035705566406, Temp: 2.6793603897094727, KL: 78.84693908691406, Loss: 0.016104085370898247, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10099/20000], Bound: 0.40467941761016846, Entropy: 139.70510864257812, Temp: 2.679425001144409, KL: 79.58363342285156, Loss: 0.016222788020968437, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10100/20000], Bound: 0.3883328437805176, Entropy: 141.17759704589844, Temp: 2.6794979572296143, KL: 75.37962341308594, Loss: 0.015058583579957485, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10101/20000], Bound: 0.3809845745563507, Entropy: 142.6286163330078, Temp: 2.6795814037323, KL: 72.5838623046875, Loss: 0.016295921057462692, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10102/20000], Bound: 0.37608906626701355, Entropy: 142.30593872070312, Temp: 2.679658889770508, KL: 71.51692199707031, Loss: 0.015659119933843613, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10103/20000], Bound: 0.36488306522369385, Entropy: 141.51641845703125, Temp: 2.679734468460083, KL: 69.05448913574219, Loss: 0.014307038858532906, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10104/20000], Bound: 0.36699002981185913, Entropy: 140.42161560058594, Temp: 2.6798157691955566, KL: 67.33807373046875, Loss: 0.018621200695633888, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10105/20000], Bound: 0.38901185989379883, Entropy: 140.1591033935547, Temp: 2.679863214492798, KL: 74.70097351074219, Loss: 0.016698351129889488, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10106/20000], Bound: 0.3999217450618744, Entropy: 140.218505859375, Temp: 2.6799087524414062, KL: 78.52543640136719, Loss: 0.01555768121033907, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10107/20000], Bound: 0.3857108950614929, Entropy: 142.01319885253906, Temp: 2.6799678802490234, KL: 74.6781005859375, Loss: 0.014947157353162766, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10108/20000], Bound: 0.3800189793109894, Entropy: 140.30886840820312, Temp: 2.6800384521484375, KL: 72.22607421875, Loss: 0.01644774340093136, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10109/20000], Bound: 0.38583505153656006, Entropy: 141.37918090820312, Temp: 2.680102825164795, KL: 73.79248046875, Loss: 0.016668029129505157, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10110/20000], Bound: 0.3600933849811554, Entropy: 141.58946228027344, Temp: 2.680162191390991, KL: 66.87281799316406, Loss: 0.015867730602622032, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10111/20000], Bound: 0.3784615993499756, Entropy: 142.40147399902344, Temp: 2.6802115440368652, KL: 71.50946044921875, Loss: 0.01694929413497448, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10112/20000], Bound: 0.37939774990081787, Entropy: 140.57839965820312, Temp: 2.6802515983581543, KL: 72.30799865722656, Loss: 0.01596277952194214, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10113/20000], Bound: 0.38025447726249695, Entropy: 141.37869262695312, Temp: 2.6802921295166016, KL: 73.038330078125, Loss: 0.015061461366713047, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10114/20000], Bound: 0.39507392048835754, Entropy: 141.43206787109375, Temp: 2.680342435836792, KL: 75.81266784667969, Loss: 0.017947524785995483, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10115/20000], Bound: 0.3642439544200897, Entropy: 143.54478454589844, Temp: 2.6803817749023438, KL: 68.69216918945312, Loss: 0.01465231366455555, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10116/20000], Bound: 0.39812570810317993, Entropy: 142.27598571777344, Temp: 2.680426836013794, KL: 77.97990417480469, Loss: 0.015587412752211094, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10117/20000], Bound: 0.3991752564907074, Entropy: 141.32958984375, Temp: 2.6804845333099365, KL: 79.22929382324219, Loss: 0.013837640173733234, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10118/20000], Bound: 0.38559219241142273, Entropy: 141.15379333496094, Temp: 2.680570125579834, KL: 73.10948181152344, Loss: 0.017814600840210915, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10119/20000], Bound: 0.36198076605796814, Entropy: 142.01966857910156, Temp: 2.680638313293457, KL: 68.21759033203125, Loss: 0.014351326040923595, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10120/20000], Bound: 0.3618294298648834, Entropy: 141.48794555664062, Temp: 2.680710554122925, KL: 66.49880981445312, Loss: 0.017478445544838905, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10121/20000], Bound: 0.36686649918556213, Entropy: 140.5672149658203, Temp: 2.680757522583008, KL: 69.99043273925781, Loss: 0.013615868985652924, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10122/20000], Bound: 0.42232730984687805, Entropy: 139.7571258544922, Temp: 2.6808204650878906, KL: 83.6878662109375, Loss: 0.01855693943798542, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10123/20000], Bound: 0.4019351899623871, Entropy: 141.5691375732422, Temp: 2.6808793544769287, KL: 77.41458129882812, Loss: 0.018756305798888206, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10124/20000], Bound: 0.37909722328186035, Entropy: 139.90371704101562, Temp: 2.680922508239746, KL: 72.90104675292969, Loss: 0.014701341278851032, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10125/20000], Bound: 0.3953641355037689, Entropy: 139.84910583496094, Temp: 2.6809775829315186, KL: 76.04399108886719, Loss: 0.017681585624814034, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10126/20000], Bound: 0.3759375810623169, Entropy: 140.95785522460938, Temp: 2.681023597717285, KL: 70.84428405761719, Loss: 0.016844816505908966, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10127/20000], Bound: 0.3921024799346924, Entropy: 141.5402374267578, Temp: 2.6810600757598877, KL: 76.0963134765625, Loss: 0.01579519920051098, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10128/20000], Bound: 0.3771907389163971, Entropy: 141.40216064453125, Temp: 2.681105613708496, KL: 71.41287231445312, Loss: 0.016455786302685738, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10129/20000], Bound: 0.3758508563041687, Entropy: 141.49549865722656, Temp: 2.681145191192627, KL: 70.66546630859375, Loss: 0.01713293045759201, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10130/20000], Bound: 0.3697265088558197, Entropy: 140.99444580078125, Temp: 2.6811728477478027, KL: 70.2891845703125, Loss: 0.014575094915926456, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10131/20000], Bound: 0.39053797721862793, Entropy: 140.7214813232422, Temp: 2.681210994720459, KL: 75.02658081054688, Loss: 0.016936177387833595, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10132/20000], Bound: 0.3816829025745392, Entropy: 141.0417022705078, Temp: 2.681246280670166, KL: 72.18960571289062, Loss: 0.017422422766685486, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10133/20000], Bound: 0.4075899124145508, Entropy: 142.30364990234375, Temp: 2.6812703609466553, KL: 78.10987854003906, Loss: 0.020617052912712097, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10134/20000], Bound: 0.36207976937294006, Entropy: 141.52430725097656, Temp: 2.681267738342285, KL: 68.154052734375, Loss: 0.014527114108204842, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10135/20000], Bound: 0.38116708397865295, Entropy: 140.2724609375, Temp: 2.6812753677368164, KL: 72.36921691894531, Loss: 0.01680976152420044, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10136/20000], Bound: 0.38211777806282043, Entropy: 142.07687377929688, Temp: 2.6812798976898193, KL: 73.46920776367188, Loss: 0.015271034091711044, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10137/20000], Bound: 0.3831286132335663, Entropy: 141.302490234375, Temp: 2.681295871734619, KL: 72.28309631347656, Loss: 0.01802869513630867, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10138/20000], Bound: 0.36019930243492126, Entropy: 140.13587951660156, Temp: 2.681298017501831, KL: 67.78132629394531, Loss: 0.014237925410270691, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10139/20000], Bound: 0.36835557222366333, Entropy: 141.6622772216797, Temp: 2.6813111305236816, KL: 70.12429809570312, Loss: 0.014158019796013832, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10140/20000], Bound: 0.3697866201400757, Entropy: 142.02772521972656, Temp: 2.6813390254974365, KL: 70.00471496582031, Loss: 0.015138908289372921, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10141/20000], Bound: 0.3828423321247101, Entropy: 140.8062744140625, Temp: 2.6813719272613525, KL: 73.49664306640625, Loss: 0.015611817128956318, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10142/20000], Bound: 0.3884654939174652, Entropy: 140.81423950195312, Temp: 2.681411027908325, KL: 75.145263671875, Loss: 0.015586807392537594, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10143/20000], Bound: 0.40133246779441833, Entropy: 141.04556274414062, Temp: 2.6814587116241455, KL: 79.53327941894531, Loss: 0.014476530253887177, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10144/20000], Bound: 0.3700466752052307, Entropy: 143.0478515625, Temp: 2.681530714035034, KL: 68.99722290039062, Loss: 0.017156988382339478, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10145/20000], Bound: 0.42017844319343567, Entropy: 140.08236694335938, Temp: 2.681584358215332, KL: 83.86846923828125, Loss: 0.0169998686760664, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10146/20000], Bound: 0.36773425340652466, Entropy: 141.32568359375, Temp: 2.681647539138794, KL: 68.53630065917969, Loss: 0.01679348200559616, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10147/20000], Bound: 0.3959730565547943, Entropy: 142.79759216308594, Temp: 2.6816956996917725, KL: 76.78108215332031, Loss: 0.01664905808866024, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10148/20000], Bound: 0.3768666684627533, Entropy: 141.38563537597656, Temp: 2.6817455291748047, KL: 73.46630859375, Loss: 0.012459294870495796, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10149/20000], Bound: 0.37797048687934875, Entropy: 141.2240447998047, Temp: 2.681826114654541, KL: 71.75764465332031, Loss: 0.016237061470746994, Learning Rate: 0.00020346692185469983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10150/20000], Bound: 0.3945104777812958, Entropy: 142.62181091308594, Temp: 2.6819000244140625, KL: 76.25267028808594, Loss: 0.01683209463953972, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10151/20000], Bound: 0.3680438995361328, Entropy: 142.31675720214844, Temp: 2.6819705963134766, KL: 70.44265747070312, Loss: 0.013405700214207172, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10152/20000], Bound: 0.4024093747138977, Entropy: 142.0287628173828, Temp: 2.6820566654205322, KL: 78.70297241210938, Loss: 0.016628872603178024, Learning Rate: 0.00020346692185469983\n",
      "Epoch [10153/20000], Bound: 0.38311877846717834, Entropy: 140.88465881347656, Temp: 2.6821177005767822, KL: 73.01637268066406, Loss: 0.01666337437927723, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10154/20000], Bound: 0.3942393362522125, Entropy: 141.14112854003906, Temp: 2.6821725368499756, KL: 77.08145141601562, Loss: 0.015140888281166553, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10155/20000], Bound: 0.3736734390258789, Entropy: 142.04185485839844, Temp: 2.6822357177734375, KL: 71.70933532714844, Loss: 0.014034271240234375, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10156/20000], Bound: 0.38783007860183716, Entropy: 141.71726989746094, Temp: 2.6823060512542725, KL: 74.59120178222656, Loss: 0.01628260873258114, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10157/20000], Bound: 0.39892828464508057, Entropy: 140.84060668945312, Temp: 2.682373285293579, KL: 76.11421203613281, Loss: 0.019528603181242943, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10158/20000], Bound: 0.3655282258987427, Entropy: 141.19898986816406, Temp: 2.6824209690093994, KL: 68.40669250488281, Loss: 0.015877697616815567, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10159/20000], Bound: 0.39546287059783936, Entropy: 143.0316619873047, Temp: 2.682462692260742, KL: 74.33207702636719, Loss: 0.02094072848558426, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10160/20000], Bound: 0.37333008646965027, Entropy: 142.12213134765625, Temp: 2.6824769973754883, KL: 71.3106689453125, Loss: 0.014596766792237759, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10161/20000], Bound: 0.4007989466190338, Entropy: 140.86996459960938, Temp: 2.682499408721924, KL: 78.68624877929688, Loss: 0.015770938247442245, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10162/20000], Bound: 0.39398008584976196, Entropy: 143.04885864257812, Temp: 2.6825315952301025, KL: 77.23294067382812, Loss: 0.01471992302685976, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10163/20000], Bound: 0.37184974551200867, Entropy: 139.552978515625, Temp: 2.6825766563415527, KL: 69.28646850585938, Loss: 0.017583148553967476, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10164/20000], Bound: 0.4050993323326111, Entropy: 141.35107421875, Temp: 2.6826071739196777, KL: 78.22468566894531, Loss: 0.019023366272449493, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10165/20000], Bound: 0.38481414318084717, Entropy: 142.98838806152344, Temp: 2.682626962661743, KL: 74.41523742675781, Loss: 0.014977331273257732, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10166/20000], Bound: 0.3875734806060791, Entropy: 141.21336364746094, Temp: 2.6826560497283936, KL: 74.77885437011719, Loss: 0.015796644613146782, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10167/20000], Bound: 0.3654114902019501, Entropy: 141.8747100830078, Temp: 2.6826889514923096, KL: 67.60472106933594, Loss: 0.017313174903392792, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10168/20000], Bound: 0.37641626596450806, Entropy: 141.59234619140625, Temp: 2.682708501815796, KL: 72.21533203125, Loss: 0.014559687115252018, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10169/20000], Bound: 0.3544345200061798, Entropy: 143.67007446289062, Temp: 2.682737112045288, KL: 65.18601989746094, Loss: 0.016086339950561523, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10170/20000], Bound: 0.3943662643432617, Entropy: 141.62489318847656, Temp: 2.6827566623687744, KL: 76.47439575195312, Loss: 0.01634794846177101, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10171/20000], Bound: 0.36864763498306274, Entropy: 141.2575225830078, Temp: 2.6827797889709473, KL: 69.92742919921875, Loss: 0.014692647382616997, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10172/20000], Bound: 0.40073639154434204, Entropy: 141.33233642578125, Temp: 2.6828083992004395, KL: 78.73463439941406, Loss: 0.015649348497390747, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10173/20000], Bound: 0.3523731231689453, Entropy: 141.41650390625, Temp: 2.682846784591675, KL: 65.69102478027344, Loss: 0.014078855514526367, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10174/20000], Bound: 0.3829435408115387, Entropy: 142.06845092773438, Temp: 2.682887315750122, KL: 74.05677795410156, Loss: 0.014636704698204994, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10175/20000], Bound: 0.38190609216690063, Entropy: 139.15879821777344, Temp: 2.682936906814575, KL: 73.42961120605469, Loss: 0.015246337279677391, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10176/20000], Bound: 0.38951951265335083, Entropy: 140.43838500976562, Temp: 2.682990312576294, KL: 75.01219177246094, Loss: 0.016424132511019707, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10177/20000], Bound: 0.3821115791797638, Entropy: 143.02120971679688, Temp: 2.68304181098938, KL: 73.70097351074219, Loss: 0.014852429740130901, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10178/20000], Bound: 0.3541332185268402, Entropy: 143.78045654296875, Temp: 2.6830992698669434, KL: 65.07005310058594, Loss: 0.016149014234542847, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10179/20000], Bound: 0.360968679189682, Entropy: 141.96713256835938, Temp: 2.6831440925598145, KL: 67.36325073242188, Loss: 0.01543499156832695, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10180/20000], Bound: 0.3949689567089081, Entropy: 140.85470581054688, Temp: 2.683184862136841, KL: 78.13314819335938, Loss: 0.013592060655355453, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10181/20000], Bound: 0.38464224338531494, Entropy: 141.69512939453125, Temp: 2.6832449436187744, KL: 74.13714599609375, Loss: 0.015408503822982311, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10182/20000], Bound: 0.3888348340988159, Entropy: 142.2195587158203, Temp: 2.683307647705078, KL: 72.52859497070312, Loss: 0.020682072266936302, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10183/20000], Bound: 0.3844813108444214, Entropy: 141.16761779785156, Temp: 2.6833395957946777, KL: 72.66020202636719, Loss: 0.01807437092065811, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10184/20000], Bound: 0.38971707224845886, Entropy: 140.08236694335938, Temp: 2.6833596229553223, KL: 73.98106384277344, Loss: 0.01845666579902172, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10185/20000], Bound: 0.3893715441226959, Entropy: 141.48187255859375, Temp: 2.683367967605591, KL: 74.89494323730469, Loss: 0.016565565019845963, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10186/20000], Bound: 0.37898120284080505, Entropy: 141.8095245361328, Temp: 2.683377981185913, KL: 73.40377807617188, Loss: 0.01372555736452341, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10187/20000], Bound: 0.3721797466278076, Entropy: 140.95223999023438, Temp: 2.6834044456481934, KL: 70.68959045410156, Loss: 0.015150566585361958, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10188/20000], Bound: 0.3881455361843109, Entropy: 140.1427764892578, Temp: 2.683434247970581, KL: 74.6539306640625, Loss: 0.01634790189564228, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10189/20000], Bound: 0.38748854398727417, Entropy: 143.03948974609375, Temp: 2.683464527130127, KL: 75.01307678222656, Loss: 0.01532180979847908, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10190/20000], Bound: 0.3721122741699219, Entropy: 141.2721405029297, Temp: 2.683501958847046, KL: 68.54623413085938, Loss: 0.01910918951034546, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10191/20000], Bound: 0.39131367206573486, Entropy: 141.17161560058594, Temp: 2.6835153102874756, KL: 75.64802551269531, Loss: 0.016223421320319176, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10192/20000], Bound: 0.38382452726364136, Entropy: 141.21853637695312, Temp: 2.683532953262329, KL: 73.60453796386719, Loss: 0.01596139557659626, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10193/20000], Bound: 0.41186782717704773, Entropy: 139.8363800048828, Temp: 2.6835532188415527, KL: 81.71827697753906, Loss: 0.01631678082048893, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10194/20000], Bound: 0.3638744354248047, Entropy: 141.59201049804688, Temp: 2.6835832595825195, KL: 68.022705078125, Loss: 0.015733029693365097, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10195/20000], Bound: 0.37393826246261597, Entropy: 141.21981811523438, Temp: 2.6836097240448, KL: 70.47248840332031, Loss: 0.016492648050189018, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10196/20000], Bound: 0.3884323835372925, Entropy: 140.19300842285156, Temp: 2.68363094329834, KL: 74.63578796386719, Loss: 0.01653956063091755, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10197/20000], Bound: 0.40106675028800964, Entropy: 144.56114196777344, Temp: 2.683652639389038, KL: 77.19381713867188, Loss: 0.018711967393755913, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10198/20000], Bound: 0.40191343426704407, Entropy: 141.37274169921875, Temp: 2.6836647987365723, KL: 77.66845703125, Loss: 0.018297351896762848, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10199/20000], Bound: 0.3842131495475769, Entropy: 142.6191864013672, Temp: 2.683671474456787, KL: 72.26753234863281, Loss: 0.01866377703845501, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10200/20000], Bound: 0.38133978843688965, Entropy: 142.06259155273438, Temp: 2.683664560317993, KL: 72.86701965332031, Loss: 0.01599632203578949, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10201/20000], Bound: 0.3994651734828949, Entropy: 138.85720825195312, Temp: 2.683661699295044, KL: 78.03021240234375, Loss: 0.016267044469714165, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10202/20000], Bound: 0.3734954297542572, Entropy: 140.5497589111328, Temp: 2.6836671829223633, KL: 70.30531311035156, Loss: 0.01656876690685749, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10203/20000], Bound: 0.40150219202041626, Entropy: 141.46250915527344, Temp: 2.683669328689575, KL: 77.41201782226562, Loss: 0.01854703016579151, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10204/20000], Bound: 0.40331947803497314, Entropy: 139.84786987304688, Temp: 2.6836647987365723, KL: 79.59626770019531, Loss: 0.015486742369830608, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10205/20000], Bound: 0.40739819407463074, Entropy: 140.7686004638672, Temp: 2.6836752891540527, KL: 80.33695983886719, Loss: 0.016382237896323204, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10206/20000], Bound: 0.37752223014831543, Entropy: 140.2974395751953, Temp: 2.68369460105896, KL: 71.67732238769531, Loss: 0.016163047403097153, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10207/20000], Bound: 0.3747357428073883, Entropy: 140.82162475585938, Temp: 2.683712959289551, KL: 68.46647644042969, Loss: 0.02065606787800789, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10208/20000], Bound: 0.38258445262908936, Entropy: 142.8929901123047, Temp: 2.6837000846862793, KL: 72.99809265136719, Loss: 0.016423257067799568, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10209/20000], Bound: 0.39599698781967163, Entropy: 142.0221710205078, Temp: 2.683689594268799, KL: 75.61308288574219, Loss: 0.018857814371585846, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10210/20000], Bound: 0.3558790385723114, Entropy: 141.39553833007812, Temp: 2.6836702823638916, KL: 65.99180603027344, Loss: 0.015341581776738167, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10211/20000], Bound: 0.3796024024486542, Entropy: 142.62969970703125, Temp: 2.683651924133301, KL: 72.94306945800781, Loss: 0.014920107088983059, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10212/20000], Bound: 0.41765934228897095, Entropy: 140.58868408203125, Temp: 2.683645248413086, KL: 83.7406005859375, Loss: 0.01582689955830574, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10213/20000], Bound: 0.3774312436580658, Entropy: 141.0590362548828, Temp: 2.6836562156677246, KL: 71.59153747558594, Loss: 0.016273807734251022, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10214/20000], Bound: 0.3958923816680908, Entropy: 141.2733917236328, Temp: 2.683666706085205, KL: 76.64041137695312, Loss: 0.016885995864868164, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10215/20000], Bound: 0.4041522741317749, Entropy: 141.0680694580078, Temp: 2.68367862701416, KL: 80.64067077636719, Loss: 0.014004518277943134, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10216/20000], Bound: 0.39148250222206116, Entropy: 141.39317321777344, Temp: 2.683713436126709, KL: 74.99542236328125, Loss: 0.0175334420055151, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10217/20000], Bound: 0.3757281005382538, Entropy: 143.04930114746094, Temp: 2.683741807937622, KL: 68.99911499023438, Loss: 0.020193569362163544, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10218/20000], Bound: 0.379341185092926, Entropy: 142.56329345703125, Temp: 2.683741331100464, KL: 72.76193237304688, Loss: 0.015118138864636421, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10219/20000], Bound: 0.4080254137516022, Entropy: 140.49078369140625, Temp: 2.6837494373321533, KL: 80.068359375, Loss: 0.017234545201063156, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10220/20000], Bound: 0.3681921362876892, Entropy: 141.14068603515625, Temp: 2.6837613582611084, KL: 69.33709716796875, Loss: 0.015560347586870193, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10221/20000], Bound: 0.3717699348926544, Entropy: 142.27435302734375, Temp: 2.683773994445801, KL: 70.08909606933594, Loss: 0.01605488918721676, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10222/20000], Bound: 0.39481666684150696, Entropy: 142.35406494140625, Temp: 2.6837849617004395, KL: 77.736083984375, Loss: 0.014254695735871792, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10223/20000], Bound: 0.36891740560531616, Entropy: 141.16087341308594, Temp: 2.683814287185669, KL: 69.75717163085938, Loss: 0.015161623246967793, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10224/20000], Bound: 0.384261816740036, Entropy: 141.73448181152344, Temp: 2.6838455200195312, KL: 73.01972961425781, Loss: 0.017290223389863968, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10225/20000], Bound: 0.40057724714279175, Entropy: 142.1728057861328, Temp: 2.6838693618774414, KL: 78.12289428710938, Loss: 0.016711924225091934, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10226/20000], Bound: 0.4307019114494324, Entropy: 140.11135864257812, Temp: 2.683896541595459, KL: 86.89207458496094, Loss: 0.017445344477891922, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10227/20000], Bound: 0.40168458223342896, Entropy: 141.5240020751953, Temp: 2.683932065963745, KL: 77.96856689453125, Loss: 0.017613857984542847, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10228/20000], Bound: 0.39529839158058167, Entropy: 142.424072265625, Temp: 2.6839640140533447, KL: 75.55976867675781, Loss: 0.01857547089457512, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10229/20000], Bound: 0.35337087512016296, Entropy: 141.86334228515625, Temp: 2.6839845180511475, KL: 65.07565307617188, Loss: 0.015750473365187645, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10230/20000], Bound: 0.36786919832229614, Entropy: 139.99880981445312, Temp: 2.6839981079101562, KL: 70.87106323242188, Loss: 0.01253415085375309, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10231/20000], Bound: 0.3913939297199249, Entropy: 142.34310913085938, Temp: 2.6840322017669678, KL: 76.04521179199219, Loss: 0.015532316640019417, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10232/20000], Bound: 0.38024693727493286, Entropy: 140.56764221191406, Temp: 2.684072732925415, KL: 73.63307189941406, Loss: 0.013985038734972477, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10233/20000], Bound: 0.3716214895248413, Entropy: 142.9335479736328, Temp: 2.6841251850128174, KL: 71.42829895019531, Loss: 0.01348438486456871, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10234/20000], Bound: 0.35123708844184875, Entropy: 141.68508911132812, Temp: 2.6841893196105957, KL: 64.19866943359375, Loss: 0.01628301478922367, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10235/20000], Bound: 0.3753691613674164, Entropy: 142.90737915039062, Temp: 2.6842379570007324, KL: 71.31948852539062, Loss: 0.015683306381106377, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10236/20000], Bound: 0.399696946144104, Entropy: 140.82183837890625, Temp: 2.6842851638793945, KL: 79.32125854492188, Loss: 0.013996695168316364, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10237/20000], Bound: 0.39978107810020447, Entropy: 140.4951171875, Temp: 2.6843502521514893, KL: 77.75135803222656, Loss: 0.0169681329280138, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10238/20000], Bound: 0.3724369704723358, Entropy: 142.36009216308594, Temp: 2.6844124794006348, KL: 68.45806884765625, Loss: 0.01945265755057335, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10239/20000], Bound: 0.401661217212677, Entropy: 140.61480712890625, Temp: 2.684446096420288, KL: 77.9710693359375, Loss: 0.017601249739527702, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10240/20000], Bound: 0.39214321970939636, Entropy: 141.77845764160156, Temp: 2.684476137161255, KL: 76.38856506347656, Loss: 0.015306780114769936, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10241/20000], Bound: 0.3891209363937378, Entropy: 141.07093811035156, Temp: 2.6845149993896484, KL: 74.5970458984375, Loss: 0.01699468120932579, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10242/20000], Bound: 0.3885997533798218, Entropy: 142.03871154785156, Temp: 2.684549331665039, KL: 75.35617065429688, Loss: 0.015297453850507736, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10243/20000], Bound: 0.4225306808948517, Entropy: 141.36961364746094, Temp: 2.6845905780792236, KL: 85.14155578613281, Loss: 0.0160067081451416, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10244/20000], Bound: 0.37718072533607483, Entropy: 142.11962890625, Temp: 2.684645652770996, KL: 69.21299743652344, Loss: 0.02057826705276966, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10245/20000], Bound: 0.3954369127750397, Entropy: 142.75450134277344, Temp: 2.684666633605957, KL: 75.53372192382812, Loss: 0.018706440925598145, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10246/20000], Bound: 0.3596894145011902, Entropy: 143.51766967773438, Temp: 2.684676170349121, KL: 65.74588012695312, Loss: 0.017791232094168663, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10247/20000], Bound: 0.3733171224594116, Entropy: 141.94754028320312, Temp: 2.684668779373169, KL: 70.50199890136719, Loss: 0.0161159448325634, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10248/20000], Bound: 0.36959660053253174, Entropy: 140.17120361328125, Temp: 2.684662103652954, KL: 69.77914428710938, Loss: 0.015487437136471272, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10249/20000], Bound: 0.3993149399757385, Entropy: 142.2592315673828, Temp: 2.6846585273742676, KL: 77.703125, Loss: 0.01680324599146843, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10250/20000], Bound: 0.39805540442466736, Entropy: 142.43101501464844, Temp: 2.684659481048584, KL: 77.27143859863281, Loss: 0.01691172644495964, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10251/20000], Bound: 0.38121944665908813, Entropy: 140.98460388183594, Temp: 2.684663772583008, KL: 72.79682922363281, Loss: 0.016071343794465065, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10252/20000], Bound: 0.3929767310619354, Entropy: 141.5558319091797, Temp: 2.6846704483032227, KL: 73.27001953125, Loss: 0.02157289907336235, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10253/20000], Bound: 0.35401126742362976, Entropy: 140.482177734375, Temp: 2.684647560119629, KL: 65.51695251464844, Loss: 0.015265066176652908, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10254/20000], Bound: 0.3906838893890381, Entropy: 141.275634765625, Temp: 2.6846258640289307, KL: 76.92840576171875, Loss: 0.013505596667528152, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10255/20000], Bound: 0.41490575671195984, Entropy: 141.5558624267578, Temp: 2.684628963470459, KL: 82.42037963867188, Loss: 0.016735808923840523, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10256/20000], Bound: 0.3879469931125641, Entropy: 140.42408752441406, Temp: 2.6846420764923096, KL: 73.49723815917969, Loss: 0.01840556412935257, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10257/20000], Bound: 0.38635990023612976, Entropy: 138.46664428710938, Temp: 2.684643507003784, KL: 73.8751220703125, Loss: 0.016840126365423203, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10258/20000], Bound: 0.391244500875473, Entropy: 141.38414001464844, Temp: 2.6846446990966797, KL: 74.46632385253906, Loss: 0.018397344276309013, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10259/20000], Bound: 0.39175164699554443, Entropy: 138.85487365722656, Temp: 2.6846365928649902, KL: 75.58154296875, Loss: 0.016597338020801544, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10260/20000], Bound: 0.39127159118652344, Entropy: 140.7643585205078, Temp: 2.6846327781677246, KL: 75.08427429199219, Loss: 0.017261149361729622, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10261/20000], Bound: 0.3967723548412323, Entropy: 142.11146545410156, Temp: 2.6846275329589844, KL: 77.21612548828125, Loss: 0.016307272017002106, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10262/20000], Bound: 0.40214839577674866, Entropy: 141.157958984375, Temp: 2.684629440307617, KL: 79.09747314453125, Loss: 0.015775438398122787, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10263/20000], Bound: 0.39136219024658203, Entropy: 143.00775146484375, Temp: 2.684643268585205, KL: 75.78572082519531, Loss: 0.016004299744963646, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10264/20000], Bound: 0.36552131175994873, Entropy: 140.0343780517578, Temp: 2.6846625804901123, KL: 69.42791748046875, Loss: 0.013990463688969612, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10265/20000], Bound: 0.36304426193237305, Entropy: 141.51409912109375, Temp: 2.6846911907196045, KL: 68.6461181640625, Loss: 0.014145324006676674, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10266/20000], Bound: 0.37365350127220154, Entropy: 141.70831298828125, Temp: 2.6847259998321533, KL: 70.17550659179688, Loss: 0.01690356619656086, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10267/20000], Bound: 0.393029123544693, Entropy: 140.55882263183594, Temp: 2.6847522258758545, KL: 75.80628967285156, Loss: 0.016878759488463402, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10268/20000], Bound: 0.357414573431015, Entropy: 140.06546020507812, Temp: 2.68477725982666, KL: 67.71861267089844, Loss: 0.01293242909014225, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10269/20000], Bound: 0.3946607708930969, Entropy: 141.6316375732422, Temp: 2.6848154067993164, KL: 76.89089965820312, Loss: 0.015753991901874542, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10270/20000], Bound: 0.387081116437912, Entropy: 140.54054260253906, Temp: 2.684859275817871, KL: 75.28193664550781, Loss: 0.014613445848226547, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10271/20000], Bound: 0.418561190366745, Entropy: 140.58946228027344, Temp: 2.684912919998169, KL: 83.32589721679688, Loss: 0.017126528546214104, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10272/20000], Bound: 0.4150889217853546, Entropy: 141.6229705810547, Temp: 2.6849706172943115, KL: 82.33078002929688, Loss: 0.017010068520903587, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10273/20000], Bound: 0.37723642587661743, Entropy: 140.49790954589844, Temp: 2.685030698776245, KL: 70.48313903808594, Loss: 0.018245618790388107, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10274/20000], Bound: 0.40756189823150635, Entropy: 141.4427947998047, Temp: 2.685072183609009, KL: 79.04043579101562, Loss: 0.018902909010648727, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10275/20000], Bound: 0.38333743810653687, Entropy: 141.45587158203125, Temp: 2.685102701187134, KL: 71.81697082519531, Loss: 0.01904156617820263, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10276/20000], Bound: 0.3512248694896698, Entropy: 142.75706481933594, Temp: 2.6851143836975098, KL: 64.74728393554688, Loss: 0.01526176929473877, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10277/20000], Bound: 0.39854708313941956, Entropy: 142.08209228515625, Temp: 2.6851227283477783, KL: 76.366455078125, Loss: 0.018872810527682304, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10278/20000], Bound: 0.4041324853897095, Entropy: 141.76815795898438, Temp: 2.6851208209991455, KL: 78.22799682617188, Loss: 0.018502390012145042, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10279/20000], Bound: 0.34669458866119385, Entropy: 144.07261657714844, Temp: 2.6851139068603516, KL: 62.86662292480469, Loss: 0.01643417961895466, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10280/20000], Bound: 0.3894988000392914, Entropy: 140.29202270507812, Temp: 2.685096025466919, KL: 75.54945373535156, Loss: 0.015432295389473438, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10281/20000], Bound: 0.3761806786060333, Entropy: 141.24642944335938, Temp: 2.6850898265838623, KL: 71.42196655273438, Loss: 0.015933338552713394, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10282/20000], Bound: 0.37823280692100525, Entropy: 141.10186767578125, Temp: 2.685086250305176, KL: 71.99261474609375, Loss: 0.01596875675022602, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10283/20000], Bound: 0.3791583180427551, Entropy: 141.73866271972656, Temp: 2.6850852966308594, KL: 72.49554443359375, Loss: 0.015528518706560135, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10284/20000], Bound: 0.3681788444519043, Entropy: 142.52105712890625, Temp: 2.6850903034210205, KL: 69.47476196289062, Loss: 0.015308203175663948, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10285/20000], Bound: 0.3999515771865845, Entropy: 142.31826782226562, Temp: 2.685098171234131, KL: 79.15483093261719, Loss: 0.014456386677920818, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10286/20000], Bound: 0.37798309326171875, Entropy: 142.40289306640625, Temp: 2.6851248741149902, KL: 71.58221435546875, Loss: 0.016599543392658234, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10287/20000], Bound: 0.3902278542518616, Entropy: 141.63548278808594, Temp: 2.6851470470428467, KL: 74.79563903808594, Loss: 0.017233779653906822, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10288/20000], Bound: 0.37854525446891785, Entropy: 140.48846435546875, Temp: 2.6851656436920166, KL: 72.44215393066406, Loss: 0.015299884602427483, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10289/20000], Bound: 0.36113405227661133, Entropy: 143.72293090820312, Temp: 2.6851892471313477, KL: 67.38706970214844, Loss: 0.015493663027882576, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10290/20000], Bound: 0.380385160446167, Entropy: 142.34249877929688, Temp: 2.6852099895477295, KL: 72.510986328125, Loss: 0.016159802675247192, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10291/20000], Bound: 0.3889870047569275, Entropy: 141.02891540527344, Temp: 2.6852307319641113, KL: 74.611328125, Loss: 0.01690172217786312, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10292/20000], Bound: 0.3849293887615204, Entropy: 141.46392822265625, Temp: 2.6852493286132812, KL: 72.27449035644531, Loss: 0.019051184877753258, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10293/20000], Bound: 0.39388972520828247, Entropy: 140.61886596679688, Temp: 2.6852505207061768, KL: 76.05416870117188, Loss: 0.016893433406949043, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10294/20000], Bound: 0.40819650888442993, Entropy: 138.68565368652344, Temp: 2.685253381729126, KL: 80.82048034667969, Loss: 0.015945404767990112, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10295/20000], Bound: 0.38620224595069885, Entropy: 140.90032958984375, Temp: 2.6852688789367676, KL: 72.85081481933594, Loss: 0.018667589873075485, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10296/20000], Bound: 0.39114245772361755, Entropy: 141.30307006835938, Temp: 2.685270309448242, KL: 76.05966186523438, Loss: 0.015380331315100193, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10297/20000], Bound: 0.3614988923072815, Entropy: 141.85171508789062, Temp: 2.6852822303771973, KL: 66.03385925292969, Loss: 0.01820496656000614, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10298/20000], Bound: 0.3927677571773529, Entropy: 142.22691345214844, Temp: 2.685274600982666, KL: 76.04121398925781, Loss: 0.016303222626447678, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10299/20000], Bound: 0.3725322186946869, Entropy: 140.0988006591797, Temp: 2.6852731704711914, KL: 71.09962463378906, Loss: 0.014590946957468987, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10300/20000], Bound: 0.38710376620292664, Entropy: 140.86080932617188, Temp: 2.685281276702881, KL: 74.625732421875, Loss: 0.015851778909564018, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10301/20000], Bound: 0.3770703077316284, Entropy: 141.6704864501953, Temp: 2.6852951049804688, KL: 71.21357727050781, Loss: 0.016798758879303932, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10302/20000], Bound: 0.372653067111969, Entropy: 140.8805389404297, Temp: 2.6853039264678955, KL: 69.09700012207031, Loss: 0.018384281545877457, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10303/20000], Bound: 0.37956690788269043, Entropy: 141.16171264648438, Temp: 2.6852965354919434, KL: 72.49337768554688, Loss: 0.01575378328561783, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10304/20000], Bound: 0.37847983837127686, Entropy: 139.46031188964844, Temp: 2.6852939128875732, KL: 71.20133972167969, Loss: 0.017576366662979126, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10305/20000], Bound: 0.37540075182914734, Entropy: 139.80908203125, Temp: 2.6852834224700928, KL: 71.99285888671875, Loss: 0.014455561526119709, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10306/20000], Bound: 0.37805449962615967, Entropy: 141.8944549560547, Temp: 2.6852853298187256, KL: 69.96221923828125, Loss: 0.01965559832751751, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10307/20000], Bound: 0.38132259249687195, Entropy: 143.355712890625, Temp: 2.685265064239502, KL: 71.90422058105469, Loss: 0.017794344574213028, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10308/20000], Bound: 0.36501839756965637, Entropy: 141.2010955810547, Temp: 2.6852381229400635, KL: 68.78887939453125, Loss: 0.014920951798558235, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10309/20000], Bound: 0.36746644973754883, Entropy: 141.26280212402344, Temp: 2.6852188110351562, KL: 69.14520263671875, Loss: 0.015546848066151142, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10310/20000], Bound: 0.3817752003669739, Entropy: 143.26254272460938, Temp: 2.6852028369903564, KL: 72.92097473144531, Loss: 0.016144245862960815, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10311/20000], Bound: 0.3901749849319458, Entropy: 141.4113006591797, Temp: 2.6851911544799805, KL: 75.80152893066406, Loss: 0.015332339331507683, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10312/20000], Bound: 0.4052903652191162, Entropy: 142.46730041503906, Temp: 2.6851913928985596, KL: 79.08273315429688, Loss: 0.017556559294462204, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10313/20000], Bound: 0.3945968449115753, Entropy: 143.35476684570312, Temp: 2.6851930618286133, KL: 75.54377746582031, Loss: 0.018231110647320747, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10314/20000], Bound: 0.41591495275497437, Entropy: 140.81008911132812, Temp: 2.685187578201294, KL: 83.14163208007812, Loss: 0.01597030833363533, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10315/20000], Bound: 0.3921841084957123, Entropy: 142.2677459716797, Temp: 2.6851985454559326, KL: 75.18486022949219, Loss: 0.017577767372131348, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10316/20000], Bound: 0.38011106848716736, Entropy: 142.432861328125, Temp: 2.6852049827575684, KL: 72.60905456542969, Loss: 0.015829849988222122, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10317/20000], Bound: 0.38519224524497986, Entropy: 143.45916748046875, Temp: 2.6852147579193115, KL: 74.57420349121094, Loss: 0.014910985715687275, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10318/20000], Bound: 0.4028507471084595, Entropy: 139.44696044921875, Temp: 2.6852352619171143, KL: 79.45974731445312, Loss: 0.015497222542762756, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10319/20000], Bound: 0.38893386721611023, Entropy: 141.9295196533203, Temp: 2.6852681636810303, KL: 73.984619140625, Loss: 0.018040088936686516, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10320/20000], Bound: 0.3824494481086731, Entropy: 142.99838256835938, Temp: 2.6852900981903076, KL: 74.71879577636719, Loss: 0.013160854578018188, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10321/20000], Bound: 0.37347546219825745, Entropy: 140.71478271484375, Temp: 2.6853325366973877, KL: 70.3011474609375, Loss: 0.01657985895872116, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10322/20000], Bound: 0.3714499771595001, Entropy: 140.87319946289062, Temp: 2.6853671073913574, KL: 70.50570678710938, Loss: 0.01512285228818655, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10323/20000], Bound: 0.40759190917015076, Entropy: 140.74790954589844, Temp: 2.685404062271118, KL: 78.94844055175781, Loss: 0.019094213843345642, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10324/20000], Bound: 0.37264707684516907, Entropy: 141.2978057861328, Temp: 2.685429573059082, KL: 70.63896179199219, Loss: 0.015511085279285908, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10325/20000], Bound: 0.3820824921131134, Entropy: 142.12405395507812, Temp: 2.6854560375213623, KL: 73.533447265625, Loss: 0.015171706676483154, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10326/20000], Bound: 0.3885103762149811, Entropy: 140.12542724609375, Temp: 2.6854889392852783, KL: 73.65234375, Loss: 0.018430326133966446, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10327/20000], Bound: 0.36185336112976074, Entropy: 141.79928588867188, Temp: 2.6855082511901855, KL: 67.56477355957031, Loss: 0.015541755594313145, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10328/20000], Bound: 0.40424013137817383, Entropy: 141.1720733642578, Temp: 2.6855249404907227, KL: 77.21244812011719, Loss: 0.02045699767768383, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10329/20000], Bound: 0.38484448194503784, Entropy: 142.07176208496094, Temp: 2.6855223178863525, KL: 74.28092956542969, Loss: 0.01527181826531887, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10330/20000], Bound: 0.39301207661628723, Entropy: 143.06800842285156, Temp: 2.6855292320251465, KL: 74.23152160644531, Loss: 0.01980876922607422, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10331/20000], Bound: 0.3643944263458252, Entropy: 141.27833557128906, Temp: 2.6855177879333496, KL: 68.74456787109375, Loss: 0.01467783935368061, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10332/20000], Bound: 0.35583746433258057, Entropy: 141.5084686279297, Temp: 2.685513496398926, KL: 65.30049133300781, Loss: 0.016621526330709457, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10333/20000], Bound: 0.3891250491142273, Entropy: 139.94065856933594, Temp: 2.685499906539917, KL: 76.30731201171875, Loss: 0.01382171455770731, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10334/20000], Bound: 0.40128037333488464, Entropy: 140.62232971191406, Temp: 2.6855082511901855, KL: 78.81907653808594, Loss: 0.015821652486920357, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10335/20000], Bound: 0.36219605803489685, Entropy: 142.12318420410156, Temp: 2.6855270862579346, KL: 66.26971435546875, Loss: 0.018132511526346207, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10336/20000], Bound: 0.3836880028247833, Entropy: 140.019287109375, Temp: 2.685526132583618, KL: 73.3946533203125, Loss: 0.01629689335823059, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10337/20000], Bound: 0.33278265595436096, Entropy: 141.0321807861328, Temp: 2.6855275630950928, KL: 59.633209228515625, Loss: 0.015393831767141819, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10338/20000], Bound: 0.39982882142066956, Entropy: 140.5618896484375, Temp: 2.685519218444824, KL: 77.36224365234375, Loss: 0.01773051917552948, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10339/20000], Bound: 0.39126572012901306, Entropy: 142.15306091308594, Temp: 2.6855101585388184, KL: 75.67529296875, Loss: 0.01616562530398369, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10340/20000], Bound: 0.3588787019252777, Entropy: 140.84576416015625, Temp: 2.6855077743530273, KL: 65.78244018554688, Loss: 0.017306221649050713, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10341/20000], Bound: 0.380169540643692, Entropy: 140.221923828125, Temp: 2.6854920387268066, KL: 72.2177734375, Loss: 0.016592370346188545, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10342/20000], Bound: 0.422665536403656, Entropy: 139.1558074951172, Temp: 2.685476779937744, KL: 84.46284484863281, Loss: 0.0173579603433609, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10343/20000], Bound: 0.3982243537902832, Entropy: 140.5806884765625, Temp: 2.685472249984741, KL: 77.6466064453125, Loss: 0.01631438359618187, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10344/20000], Bound: 0.3854035437107086, Entropy: 139.36724853515625, Temp: 2.6854751110076904, KL: 74.26983642578125, Loss: 0.015594612807035446, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10345/20000], Bound: 0.40249189734458923, Entropy: 138.67034912109375, Temp: 2.6854851245880127, KL: 78.29104614257812, Loss: 0.01747654564678669, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10346/20000], Bound: 0.4135653078556061, Entropy: 140.3000030517578, Temp: 2.685495138168335, KL: 82.01356506347656, Loss: 0.016745252534747124, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10347/20000], Bound: 0.42666804790496826, Entropy: 140.30844116210938, Temp: 2.685513973236084, KL: 85.74394226074219, Loss: 0.017271358519792557, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10348/20000], Bound: 0.37308013439178467, Entropy: 140.23394775390625, Temp: 2.6855416297912598, KL: 69.92579650878906, Loss: 0.017070118337869644, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10349/20000], Bound: 0.41489270329475403, Entropy: 141.0990753173828, Temp: 2.6855599880218506, KL: 81.47830200195312, Loss: 0.018492592498660088, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10350/20000], Bound: 0.39158886671066284, Entropy: 140.39353942871094, Temp: 2.685575246810913, KL: 76.81050109863281, Loss: 0.01422923058271408, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10351/20000], Bound: 0.37013500928878784, Entropy: 142.09519958496094, Temp: 2.685607671737671, KL: 68.82926940917969, Loss: 0.01754901185631752, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10352/20000], Bound: 0.379503458738327, Entropy: 142.07208251953125, Temp: 2.6856257915496826, KL: 72.5479736328125, Loss: 0.015621067956089973, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10353/20000], Bound: 0.382673054933548, Entropy: 141.58169555664062, Temp: 2.6856472492218018, KL: 73.49172973632812, Loss: 0.015569514594972134, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10354/20000], Bound: 0.4017888605594635, Entropy: 142.95504760742188, Temp: 2.685673236846924, KL: 79.50155639648438, Loss: 0.014834631234407425, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10355/20000], Bound: 0.38727736473083496, Entropy: 141.42660522460938, Temp: 2.6857147216796875, KL: 74.83352661132812, Loss: 0.015563298016786575, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10356/20000], Bound: 0.3900146186351776, Entropy: 141.03977966308594, Temp: 2.685760259628296, KL: 74.78729248046875, Loss: 0.017138706520199776, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10357/20000], Bound: 0.4058760702610016, Entropy: 141.909912109375, Temp: 2.685800075531006, KL: 80.37693786621094, Loss: 0.015479987487196922, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10358/20000], Bound: 0.37716153264045715, Entropy: 140.91062927246094, Temp: 2.6858510971069336, KL: 72.31350708007812, Loss: 0.014804656617343426, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10359/20000], Bound: 0.3650786280632019, Entropy: 140.24041748046875, Temp: 2.6859068870544434, KL: 69.29679870605469, Loss: 0.014012758620083332, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10360/20000], Bound: 0.4048711955547333, Entropy: 144.85218811035156, Temp: 2.6859681606292725, KL: 78.48614501953125, Loss: 0.01844131387770176, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10361/20000], Bound: 0.3521330654621124, Entropy: 142.16781616210938, Temp: 2.686018466949463, KL: 64.74819946289062, Loss: 0.015735594555735588, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10362/20000], Bound: 0.3585135042667389, Entropy: 142.45242309570312, Temp: 2.686058759689331, KL: 66.06986999511719, Loss: 0.016584889963269234, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10363/20000], Bound: 0.3965636193752289, Entropy: 140.7670440673828, Temp: 2.686086416244507, KL: 76.15689086914062, Loss: 0.01817859522998333, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10364/20000], Bound: 0.3891664743423462, Entropy: 140.71194458007812, Temp: 2.686105251312256, KL: 74.79672241210938, Loss: 0.01666237972676754, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10365/20000], Bound: 0.3722648322582245, Entropy: 141.2609405517578, Temp: 2.686123847961426, KL: 71.26612854003906, Loss: 0.014146615751087666, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10366/20000], Bound: 0.4007253050804138, Entropy: 140.4913787841797, Temp: 2.6861531734466553, KL: 77.04046630859375, Loss: 0.018831565976142883, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10367/20000], Bound: 0.38773202896118164, Entropy: 141.5540008544922, Temp: 2.6861705780029297, KL: 73.05888366699219, Loss: 0.019117828458547592, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10368/20000], Bound: 0.3898276388645172, Entropy: 141.45018005371094, Temp: 2.686171293258667, KL: 75.12950134277344, Loss: 0.016403615474700928, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10369/20000], Bound: 0.36152470111846924, Entropy: 142.22946166992188, Temp: 2.6861753463745117, KL: 66.858154296875, Loss: 0.016690436750650406, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10370/20000], Bound: 0.3725661039352417, Entropy: 141.31045532226562, Temp: 2.6861705780029297, KL: 69.25340270996094, Loss: 0.018053585663437843, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10371/20000], Bound: 0.3735951781272888, Entropy: 140.7781219482422, Temp: 2.686152696609497, KL: 70.44070434570312, Loss: 0.01639065518975258, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10372/20000], Bound: 0.3673607409000397, Entropy: 141.822021484375, Temp: 2.6861345767974854, KL: 67.90310668945312, Loss: 0.017810823395848274, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10373/20000], Bound: 0.34223565459251404, Entropy: 141.4954071044922, Temp: 2.6861040592193604, KL: 61.34454345703125, Loss: 0.016996117308735847, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10374/20000], Bound: 0.38004881143569946, Entropy: 140.73133850097656, Temp: 2.6860594749450684, KL: 71.75187683105469, Loss: 0.017399735748767853, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10375/20000], Bound: 0.394074946641922, Entropy: 142.1269073486328, Temp: 2.6860125064849854, KL: 75.55804443359375, Loss: 0.017925746738910675, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10376/20000], Bound: 0.3822519779205322, Entropy: 142.02191162109375, Temp: 2.685965061187744, KL: 71.99757385253906, Loss: 0.018126901239156723, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10377/20000], Bound: 0.3826353847980499, Entropy: 141.268310546875, Temp: 2.6859121322631836, KL: 72.42111206054688, Loss: 0.017544684931635857, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10378/20000], Bound: 0.39597487449645996, Entropy: 140.36363220214844, Temp: 2.6858577728271484, KL: 77.01296997070312, Loss: 0.016258973628282547, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10379/20000], Bound: 0.3948175311088562, Entropy: 140.05162048339844, Temp: 2.6858153343200684, KL: 76.61607360839844, Loss: 0.01636163704097271, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10380/20000], Bound: 0.3887333869934082, Entropy: 140.90525817871094, Temp: 2.6857826709747314, KL: 74.05853271484375, Loss: 0.017797937616705894, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10381/20000], Bound: 0.3385131359100342, Entropy: 141.59552001953125, Temp: 2.6857473850250244, KL: 60.06227111816406, Loss: 0.017489520832896233, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10382/20000], Bound: 0.3993525803089142, Entropy: 141.94993591308594, Temp: 2.6856935024261475, KL: 77.59767150878906, Loss: 0.017030663788318634, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10383/20000], Bound: 0.3629946708679199, Entropy: 143.3370819091797, Temp: 2.68564772605896, KL: 68.28213500976562, Loss: 0.014805254526436329, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10384/20000], Bound: 0.38353875279426575, Entropy: 140.90756225585938, Temp: 2.6856114864349365, KL: 72.91703796386719, Loss: 0.017106270417571068, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10385/20000], Bound: 0.3839430510997772, Entropy: 141.6852569580078, Temp: 2.685575485229492, KL: 73.67118835449219, Loss: 0.015920231118798256, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10386/20000], Bound: 0.38295504450798035, Entropy: 142.77162170410156, Temp: 2.6855478286743164, KL: 71.63188171386719, Loss: 0.019183387979865074, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10387/20000], Bound: 0.3731193542480469, Entropy: 142.42861938476562, Temp: 2.6855056285858154, KL: 70.19114685058594, Loss: 0.016596641391515732, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10388/20000], Bound: 0.4024050831794739, Entropy: 138.88938903808594, Temp: 2.6854639053344727, KL: 80.31034851074219, Loss: 0.013668430969119072, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10389/20000], Bound: 0.378713995218277, Entropy: 141.8888397216797, Temp: 2.685452699661255, KL: 73.01210021972656, Loss: 0.014331764541566372, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10390/20000], Bound: 0.35536712408065796, Entropy: 142.3754425048828, Temp: 2.6854560375213623, KL: 64.68486022949219, Loss: 0.017523275688290596, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10391/20000], Bound: 0.38066786527633667, Entropy: 140.6468963623047, Temp: 2.6854429244995117, KL: 72.91522216796875, Loss: 0.015561242587864399, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10392/20000], Bound: 0.40495359897613525, Entropy: 140.37911987304688, Temp: 2.6854372024536133, KL: 79.84036254882812, Loss: 0.015960682183504105, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10393/20000], Bound: 0.38711363077163696, Entropy: 140.0601348876953, Temp: 2.685443639755249, KL: 74.940673828125, Loss: 0.0152722904458642, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10394/20000], Bound: 0.37538501620292664, Entropy: 145.10110473632812, Temp: 2.685460090637207, KL: 71.68373107910156, Loss: 0.015024370513856411, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10395/20000], Bound: 0.39833498001098633, Entropy: 142.04885864257812, Temp: 2.6854822635650635, KL: 76.63554382324219, Loss: 0.01825798861682415, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10396/20000], Bound: 0.39945992827415466, Entropy: 141.7096710205078, Temp: 2.6854968070983887, KL: 78.44557189941406, Loss: 0.015509352087974548, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10397/20000], Bound: 0.38769784569740295, Entropy: 140.54425048828125, Temp: 2.6855227947235107, KL: 75.09281921386719, Loss: 0.015307030640542507, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10398/20000], Bound: 0.3871418237686157, Entropy: 141.68612670898438, Temp: 2.685556411743164, KL: 75.27226257324219, Loss: 0.014671352691948414, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10399/20000], Bound: 0.37636905908584595, Entropy: 142.06797790527344, Temp: 2.685600757598877, KL: 70.18475341796875, Loss: 0.01834186539053917, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10400/20000], Bound: 0.398466020822525, Entropy: 142.07164001464844, Temp: 2.68562650680542, KL: 78.40866088867188, Loss: 0.015030501410365105, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10401/20000], Bound: 0.36148178577423096, Entropy: 143.12542724609375, Temp: 2.6856658458709717, KL: 66.98382568359375, Loss: 0.016430139541625977, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10402/20000], Bound: 0.4041540026664734, Entropy: 140.46803283691406, Temp: 2.6856942176818848, KL: 78.49655151367188, Loss: 0.018019909039139748, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10403/20000], Bound: 0.39274337887763977, Entropy: 141.86912536621094, Temp: 2.685718059539795, KL: 76.10284423828125, Loss: 0.01617939956486225, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10404/20000], Bound: 0.397165983915329, Entropy: 141.0553741455078, Temp: 2.6857454776763916, KL: 77.74923706054688, Loss: 0.015542641282081604, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10405/20000], Bound: 0.3795304000377655, Entropy: 143.05569458007812, Temp: 2.6857821941375732, KL: 71.97270202636719, Loss: 0.0167078897356987, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10406/20000], Bound: 0.38225993514060974, Entropy: 141.878173828125, Temp: 2.6858129501342773, KL: 72.17393493652344, Loss: 0.017801601439714432, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10407/20000], Bound: 0.4128623902797699, Entropy: 141.1688995361328, Temp: 2.6858322620391846, KL: 81.05972290039062, Loss: 0.018127992749214172, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10408/20000], Bound: 0.39541298151016235, Entropy: 141.10572814941406, Temp: 2.685850143432617, KL: 76.2015380859375, Loss: 0.017460625618696213, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10409/20000], Bound: 0.3702738881111145, Entropy: 142.48178100585938, Temp: 2.6858644485473633, KL: 70.16458129882812, Loss: 0.015138767659664154, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10410/20000], Bound: 0.3737713098526001, Entropy: 140.74920654296875, Temp: 2.685882806777954, KL: 72.07376098632812, Loss: 0.01344203483313322, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10411/20000], Bound: 0.37899765372276306, Entropy: 140.96632385253906, Temp: 2.6859171390533447, KL: 71.82096862792969, Loss: 0.016705675050616264, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10412/20000], Bound: 0.37052199244499207, Entropy: 141.4606475830078, Temp: 2.685945749282837, KL: 69.52330017089844, Loss: 0.016464684158563614, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10413/20000], Bound: 0.3912489414215088, Entropy: 142.634521484375, Temp: 2.6859679222106934, KL: 74.53707885742188, Loss: 0.018279695883393288, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10414/20000], Bound: 0.36894187331199646, Entropy: 141.3455810546875, Temp: 2.685979127883911, KL: 70.24884033203125, Loss: 0.014278021641075611, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10415/20000], Bound: 0.40248045325279236, Entropy: 141.64016723632812, Temp: 2.686000108718872, KL: 78.47877502441406, Loss: 0.017125794664025307, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10416/20000], Bound: 0.40185192227363586, Entropy: 143.10231018066406, Temp: 2.6860220432281494, KL: 78.05363464355469, Loss: 0.017568660899996758, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10417/20000], Bound: 0.378778338432312, Entropy: 141.2603302001953, Temp: 2.686042070388794, KL: 71.76542663574219, Loss: 0.016692548990249634, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10418/20000], Bound: 0.3747802972793579, Entropy: 141.10702514648438, Temp: 2.6860575675964355, KL: 70.67013549804688, Loss: 0.01659400574862957, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10419/20000], Bound: 0.36949118971824646, Entropy: 142.15045166015625, Temp: 2.686068534851074, KL: 69.38104248046875, Loss: 0.01618470437824726, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10420/20000], Bound: 0.3781668543815613, Entropy: 141.66705322265625, Temp: 2.6860761642456055, KL: 70.29339599609375, Loss: 0.019105195999145508, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10421/20000], Bound: 0.41314250230789185, Entropy: 142.39007568359375, Temp: 2.6860642433166504, KL: 81.25148010253906, Loss: 0.01793142594397068, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10422/20000], Bound: 0.39593830704689026, Entropy: 141.20921325683594, Temp: 2.6860554218292236, KL: 75.78034973144531, Loss: 0.0185353122651577, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10423/20000], Bound: 0.392655611038208, Entropy: 142.05384826660156, Temp: 2.6860387325286865, KL: 77.38957214355469, Loss: 0.013739281333982944, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10424/20000], Bound: 0.4239855408668518, Entropy: 141.69873046875, Temp: 2.6860461235046387, KL: 84.98504638671875, Loss: 0.017148563638329506, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10425/20000], Bound: 0.38122496008872986, Entropy: 142.132080078125, Temp: 2.6860640048980713, KL: 73.44151306152344, Loss: 0.014886917546391487, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10426/20000], Bound: 0.3786054849624634, Entropy: 143.0790252685547, Temp: 2.6860907077789307, KL: 71.94137573242188, Loss: 0.016272753477096558, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10427/20000], Bound: 0.3895064890384674, Entropy: 142.55419921875, Temp: 2.686115264892578, KL: 76.04931640625, Loss: 0.014515955001115799, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10428/20000], Bound: 0.37653177976608276, Entropy: 141.34703063964844, Temp: 2.6861534118652344, KL: 69.91632080078125, Loss: 0.01893283985555172, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10429/20000], Bound: 0.3843516707420349, Entropy: 144.54232788085938, Temp: 2.686169385910034, KL: 74.63737487792969, Loss: 0.014348074793815613, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10430/20000], Bound: 0.37231242656707764, Entropy: 141.57568359375, Temp: 2.686199188232422, KL: 69.62950134277344, Loss: 0.01721899025142193, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10431/20000], Bound: 0.38718777894973755, Entropy: 141.5792999267578, Temp: 2.686217784881592, KL: 74.76846313476562, Loss: 0.015640584751963615, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10432/20000], Bound: 0.385812908411026, Entropy: 140.59271240234375, Temp: 2.6862423419952393, KL: 74.12240600585938, Loss: 0.016097910702228546, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10433/20000], Bound: 0.3686842620372772, Entropy: 142.2748565673828, Temp: 2.6862688064575195, KL: 68.64646911621094, Loss: 0.01712697744369507, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10434/20000], Bound: 0.37763872742652893, Entropy: 142.78231811523438, Temp: 2.6862833499908447, KL: 72.64056396484375, Loss: 0.014455239288508892, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10435/20000], Bound: 0.36355140805244446, Entropy: 142.3397674560547, Temp: 2.6863090991973877, KL: 67.92857360839844, Loss: 0.015760859474539757, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10436/20000], Bound: 0.35872578620910645, Entropy: 140.21131896972656, Temp: 2.686330556869507, KL: 65.87226867675781, Loss: 0.01706528849899769, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10437/20000], Bound: 0.40844884514808655, Entropy: 141.1229248046875, Temp: 2.686337947845459, KL: 78.83030700683594, Loss: 0.01980265974998474, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10438/20000], Bound: 0.3758672773838043, Entropy: 142.81605529785156, Temp: 2.6863322257995605, KL: 70.874755859375, Loss: 0.01679537631571293, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10439/20000], Bound: 0.3610667288303375, Entropy: 144.09747314453125, Temp: 2.6863229274749756, KL: 66.38557434082031, Loss: 0.017331667244434357, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10440/20000], Bound: 0.39410898089408875, Entropy: 139.59774780273438, Temp: 2.6863012313842773, KL: 77.98924255371094, Loss: 0.013421852141618729, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10441/20000], Bound: 0.38427454233169556, Entropy: 141.92556762695312, Temp: 2.686307191848755, KL: 73.11485290527344, Loss: 0.017141595482826233, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10442/20000], Bound: 0.37927016615867615, Entropy: 142.15097045898438, Temp: 2.6863088607788086, KL: 73.22628784179688, Loss: 0.01423955149948597, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10443/20000], Bound: 0.37926745414733887, Entropy: 141.83407592773438, Temp: 2.6863248348236084, KL: 71.67494201660156, Loss: 0.01712573692202568, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10444/20000], Bound: 0.38674265146255493, Entropy: 142.2256317138672, Temp: 2.6863341331481934, KL: 73.44398498535156, Loss: 0.017865397036075592, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10445/20000], Bound: 0.3974778354167938, Entropy: 139.837646484375, Temp: 2.686335325241089, KL: 78.67672729492188, Loss: 0.013994147069752216, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10446/20000], Bound: 0.38291865587234497, Entropy: 142.33892822265625, Temp: 2.686358690261841, KL: 72.23126220703125, Loss: 0.018054628744721413, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10447/20000], Bound: 0.38902097940444946, Entropy: 141.99447631835938, Temp: 2.6863696575164795, KL: 75.33868408203125, Loss: 0.015576913021504879, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10448/20000], Bound: 0.395080029964447, Entropy: 141.01675415039062, Temp: 2.6863882541656494, KL: 75.89988708496094, Loss: 0.017844311892986298, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10449/20000], Bound: 0.3603178858757019, Entropy: 141.67494201660156, Temp: 2.6864006519317627, KL: 66.95188903808594, Loss: 0.015886884182691574, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10450/20000], Bound: 0.3898760974407196, Entropy: 140.87716674804688, Temp: 2.686408519744873, KL: 75.54205322265625, Loss: 0.015664367005228996, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10451/20000], Bound: 0.3740905225276947, Entropy: 141.94699096679688, Temp: 2.6864240169525146, KL: 71.02679443359375, Loss: 0.015565810725092888, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10452/20000], Bound: 0.3853311538696289, Entropy: 139.93020629882812, Temp: 2.686441659927368, KL: 75.26370239257812, Loss: 0.013714752160012722, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10453/20000], Bound: 0.39945825934410095, Entropy: 139.2534942626953, Temp: 2.6864776611328125, KL: 78.44076538085938, Loss: 0.015527572482824326, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10454/20000], Bound: 0.3867405652999878, Entropy: 142.87794494628906, Temp: 2.6865227222442627, KL: 72.23150634765625, Loss: 0.020122531801462173, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10455/20000], Bound: 0.3656448721885681, Entropy: 142.1882781982422, Temp: 2.6865406036376953, KL: 67.0115966796875, Loss: 0.01856921799480915, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10456/20000], Bound: 0.39359086751937866, Entropy: 141.7716522216797, Temp: 2.6865367889404297, KL: 76.45655822753906, Loss: 0.015992984175682068, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10457/20000], Bound: 0.382022887468338, Entropy: 140.98297119140625, Temp: 2.6865410804748535, KL: 74.93960571289062, Loss: 0.01253277063369751, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10458/20000], Bound: 0.3794781565666199, Entropy: 143.39340209960938, Temp: 2.6865715980529785, KL: 71.75921630859375, Loss: 0.01708405464887619, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10459/20000], Bound: 0.3998614549636841, Entropy: 140.72250366210938, Temp: 2.6865947246551514, KL: 78.2789306640625, Loss: 0.01605282537639141, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10460/20000], Bound: 0.4007336497306824, Entropy: 143.92210388183594, Temp: 2.686624526977539, KL: 77.76052856445312, Loss: 0.017500465735793114, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10461/20000], Bound: 0.3923187851905823, Entropy: 141.31982421875, Temp: 2.6866517066955566, KL: 74.64755249023438, Loss: 0.01866462454199791, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10462/20000], Bound: 0.38270264863967896, Entropy: 141.9925079345703, Temp: 2.6866652965545654, KL: 72.90327453613281, Loss: 0.016690058633685112, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10463/20000], Bound: 0.42473143339157104, Entropy: 143.4373016357422, Temp: 2.686676502227783, KL: 84.10279846191406, Loss: 0.01922556757926941, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10464/20000], Bound: 0.3916245400905609, Entropy: 140.29202270507812, Temp: 2.6866841316223145, KL: 74.08712768554688, Loss: 0.019328448921442032, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10465/20000], Bound: 0.39394327998161316, Entropy: 141.3673095703125, Temp: 2.686675548553467, KL: 75.59027099609375, Loss: 0.017799636349081993, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10466/20000], Bound: 0.3724766969680786, Entropy: 143.5731964111328, Temp: 2.6866631507873535, KL: 70.15968322753906, Loss: 0.016323277726769447, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10467/20000], Bound: 0.381630003452301, Entropy: 142.27886962890625, Temp: 2.686650037765503, KL: 71.72274780273438, Loss: 0.018309103325009346, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10468/20000], Bound: 0.3631032705307007, Entropy: 140.77867126464844, Temp: 2.6866259574890137, KL: 67.72456359863281, Loss: 0.01590808667242527, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10469/20000], Bound: 0.4168524742126465, Entropy: 139.18458557128906, Temp: 2.686601400375366, KL: 81.38540649414062, Loss: 0.01978607289493084, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10470/20000], Bound: 0.354356050491333, Entropy: 142.00621032714844, Temp: 2.686570405960083, KL: 65.35435485839844, Loss: 0.015761030837893486, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10471/20000], Bound: 0.38472676277160645, Entropy: 141.03466796875, Temp: 2.686537504196167, KL: 74.05989074707031, Loss: 0.015629224479198456, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10472/20000], Bound: 0.3720118999481201, Entropy: 141.5712127685547, Temp: 2.6865148544311523, KL: 69.32798767089844, Loss: 0.01762307807803154, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10473/20000], Bound: 0.3780783712863922, Entropy: 142.76284790039062, Temp: 2.686483383178711, KL: 71.13258361816406, Loss: 0.017499102279543877, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10474/20000], Bound: 0.3663037419319153, Entropy: 142.76904296875, Temp: 2.6864469051361084, KL: 68.96446228027344, Loss: 0.01528070867061615, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10475/20000], Bound: 0.404620885848999, Entropy: 141.21263122558594, Temp: 2.6864168643951416, KL: 79.80195617675781, Loss: 0.015857189893722534, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10476/20000], Bound: 0.37001922726631165, Entropy: 141.9547882080078, Temp: 2.6864023208618164, KL: 68.38670349121094, Loss: 0.01831761747598648, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10477/20000], Bound: 0.4178268015384674, Entropy: 138.66476440429688, Temp: 2.6863725185394287, KL: 83.2938232421875, Loss: 0.01678488962352276, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10478/20000], Bound: 0.4143458306789398, Entropy: 140.2315216064453, Temp: 2.686357021331787, KL: 79.41545104980469, Loss: 0.022031115368008614, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10479/20000], Bound: 0.39387452602386475, Entropy: 139.7021484375, Temp: 2.6863179206848145, KL: 74.25851440429688, Loss: 0.020237447693943977, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10480/20000], Bound: 0.39245352149009705, Entropy: 139.56491088867188, Temp: 2.6862618923187256, KL: 74.76997375488281, Loss: 0.01850702054798603, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10481/20000], Bound: 0.40124648809432983, Entropy: 140.25750732421875, Temp: 2.686202049255371, KL: 78.21400451660156, Loss: 0.016936302185058594, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10482/20000], Bound: 0.39735642075538635, Entropy: 141.53245544433594, Temp: 2.686152219772339, KL: 76.35302734375, Loss: 0.018250565975904465, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10483/20000], Bound: 0.3738284111022949, Entropy: 142.8804473876953, Temp: 2.686101198196411, KL: 70.06527709960938, Loss: 0.017213186249136925, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10484/20000], Bound: 0.41198450326919556, Entropy: 141.64071655273438, Temp: 2.6860475540161133, KL: 80.84681701660156, Loss: 0.018031859770417213, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10485/20000], Bound: 0.3710256516933441, Entropy: 142.8074951171875, Temp: 2.686000108718872, KL: 71.00543212890625, Loss: 0.013973088935017586, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10486/20000], Bound: 0.3804006278514862, Entropy: 138.8980255126953, Temp: 2.6859707832336426, KL: 71.30685424804688, Loss: 0.01841641403734684, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10487/20000], Bound: 0.3766896426677704, Entropy: 141.7588653564453, Temp: 2.6859309673309326, KL: 73.04177856445312, Loss: 0.013197313994169235, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10488/20000], Bound: 0.3972387909889221, Entropy: 140.6893310546875, Temp: 2.685915946960449, KL: 77.39564514160156, Loss: 0.016242731362581253, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10489/20000], Bound: 0.41125452518463135, Entropy: 140.45741271972656, Temp: 2.6859097480773926, KL: 81.93614196777344, Loss: 0.015591762028634548, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10490/20000], Bound: 0.38221535086631775, Entropy: 141.7481231689453, Temp: 2.6859207153320312, KL: 73.62667846679688, Loss: 0.015074126422405243, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10491/20000], Bound: 0.3522902727127075, Entropy: 141.35557556152344, Temp: 2.6859405040740967, KL: 64.85635375976562, Loss: 0.015614830888807774, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10492/20000], Bound: 0.3947148025035858, Entropy: 141.35208129882812, Temp: 2.6859538555145264, KL: 75.52536010742188, Loss: 0.018337013199925423, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10493/20000], Bound: 0.39310309290885925, Entropy: 141.44383239746094, Temp: 2.6859583854675293, KL: 77.68304443359375, Loss: 0.013437045738101006, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10494/20000], Bound: 0.3796887695789337, Entropy: 143.3417510986328, Temp: 2.6859869956970215, KL: 71.79609680175781, Loss: 0.017123453319072723, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10495/20000], Bound: 0.39259254932403564, Entropy: 142.19297790527344, Temp: 2.6860077381134033, KL: 75.55732727050781, Loss: 0.01711515337228775, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10496/20000], Bound: 0.377545565366745, Entropy: 142.30064392089844, Temp: 2.6860265731811523, KL: 71.33827209472656, Loss: 0.01682712323963642, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10497/20000], Bound: 0.4078351557254791, Entropy: 141.59432983398438, Temp: 2.686039686203003, KL: 81.03410339355469, Loss: 0.015353973954916, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10498/20000], Bound: 0.38852307200431824, Entropy: 141.806640625, Temp: 2.686068534851074, KL: 75.044921875, Loss: 0.015849994495511055, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10499/20000], Bound: 0.39076733589172363, Entropy: 142.43211364746094, Temp: 2.6861014366149902, KL: 73.88261413574219, Loss: 0.019236277788877487, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10500/20000], Bound: 0.3876381516456604, Entropy: 138.98422241210938, Temp: 2.6861157417297363, KL: 74.69866943359375, Loss: 0.01601403020322323, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10501/20000], Bound: 0.3658822178840637, Entropy: 141.60986328125, Temp: 2.6861343383789062, KL: 68.99002075195312, Loss: 0.01500854454934597, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10502/20000], Bound: 0.4110621511936188, Entropy: 139.43145751953125, Temp: 2.686155319213867, KL: 80.417724609375, Loss: 0.01831270381808281, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10503/20000], Bound: 0.3714239001274109, Entropy: 140.21908569335938, Temp: 2.6861729621887207, KL: 70.39649963378906, Loss: 0.015319392085075378, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10504/20000], Bound: 0.39605554938316345, Entropy: 140.02630615234375, Temp: 2.6861932277679443, KL: 75.51026916503906, Loss: 0.01910368911921978, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10505/20000], Bound: 0.3556263744831085, Entropy: 142.5807342529297, Temp: 2.6861989498138428, KL: 65.93147277832031, Loss: 0.01534252893179655, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10506/20000], Bound: 0.4044833481311798, Entropy: 139.04302978515625, Temp: 2.6862027645111084, KL: 79.22784423828125, Loss: 0.016846992075443268, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10507/20000], Bound: 0.3999181091785431, Entropy: 141.0395965576172, Temp: 2.6862120628356934, KL: 78.51419067382812, Loss: 0.015642357990145683, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10508/20000], Bound: 0.37686118483543396, Entropy: 141.5618438720703, Temp: 2.686232566833496, KL: 71.79510498046875, Loss: 0.015612508170306683, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10509/20000], Bound: 0.42168721556663513, Entropy: 142.32936096191406, Temp: 2.68625545501709, KL: 83.99162292480469, Loss: 0.017684301361441612, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10510/20000], Bound: 0.38693755865097046, Entropy: 142.9461669921875, Temp: 2.6862826347351074, KL: 74.13018798828125, Loss: 0.01669347658753395, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10511/20000], Bound: 0.395332396030426, Entropy: 142.078857421875, Temp: 2.686307907104492, KL: 77.4864501953125, Loss: 0.01502905786037445, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10512/20000], Bound: 0.4079347848892212, Entropy: 142.7098846435547, Temp: 2.686345100402832, KL: 80.88444519042969, Loss: 0.015691611915826797, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10513/20000], Bound: 0.3843274712562561, Entropy: 140.80398559570312, Temp: 2.6863934993743896, KL: 74.99264526367188, Loss: 0.013675926253199577, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10514/20000], Bound: 0.36966946721076965, Entropy: 140.1351318359375, Temp: 2.6864571571350098, KL: 69.98674011230469, Loss: 0.0151549456641078, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10515/20000], Bound: 0.39226630330085754, Entropy: 140.77090454101562, Temp: 2.6865196228027344, KL: 76.28421020507812, Loss: 0.015588688664138317, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10516/20000], Bound: 0.37772291898727417, Entropy: 142.32920837402344, Temp: 2.6865854263305664, KL: 71.49658203125, Loss: 0.01663219928741455, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10517/20000], Bound: 0.34977948665618896, Entropy: 141.4046630859375, Temp: 2.686642646789551, KL: 64.30445861816406, Loss: 0.01535293273627758, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10518/20000], Bound: 0.3780346214771271, Entropy: 139.64663696289062, Temp: 2.686690330505371, KL: 71.65449523925781, Loss: 0.016506075859069824, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10519/20000], Bound: 0.39348170161247253, Entropy: 141.74234008789062, Temp: 2.6867318153381348, KL: 75.67347717285156, Loss: 0.01739245094358921, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10520/20000], Bound: 0.39080244302749634, Entropy: 142.3591766357422, Temp: 2.686767339706421, KL: 75.03388977050781, Loss: 0.0171185452491045, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10521/20000], Bound: 0.39129164814949036, Entropy: 140.24954223632812, Temp: 2.686798572540283, KL: 77.37521362304688, Loss: 0.013028719462454319, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10522/20000], Bound: 0.40339547395706177, Entropy: 139.39540100097656, Temp: 2.6868534088134766, KL: 79.47807312011719, Loss: 0.01578286848962307, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10523/20000], Bound: 0.3649751842021942, Entropy: 140.88385009765625, Temp: 2.686915397644043, KL: 68.39743041992188, Loss: 0.0156408678740263, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10524/20000], Bound: 0.3848886787891388, Entropy: 142.39627075195312, Temp: 2.6869709491729736, KL: 73.83891296386719, Loss: 0.01613207347691059, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10525/20000], Bound: 0.4300275146961212, Entropy: 140.45840454101562, Temp: 2.6870245933532715, KL: 84.99647521972656, Loss: 0.02061893604695797, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10526/20000], Bound: 0.3849062919616699, Entropy: 141.68447875976562, Temp: 2.687062978744507, KL: 73.77238464355469, Loss: 0.016266217455267906, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10527/20000], Bound: 0.3981015980243683, Entropy: 139.80111694335938, Temp: 2.6871001720428467, KL: 77.12281799316406, Loss: 0.017237624153494835, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10528/20000], Bound: 0.4035204350948334, Entropy: 140.27456665039062, Temp: 2.687134265899658, KL: 78.95429992675781, Loss: 0.01682988367974758, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10529/20000], Bound: 0.396813303232193, Entropy: 139.48855590820312, Temp: 2.6871707439422607, KL: 76.22419738769531, Loss: 0.01820068620145321, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10530/20000], Bound: 0.409793883562088, Entropy: 142.35057067871094, Temp: 2.68719744682312, KL: 80.15243530273438, Loss: 0.018104543909430504, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10531/20000], Bound: 0.3959946036338806, Entropy: 139.88406372070312, Temp: 2.687220811843872, KL: 76.81398010253906, Loss: 0.016653507947921753, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10532/20000], Bound: 0.369934618473053, Entropy: 139.88929748535156, Temp: 2.6872458457946777, KL: 68.71359252929688, Loss: 0.017670942470431328, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10533/20000], Bound: 0.3857361674308777, Entropy: 141.4268035888672, Temp: 2.687255859375, KL: 73.5994873046875, Loss: 0.017038729041814804, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10534/20000], Bound: 0.35520732402801514, Entropy: 143.37249755859375, Temp: 2.687262535095215, KL: 66.3895263671875, Loss: 0.014281198382377625, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10535/20000], Bound: 0.40793803334236145, Entropy: 142.5720672607422, Temp: 2.687274217605591, KL: 80.88273620605469, Loss: 0.015706686303019524, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10536/20000], Bound: 0.36062297224998474, Entropy: 141.38404846191406, Temp: 2.6872994899749756, KL: 67.29252624511719, Loss: 0.01541950460523367, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10537/20000], Bound: 0.36450138688087463, Entropy: 142.33522033691406, Temp: 2.687321901321411, KL: 70.07681274414062, Loss: 0.01227065734565258, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10538/20000], Bound: 0.3967779278755188, Entropy: 140.73626708984375, Temp: 2.6873650550842285, KL: 75.92230224609375, Loss: 0.018744688481092453, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10539/20000], Bound: 0.3767832815647125, Entropy: 140.26329040527344, Temp: 2.687394142150879, KL: 71.67668151855469, Loss: 0.01580151915550232, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10540/20000], Bound: 0.39507046341896057, Entropy: 139.8951873779297, Temp: 2.68742299079895, KL: 77.37004089355469, Loss: 0.015113316476345062, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10541/20000], Bound: 0.41168534755706787, Entropy: 139.9095458984375, Temp: 2.6874630451202393, KL: 82.56866455078125, Loss: 0.014674520120024681, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10542/20000], Bound: 0.381300151348114, Entropy: 142.55029296875, Temp: 2.6875221729278564, KL: 72.91290283203125, Loss: 0.015924569219350815, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10543/20000], Bound: 0.36679136753082275, Entropy: 141.2249755859375, Temp: 2.6875791549682617, KL: 69.90211486816406, Loss: 0.01380261592566967, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10544/20000], Bound: 0.39020371437072754, Entropy: 142.23715209960938, Temp: 2.68764328956604, KL: 75.43553161621094, Loss: 0.016052981838583946, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10545/20000], Bound: 0.35147979855537415, Entropy: 141.36167907714844, Temp: 2.6877071857452393, KL: 65.39363098144531, Loss: 0.014210409484803677, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10546/20000], Bound: 0.3627880811691284, Entropy: 142.3502655029297, Temp: 2.6877694129943848, KL: 66.56240844726562, Loss: 0.01791398413479328, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10547/20000], Bound: 0.3988705575466156, Entropy: 141.81985473632812, Temp: 2.6878082752227783, KL: 76.80197143554688, Loss: 0.01826539635658264, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10548/20000], Bound: 0.38746798038482666, Entropy: 139.6487579345703, Temp: 2.687837839126587, KL: 73.92141723632812, Loss: 0.01738378219306469, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10549/20000], Bound: 0.3876056969165802, Entropy: 140.7626953125, Temp: 2.6878602504730225, KL: 76.01849365234375, Loss: 0.013557680882513523, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10550/20000], Bound: 0.37477684020996094, Entropy: 142.55474853515625, Temp: 2.6879022121429443, KL: 71.02125549316406, Loss: 0.015954548493027687, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10551/20000], Bound: 0.3827457129955292, Entropy: 142.00965881347656, Temp: 2.687941074371338, KL: 72.78451538085938, Loss: 0.016945499926805496, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10552/20000], Bound: 0.35893702507019043, Entropy: 140.6491241455078, Temp: 2.6879732608795166, KL: 66.02595520019531, Loss: 0.016901444643735886, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10553/20000], Bound: 0.36850959062576294, Entropy: 142.89581298828125, Temp: 2.687991142272949, KL: 69.88191223144531, Loss: 0.014750163070857525, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10554/20000], Bound: 0.377536803483963, Entropy: 139.53167724609375, Temp: 2.688014030456543, KL: 72.31636047363281, Loss: 0.015019983984529972, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10555/20000], Bound: 0.3710830807685852, Entropy: 141.59141540527344, Temp: 2.6880435943603516, KL: 69.92610168457031, Loss: 0.01602989435195923, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10556/20000], Bound: 0.387044221162796, Entropy: 142.09765625, Temp: 2.6880691051483154, KL: 72.53810119628906, Loss: 0.019728971645236015, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10557/20000], Bound: 0.3805457353591919, Entropy: 140.76025390625, Temp: 2.6880717277526855, KL: 71.99757385253906, Loss: 0.017226586118340492, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10558/20000], Bound: 0.357136070728302, Entropy: 141.55712890625, Temp: 2.688068389892578, KL: 65.741943359375, Loss: 0.016493331640958786, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10559/20000], Bound: 0.3975323438644409, Entropy: 140.0459747314453, Temp: 2.688056468963623, KL: 78.29737854003906, Loss: 0.014748318120837212, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10560/20000], Bound: 0.37409982085227966, Entropy: 140.5735626220703, Temp: 2.688063144683838, KL: 69.24873352050781, Loss: 0.01889241859316826, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10561/20000], Bound: 0.3938167691230774, Entropy: 140.5503387451172, Temp: 2.688049793243408, KL: 75.93641662597656, Loss: 0.017099004238843918, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10562/20000], Bound: 0.39939433336257935, Entropy: 140.59423828125, Temp: 2.688037872314453, KL: 77.26454162597656, Loss: 0.01769627444446087, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10563/20000], Bound: 0.3692943751811981, Entropy: 140.92189025878906, Temp: 2.6880249977111816, KL: 69.67216491699219, Loss: 0.015555270947515965, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10564/20000], Bound: 0.4167026877403259, Entropy: 140.603271484375, Temp: 2.6880154609680176, KL: 80.54899597167969, Loss: 0.02127099223434925, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10565/20000], Bound: 0.3777623772621155, Entropy: 140.40151977539062, Temp: 2.6879873275756836, KL: 71.79179382324219, Loss: 0.016116192564368248, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10566/20000], Bound: 0.3937874734401703, Entropy: 138.42962646484375, Temp: 2.6879630088806152, KL: 77.36392211914062, Loss: 0.014426798559725285, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10567/20000], Bound: 0.3909378945827484, Entropy: 141.0222625732422, Temp: 2.6879594326019287, KL: 75.24992370605469, Loss: 0.01680150255560875, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10568/20000], Bound: 0.378440797328949, Entropy: 141.1340789794922, Temp: 2.6879570484161377, KL: 71.87403869628906, Loss: 0.016326162964105606, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10569/20000], Bound: 0.35673394799232483, Entropy: 141.85275268554688, Temp: 2.687954902648926, KL: 66.40934753417969, Loss: 0.015042083337903023, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10570/20000], Bound: 0.3940756618976593, Entropy: 139.7167510986328, Temp: 2.6879537105560303, KL: 77.22244262695312, Loss: 0.014847769401967525, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10571/20000], Bound: 0.3689211905002594, Entropy: 140.06700134277344, Temp: 2.6879684925079346, KL: 68.91348266601562, Loss: 0.016768796369433403, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10572/20000], Bound: 0.38042014837265015, Entropy: 140.2079620361328, Temp: 2.6879749298095703, KL: 72.77731323242188, Loss: 0.015707848593592644, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10573/20000], Bound: 0.39841267466545105, Entropy: 141.54212951660156, Temp: 2.687985420227051, KL: 78.00617980957031, Loss: 0.01577443815767765, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10574/20000], Bound: 0.42268577218055725, Entropy: 144.48138427734375, Temp: 2.6880059242248535, KL: 85.46165466308594, Loss: 0.015539979562163353, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10575/20000], Bound: 0.3484819233417511, Entropy: 141.86705017089844, Temp: 2.6880452632904053, KL: 63.28767395019531, Loss: 0.016587771475315094, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10576/20000], Bound: 0.3591122031211853, Entropy: 142.47276306152344, Temp: 2.68806791305542, KL: 66.37762451171875, Loss: 0.016339270398020744, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10577/20000], Bound: 0.40554022789001465, Entropy: 141.40806579589844, Temp: 2.6880810260772705, KL: 79.90669250488281, Loss: 0.016191983595490456, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10578/20000], Bound: 0.38318726420402527, Entropy: 140.2699737548828, Temp: 2.688103199005127, KL: 70.65568542480469, Loss: 0.021144689992070198, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10579/20000], Bound: 0.3799762427806854, Entropy: 142.47711181640625, Temp: 2.68809175491333, KL: 72.20187377929688, Loss: 0.016540898010134697, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10580/20000], Bound: 0.3927238881587982, Entropy: 142.81605529785156, Temp: 2.688080310821533, KL: 75.34591674804688, Loss: 0.017599543556571007, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10581/20000], Bound: 0.3840593695640564, Entropy: 141.52304077148438, Temp: 2.688066244125366, KL: 73.58427429199219, Loss: 0.016167687252163887, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10582/20000], Bound: 0.3752957582473755, Entropy: 140.94943237304688, Temp: 2.688056707382202, KL: 70.06047058105469, Loss: 0.018019629642367363, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10583/20000], Bound: 0.41524505615234375, Entropy: 141.6586456298828, Temp: 2.688035011291504, KL: 83.35658264160156, Loss: 0.015223476104438305, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10584/20000], Bound: 0.3840431869029999, Entropy: 141.76548767089844, Temp: 2.6880359649658203, KL: 73.65638732910156, Loss: 0.016024528071284294, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10585/20000], Bound: 0.4171086549758911, Entropy: 141.63070678710938, Temp: 2.6880409717559814, KL: 83.96487426757812, Loss: 0.015147601254284382, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10586/20000], Bound: 0.3960537314414978, Entropy: 140.20301818847656, Temp: 2.6880667209625244, KL: 77.78550720214844, Loss: 0.01488707959651947, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10587/20000], Bound: 0.38875719904899597, Entropy: 141.29122924804688, Temp: 2.688106060028076, KL: 75.52691650390625, Loss: 0.015100118704140186, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10588/20000], Bound: 0.3666279911994934, Entropy: 143.55494689941406, Temp: 2.6881535053253174, KL: 68.30917358398438, Loss: 0.01668464206159115, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10589/20000], Bound: 0.3899105191230774, Entropy: 141.2734375, Temp: 2.6881890296936035, KL: 74.57342529296875, Loss: 0.01750197634100914, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10590/20000], Bound: 0.390063613653183, Entropy: 141.06085205078125, Temp: 2.6882171630859375, KL: 74.87124633789062, Loss: 0.017031695693731308, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10591/20000], Bound: 0.3897484838962555, Entropy: 142.77218627929688, Temp: 2.688241481781006, KL: 75.46925354003906, Loss: 0.015748044475913048, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10592/20000], Bound: 0.3774315118789673, Entropy: 142.080322265625, Temp: 2.6882712841033936, KL: 72.09506225585938, Loss: 0.015377597883343697, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10593/20000], Bound: 0.37587231397628784, Entropy: 140.1464080810547, Temp: 2.6883039474487305, KL: 71.40185546875, Loss: 0.015834279358386993, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10594/20000], Bound: 0.389769971370697, Entropy: 143.08114624023438, Temp: 2.688335657119751, KL: 73.85081481933594, Loss: 0.01877075806260109, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10595/20000], Bound: 0.3869352638721466, Entropy: 141.41995239257812, Temp: 2.6883513927459717, KL: 73.04325866699219, Loss: 0.01873263716697693, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10596/20000], Bound: 0.37920698523521423, Entropy: 142.76812744140625, Temp: 2.68835186958313, KL: 71.74404907226562, Loss: 0.016981981694698334, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10597/20000], Bound: 0.4009550213813782, Entropy: 140.51026916503906, Temp: 2.6883480548858643, KL: 77.94662475585938, Loss: 0.017293529585003853, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10598/20000], Bound: 0.37182334065437317, Entropy: 140.20263671875, Temp: 2.6883456707000732, KL: 69.72360229492188, Loss: 0.016801603138446808, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10599/20000], Bound: 0.368150919675827, Entropy: 142.8221893310547, Temp: 2.688337802886963, KL: 69.54612731933594, Loss: 0.015188364312052727, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10600/20000], Bound: 0.3752727508544922, Entropy: 141.01780700683594, Temp: 2.6883344650268555, KL: 71.14527893066406, Loss: 0.015991900116205215, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10601/20000], Bound: 0.3936117887496948, Entropy: 140.07701110839844, Temp: 2.6883325576782227, KL: 76.53439331054688, Loss: 0.015877263620495796, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10602/20000], Bound: 0.38318437337875366, Entropy: 141.59326171875, Temp: 2.6883392333984375, KL: 72.92752075195312, Loss: 0.016919488087296486, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10603/20000], Bound: 0.3872276544570923, Entropy: 140.7892303466797, Temp: 2.688342571258545, KL: 74.08842468261719, Loss: 0.016947230324149132, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10604/20000], Bound: 0.4050371050834656, Entropy: 141.2468719482422, Temp: 2.6883444786071777, KL: 80.18653869628906, Loss: 0.015393990091979504, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10605/20000], Bound: 0.40246933698654175, Entropy: 140.50718688964844, Temp: 2.688361644744873, KL: 79.45016479492188, Loss: 0.015336422249674797, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10606/20000], Bound: 0.3759676516056061, Entropy: 141.1715850830078, Temp: 2.6883926391601562, KL: 70.83009338378906, Loss: 0.01694929413497448, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10607/20000], Bound: 0.35554683208465576, Entropy: 143.1781768798828, Temp: 2.6884148120880127, KL: 65.67764282226562, Loss: 0.0157905463129282, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10608/20000], Bound: 0.4290030598640442, Entropy: 140.7230224609375, Temp: 2.688430070877075, KL: 86.92486572265625, Loss: 0.016455013304948807, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10609/20000], Bound: 0.3731463849544525, Entropy: 142.25833129882812, Temp: 2.6884613037109375, KL: 70.88389587402344, Loss: 0.015347208827733994, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10610/20000], Bound: 0.3907792568206787, Entropy: 140.49034118652344, Temp: 2.6884937286376953, KL: 76.27566528320312, Loss: 0.01481233537197113, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10611/20000], Bound: 0.3753909468650818, Entropy: 143.11924743652344, Temp: 2.68853759765625, KL: 71.73953247070312, Loss: 0.014951544813811779, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10612/20000], Bound: 0.3947173058986664, Entropy: 143.72694396972656, Temp: 2.6885855197906494, KL: 75.57620239257812, Loss: 0.018267497420310974, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10613/20000], Bound: 0.3798997402191162, Entropy: 140.55072021484375, Temp: 2.6886208057403564, KL: 71.72665405273438, Loss: 0.017388222739100456, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10614/20000], Bound: 0.38966652750968933, Entropy: 141.3774871826172, Temp: 2.688645601272583, KL: 76.80403137207031, Loss: 0.013225042261183262, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10615/20000], Bound: 0.3799951672554016, Entropy: 141.4003448486328, Temp: 2.688692808151245, KL: 72.47190856933594, Loss: 0.016054119914770126, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10616/20000], Bound: 0.3744646906852722, Entropy: 140.5665740966797, Temp: 2.688737392425537, KL: 70.85560607910156, Loss: 0.016103532165288925, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10617/20000], Bound: 0.38306015729904175, Entropy: 141.6486053466797, Temp: 2.6887776851654053, KL: 72.29182434082031, Loss: 0.018038485199213028, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10618/20000], Bound: 0.3856145143508911, Entropy: 138.3634490966797, Temp: 2.6888034343719482, KL: 74.25764465332031, Loss: 0.01576279103755951, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10619/20000], Bound: 0.4089318513870239, Entropy: 139.5858612060547, Temp: 2.6888327598571777, KL: 81.00514221191406, Loss: 0.01605214737355709, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10620/20000], Bound: 0.41980525851249695, Entropy: 139.9510498046875, Temp: 2.6888718605041504, KL: 81.73262023925781, Loss: 0.020840534940361977, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10621/20000], Bound: 0.3935442566871643, Entropy: 141.0153350830078, Temp: 2.6888914108276367, KL: 76.85981750488281, Loss: 0.015240656211972237, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10622/20000], Bound: 0.37829074263572693, Entropy: 142.36846923828125, Temp: 2.6889216899871826, KL: 71.69587707519531, Loss: 0.016585495322942734, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10623/20000], Bound: 0.40554288029670715, Entropy: 142.33079528808594, Temp: 2.6889467239379883, KL: 79.10301208496094, Loss: 0.017696984112262726, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10624/20000], Bound: 0.4020061492919922, Entropy: 139.36016845703125, Temp: 2.688969612121582, KL: 78.87937927246094, Loss: 0.016147391870617867, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10625/20000], Bound: 0.38578709959983826, Entropy: 142.55372619628906, Temp: 2.688999652862549, KL: 74.36456298828125, Loss: 0.01565922051668167, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10626/20000], Bound: 0.38412153720855713, Entropy: 141.5441131591797, Temp: 2.6890337467193604, KL: 71.94465637207031, Loss: 0.01925874873995781, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10627/20000], Bound: 0.3800879418849945, Entropy: 141.5741729736328, Temp: 2.6890459060668945, KL: 69.5419921875, Loss: 0.021554941311478615, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10628/20000], Bound: 0.3990168571472168, Entropy: 139.2059783935547, Temp: 2.689021110534668, KL: 77.06880187988281, Loss: 0.01786119118332863, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10629/20000], Bound: 0.36948323249816895, Entropy: 141.4322967529297, Temp: 2.688995122909546, KL: 69.98699951171875, Loss: 0.015077882446348667, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10630/20000], Bound: 0.3792363107204437, Entropy: 139.85435485839844, Temp: 2.6889772415161133, KL: 73.27720642089844, Loss: 0.01415218785405159, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10631/20000], Bound: 0.3683127164840698, Entropy: 141.556640625, Temp: 2.6889758110046387, KL: 67.26625061035156, Loss: 0.01951846107840538, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10632/20000], Bound: 0.3848009407520294, Entropy: 141.4140167236328, Temp: 2.6889488697052, KL: 74.97126770019531, Loss: 0.013997227884829044, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10633/20000], Bound: 0.3841281235218048, Entropy: 142.61962890625, Temp: 2.6889424324035645, KL: 74.41828918457031, Loss: 0.014661948196589947, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10634/20000], Bound: 0.3978063464164734, Entropy: 141.64935302734375, Temp: 2.6889495849609375, KL: 76.69059753417969, Loss: 0.017896384000778198, Learning Rate: 0.00014242684529828988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10635/20000], Bound: 0.39209219813346863, Entropy: 141.04440307617188, Temp: 2.6889522075653076, KL: 75.42665100097656, Loss: 0.01711212284862995, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10636/20000], Bound: 0.41382068395614624, Entropy: 141.53469848632812, Temp: 2.6889541149139404, KL: 79.9591064453125, Loss: 0.020746883004903793, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10637/20000], Bound: 0.3821897804737091, Entropy: 141.83407592773438, Temp: 2.6889383792877197, KL: 72.44638061523438, Loss: 0.017283501103520393, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10638/20000], Bound: 0.3855006992816925, Entropy: 140.75277709960938, Temp: 2.6889185905456543, KL: 73.17767333984375, Loss: 0.017710497602820396, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10639/20000], Bound: 0.36275383830070496, Entropy: 140.6288299560547, Temp: 2.6888935565948486, KL: 68.09696960449219, Loss: 0.015050634741783142, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10640/20000], Bound: 0.39036867022514343, Entropy: 141.25616455078125, Temp: 2.6888740062713623, KL: 74.79771423339844, Loss: 0.0173406470566988, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10641/20000], Bound: 0.3909871578216553, Entropy: 142.46368408203125, Temp: 2.6888532638549805, KL: 74.66188049316406, Loss: 0.01793016493320465, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10642/20000], Bound: 0.3937437832355499, Entropy: 141.53976440429688, Temp: 2.6888279914855957, KL: 74.35655212402344, Loss: 0.020004181191325188, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10643/20000], Bound: 0.40088996291160583, Entropy: 142.89134216308594, Temp: 2.6887853145599365, KL: 79.31716918945312, Loss: 0.014713194221258163, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10644/20000], Bound: 0.3728336989879608, Entropy: 141.49012756347656, Temp: 2.68876576423645, KL: 70.40309143066406, Loss: 0.016077803447842598, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10645/20000], Bound: 0.38702142238616943, Entropy: 143.03240966796875, Temp: 2.6887476444244385, KL: 72.91780090332031, Loss: 0.019015947356820107, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10646/20000], Bound: 0.39441829919815063, Entropy: 140.02542114257812, Temp: 2.688715696334839, KL: 76.54888916015625, Loss: 0.01629587821662426, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10647/20000], Bound: 0.3839834928512573, Entropy: 140.76922607421875, Temp: 2.688692808151245, KL: 73.37994384765625, Loss: 0.016512388363480568, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10648/20000], Bound: 0.4084216356277466, Entropy: 139.75643920898438, Temp: 2.6886725425720215, KL: 78.36892700195312, Loss: 0.02066720277070999, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10649/20000], Bound: 0.4008769690990448, Entropy: 141.33651733398438, Temp: 2.68863582611084, KL: 76.66398620605469, Loss: 0.019638419151306152, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10650/20000], Bound: 0.3655630350112915, Entropy: 139.92152404785156, Temp: 2.6885876655578613, KL: 70.05401611328125, Loss: 0.012882757931947708, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10651/20000], Bound: 0.3995775580406189, Entropy: 142.45358276367188, Temp: 2.688563346862793, KL: 77.00592041015625, Loss: 0.018283477053046227, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10652/20000], Bound: 0.39795854687690735, Entropy: 139.82696533203125, Temp: 2.6885359287261963, KL: 76.81436157226562, Loss: 0.01774616725742817, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10653/20000], Bound: 0.3724878430366516, Entropy: 140.1585693359375, Temp: 2.6885082721710205, KL: 69.94166564941406, Loss: 0.016750114038586617, Learning Rate: 0.00014242684529828988\n",
      "Epoch [10654/20000], Bound: 0.38169631361961365, Entropy: 142.5133514404297, Temp: 2.6884870529174805, KL: 74.54429626464844, Loss: 0.013112355023622513, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10655/20000], Bound: 0.37624821066856384, Entropy: 143.21615600585938, Temp: 2.6884841918945312, KL: 71.13877868652344, Loss: 0.01652573235332966, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10656/20000], Bound: 0.38695594668388367, Entropy: 141.4742889404297, Temp: 2.6884796619415283, KL: 74.38578796386719, Loss: 0.016248080879449844, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10657/20000], Bound: 0.3866160213947296, Entropy: 139.97262573242188, Temp: 2.6884782314300537, KL: 73.83969116210938, Loss: 0.017079437151551247, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10658/20000], Bound: 0.3789038360118866, Entropy: 140.34237670898438, Temp: 2.6884751319885254, KL: 72.6170654296875, Loss: 0.015196871012449265, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10659/20000], Bound: 0.37128573656082153, Entropy: 140.19857788085938, Temp: 2.6884777545928955, KL: 70.78622436523438, Loss: 0.01454134937375784, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10660/20000], Bound: 0.430568128824234, Entropy: 141.93582153320312, Temp: 2.6884868144989014, KL: 87.78355407714844, Loss: 0.015763841569423676, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10661/20000], Bound: 0.4038547873497009, Entropy: 140.61351013183594, Temp: 2.6885106563568115, KL: 80.03266906738281, Loss: 0.01502405945211649, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10662/20000], Bound: 0.3762919008731842, Entropy: 142.39205932617188, Temp: 2.688544511795044, KL: 70.79559326171875, Loss: 0.017187809571623802, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10663/20000], Bound: 0.3565819561481476, Entropy: 139.84335327148438, Temp: 2.688570261001587, KL: 67.26263427734375, Loss: 0.013381163589656353, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10664/20000], Bound: 0.39636120200157166, Entropy: 139.773193359375, Temp: 2.6886022090911865, KL: 77.60093688964844, Loss: 0.015404864214360714, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10665/20000], Bound: 0.3890317976474762, Entropy: 141.33413696289062, Temp: 2.6886394023895264, KL: 75.34405517578125, Loss: 0.015594739466905594, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10666/20000], Bound: 0.36402836441993713, Entropy: 140.1818389892578, Temp: 2.6886792182922363, KL: 67.48397827148438, Loss: 0.01685691997408867, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10667/20000], Bound: 0.37300175428390503, Entropy: 141.73439025878906, Temp: 2.688708543777466, KL: 70.06108093261719, Loss: 0.016802627593278885, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10668/20000], Bound: 0.3947817087173462, Entropy: 141.35479736328125, Temp: 2.6887311935424805, KL: 76.21383666992188, Loss: 0.017118385061621666, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10669/20000], Bound: 0.4018896818161011, Entropy: 140.86045837402344, Temp: 2.688751459121704, KL: 78.00672912597656, Loss: 0.01770336739718914, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10670/20000], Bound: 0.39736923575401306, Entropy: 142.88919067382812, Temp: 2.6887691020965576, KL: 78.53025817871094, Loss: 0.014232947491109371, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10671/20000], Bound: 0.36766764521598816, Entropy: 140.83494567871094, Temp: 2.6887993812561035, KL: 69.73495483398438, Loss: 0.014586200006306171, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10672/20000], Bound: 0.3701569437980652, Entropy: 142.24330139160156, Temp: 2.6888322830200195, KL: 70.01263427734375, Loss: 0.01538518164306879, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10673/20000], Bound: 0.41536492109298706, Entropy: 143.04759216308594, Temp: 2.6888644695281982, KL: 81.41525268554688, Loss: 0.01891067437827587, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10674/20000], Bound: 0.38121458888053894, Entropy: 140.54689025878906, Temp: 2.688890218734741, KL: 73.5833740234375, Loss: 0.014644157141447067, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10675/20000], Bound: 0.37294653058052063, Entropy: 141.01248168945312, Temp: 2.6889219284057617, KL: 71.26014709472656, Loss: 0.014545395039021969, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10676/20000], Bound: 0.38712042570114136, Entropy: 141.8849639892578, Temp: 2.688957452774048, KL: 72.24221801757812, Loss: 0.020327577367424965, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10677/20000], Bound: 0.36400434374809265, Entropy: 142.2200927734375, Temp: 2.6889724731445312, KL: 68.89871215820312, Loss: 0.014215956442058086, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10678/20000], Bound: 0.3769296705722809, Entropy: 141.68124389648438, Temp: 2.6889922618865967, KL: 70.86175537109375, Loss: 0.017409102991223335, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10679/20000], Bound: 0.40257588028907776, Entropy: 140.26095581054688, Temp: 2.6890041828155518, KL: 77.22343444824219, Loss: 0.01954277791082859, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10680/20000], Bound: 0.38964810967445374, Entropy: 142.11341857910156, Temp: 2.6890056133270264, KL: 74.353759765625, Loss: 0.017774904146790504, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10681/20000], Bound: 0.3932510018348694, Entropy: 139.43093872070312, Temp: 2.689002275466919, KL: 77.02734375, Loss: 0.014769782312214375, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10682/20000], Bound: 0.3294064998626709, Entropy: 144.57687377929688, Temp: 2.6890106201171875, KL: 57.519805908203125, Loss: 0.017653532326221466, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10683/20000], Bound: 0.40285640954971313, Entropy: 140.54530334472656, Temp: 2.6889986991882324, KL: 79.49003601074219, Loss: 0.015483791939914227, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10684/20000], Bound: 0.3752136528491974, Entropy: 142.07977294921875, Temp: 2.688997983932495, KL: 70.87440490722656, Loss: 0.016469823196530342, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10685/20000], Bound: 0.3830149471759796, Entropy: 142.88067626953125, Temp: 2.688995361328125, KL: 73.79035949707031, Loss: 0.015229503624141216, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10686/20000], Bound: 0.367517352104187, Entropy: 140.03524780273438, Temp: 2.6889994144439697, KL: 69.60139465332031, Loss: 0.01475706696510315, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10687/20000], Bound: 0.390746533870697, Entropy: 141.8374481201172, Temp: 2.6890079975128174, KL: 76.2801513671875, Loss: 0.014791293069720268, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10688/20000], Bound: 0.3989907205104828, Entropy: 139.79580688476562, Temp: 2.689025640487671, KL: 78.74374389648438, Loss: 0.014732422307133675, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10689/20000], Bound: 0.3910236954689026, Entropy: 140.80189514160156, Temp: 2.689054250717163, KL: 76.15516662597656, Loss: 0.015175304375588894, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10690/20000], Bound: 0.408133327960968, Entropy: 140.916015625, Temp: 2.6890885829925537, KL: 79.23507690429688, Loss: 0.018899107351899147, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10691/20000], Bound: 0.3696914315223694, Entropy: 141.68345642089844, Temp: 2.689114570617676, KL: 70.48663330078125, Loss: 0.01426001638174057, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10692/20000], Bound: 0.3738163709640503, Entropy: 142.85409545898438, Temp: 2.689145565032959, KL: 70.99679565429688, Loss: 0.015499467961490154, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10693/20000], Bound: 0.37826165556907654, Entropy: 141.69436645507812, Temp: 2.689175844192505, KL: 71.69145202636719, Loss: 0.0165803674608469, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10694/20000], Bound: 0.3752218186855316, Entropy: 143.5555877685547, Temp: 2.689202070236206, KL: 71.2884521484375, Loss: 0.015706099569797516, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10695/20000], Bound: 0.3804742991924286, Entropy: 139.83583068847656, Temp: 2.689227342605591, KL: 71.02362060546875, Loss: 0.019008854404091835, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10696/20000], Bound: 0.35044246912002563, Entropy: 141.71734619140625, Temp: 2.689237117767334, KL: 63.10557556152344, Loss: 0.017942314967513084, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10697/20000], Bound: 0.36365848779678345, Entropy: 141.84799194335938, Temp: 2.6892311573028564, KL: 67.06178283691406, Loss: 0.01745213195681572, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10698/20000], Bound: 0.3902203440666199, Entropy: 141.59088134765625, Temp: 2.689215898513794, KL: 74.60501098632812, Loss: 0.017621248960494995, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10699/20000], Bound: 0.3825591802597046, Entropy: 141.49855041503906, Temp: 2.6891987323760986, KL: 72.52572631835938, Loss: 0.01733716018497944, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10700/20000], Bound: 0.3734533488750458, Entropy: 141.5328826904297, Temp: 2.6891794204711914, KL: 70.98580932617188, Loss: 0.015327190980315208, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10701/20000], Bound: 0.3785378336906433, Entropy: 141.79342651367188, Temp: 2.6891653537750244, KL: 69.09660339355469, Loss: 0.02155275270342827, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10702/20000], Bound: 0.4162485599517822, Entropy: 140.46499633789062, Temp: 2.689127206802368, KL: 80.88203430175781, Loss: 0.020404867827892303, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10703/20000], Bound: 0.39235034584999084, Entropy: 141.372314453125, Temp: 2.6890830993652344, KL: 74.56097412109375, Loss: 0.01886400394141674, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10704/20000], Bound: 0.37366271018981934, Entropy: 139.35867309570312, Temp: 2.6890347003936768, KL: 70.26048278808594, Loss: 0.016785910353064537, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10705/20000], Bound: 0.3762088119983673, Entropy: 141.1735382080078, Temp: 2.6889872550964355, KL: 72.87228393554688, Loss: 0.013285641558468342, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10706/20000], Bound: 0.37556371092796326, Entropy: 141.78744506835938, Temp: 2.6889586448669434, KL: 71.02598571777344, Loss: 0.01637427695095539, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10707/20000], Bound: 0.36743950843811035, Entropy: 140.28919982910156, Temp: 2.688931465148926, KL: 70.12677001953125, Loss: 0.013738526031374931, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10708/20000], Bound: 0.37521249055862427, Entropy: 139.51658630371094, Temp: 2.6889169216156006, KL: 72.567138671875, Loss: 0.013320907950401306, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10709/20000], Bound: 0.38649243116378784, Entropy: 140.15499877929688, Temp: 2.6889171600341797, KL: 75.24714660644531, Loss: 0.014399223029613495, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10710/20000], Bound: 0.3692653775215149, Entropy: 143.15609741210938, Temp: 2.6889283657073975, KL: 69.20777893066406, Loss: 0.016411107033491135, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10711/20000], Bound: 0.35773834586143494, Entropy: 142.88291931152344, Temp: 2.6889355182647705, KL: 66.19308471679688, Loss: 0.015973903238773346, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10712/20000], Bound: 0.39137861132621765, Entropy: 140.99685668945312, Temp: 2.6889381408691406, KL: 75.51602172851562, Loss: 0.016556207090616226, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10713/20000], Bound: 0.3907903730869293, Entropy: 140.23619079589844, Temp: 2.6889426708221436, KL: 75.33491516113281, Loss: 0.01657218113541603, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10714/20000], Bound: 0.37368202209472656, Entropy: 140.9901580810547, Temp: 2.688948392868042, KL: 71.014892578125, Loss: 0.015392658300697803, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10715/20000], Bound: 0.3737676739692688, Entropy: 141.38177490234375, Temp: 2.6889564990997314, KL: 69.97384643554688, Loss: 0.017374109476804733, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10716/20000], Bound: 0.3955591022968292, Entropy: 140.38279724121094, Temp: 2.688957452774048, KL: 76.20857238769531, Loss: 0.017556840553879738, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10717/20000], Bound: 0.3801422119140625, Entropy: 140.85971069335938, Temp: 2.6889564990997314, KL: 72.56289672851562, Loss: 0.01596623845398426, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10718/20000], Bound: 0.3703232705593109, Entropy: 139.06309509277344, Temp: 2.688957691192627, KL: 70.71035766601562, Loss: 0.014176883734762669, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10719/20000], Bound: 0.37814074754714966, Entropy: 142.7027587890625, Temp: 2.688966989517212, KL: 72.00199890136719, Loss: 0.01593637280166149, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10720/20000], Bound: 0.38478928804397583, Entropy: 141.40390014648438, Temp: 2.688977003097534, KL: 74.27833557128906, Loss: 0.015279686078429222, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10721/20000], Bound: 0.39771148562431335, Entropy: 139.58509826660156, Temp: 2.688992500305176, KL: 79.48033142089844, Loss: 0.012657161802053452, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10722/20000], Bound: 0.39732488989830017, Entropy: 141.58328247070312, Temp: 2.689028263092041, KL: 78.19192504882812, Loss: 0.014840390533208847, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10723/20000], Bound: 0.39123958349227905, Entropy: 141.43519592285156, Temp: 2.689072370529175, KL: 74.57077026367188, Loss: 0.018239207565784454, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10724/20000], Bound: 0.38943085074424744, Entropy: 143.1228485107422, Temp: 2.689105987548828, KL: 73.59719848632812, Loss: 0.019064292311668396, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10725/20000], Bound: 0.38439804315567017, Entropy: 141.5345458984375, Temp: 2.6891255378723145, KL: 72.33822631835938, Loss: 0.018677042797207832, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10726/20000], Bound: 0.3833344876766205, Entropy: 139.76841735839844, Temp: 2.6891329288482666, KL: 73.19192504882812, Loss: 0.016515785828232765, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10727/20000], Bound: 0.3878716826438904, Entropy: 142.283203125, Temp: 2.6891398429870605, KL: 75.31880187988281, Loss: 0.01501620002090931, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10728/20000], Bound: 0.3726031184196472, Entropy: 142.4033966064453, Temp: 2.689154624938965, KL: 69.15087890625, Loss: 0.018286924809217453, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10729/20000], Bound: 0.3960278630256653, Entropy: 142.1166534423828, Temp: 2.6891565322875977, KL: 77.98092651367188, Loss: 0.014520764350891113, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10730/20000], Bound: 0.4014405310153961, Entropy: 142.76849365234375, Temp: 2.689171314239502, KL: 77.18983459472656, Loss: 0.018977444618940353, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10731/20000], Bound: 0.37147483229637146, Entropy: 141.9140625, Temp: 2.6891775131225586, KL: 70.73152160644531, Loss: 0.014749537222087383, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10732/20000], Bound: 0.41059768199920654, Entropy: 140.99574279785156, Temp: 2.6891887187957764, KL: 80.13058471679688, Loss: 0.01861618086695671, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10733/20000], Bound: 0.3823302388191223, Entropy: 140.54779052734375, Temp: 2.6891961097717285, KL: 74.19984436035156, Loss: 0.014101170934736729, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10734/20000], Bound: 0.42243751883506775, Entropy: 139.3941650390625, Temp: 2.6892142295837402, KL: 83.89451599121094, Loss: 0.018326090648770332, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10735/20000], Bound: 0.4148302376270294, Entropy: 142.1644744873047, Temp: 2.6892318725585938, KL: 82.08033752441406, Loss: 0.017375575378537178, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10736/20000], Bound: 0.38721907138824463, Entropy: 141.76548767089844, Temp: 2.6892518997192383, KL: 73.28665161132812, Loss: 0.018441488966345787, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10737/20000], Bound: 0.3799135088920593, Entropy: 143.56019592285156, Temp: 2.6892616748809814, KL: 72.02534484863281, Loss: 0.016845667734742165, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10738/20000], Bound: 0.40150025486946106, Entropy: 142.0561065673828, Temp: 2.689268112182617, KL: 76.11517333984375, Loss: 0.02100946381688118, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10739/20000], Bound: 0.3849456012248993, Entropy: 142.46092224121094, Temp: 2.6892571449279785, KL: 74.69387817382812, Loss: 0.01459419447928667, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10740/20000], Bound: 0.3837476372718811, Entropy: 138.88758850097656, Temp: 2.6892571449279785, KL: 72.20944213867188, Loss: 0.018566440790891647, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10741/20000], Bound: 0.3979659676551819, Entropy: 140.1746063232422, Temp: 2.6892476081848145, KL: 76.7529296875, Loss: 0.01787116564810276, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10742/20000], Bound: 0.3826424181461334, Entropy: 141.6222686767578, Temp: 2.6892364025115967, KL: 73.09715270996094, Loss: 0.016319872811436653, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10743/20000], Bound: 0.40654295682907104, Entropy: 140.19390869140625, Temp: 2.6892271041870117, KL: 79.4879150390625, Loss: 0.017541753128170967, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10744/20000], Bound: 0.38594749569892883, Entropy: 142.58714294433594, Temp: 2.6892197132110596, KL: 74.8895263671875, Loss: 0.014772036112844944, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10745/20000], Bound: 0.3850906491279602, Entropy: 141.6422882080078, Temp: 2.6892223358154297, KL: 74.76564025878906, Loss: 0.014538845978677273, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10746/20000], Bound: 0.392702579498291, Entropy: 140.42459106445312, Temp: 2.689234733581543, KL: 74.9339599609375, Loss: 0.01836433820426464, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10747/20000], Bound: 0.3973733186721802, Entropy: 141.8497772216797, Temp: 2.689239740371704, KL: 75.95379638671875, Loss: 0.019030515104532242, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10748/20000], Bound: 0.40345337986946106, Entropy: 141.19244384765625, Temp: 2.6892356872558594, KL: 80.17945861816406, Loss: 0.014535930939018726, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10749/20000], Bound: 0.3910101354122162, Entropy: 142.44325256347656, Temp: 2.689246892929077, KL: 74.92095947265625, Loss: 0.017464522272348404, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10750/20000], Bound: 0.40439632534980774, Entropy: 143.04519653320312, Temp: 2.6892542839050293, KL: 78.77902221679688, Loss: 0.017664054408669472, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10751/20000], Bound: 0.38142645359039307, Entropy: 141.85594177246094, Temp: 2.689260959625244, KL: 72.38862609863281, Loss: 0.016982942819595337, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10752/20000], Bound: 0.34925347566604614, Entropy: 142.45179748535156, Temp: 2.6892642974853516, KL: 65.09236145019531, Loss: 0.013637062162160873, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10753/20000], Bound: 0.37795835733413696, Entropy: 138.76641845703125, Temp: 2.6892731189727783, KL: 72.62666320800781, Loss: 0.014680055901408195, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10754/20000], Bound: 0.3688422441482544, Entropy: 140.60986328125, Temp: 2.689288377761841, KL: 69.27626037597656, Loss: 0.01606314443051815, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10755/20000], Bound: 0.401498943567276, Entropy: 139.2318572998047, Temp: 2.689300775527954, KL: 79.04116821289062, Loss: 0.015568944625556469, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10756/20000], Bound: 0.39297616481781006, Entropy: 141.6982879638672, Temp: 2.689321279525757, KL: 73.99250793457031, Loss: 0.02026505023241043, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10757/20000], Bound: 0.38086533546447754, Entropy: 141.4933319091797, Temp: 2.689324140548706, KL: 71.85781860351562, Loss: 0.017668746411800385, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10758/20000], Bound: 0.39997532963752747, Entropy: 139.44578552246094, Temp: 2.6893205642700195, KL: 78.3775634765625, Loss: 0.01596001908183098, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10759/20000], Bound: 0.37491312623023987, Entropy: 144.84988403320312, Temp: 2.6893246173858643, KL: 70.79507446289062, Loss: 0.016459936276078224, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10760/20000], Bound: 0.40415531396865845, Entropy: 139.58450317382812, Temp: 2.689326286315918, KL: 78.70158386230469, Loss: 0.01767468824982643, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10761/20000], Bound: 0.37767624855041504, Entropy: 141.38685607910156, Temp: 2.6893277168273926, KL: 71.79670715332031, Loss: 0.016072692349553108, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10762/20000], Bound: 0.3859628438949585, Entropy: 140.6376495361328, Temp: 2.6893296241760254, KL: 74.32777404785156, Loss: 0.015825828537344933, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10763/20000], Bound: 0.40278664231300354, Entropy: 140.12928771972656, Temp: 2.689335823059082, KL: 78.663818359375, Loss: 0.016984758898615837, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10764/20000], Bound: 0.3972918391227722, Entropy: 141.74496459960938, Temp: 2.6893439292907715, KL: 75.634521484375, Loss: 0.019580161198973656, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10765/20000], Bound: 0.3794403076171875, Entropy: 141.69482421875, Temp: 2.689340353012085, KL: 72.63896179199219, Loss: 0.015451665036380291, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10766/20000], Bound: 0.4025912582874298, Entropy: 141.09396362304688, Temp: 2.6893413066864014, KL: 77.06961059570312, Loss: 0.01984032429754734, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10767/20000], Bound: 0.3872210383415222, Entropy: 139.78323364257812, Temp: 2.689331293106079, KL: 76.67179870605469, Loss: 0.012149535119533539, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10768/20000], Bound: 0.40840035676956177, Entropy: 141.0797882080078, Temp: 2.6893444061279297, KL: 81.39987182617188, Loss: 0.015026239678263664, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10769/20000], Bound: 0.37847790122032166, Entropy: 141.3110809326172, Temp: 2.6893696784973145, KL: 71.430908203125, Loss: 0.01718216948211193, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10770/20000], Bound: 0.37229710817337036, Entropy: 143.28167724609375, Temp: 2.6893880367279053, KL: 69.79989624023438, Loss: 0.016919612884521484, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10771/20000], Bound: 0.3957359492778778, Entropy: 140.3605499267578, Temp: 2.6894001960754395, KL: 75.49620056152344, Loss: 0.01898249424993992, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10772/20000], Bound: 0.39770862460136414, Entropy: 139.84779357910156, Temp: 2.6894023418426514, KL: 76.30154418945312, Loss: 0.018570072948932648, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10773/20000], Bound: 0.3719140589237213, Entropy: 141.9584503173828, Temp: 2.6893982887268066, KL: 69.25328063964844, Loss: 0.01773269847035408, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10774/20000], Bound: 0.36261481046676636, Entropy: 142.09103393554688, Temp: 2.6893856525421143, KL: 64.79512023925781, Loss: 0.021120551973581314, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10775/20000], Bound: 0.4159967601299286, Entropy: 140.90267944335938, Temp: 2.6893467903137207, KL: 81.96006774902344, Loss: 0.018260139971971512, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10776/20000], Bound: 0.37306803464889526, Entropy: 142.24465942382812, Temp: 2.689312219619751, KL: 69.84828186035156, Loss: 0.017238441854715347, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10777/20000], Bound: 0.39099937677383423, Entropy: 141.23109436035156, Temp: 2.689274787902832, KL: 75.22178649902344, Loss: 0.016899563372135162, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10778/20000], Bound: 0.38969987630844116, Entropy: 140.48670959472656, Temp: 2.689241409301758, KL: 74.240234375, Loss: 0.018016261979937553, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10779/20000], Bound: 0.3801209330558777, Entropy: 140.56039428710938, Temp: 2.6892058849334717, KL: 71.77388000488281, Loss: 0.017423996701836586, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10780/20000], Bound: 0.36244919896125793, Entropy: 142.0951385498047, Temp: 2.689168930053711, KL: 66.27313232421875, Loss: 0.018284520134329796, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10781/20000], Bound: 0.4100116193294525, Entropy: 143.24424743652344, Temp: 2.689121723175049, KL: 80.73178100585938, Loss: 0.017168758437037468, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10782/20000], Bound: 0.3773525059223175, Entropy: 142.1868133544922, Temp: 2.6890830993652344, KL: 71.48348999023438, Loss: 0.016479840502142906, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10783/20000], Bound: 0.4004228413105011, Entropy: 141.57940673828125, Temp: 2.68904709815979, KL: 79.11598205566406, Loss: 0.014831657521426678, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10784/20000], Bound: 0.38829731941223145, Entropy: 140.316162109375, Temp: 2.6890270709991455, KL: 74.7344970703125, Loss: 0.01633268967270851, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10785/20000], Bound: 0.3725038170814514, Entropy: 142.8896484375, Temp: 2.689011335372925, KL: 70.8662109375, Loss: 0.015043570660054684, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10786/20000], Bound: 0.38821011781692505, Entropy: 141.5424346923828, Temp: 2.6890015602111816, KL: 75.34616088867188, Loss: 0.015147755853831768, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10787/20000], Bound: 0.3715328574180603, Entropy: 141.9326171875, Temp: 2.6890010833740234, KL: 70.44261169433594, Loss: 0.015315939672291279, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10788/20000], Bound: 0.39916884899139404, Entropy: 142.3910675048828, Temp: 2.6890032291412354, KL: 75.91064453125, Loss: 0.020098455250263214, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10789/20000], Bound: 0.37135863304138184, Entropy: 141.25323486328125, Temp: 2.688992500305176, KL: 70.62832641601562, Loss: 0.014878184534609318, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10790/20000], Bound: 0.34886717796325684, Entropy: 142.8040771484375, Temp: 2.6889874935150146, KL: 64.8170166015625, Loss: 0.013948378153145313, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10791/20000], Bound: 0.35489019751548767, Entropy: 142.35780334472656, Temp: 2.6889870166778564, KL: 66.19515991210938, Loss: 0.014492333866655827, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10792/20000], Bound: 0.3713192343711853, Entropy: 140.81846618652344, Temp: 2.6889894008636475, KL: 70.5400390625, Loss: 0.015021422877907753, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10793/20000], Bound: 0.38148918747901917, Entropy: 142.08245849609375, Temp: 2.688995838165283, KL: 73.60243225097656, Loss: 0.014757413417100906, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10794/20000], Bound: 0.3850754201412201, Entropy: 141.47967529296875, Temp: 2.68900990486145, KL: 75.23336791992188, Loss: 0.013658876530826092, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10795/20000], Bound: 0.3850025534629822, Entropy: 142.4846649169922, Temp: 2.6890368461608887, KL: 74.72462463378906, Loss: 0.014565682969987392, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10796/20000], Bound: 0.3955920934677124, Entropy: 141.2172088623047, Temp: 2.6890709400177, KL: 76.025634765625, Loss: 0.017916159704327583, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10797/20000], Bound: 0.3933129608631134, Entropy: 141.11825561523438, Temp: 2.689098358154297, KL: 76.43930053710938, Loss: 0.015898043289780617, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10798/20000], Bound: 0.40099748969078064, Entropy: 140.7549591064453, Temp: 2.6891286373138428, KL: 79.34722900390625, Loss: 0.014720458537340164, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10799/20000], Bound: 0.40516942739486694, Entropy: 141.60781860351562, Temp: 2.689168930053711, KL: 77.28472900390625, Loss: 0.020871859043836594, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10800/20000], Bound: 0.38277724385261536, Entropy: 140.81863403320312, Temp: 2.68919038772583, KL: 73.86408996582031, Loss: 0.01496609766036272, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10801/20000], Bound: 0.36975109577178955, Entropy: 142.0668182373047, Temp: 2.6892170906066895, KL: 69.8074951171875, Loss: 0.015555203892290592, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10802/20000], Bound: 0.408741295337677, Entropy: 141.22105407714844, Temp: 2.689242362976074, KL: 79.15289306640625, Loss: 0.019393648952245712, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10803/20000], Bound: 0.36890459060668945, Entropy: 144.83790588378906, Temp: 2.689257860183716, KL: 70.54048156738281, Loss: 0.013745347037911415, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10804/20000], Bound: 0.41122689843177795, Entropy: 140.17074584960938, Temp: 2.689281940460205, KL: 81.697021484375, Loss: 0.016058167442679405, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10805/20000], Bound: 0.3809269070625305, Entropy: 140.2194061279297, Temp: 2.6893129348754883, KL: 73.56993103027344, Loss: 0.014518525451421738, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10806/20000], Bound: 0.4021584689617157, Entropy: 140.80894470214844, Temp: 2.68934965133667, KL: 79.09915161132812, Loss: 0.01582709513604641, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10807/20000], Bound: 0.3863734304904938, Entropy: 141.3410186767578, Temp: 2.6893911361694336, KL: 75.09622192382812, Loss: 0.014620058238506317, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10808/20000], Bound: 0.39689475297927856, Entropy: 141.98739624023438, Temp: 2.689438581466675, KL: 78.35386657714844, Loss: 0.014306904748082161, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10809/20000], Bound: 0.3957997262477875, Entropy: 142.40335083007812, Temp: 2.689495086669922, KL: 78.3516845703125, Loss: 0.013709740713238716, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10810/20000], Bound: 0.3683745861053467, Entropy: 141.43353271484375, Temp: 2.6895627975463867, KL: 67.11201477050781, Loss: 0.019841918721795082, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10811/20000], Bound: 0.35944586992263794, Entropy: 142.0811309814453, Temp: 2.689603805541992, KL: 65.78598022460938, Loss: 0.017624646425247192, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10812/20000], Bound: 0.39887920022010803, Entropy: 139.93545532226562, Temp: 2.689629077911377, KL: 75.37065124511719, Loss: 0.02094782516360283, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10813/20000], Bound: 0.3810237944126129, Entropy: 141.25531005859375, Temp: 2.6896347999572754, KL: 72.7435302734375, Loss: 0.016109969466924667, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10814/20000], Bound: 0.3686469495296478, Entropy: 142.90029907226562, Temp: 2.6896414756774902, KL: 69.29351806640625, Loss: 0.015930842608213425, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10815/20000], Bound: 0.39064672589302063, Entropy: 141.34368896484375, Temp: 2.6896464824676514, KL: 75.51791381835938, Loss: 0.016160275787115097, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10816/20000], Bound: 0.38770970702171326, Entropy: 139.63096618652344, Temp: 2.68965482711792, KL: 74.57316589355469, Loss: 0.016319459304213524, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10817/20000], Bound: 0.3743177056312561, Entropy: 142.67674255371094, Temp: 2.689664363861084, KL: 70.427490234375, Loss: 0.016829097643494606, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10818/20000], Bound: 0.3807051479816437, Entropy: 141.64239501953125, Temp: 2.689669370651245, KL: 73.34185791015625, Loss: 0.014826802536845207, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10819/20000], Bound: 0.35958850383758545, Entropy: 141.8494415283203, Temp: 2.689681053161621, KL: 67.85050964355469, Loss: 0.013861658051609993, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10820/20000], Bound: 0.389193594455719, Entropy: 142.22537231445312, Temp: 2.6896986961364746, KL: 74.39321899414062, Loss: 0.017460431903600693, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10821/20000], Bound: 0.3913061022758484, Entropy: 141.4778594970703, Temp: 2.689711570739746, KL: 74.79533386230469, Loss: 0.017863618209958076, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10822/20000], Bound: 0.38169100880622864, Entropy: 141.80345153808594, Temp: 2.689718723297119, KL: 72.29295349121094, Loss: 0.017306994646787643, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10823/20000], Bound: 0.3523821532726288, Entropy: 141.42686462402344, Temp: 2.6897213459014893, KL: 64.28375244140625, Loss: 0.016754860058426857, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10824/20000], Bound: 0.3815159499645233, Entropy: 139.0438690185547, Temp: 2.6897144317626953, KL: 72.91188049316406, Loss: 0.016062304377555847, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10825/20000], Bound: 0.4011954367160797, Entropy: 139.76138305664062, Temp: 2.6897099018096924, KL: 78.93721008300781, Loss: 0.015598430298268795, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10826/20000], Bound: 0.3929860591888428, Entropy: 141.87522888183594, Temp: 2.6897149085998535, KL: 76.17359924316406, Loss: 0.016219157725572586, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10827/20000], Bound: 0.37554123997688293, Entropy: 141.5717010498047, Temp: 2.689723491668701, KL: 71.40126037597656, Loss: 0.015671217814087868, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10828/20000], Bound: 0.3944917917251587, Entropy: 141.21006774902344, Temp: 2.6897332668304443, KL: 75.7022705078125, Loss: 0.01791985146701336, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10829/20000], Bound: 0.37782976031303406, Entropy: 140.57086181640625, Temp: 2.6897380352020264, KL: 72.77764892578125, Loss: 0.014334918931126595, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10830/20000], Bound: 0.4051106572151184, Entropy: 140.6431884765625, Temp: 2.6897518634796143, KL: 79.48721313476562, Loss: 0.01675000786781311, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10831/20000], Bound: 0.3955618143081665, Entropy: 140.6380615234375, Temp: 2.6897687911987305, KL: 74.1180419921875, Loss: 0.021451978012919426, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10832/20000], Bound: 0.3940899074077606, Entropy: 140.64222717285156, Temp: 2.6897635459899902, KL: 75.32752990722656, Loss: 0.01839652843773365, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10833/20000], Bound: 0.38795891404151917, Entropy: 142.48036193847656, Temp: 2.6897525787353516, KL: 74.69119262695312, Loss: 0.01623619720339775, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10834/20000], Bound: 0.406652569770813, Entropy: 142.45953369140625, Temp: 2.6897451877593994, KL: 80.19163513183594, Loss: 0.01629994437098503, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10835/20000], Bound: 0.4055003523826599, Entropy: 139.74876403808594, Temp: 2.6897456645965576, KL: 78.65583801269531, Loss: 0.018512390553951263, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10836/20000], Bound: 0.3755887746810913, Entropy: 141.57411193847656, Temp: 2.68974232673645, KL: 71.65242004394531, Loss: 0.015229813754558563, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10837/20000], Bound: 0.3928021490573883, Entropy: 141.68211364746094, Temp: 2.689743757247925, KL: 76.66122436523438, Loss: 0.015212438069283962, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10838/20000], Bound: 0.36281993985176086, Entropy: 143.3346710205078, Temp: 2.689753532409668, KL: 68.32168579101562, Loss: 0.014674610458314419, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10839/20000], Bound: 0.3666093349456787, Entropy: 140.4449462890625, Temp: 2.6897661685943604, KL: 68.89064025878906, Loss: 0.01560656726360321, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10840/20000], Bound: 0.3883277475833893, Entropy: 141.3202362060547, Temp: 2.6897778511047363, KL: 73.11123657226562, Loss: 0.019373642280697823, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10841/20000], Bound: 0.38264548778533936, Entropy: 139.45310974121094, Temp: 2.6897757053375244, KL: 72.81446838378906, Loss: 0.01685182936489582, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10842/20000], Bound: 0.3985239267349243, Entropy: 140.4866180419922, Temp: 2.689772129058838, KL: 77.59812927246094, Loss: 0.016612419858574867, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10843/20000], Bound: 0.393572062253952, Entropy: 141.84788513183594, Temp: 2.689772367477417, KL: 74.78034973144531, Loss: 0.01913023367524147, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10844/20000], Bound: 0.3792324960231781, Entropy: 142.32032775878906, Temp: 2.689762830734253, KL: 73.55776977539062, Loss: 0.013636086136102676, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10845/20000], Bound: 0.3922780752182007, Entropy: 139.44773864746094, Temp: 2.6897671222686768, KL: 75.25471496582031, Loss: 0.017540840432047844, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10846/20000], Bound: 0.3601934611797333, Entropy: 144.0579071044922, Temp: 2.6897683143615723, KL: 66.17221069335938, Loss: 0.01729763299226761, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10847/20000], Bound: 0.41552475094795227, Entropy: 138.62925720214844, Temp: 2.6897599697113037, KL: 81.54808044433594, Loss: 0.018763171508908272, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10848/20000], Bound: 0.37614142894744873, Entropy: 141.1107635498047, Temp: 2.6897499561309814, KL: 70.61251831054688, Loss: 0.01745777018368244, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10849/20000], Bound: 0.34576693177223206, Entropy: 141.77894592285156, Temp: 2.68973445892334, KL: 63.87384033203125, Loss: 0.014118828810751438, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10850/20000], Bound: 0.37394842505455017, Entropy: 143.07899475097656, Temp: 2.689723014831543, KL: 69.76455688476562, Loss: 0.01786540448665619, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10851/20000], Bound: 0.37213751673698425, Entropy: 140.95750427246094, Temp: 2.6897037029266357, KL: 70.70936584472656, Loss: 0.015146836638450623, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10852/20000], Bound: 0.4016723334789276, Entropy: 141.01441955566406, Temp: 2.689690113067627, KL: 79.0587158203125, Loss: 0.015636395663022995, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10853/20000], Bound: 0.3808882534503937, Entropy: 140.63694763183594, Temp: 2.6896870136260986, KL: 73.94223022460938, Loss: 0.01380924228578806, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10854/20000], Bound: 0.4007702171802521, Entropy: 140.43458557128906, Temp: 2.689696788787842, KL: 78.98884582519531, Loss: 0.015267004258930683, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10855/20000], Bound: 0.40389519929885864, Entropy: 140.44219970703125, Temp: 2.68971586227417, KL: 79.37939453125, Loss: 0.016273945569992065, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10856/20000], Bound: 0.33693239092826843, Entropy: 142.76974487304688, Temp: 2.68973970413208, KL: 61.445098876953125, Loss: 0.014143506996333599, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10857/20000], Bound: 0.3674863874912262, Entropy: 140.8095703125, Temp: 2.68976092338562, KL: 68.57159423828125, Loss: 0.016661597415804863, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10858/20000], Bound: 0.3835268020629883, Entropy: 141.60574340820312, Temp: 2.6897754669189453, KL: 73.64900207519531, Loss: 0.015775565057992935, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10859/20000], Bound: 0.4120350480079651, Entropy: 141.00466918945312, Temp: 2.6897921562194824, KL: 81.45086669921875, Loss: 0.01697567291557789, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10860/20000], Bound: 0.4207870066165924, Entropy: 140.7011260986328, Temp: 2.689812421798706, KL: 83.90054321289062, Loss: 0.017379075288772583, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10861/20000], Bound: 0.4010499119758606, Entropy: 142.3950653076172, Temp: 2.689836263656616, KL: 78.30575561523438, Loss: 0.016692956909537315, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10862/20000], Bound: 0.4033144414424896, Entropy: 139.34402465820312, Temp: 2.689861536026001, KL: 80.31309509277344, Loss: 0.014217203482985497, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10863/20000], Bound: 0.39932021498680115, Entropy: 139.37490844726562, Temp: 2.6899003982543945, KL: 76.57173156738281, Loss: 0.018960844725370407, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10864/20000], Bound: 0.3850373327732086, Entropy: 140.09356689453125, Temp: 2.689927816390991, KL: 73.74610900878906, Loss: 0.016412021592259407, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10865/20000], Bound: 0.3924766778945923, Entropy: 141.20352172851562, Temp: 2.689953327178955, KL: 75.70655822753906, Loss: 0.016811134293675423, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10866/20000], Bound: 0.42313897609710693, Entropy: 141.19615173339844, Temp: 2.6899774074554443, KL: 84.83840942382812, Loss: 0.016981029883027077, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10867/20000], Bound: 0.3691463768482208, Entropy: 141.15362548828125, Temp: 2.690006971359253, KL: 69.96328735351562, Loss: 0.014952690340578556, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10868/20000], Bound: 0.37690478563308716, Entropy: 142.0514678955078, Temp: 2.690037727355957, KL: 70.16720581054688, Loss: 0.018695352599024773, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10869/20000], Bound: 0.41013407707214355, Entropy: 141.2904052734375, Temp: 2.6900532245635986, KL: 80.23104858398438, Loss: 0.018177799880504608, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10870/20000], Bound: 0.3876733183860779, Entropy: 141.38511657714844, Temp: 2.69006609916687, KL: 74.89581298828125, Loss: 0.015703782439231873, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10871/20000], Bound: 0.37407079339027405, Entropy: 142.08189392089844, Temp: 2.6900827884674072, KL: 70.57131958007812, Loss: 0.016433829441666603, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10872/20000], Bound: 0.39417752623558044, Entropy: 142.1327667236328, Temp: 2.690095901489258, KL: 76.46908569335938, Loss: 0.016325660049915314, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10873/20000], Bound: 0.39772137999534607, Entropy: 141.6596221923828, Temp: 2.6901116371154785, KL: 78.03237915039062, Loss: 0.01536647416651249, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10874/20000], Bound: 0.40196526050567627, Entropy: 140.67202758789062, Temp: 2.6901347637176514, KL: 78.50679016113281, Loss: 0.016829142346978188, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10875/20000], Bound: 0.3959878981113434, Entropy: 139.85757446289062, Temp: 2.6901588439941406, KL: 77.30641174316406, Loss: 0.01576298289000988, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10876/20000], Bound: 0.38411277532577515, Entropy: 139.82945251464844, Temp: 2.690187454223633, KL: 74.62471008300781, Loss: 0.014282026328146458, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10877/20000], Bound: 0.38015487790107727, Entropy: 142.67564392089844, Temp: 2.6902244091033936, KL: 72.04116821289062, Loss: 0.016953997313976288, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10878/20000], Bound: 0.3850317597389221, Entropy: 139.37791442871094, Temp: 2.6902546882629395, KL: 73.40765380859375, Loss: 0.017040982842445374, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10879/20000], Bound: 0.4086817800998688, Entropy: 139.66014099121094, Temp: 2.690279722213745, KL: 79.15341186523438, Loss: 0.019369201734662056, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10880/20000], Bound: 0.3987567126750946, Entropy: 140.68844604492188, Temp: 2.6902952194213867, KL: 77.55819702148438, Loss: 0.01682012341916561, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10881/20000], Bound: 0.37275248765945435, Entropy: 140.5345916748047, Temp: 2.6903116703033447, KL: 70.97030639648438, Loss: 0.014993562363088131, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10882/20000], Bound: 0.368526816368103, Entropy: 140.7134552001953, Temp: 2.690331220626831, KL: 69.02973937988281, Loss: 0.01636335998773575, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10883/20000], Bound: 0.40454262495040894, Entropy: 140.0084991455078, Temp: 2.6903460025787354, KL: 79.43785095214844, Loss: 0.016531748697161674, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10884/20000], Bound: 0.3633594512939453, Entropy: 141.4930877685547, Temp: 2.690364360809326, KL: 65.38783264160156, Loss: 0.020414812490344048, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10885/20000], Bound: 0.3936052620410919, Entropy: 142.69314575195312, Temp: 2.690356492996216, KL: 77.12211608886719, Loss: 0.014801227487623692, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10886/20000], Bound: 0.3930366635322571, Entropy: 140.27914428710938, Temp: 2.6903605461120605, KL: 75.93125915527344, Loss: 0.016703402623534203, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10887/20000], Bound: 0.3751102387905121, Entropy: 140.03550720214844, Temp: 2.690365791320801, KL: 72.45677185058594, Loss: 0.013485436327755451, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10888/20000], Bound: 0.37182414531707764, Entropy: 141.55519104003906, Temp: 2.690383195877075, KL: 68.34597778320312, Loss: 0.01937888376414776, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10889/20000], Bound: 0.4212576448917389, Entropy: 142.4874725341797, Temp: 2.6903817653656006, KL: 84.68601989746094, Loss: 0.01619396172463894, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10890/20000], Bound: 0.39174601435661316, Entropy: 140.7672576904297, Temp: 2.690391778945923, KL: 75.23332214355469, Loss: 0.01729573868215084, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10891/20000], Bound: 0.3702009916305542, Entropy: 141.5967254638672, Temp: 2.690399408340454, KL: 69.804443359375, Loss: 0.01580878160893917, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10892/20000], Bound: 0.358467161655426, Entropy: 142.2228546142578, Temp: 2.690406322479248, KL: 63.8118896484375, Loss: 0.020789451897144318, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10893/20000], Bound: 0.38010919094085693, Entropy: 139.18157958984375, Temp: 2.690385103225708, KL: 71.79200744628906, Loss: 0.017393939197063446, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10894/20000], Bound: 0.38536179065704346, Entropy: 140.93162536621094, Temp: 2.6903607845306396, KL: 73.83247375488281, Loss: 0.01643083244562149, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10895/20000], Bound: 0.37782028317451477, Entropy: 141.50390625, Temp: 2.690340042114258, KL: 70.23136901855469, Loss: 0.01906776987016201, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10896/20000], Bound: 0.383571982383728, Entropy: 141.59469604492188, Temp: 2.6903076171875, KL: 73.24992370605469, Loss: 0.016546521335840225, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10897/20000], Bound: 0.38506120443344116, Entropy: 139.45664978027344, Temp: 2.6902782917022705, KL: 72.52297973632812, Loss: 0.018701327964663506, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10898/20000], Bound: 0.41610509157180786, Entropy: 140.92333984375, Temp: 2.690241813659668, KL: 83.13224792480469, Loss: 0.016152160242199898, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10899/20000], Bound: 0.35299941897392273, Entropy: 140.2621307373047, Temp: 2.6902191638946533, KL: 65.73495483398438, Loss: 0.01437983475625515, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10900/20000], Bound: 0.3904150128364563, Entropy: 139.76242065429688, Temp: 2.6902012825012207, KL: 74.499755859375, Loss: 0.017931655049324036, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10901/20000], Bound: 0.39828523993492126, Entropy: 141.39588928222656, Temp: 2.6901803016662598, KL: 77.39453125, Loss: 0.016863280907273293, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10902/20000], Bound: 0.38453951478004456, Entropy: 140.82000732421875, Temp: 2.6901636123657227, KL: 74.15065002441406, Loss: 0.015393301844596863, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10903/20000], Bound: 0.39988404512405396, Entropy: 140.10203552246094, Temp: 2.6901543140411377, KL: 78.89939880371094, Loss: 0.014948165975511074, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10904/20000], Bound: 0.39005476236343384, Entropy: 141.0135498046875, Temp: 2.690157890319824, KL: 75.68632507324219, Loss: 0.015529713593423367, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10905/20000], Bound: 0.386526882648468, Entropy: 140.33749389648438, Temp: 2.6901674270629883, KL: 74.70523071289062, Loss: 0.015437458641827106, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10906/20000], Bound: 0.3911760449409485, Entropy: 141.86016845703125, Temp: 2.6901822090148926, KL: 74.74404907226562, Loss: 0.017892254516482353, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10907/20000], Bound: 0.3965817987918854, Entropy: 138.82333374023438, Temp: 2.6901907920837402, KL: 76.32136535644531, Loss: 0.01792041026055813, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10908/20000], Bound: 0.39035436511039734, Entropy: 142.1320343017578, Temp: 2.690195322036743, KL: 73.94212341308594, Loss: 0.01893496885895729, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10909/20000], Bound: 0.378935843706131, Entropy: 139.97840881347656, Temp: 2.6901895999908447, KL: 72.12846374511719, Loss: 0.01613783650100231, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10910/20000], Bound: 0.3983210325241089, Entropy: 141.42684936523438, Temp: 2.690185070037842, KL: 77.44229125976562, Loss: 0.016794295981526375, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10911/20000], Bound: 0.3710891902446747, Entropy: 141.58184814453125, Temp: 2.690183401107788, KL: 70.71060180664062, Loss: 0.014592977240681648, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10912/20000], Bound: 0.37799564003944397, Entropy: 140.4349822998047, Temp: 2.690188407897949, KL: 71.67057800292969, Loss: 0.016485488042235374, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10913/20000], Bound: 0.3800051808357239, Entropy: 142.35018920898438, Temp: 2.6901915073394775, KL: 72.34281921386719, Loss: 0.016312727704644203, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10914/20000], Bound: 0.3792371451854706, Entropy: 141.1059112548828, Temp: 2.690194606781006, KL: 72.15608215332031, Loss: 0.016247956082224846, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10915/20000], Bound: 0.3708505630493164, Entropy: 142.32891845703125, Temp: 2.690197467803955, KL: 68.79025268554688, Loss: 0.018035883083939552, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10916/20000], Bound: 0.39295947551727295, Entropy: 140.38754272460938, Temp: 2.6901895999908447, KL: 75.52647399902344, Loss: 0.0174119733273983, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10917/20000], Bound: 0.39033690094947815, Entropy: 140.2978515625, Temp: 2.690180540084839, KL: 74.75749206542969, Loss: 0.01740988902747631, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10918/20000], Bound: 0.3644503355026245, Entropy: 141.36782836914062, Temp: 2.6901700496673584, KL: 67.83244323730469, Loss: 0.016442008316516876, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10919/20000], Bound: 0.37734365463256836, Entropy: 141.45359802246094, Temp: 2.6901559829711914, KL: 71.633544921875, Loss: 0.01620539464056492, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10920/20000], Bound: 0.37944483757019043, Entropy: 142.1381072998047, Temp: 2.690143346786499, KL: 72.260986328125, Loss: 0.016163865104317665, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10921/20000], Bound: 0.37229418754577637, Entropy: 142.489501953125, Temp: 2.6901326179504395, KL: 70.61457824707031, Loss: 0.015409892424941063, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10922/20000], Bound: 0.3736485540866852, Entropy: 140.54917907714844, Temp: 2.6901257038116455, KL: 71.09089660644531, Loss: 0.015243898145854473, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10923/20000], Bound: 0.40286779403686523, Entropy: 143.133544921875, Temp: 2.6901233196258545, KL: 79.58808898925781, Loss: 0.015319706872105598, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10924/20000], Bound: 0.36906787753105164, Entropy: 142.38909912109375, Temp: 2.690131902694702, KL: 69.35429382324219, Loss: 0.016044216230511665, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10925/20000], Bound: 0.36669549345970154, Entropy: 139.42178344726562, Temp: 2.690138339996338, KL: 68.08157348632812, Loss: 0.01715877093374729, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10926/20000], Bound: 0.3998892307281494, Entropy: 143.21954345703125, Temp: 2.690136671066284, KL: 79.18547058105469, Loss: 0.014419138431549072, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10927/20000], Bound: 0.3963184952735901, Entropy: 137.7864227294922, Temp: 2.6901495456695557, KL: 77.78532409667969, Loss: 0.015054377727210522, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10928/20000], Bound: 0.38040488958358765, Entropy: 141.99053955078125, Temp: 2.690171480178833, KL: 71.43746948242188, Loss: 0.01820981502532959, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10929/20000], Bound: 0.3854209780693054, Entropy: 141.82382202148438, Temp: 2.6901824474334717, KL: 74.48333740234375, Loss: 0.015251492150127888, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10930/20000], Bound: 0.4030529260635376, Entropy: 142.48934936523438, Temp: 2.6901988983154297, KL: 80.14471435546875, Loss: 0.0143887372687459, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10931/20000], Bound: 0.3674980103969574, Entropy: 143.07736206054688, Temp: 2.6902291774749756, KL: 69.24630737304688, Loss: 0.015417428687214851, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10932/20000], Bound: 0.3772333860397339, Entropy: 140.41053771972656, Temp: 2.6902575492858887, KL: 71.25465393066406, Loss: 0.016851548105478287, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10933/20000], Bound: 0.3990758955478668, Entropy: 141.3404998779297, Temp: 2.690279960632324, KL: 77.90005493164062, Loss: 0.016360707581043243, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10934/20000], Bound: 0.40364909172058105, Entropy: 142.61727905273438, Temp: 2.69030499458313, KL: 80.24798583984375, Loss: 0.014528941363096237, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10935/20000], Bound: 0.37625327706336975, Entropy: 140.22547912597656, Temp: 2.690342426300049, KL: 73.3505859375, Loss: 0.012433560565114021, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10936/20000], Bound: 0.37089037895202637, Entropy: 140.7364501953125, Temp: 2.690394163131714, KL: 71.39080810546875, Loss: 0.013225434347987175, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10937/20000], Bound: 0.38086700439453125, Entropy: 142.6676483154297, Temp: 2.69045352935791, KL: 73.67919921875, Loss: 0.014294135384261608, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10938/20000], Bound: 0.38331547379493713, Entropy: 141.65272521972656, Temp: 2.690516948699951, KL: 74.94563293457031, Loss: 0.013258807361125946, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10939/20000], Bound: 0.372557133436203, Entropy: 140.86392211914062, Temp: 2.6905901432037354, KL: 70.68118286132812, Loss: 0.01542961597442627, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10940/20000], Bound: 0.40062186121940613, Entropy: 141.80384826660156, Temp: 2.6906585693359375, KL: 79.82681274414062, Loss: 0.013637827709317207, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10941/20000], Bound: 0.3866211175918579, Entropy: 142.07192993164062, Temp: 2.6907379627227783, KL: 73.52241516113281, Loss: 0.017691897228360176, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10942/20000], Bound: 0.3863320052623749, Entropy: 141.31307983398438, Temp: 2.690804958343506, KL: 74.33656311035156, Loss: 0.016023064032197, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10943/20000], Bound: 0.36836618185043335, Entropy: 141.87747192382812, Temp: 2.6908681392669678, KL: 68.073486328125, Loss: 0.018059788271784782, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10944/20000], Bound: 0.36082515120506287, Entropy: 143.93504333496094, Temp: 2.6909139156341553, KL: 66.83331298828125, Loss: 0.016407152637839317, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10945/20000], Bound: 0.3676689565181732, Entropy: 140.54827880859375, Temp: 2.6909496784210205, KL: 68.14218139648438, Loss: 0.017565084621310234, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10946/20000], Bound: 0.3855729103088379, Entropy: 141.3346710205078, Temp: 2.6909725666046143, KL: 73.68635559082031, Loss: 0.01682199351489544, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10947/20000], Bound: 0.4089730679988861, Entropy: 139.9405975341797, Temp: 2.6909921169281006, KL: 80.73185729980469, Loss: 0.016606083139777184, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10948/20000], Bound: 0.38595977425575256, Entropy: 140.3004608154297, Temp: 2.6910157203674316, KL: 73.59320068359375, Loss: 0.017204761505126953, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10949/20000], Bound: 0.3773591220378876, Entropy: 143.920166015625, Temp: 2.6910345554351807, KL: 72.27459716796875, Loss: 0.015030154027044773, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10950/20000], Bound: 0.39439329504966736, Entropy: 140.72854614257812, Temp: 2.691056966781616, KL: 76.22872924804688, Loss: 0.016899695619940758, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10951/20000], Bound: 0.3897682726383209, Entropy: 140.22085571289062, Temp: 2.691077947616577, KL: 74.51687622070312, Loss: 0.01755553111433983, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10952/20000], Bound: 0.3990424871444702, Entropy: 138.66282653808594, Temp: 2.691093683242798, KL: 78.53807067871094, Loss: 0.015164925716817379, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10953/20000], Bound: 0.3900575339794159, Entropy: 142.7764434814453, Temp: 2.6911182403564453, KL: 75.21192932128906, Loss: 0.016421912238001823, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10954/20000], Bound: 0.41598325967788696, Entropy: 141.19288635253906, Temp: 2.6911425590515137, KL: 83.55035400390625, Loss: 0.015316416509449482, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10955/20000], Bound: 0.3920219838619232, Entropy: 141.5438690185547, Temp: 2.691178321838379, KL: 76.64462280273438, Loss: 0.014831463806331158, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10956/20000], Bound: 0.39706915616989136, Entropy: 141.77490234375, Temp: 2.691220998764038, KL: 74.69454956054688, Loss: 0.021220248192548752, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10957/20000], Bound: 0.3822525143623352, Entropy: 142.1857147216797, Temp: 2.6912403106689453, KL: 72.58204650878906, Loss: 0.017084896564483643, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10958/20000], Bound: 0.38210296630859375, Entropy: 139.6649627685547, Temp: 2.6912543773651123, KL: 73.39515686035156, Loss: 0.015493889339268208, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10959/20000], Bound: 0.37906816601753235, Entropy: 142.11703491210938, Temp: 2.6912715435028076, KL: 71.33981323242188, Loss: 0.01768336445093155, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10960/20000], Bound: 0.3959207236766815, Entropy: 142.3544158935547, Temp: 2.6912803649902344, KL: 77.41421508789062, Loss: 0.01553697045892477, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10961/20000], Bound: 0.3857775330543518, Entropy: 142.23300170898438, Temp: 2.691296100616455, KL: 73.76220703125, Loss: 0.016794683411717415, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10962/20000], Bound: 0.3878217935562134, Entropy: 139.0313262939453, Temp: 2.6913094520568848, KL: 73.17207336425781, Loss: 0.01899849995970726, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10963/20000], Bound: 0.39167365431785583, Entropy: 138.835693359375, Temp: 2.691310405731201, KL: 75.06074523925781, Loss: 0.017585238441824913, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10964/20000], Bound: 0.3253772556781769, Entropy: 143.31597900390625, Temp: 2.6913082599639893, KL: 56.48503112792969, Loss: 0.017579777166247368, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10965/20000], Bound: 0.38768303394317627, Entropy: 141.29052734375, Temp: 2.691286087036133, KL: 73.35481262207031, Loss: 0.01858355849981308, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10966/20000], Bound: 0.388592392206192, Entropy: 141.83987426757812, Temp: 2.6912572383880615, KL: 74.3790283203125, Loss: 0.01717401295900345, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10967/20000], Bound: 0.3803875148296356, Entropy: 142.33763122558594, Temp: 2.6912291049957275, KL: 72.02337646484375, Loss: 0.01712045632302761, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10968/20000], Bound: 0.3460879623889923, Entropy: 140.14517211914062, Temp: 2.6912002563476562, KL: 63.61955261230469, Loss: 0.01476659718900919, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10969/20000], Bound: 0.39389386773109436, Entropy: 139.8075408935547, Temp: 2.6911730766296387, KL: 76.45445251464844, Loss: 0.016207944601774216, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10970/20000], Bound: 0.3508301079273224, Entropy: 142.8022003173828, Temp: 2.691153049468994, KL: 64.19486999511719, Loss: 0.016130313277244568, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10971/20000], Bound: 0.38196489214897156, Entropy: 141.7648468017578, Temp: 2.6911282539367676, KL: 72.15461730957031, Loss: 0.017723344266414642, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10972/20000], Bound: 0.399473637342453, Entropy: 140.84521484375, Temp: 2.6910998821258545, KL: 77.90284729003906, Loss: 0.01658310554921627, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10973/20000], Bound: 0.36500680446624756, Entropy: 141.69285583496094, Temp: 2.691077709197998, KL: 67.35652160644531, Loss: 0.017625395208597183, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10974/20000], Bound: 0.3751159608364105, Entropy: 141.11859130859375, Temp: 2.6910479068756104, KL: 71.64871215820312, Loss: 0.014996401034295559, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10975/20000], Bound: 0.3798684775829315, Entropy: 141.6790771484375, Temp: 2.691025972366333, KL: 73.53256225585938, Loss: 0.014036163687705994, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10976/20000], Bound: 0.3994084596633911, Entropy: 139.69607543945312, Temp: 2.6910176277160645, KL: 77.98027038574219, Loss: 0.016402455046772957, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10977/20000], Bound: 0.3803649842739105, Entropy: 143.05227661132812, Temp: 2.6910147666931152, KL: 74.17283630371094, Loss: 0.013112794607877731, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10978/20000], Bound: 0.4132491648197174, Entropy: 140.99485778808594, Temp: 2.691027879714966, KL: 82.26226806640625, Loss: 0.016164619475603104, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10979/20000], Bound: 0.36943379044532776, Entropy: 143.23495483398438, Temp: 2.6910488605499268, KL: 67.43478393554688, Loss: 0.019811531528830528, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10980/20000], Bound: 0.39246606826782227, Entropy: 142.6371612548828, Temp: 2.6910481452941895, KL: 75.87818908691406, Loss: 0.01649671606719494, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10981/20000], Bound: 0.3813720941543579, Entropy: 141.17852783203125, Temp: 2.6910500526428223, KL: 72.32475280761719, Loss: 0.01708781160414219, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10982/20000], Bound: 0.3733590543270111, Entropy: 143.44407653808594, Temp: 2.6910481452941895, KL: 71.29908752441406, Loss: 0.014711369760334492, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10983/20000], Bound: 0.4084375500679016, Entropy: 141.4461212158203, Temp: 2.6910529136657715, KL: 81.78665161132812, Loss: 0.014347243122756481, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10984/20000], Bound: 0.3765552043914795, Entropy: 139.73480224609375, Temp: 2.6910738945007324, KL: 71.40660095214844, Loss: 0.016213886439800262, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10985/20000], Bound: 0.38586828112602234, Entropy: 141.944580078125, Temp: 2.6910922527313232, KL: 75.10392761230469, Loss: 0.014349040575325489, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10986/20000], Bound: 0.3785432279109955, Entropy: 141.25582885742188, Temp: 2.691120147705078, KL: 72.71565246582031, Loss: 0.014844792895019054, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10987/20000], Bound: 0.3691006600856781, Entropy: 142.31954956054688, Temp: 2.6911518573760986, KL: 69.30245971679688, Loss: 0.01616622693836689, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10988/20000], Bound: 0.3698435127735138, Entropy: 140.0774383544922, Temp: 2.691178321838379, KL: 68.79913330078125, Loss: 0.017494039610028267, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10989/20000], Bound: 0.39102157950401306, Entropy: 141.547607421875, Temp: 2.6911942958831787, KL: 74.36968994140625, Loss: 0.0185125470161438, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10990/20000], Bound: 0.3761662244796753, Entropy: 140.14915466308594, Temp: 2.6912007331848145, KL: 70.52679443359375, Loss: 0.01764203980565071, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10991/20000], Bound: 0.3908170163631439, Entropy: 142.96958923339844, Temp: 2.69119930267334, KL: 75.46743774414062, Loss: 0.016361571848392487, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10992/20000], Bound: 0.3737722635269165, Entropy: 142.26026916503906, Temp: 2.6912002563476562, KL: 71.34814453125, Loss: 0.014841186814010143, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10993/20000], Bound: 0.3794105648994446, Entropy: 142.96336364746094, Temp: 2.691206932067871, KL: 72.31533813476562, Loss: 0.01605389267206192, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10994/20000], Bound: 0.4033513367176056, Entropy: 139.1542510986328, Temp: 2.691214084625244, KL: 78.13188171386719, Loss: 0.01830505020916462, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10995/20000], Bound: 0.38353168964385986, Entropy: 140.81187438964844, Temp: 2.6912169456481934, KL: 74.67317199707031, Loss: 0.013888636603951454, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10996/20000], Bound: 0.3906457722187042, Entropy: 141.4163360595703, Temp: 2.691232204437256, KL: 75.35044860839844, Loss: 0.016485948115587234, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10997/20000], Bound: 0.37945058941841125, Entropy: 142.4582061767578, Temp: 2.6912481784820557, KL: 72.98748779296875, Loss: 0.014826924540102482, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10998/20000], Bound: 0.3799471855163574, Entropy: 141.3101348876953, Temp: 2.691269636154175, KL: 72.45443725585938, Loss: 0.016083745285868645, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [10999/20000], Bound: 0.3684771955013275, Entropy: 141.85000610351562, Temp: 2.6912901401519775, KL: 70.28872680664062, Loss: 0.014005927368998528, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11000/20000], Bound: 0.40730199217796326, Entropy: 142.00135803222656, Temp: 2.691316843032837, KL: 81.00265502929688, Loss: 0.015172178857028484, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11001/20000], Bound: 0.41093945503234863, Entropy: 140.621826171875, Temp: 2.6913535594940186, KL: 82.04156494140625, Loss: 0.015279135666787624, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11002/20000], Bound: 0.3551290035247803, Entropy: 140.04493713378906, Temp: 2.691399574279785, KL: 66.62333679199219, Loss: 0.013839777559041977, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11003/20000], Bound: 0.35862770676612854, Entropy: 141.04505920410156, Temp: 2.6914470195770264, KL: 67.01646423339844, Loss: 0.014925850555300713, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11004/20000], Bound: 0.386234849691391, Entropy: 140.80003356933594, Temp: 2.691490888595581, KL: 74.40327453613281, Loss: 0.01585288532078266, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11005/20000], Bound: 0.39431923627853394, Entropy: 140.4733123779297, Temp: 2.6915345191955566, KL: 76.10289001464844, Loss: 0.017097437754273415, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11006/20000], Bound: 0.3875606954097748, Entropy: 143.28428649902344, Temp: 2.6915736198425293, KL: 72.91279602050781, Loss: 0.019340766593813896, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11007/20000], Bound: 0.3530694842338562, Entropy: 142.06092834472656, Temp: 2.691596031188965, KL: 65.61540222167969, Loss: 0.014649009332060814, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11008/20000], Bound: 0.39266425371170044, Entropy: 142.31961059570312, Temp: 2.691617727279663, KL: 75.47917175292969, Loss: 0.017351550981402397, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11009/20000], Bound: 0.36197972297668457, Entropy: 142.56634521484375, Temp: 2.6916353702545166, KL: 68.3017578125, Loss: 0.014287874102592468, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11010/20000], Bound: 0.3844594955444336, Entropy: 142.47042846679688, Temp: 2.6916568279266357, KL: 74.70964050292969, Loss: 0.014325723052024841, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11011/20000], Bound: 0.3797574043273926, Entropy: 141.2733154296875, Temp: 2.6916868686676025, KL: 72.32098388671875, Loss: 0.01623356342315674, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11012/20000], Bound: 0.40731459856033325, Entropy: 140.54373168945312, Temp: 2.69171404838562, KL: 81.87789916992188, Loss: 0.01355768647044897, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11013/20000], Bound: 0.372158408164978, Entropy: 143.51441955566406, Temp: 2.6917591094970703, KL: 68.71182250976562, Loss: 0.018886331468820572, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11014/20000], Bound: 0.37112727761268616, Entropy: 142.3065948486328, Temp: 2.6917848587036133, KL: 69.95184326171875, Loss: 0.01603677123785019, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11015/20000], Bound: 0.37782523036003113, Entropy: 140.7294464111328, Temp: 2.6918070316314697, KL: 69.97662353515625, Loss: 0.019554756581783295, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11016/20000], Bound: 0.4172658324241638, Entropy: 141.89137268066406, Temp: 2.6918108463287354, KL: 84.61454772949219, Loss: 0.014073551632463932, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11017/20000], Bound: 0.39154767990112305, Entropy: 142.5237274169922, Temp: 2.6918344497680664, KL: 75.4522705078125, Loss: 0.016794022172689438, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11018/20000], Bound: 0.3811069130897522, Entropy: 143.24664306640625, Temp: 2.6918561458587646, KL: 73.64985656738281, Loss: 0.014490903355181217, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11019/20000], Bound: 0.3940877616405487, Entropy: 141.61837768554688, Temp: 2.691884994506836, KL: 75.7386474609375, Loss: 0.01765051670372486, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11020/20000], Bound: 0.35357123613357544, Entropy: 142.57814025878906, Temp: 2.691908121109009, KL: 63.514312744140625, Loss: 0.01881313882768154, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11021/20000], Bound: 0.38797685503959656, Entropy: 140.43585205078125, Temp: 2.6919100284576416, KL: 73.19093322753906, Loss: 0.019052544608712196, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11022/20000], Bound: 0.4113365709781647, Entropy: 141.8959197998047, Temp: 2.6919002532958984, KL: 80.45098876953125, Loss: 0.018462536856532097, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11023/20000], Bound: 0.40007758140563965, Entropy: 141.3874969482422, Temp: 2.6918892860412598, KL: 77.92958068847656, Loss: 0.01687462627887726, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11024/20000], Bound: 0.3515623211860657, Entropy: 142.43421936035156, Temp: 2.6918818950653076, KL: 64.68168640136719, Loss: 0.01560832466930151, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11025/20000], Bound: 0.37580665946006775, Entropy: 141.84332275390625, Temp: 2.691871404647827, KL: 72.56578063964844, Loss: 0.013668409548699856, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11026/20000], Bound: 0.4022209644317627, Entropy: 140.75689697265625, Temp: 2.6918740272521973, KL: 78.61686706542969, Loss: 0.01678362302482128, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11027/20000], Bound: 0.4040643572807312, Entropy: 142.23165893554688, Temp: 2.6918795108795166, KL: 78.03181457519531, Loss: 0.018893150612711906, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11028/20000], Bound: 0.39908480644226074, Entropy: 140.00491333007812, Temp: 2.691878318786621, KL: 76.68162536621094, Loss: 0.018644632771611214, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11029/20000], Bound: 0.369403600692749, Entropy: 143.5712127685547, Temp: 2.691870927810669, KL: 69.791015625, Loss: 0.015424590557813644, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11030/20000], Bound: 0.37248632311820984, Entropy: 140.9962921142578, Temp: 2.691866159439087, KL: 70.56803894042969, Loss: 0.015613257884979248, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11031/20000], Bound: 0.37647467851638794, Entropy: 144.01747131347656, Temp: 2.6918632984161377, KL: 72.62025451660156, Loss: 0.01392341498285532, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11032/20000], Bound: 0.39561727643013, Entropy: 140.26502990722656, Temp: 2.691871404647827, KL: 77.32008361816406, Loss: 0.015551215969026089, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11033/20000], Bound: 0.3969413638114929, Entropy: 139.70684814453125, Temp: 2.6918866634368896, KL: 77.02204895019531, Loss: 0.016832146793603897, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11034/20000], Bound: 0.37868231534957886, Entropy: 142.70074462890625, Temp: 2.691901922225952, KL: 72.41925048828125, Loss: 0.015476968139410019, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11035/20000], Bound: 0.3704487979412079, Entropy: 143.82452392578125, Temp: 2.6919195652008057, KL: 69.54150390625, Loss: 0.01644097827374935, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11036/20000], Bound: 0.39010104537010193, Entropy: 139.76478576660156, Temp: 2.691932201385498, KL: 74.80352783203125, Loss: 0.017211725935339928, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11037/20000], Bound: 0.4083571135997772, Entropy: 141.59056091308594, Temp: 2.691941976547241, KL: 79.99795532226562, Loss: 0.01763460598886013, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11038/20000], Bound: 0.4002593159675598, Entropy: 141.34425354003906, Temp: 2.6919517517089844, KL: 77.21919250488281, Loss: 0.018295127898454666, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11039/20000], Bound: 0.39232513308525085, Entropy: 142.07794189453125, Temp: 2.691956043243408, KL: 76.41459655761719, Loss: 0.015432002954185009, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11040/20000], Bound: 0.3640170991420746, Entropy: 140.57972717285156, Temp: 2.691967487335205, KL: 67.54875183105469, Loss: 0.01675577647984028, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11041/20000], Bound: 0.39626359939575195, Entropy: 140.73883056640625, Temp: 2.69197154045105, KL: 76.92399597167969, Loss: 0.01664271391928196, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11042/20000], Bound: 0.39353981614112854, Entropy: 140.8964080810547, Temp: 2.6919777393341064, KL: 76.81593322753906, Loss: 0.01535060815513134, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11043/20000], Bound: 0.4082677364349365, Entropy: 142.49591064453125, Temp: 2.6919915676116943, KL: 79.66201782226562, Loss: 0.018209125846624374, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11044/20000], Bound: 0.3918936848640442, Entropy: 139.8363494873047, Temp: 2.692002058029175, KL: 75.87437438964844, Loss: 0.016200324520468712, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11045/20000], Bound: 0.4052655100822449, Entropy: 142.13230895996094, Temp: 2.6920149326324463, KL: 79.7987060546875, Loss: 0.016280658543109894, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11046/20000], Bound: 0.4062313437461853, Entropy: 139.4638214111328, Temp: 2.692033529281616, KL: 79.91993713378906, Loss: 0.01659361645579338, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11047/20000], Bound: 0.3662028908729553, Entropy: 142.19656372070312, Temp: 2.6920554637908936, KL: 68.81620788574219, Loss: 0.015549903735518456, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11048/20000], Bound: 0.378826767206192, Entropy: 141.46102905273438, Temp: 2.692075490951538, KL: 74.14361572265625, Loss: 0.012353204190731049, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11049/20000], Bound: 0.3965345323085785, Entropy: 139.83175659179688, Temp: 2.692112684249878, KL: 76.45472717285156, Loss: 0.017664456740021706, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11050/20000], Bound: 0.3791836202144623, Entropy: 142.15785217285156, Temp: 2.6921439170837402, KL: 71.18937683105469, Loss: 0.01803177408874035, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11051/20000], Bound: 0.40103256702423096, Entropy: 142.15579223632812, Temp: 2.6921629905700684, KL: 78.71478271484375, Loss: 0.015946852043271065, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11052/20000], Bound: 0.3775081932544708, Entropy: 141.6586151123047, Temp: 2.692187786102295, KL: 72.27285766601562, Loss: 0.015123513527214527, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11053/20000], Bound: 0.3888408839702606, Entropy: 139.6885528564453, Temp: 2.6922152042388916, KL: 73.36593627929688, Loss: 0.019199064001441002, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11054/20000], Bound: 0.38306358456611633, Entropy: 141.01914978027344, Temp: 2.692227840423584, KL: 71.83102416992188, Loss: 0.018924815580248833, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11055/20000], Bound: 0.36875781416893005, Entropy: 143.1205596923828, Temp: 2.692227363586426, KL: 67.970703125, Loss: 0.018467340618371964, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11056/20000], Bound: 0.39978376030921936, Entropy: 140.23681640625, Temp: 2.692213296890259, KL: 77.41372680664062, Loss: 0.01767362467944622, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11057/20000], Bound: 0.37754005193710327, Entropy: 141.3596649169922, Temp: 2.692199230194092, KL: 72.61280822753906, Loss: 0.014509296044707298, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11058/20000], Bound: 0.3984900414943695, Entropy: 139.78756713867188, Temp: 2.692194938659668, KL: 77.33457946777344, Loss: 0.01710698753595352, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11059/20000], Bound: 0.3616822361946106, Entropy: 142.9691925048828, Temp: 2.6921918392181396, KL: 66.60275268554688, Loss: 0.017292581498622894, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11060/20000], Bound: 0.3924022614955902, Entropy: 140.83421325683594, Temp: 2.6921792030334473, KL: 75.26850891113281, Loss: 0.017604880034923553, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11061/20000], Bound: 0.38784128427505493, Entropy: 141.91880798339844, Temp: 2.6921651363372803, KL: 75.09677124023438, Loss: 0.015441500581800938, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11062/20000], Bound: 0.3700203597545624, Entropy: 141.4027099609375, Temp: 2.6921584606170654, KL: 69.04428100585938, Loss: 0.017139803618192673, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11063/20000], Bound: 0.3780280649662018, Entropy: 141.82211303710938, Temp: 2.692145824432373, KL: 70.80632019042969, Loss: 0.018124766647815704, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11064/20000], Bound: 0.3652227818965912, Entropy: 141.77037048339844, Temp: 2.692125082015991, KL: 66.86271667480469, Loss: 0.01866365596652031, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11065/20000], Bound: 0.40116313099861145, Entropy: 140.15574645996094, Temp: 2.6920909881591797, KL: 78.77870178222656, Loss: 0.015899568796157837, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11066/20000], Bound: 0.40340033173561096, Entropy: 142.56459045410156, Temp: 2.692068099975586, KL: 78.38198852539062, Loss: 0.017875809222459793, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11067/20000], Bound: 0.3927517235279083, Entropy: 141.0915069580078, Temp: 2.6920454502105713, KL: 76.34866333007812, Loss: 0.015788329765200615, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11068/20000], Bound: 0.3620324730873108, Entropy: 141.17967224121094, Temp: 2.692031145095825, KL: 67.73735046386719, Loss: 0.01536710187792778, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11069/20000], Bound: 0.3753398656845093, Entropy: 141.8376007080078, Temp: 2.6920180320739746, KL: 70.85075378417969, Loss: 0.016606424003839493, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11070/20000], Bound: 0.38609224557876587, Entropy: 141.8330841064453, Temp: 2.6920039653778076, KL: 75.1318359375, Loss: 0.014427350834012032, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11071/20000], Bound: 0.40521588921546936, Entropy: 141.81837463378906, Temp: 2.692002058029175, KL: 78.88728332519531, Loss: 0.01794573850929737, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11072/20000], Bound: 0.35764098167419434, Entropy: 143.3552703857422, Temp: 2.6919987201690674, KL: 66.23124694824219, Loss: 0.015875697135925293, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11073/20000], Bound: 0.3936688005924225, Entropy: 140.6852264404297, Temp: 2.6919925212860107, KL: 75.27763366699219, Loss: 0.018278473988175392, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11074/20000], Bound: 0.39663782715797424, Entropy: 142.3332977294922, Temp: 2.6919806003570557, KL: 77.71040344238281, Loss: 0.015387693420052528, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11075/20000], Bound: 0.403123676776886, Entropy: 142.15786743164062, Temp: 2.691978931427002, KL: 79.56950378417969, Loss: 0.015515764243900776, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11076/20000], Bound: 0.413870245218277, Entropy: 141.75352478027344, Temp: 2.6919870376586914, KL: 82.5537109375, Loss: 0.015983814373612404, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11077/20000], Bound: 0.38928914070129395, Entropy: 141.5689239501953, Temp: 2.692004680633545, KL: 74.69435119628906, Loss: 0.01697360724210739, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11078/20000], Bound: 0.360110342502594, Entropy: 139.84109497070312, Temp: 2.6920197010040283, KL: 67.43359375, Loss: 0.014927799813449383, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11079/20000], Bound: 0.4006548225879669, Entropy: 141.29405212402344, Temp: 2.69203519821167, KL: 80.69992065429688, Loss: 0.012049581855535507, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11080/20000], Bound: 0.386127233505249, Entropy: 141.27610778808594, Temp: 2.692074775695801, KL: 72.10060119628906, Loss: 0.020076852291822433, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11081/20000], Bound: 0.38844966888427734, Entropy: 141.58551025390625, Temp: 2.692093849182129, KL: 74.41801452636719, Loss: 0.017031587660312653, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11082/20000], Bound: 0.3774149715900421, Entropy: 141.6636199951172, Temp: 2.6921095848083496, KL: 68.33843994140625, Loss: 0.02238031104207039, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11083/20000], Bound: 0.3965862989425659, Entropy: 139.9164581298828, Temp: 2.6920933723449707, KL: 77.24717712402344, Loss: 0.016220876947045326, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11084/20000], Bound: 0.38052526116371155, Entropy: 141.10406494140625, Temp: 2.6920833587646484, KL: 73.040771484375, Loss: 0.015312017872929573, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11085/20000], Bound: 0.3871329426765442, Entropy: 142.8713836669922, Temp: 2.6920793056488037, KL: 75.01893615722656, Loss: 0.015201241709291935, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11086/20000], Bound: 0.3762218952178955, Entropy: 143.8853302001953, Temp: 2.6920831203460693, KL: 70.48745727539062, Loss: 0.017751866951584816, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11087/20000], Bound: 0.37756940722465515, Entropy: 142.4431915283203, Temp: 2.6920783519744873, KL: 72.40740966796875, Loss: 0.01490531675517559, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11088/20000], Bound: 0.3725954592227936, Entropy: 141.191650390625, Temp: 2.692080497741699, KL: 70.28759765625, Loss: 0.016193853691220284, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11089/20000], Bound: 0.38242876529693604, Entropy: 142.57704162597656, Temp: 2.6920809745788574, KL: 74.11283874511719, Loss: 0.014343826100230217, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11090/20000], Bound: 0.3896835446357727, Entropy: 140.70518493652344, Temp: 2.692091703414917, KL: 74.31060791015625, Loss: 0.01790156401693821, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11091/20000], Bound: 0.37596407532691956, Entropy: 141.2218475341797, Temp: 2.692095994949341, KL: 70.47541809082031, Loss: 0.01763683557510376, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11092/20000], Bound: 0.4076533913612366, Entropy: 140.89337158203125, Temp: 2.6920924186706543, KL: 79.53298950195312, Loss: 0.018106406554579735, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11093/20000], Bound: 0.3968465030193329, Entropy: 141.22752380371094, Temp: 2.6920878887176514, KL: 76.47331237792969, Loss: 0.017801126465201378, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11094/20000], Bound: 0.4064795672893524, Entropy: 140.62255859375, Temp: 2.692080497741699, KL: 80.21131896972656, Loss: 0.016191324219107628, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11095/20000], Bound: 0.40326988697052, Entropy: 142.6706085205078, Temp: 2.6920816898345947, KL: 79.4486083984375, Loss: 0.0158225167542696, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11096/20000], Bound: 0.38622090220451355, Entropy: 140.3861846923828, Temp: 2.6920905113220215, KL: 73.8565673828125, Loss: 0.016866324469447136, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11097/20000], Bound: 0.404439777135849, Entropy: 140.45870971679688, Temp: 2.6920976638793945, KL: 77.06005859375, Loss: 0.0209086574614048, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11098/20000], Bound: 0.3998657464981079, Entropy: 141.0606231689453, Temp: 2.6920883655548096, KL: 77.10108947753906, Loss: 0.01829838939011097, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11099/20000], Bound: 0.4273608326911926, Entropy: 141.87777709960938, Temp: 2.692075490951538, KL: 85.45901489257812, Loss: 0.01827440783381462, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11100/20000], Bound: 0.3746154308319092, Entropy: 140.6255340576172, Temp: 2.6920666694641113, KL: 70.23590087890625, Loss: 0.017363114282488823, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11101/20000], Bound: 0.3783723711967468, Entropy: 140.36187744140625, Temp: 2.692052125930786, KL: 71.97785949707031, Loss: 0.016132252290844917, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11102/20000], Bound: 0.3911097049713135, Entropy: 141.66563415527344, Temp: 2.692039728164673, KL: 75.77279663085938, Loss: 0.015961816534399986, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11103/20000], Bound: 0.3579680323600769, Entropy: 143.04583740234375, Temp: 2.692033052444458, KL: 67.04145812988281, Loss: 0.014541089534759521, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11104/20000], Bound: 0.40495702624320984, Entropy: 140.784423828125, Temp: 2.692030191421509, KL: 78.17071533203125, Loss: 0.019132880493998528, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11105/20000], Bound: 0.3795689642429352, Entropy: 143.41702270507812, Temp: 2.6920204162597656, KL: 71.39346313476562, Loss: 0.017858188599348068, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11106/20000], Bound: 0.39645472168922424, Entropy: 139.6610565185547, Temp: 2.6920039653778076, KL: 77.72035217285156, Loss: 0.015268868766725063, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11107/20000], Bound: 0.4035056531429291, Entropy: 142.44847106933594, Temp: 2.6919984817504883, KL: 79.57786560058594, Loss: 0.015712440013885498, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11108/20000], Bound: 0.3833424746990204, Entropy: 142.32017517089844, Temp: 2.692002534866333, KL: 71.27845764160156, Loss: 0.0200995821505785, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11109/20000], Bound: 0.41064414381980896, Entropy: 143.18199157714844, Temp: 2.691988468170166, KL: 81.54139709472656, Loss: 0.01604941487312317, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11110/20000], Bound: 0.36311864852905273, Entropy: 142.06570434570312, Temp: 2.6919848918914795, KL: 66.64694213867188, Loss: 0.01796022057533264, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11111/20000], Bound: 0.37135007977485657, Entropy: 142.22998046875, Temp: 2.691969394683838, KL: 70.56138610839844, Loss: 0.015024157240986824, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11112/20000], Bound: 0.38639208674430847, Entropy: 141.6830291748047, Temp: 2.6919593811035156, KL: 73.69044494628906, Loss: 0.017266353592276573, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11113/20000], Bound: 0.40987181663513184, Entropy: 138.73052978515625, Temp: 2.6919472217559814, KL: 80.4095458984375, Loss: 0.017718136310577393, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11114/20000], Bound: 0.37035122513771057, Entropy: 141.28887939453125, Temp: 2.6919374465942383, KL: 69.44142150878906, Loss: 0.01657538115978241, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11115/20000], Bound: 0.37896108627319336, Entropy: 142.57400512695312, Temp: 2.691924810409546, KL: 70.87429809570312, Loss: 0.018496042117476463, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11116/20000], Bound: 0.3782126009464264, Entropy: 139.94554138183594, Temp: 2.6919026374816895, KL: 72.00685119628906, Loss: 0.01599164493381977, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11117/20000], Bound: 0.3964250385761261, Entropy: 142.05880737304688, Temp: 2.6918838024139404, KL: 76.64349365234375, Loss: 0.01725151017308235, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11118/20000], Bound: 0.3964213728904724, Entropy: 140.40896606445312, Temp: 2.691866397857666, KL: 75.59095764160156, Loss: 0.019204381853342056, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11119/20000], Bound: 0.40634942054748535, Entropy: 141.7269744873047, Temp: 2.6918411254882812, KL: 81.93013000488281, Loss: 0.01292363740503788, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11120/20000], Bound: 0.39703992009162903, Entropy: 140.9064483642578, Temp: 2.6918413639068604, KL: 77.86909484863281, Loss: 0.015312525443732738, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11121/20000], Bound: 0.39757126569747925, Entropy: 141.2511749267578, Temp: 2.6918509006500244, KL: 77.16453552246094, Loss: 0.016913622617721558, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11122/20000], Bound: 0.37574389576911926, Entropy: 142.4259490966797, Temp: 2.6918609142303467, KL: 70.81986999511719, Loss: 0.0168777983635664, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11123/20000], Bound: 0.3906131684780121, Entropy: 140.73245239257812, Temp: 2.691866397857666, KL: 75.66395568847656, Loss: 0.01589176431298256, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11124/20000], Bound: 0.40123870968818665, Entropy: 142.6549530029297, Temp: 2.69187593460083, KL: 78.97837829589844, Loss: 0.015568296425044537, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11125/20000], Bound: 0.39690226316452026, Entropy: 140.16049194335938, Temp: 2.6918938159942627, KL: 77.2762451171875, Loss: 0.016338614746928215, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11126/20000], Bound: 0.38912084698677063, Entropy: 141.51756286621094, Temp: 2.6919138431549072, KL: 75.6746826171875, Loss: 0.015060406178236008, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11127/20000], Bound: 0.39201101660728455, Entropy: 141.74546813964844, Temp: 2.6919405460357666, KL: 76.87985229492188, Loss: 0.014396212995052338, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11128/20000], Bound: 0.4111354351043701, Entropy: 142.8436737060547, Temp: 2.69197678565979, KL: 79.99600219726562, Loss: 0.01919541507959366, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11129/20000], Bound: 0.3840247094631195, Entropy: 140.44915771484375, Temp: 2.6920037269592285, KL: 74.08383178710938, Loss: 0.015256857499480247, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11130/20000], Bound: 0.39596372842788696, Entropy: 142.63218688964844, Temp: 2.6920342445373535, KL: 77.19081115722656, Loss: 0.015983112156391144, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11131/20000], Bound: 0.38818901777267456, Entropy: 141.3682403564453, Temp: 2.6920673847198486, KL: 75.47802734375, Loss: 0.014921135269105434, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11132/20000], Bound: 0.3787235617637634, Entropy: 140.50242614746094, Temp: 2.692106008529663, KL: 71.55891418457031, Loss: 0.017098791897296906, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11133/20000], Bound: 0.3691911995410919, Entropy: 142.19288635253906, Temp: 2.692136526107788, KL: 69.21414184570312, Loss: 0.016386086121201515, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11134/20000], Bound: 0.4076269268989563, Entropy: 140.65760803222656, Temp: 2.6921610832214355, KL: 80.32672119140625, Loss: 0.016618160530924797, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11135/20000], Bound: 0.4050692617893219, Entropy: 139.12901306152344, Temp: 2.6921887397766113, KL: 79.303466796875, Loss: 0.017093025147914886, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11136/20000], Bound: 0.33832570910453796, Entropy: 143.43080139160156, Temp: 2.692216157913208, KL: 60.65516662597656, Loss: 0.016332289204001427, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11137/20000], Bound: 0.3807789981365204, Entropy: 138.68641662597656, Temp: 2.692230224609375, KL: 72.17626953125, Loss: 0.017055107280611992, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11138/20000], Bound: 0.38730138540267944, Entropy: 142.67001342773438, Temp: 2.692239284515381, KL: 73.37138366699219, Loss: 0.01835385523736477, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11139/20000], Bound: 0.3697112798690796, Entropy: 142.05311584472656, Temp: 2.692239284515381, KL: 69.17442321777344, Loss: 0.01673538237810135, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11140/20000], Bound: 0.3806358277797699, Entropy: 141.0712127685547, Temp: 2.692234754562378, KL: 71.61488342285156, Loss: 0.018020929768681526, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11141/20000], Bound: 0.36556708812713623, Entropy: 141.6906280517578, Temp: 2.6922221183776855, KL: 68.3197021484375, Loss: 0.016139252111315727, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11142/20000], Bound: 0.39179763197898865, Entropy: 140.09161376953125, Temp: 2.6922082901000977, KL: 74.07781982421875, Loss: 0.019486483186483383, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11143/20000], Bound: 0.3982223868370056, Entropy: 140.0796661376953, Temp: 2.692183017730713, KL: 76.73359680175781, Loss: 0.018075577914714813, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11144/20000], Bound: 0.37124520540237427, Entropy: 140.00833129882812, Temp: 2.6921565532684326, KL: 70.86807250976562, Loss: 0.014400653541088104, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11145/20000], Bound: 0.38453409075737, Entropy: 143.24830627441406, Temp: 2.6921401023864746, KL: 73.33456420898438, Loss: 0.016924602910876274, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11146/20000], Bound: 0.39222657680511475, Entropy: 141.03475952148438, Temp: 2.6921229362487793, KL: 75.34609985351562, Loss: 0.017364317551255226, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11147/20000], Bound: 0.36932799220085144, Entropy: 141.2178497314453, Temp: 2.692106008529663, KL: 69.35191345214844, Loss: 0.0162021704018116, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11148/20000], Bound: 0.3976629376411438, Entropy: 141.5972900390625, Temp: 2.6920883655548096, KL: 78.26319885253906, Loss: 0.0149257592856884, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11149/20000], Bound: 0.39471471309661865, Entropy: 141.030029296875, Temp: 2.6920838356018066, KL: 77.71916198730469, Loss: 0.014317321591079235, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11150/20000], Bound: 0.39097923040390015, Entropy: 138.76622009277344, Temp: 2.6920931339263916, KL: 74.51356506347656, Loss: 0.018229978159070015, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11151/20000], Bound: 0.4082247018814087, Entropy: 141.6593475341797, Temp: 2.6920950412750244, KL: 79.61576843261719, Loss: 0.01827194355428219, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11152/20000], Bound: 0.39099636673927307, Entropy: 139.79843139648438, Temp: 2.692094564437866, KL: 74.93827819824219, Loss: 0.01745053008198738, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11153/20000], Bound: 0.37789812684059143, Entropy: 141.90951538085938, Temp: 2.692091703414917, KL: 70.98184204101562, Loss: 0.017728835344314575, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11154/20000], Bound: 0.36632925271987915, Entropy: 139.76324462890625, Temp: 2.6920812129974365, KL: 69.41989135742188, Loss: 0.01449535135179758, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11155/20000], Bound: 0.39099401235580444, Entropy: 139.86192321777344, Temp: 2.69207763671875, KL: 75.38334655761719, Loss: 0.016622448340058327, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11156/20000], Bound: 0.3701907992362976, Entropy: 141.93136596679688, Temp: 2.692075490951538, KL: 70.1005859375, Loss: 0.015267390757799149, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11157/20000], Bound: 0.36708253622055054, Entropy: 139.4545440673828, Temp: 2.6920762062072754, KL: 68.71401977539062, Loss: 0.016202671453356743, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11158/20000], Bound: 0.40414586663246155, Entropy: 138.93115234375, Temp: 2.6920740604400635, KL: 79.116455078125, Loss: 0.0169257540255785, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11159/20000], Bound: 0.3976474404335022, Entropy: 140.36402893066406, Temp: 2.692075252532959, KL: 77.1297607421875, Loss: 0.017022261396050453, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11160/20000], Bound: 0.3765184283256531, Entropy: 141.41830444335938, Temp: 2.692077875137329, KL: 72.20991516113281, Loss: 0.014710881747305393, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11161/20000], Bound: 0.38471972942352295, Entropy: 142.53758239746094, Temp: 2.692086935043335, KL: 74.25556945800781, Loss: 0.015313757583498955, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11162/20000], Bound: 0.3641197979450226, Entropy: 142.38340759277344, Temp: 2.692101240158081, KL: 69.67253112792969, Loss: 0.012866172939538956, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11163/20000], Bound: 0.3858158588409424, Entropy: 141.81597900390625, Temp: 2.692127227783203, KL: 72.97463989257812, Loss: 0.01828550361096859, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11164/20000], Bound: 0.3837704658508301, Entropy: 142.2801971435547, Temp: 2.6921424865722656, KL: 72.32034301757812, Loss: 0.018396295607089996, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11165/20000], Bound: 0.392946720123291, Entropy: 140.3905487060547, Temp: 2.6921467781066895, KL: 75.24382019042969, Loss: 0.017947835847735405, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11166/20000], Bound: 0.39579257369041443, Entropy: 139.9843292236328, Temp: 2.6921463012695312, KL: 76.4190673828125, Loss: 0.017323561012744904, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11167/20000], Bound: 0.3556693494319916, Entropy: 141.6391143798828, Temp: 2.6921446323394775, KL: 66.46415710449219, Loss: 0.014421353116631508, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11168/20000], Bound: 0.3943958878517151, Entropy: 143.92454528808594, Temp: 2.6921465396881104, KL: 75.99583435058594, Loss: 0.017343925312161446, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11169/20000], Bound: 0.38232412934303284, Entropy: 141.54920959472656, Temp: 2.6921465396881104, KL: 71.405517578125, Loss: 0.019316351041197777, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11170/20000], Bound: 0.39206284284591675, Entropy: 140.7923583984375, Temp: 2.6921327114105225, KL: 75.47540283203125, Loss: 0.01703489199280739, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11171/20000], Bound: 0.3837733864784241, Entropy: 140.9009552001953, Temp: 2.692119836807251, KL: 74.71820068359375, Loss: 0.013944210484623909, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11172/20000], Bound: 0.3933961093425751, Entropy: 142.12249755859375, Temp: 2.6921205520629883, KL: 77.14212036132812, Loss: 0.01466760691255331, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11173/20000], Bound: 0.38711974024772644, Entropy: 141.17892456054688, Temp: 2.6921329498291016, KL: 74.52821350097656, Loss: 0.01610599085688591, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11174/20000], Bound: 0.3708113431930542, Entropy: 141.20953369140625, Temp: 2.6921470165252686, KL: 70.730712890625, Loss: 0.014425982721149921, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11175/20000], Bound: 0.3973868191242218, Entropy: 141.36732482910156, Temp: 2.692166566848755, KL: 78.28225708007812, Loss: 0.014739313162863255, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11176/20000], Bound: 0.4051341414451599, Entropy: 140.56475830078125, Temp: 2.6921963691711426, KL: 79.66288757324219, Loss: 0.016461670398712158, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11177/20000], Bound: 0.37326592206954956, Entropy: 142.50845336914062, Temp: 2.6922290325164795, KL: 69.96775817871094, Loss: 0.017144983634352684, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11178/20000], Bound: 0.3974064290523529, Entropy: 139.7487335205078, Temp: 2.6922526359558105, KL: 77.66966247558594, Loss: 0.01588865928351879, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11179/20000], Bound: 0.3906707167625427, Entropy: 142.31204223632812, Temp: 2.6922805309295654, KL: 75.59947204589844, Loss: 0.016046833246946335, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11180/20000], Bound: 0.3753420412540436, Entropy: 139.6973419189453, Temp: 2.692309617996216, KL: 72.23577880859375, Loss: 0.01403783168643713, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11181/20000], Bound: 0.3747740089893341, Entropy: 141.63943481445312, Temp: 2.6923458576202393, KL: 70.18496704101562, Loss: 0.017544332891702652, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11182/20000], Bound: 0.39076271653175354, Entropy: 140.00390625, Temp: 2.692370891571045, KL: 76.11708068847656, Loss: 0.01513653714209795, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11183/20000], Bound: 0.38010215759277344, Entropy: 141.85890197753906, Temp: 2.6924021244049072, KL: 72.4996337890625, Loss: 0.016092898324131966, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11184/20000], Bound: 0.3490856885910034, Entropy: 141.75160217285156, Temp: 2.6924312114715576, KL: 63.80908203125, Loss: 0.015959320589900017, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11185/20000], Bound: 0.37354129552841187, Entropy: 143.02879333496094, Temp: 2.692451238632202, KL: 71.12783813476562, Loss: 0.01513872854411602, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11186/20000], Bound: 0.3816448450088501, Entropy: 140.49305725097656, Temp: 2.6924731731414795, KL: 73.46122741699219, Loss: 0.01513607706874609, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11187/20000], Bound: 0.39858436584472656, Entropy: 140.18125915527344, Temp: 2.6924991607666016, KL: 78.06884765625, Loss: 0.01579832099378109, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11188/20000], Bound: 0.3504813015460968, Entropy: 140.4430389404297, Temp: 2.6925299167633057, KL: 63.64787292480469, Loss: 0.01697644032537937, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11189/20000], Bound: 0.395870178937912, Entropy: 141.39137268066406, Temp: 2.6925466060638428, KL: 76.42939758300781, Loss: 0.017350714653730392, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11190/20000], Bound: 0.34959107637405396, Entropy: 141.7126922607422, Temp: 2.6925606727600098, KL: 62.45039367675781, Loss: 0.018742788583040237, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11191/20000], Bound: 0.3693368434906006, Entropy: 142.68199157714844, Temp: 2.6925530433654785, KL: 68.56642150878906, Loss: 0.017669139429926872, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11192/20000], Bound: 0.35932010412216187, Entropy: 142.62538146972656, Temp: 2.6925365924835205, KL: 66.44148254394531, Loss: 0.01636260747909546, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11193/20000], Bound: 0.38801053166389465, Entropy: 139.69602966308594, Temp: 2.6925160884857178, KL: 73.00704956054688, Loss: 0.019417252391576767, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11194/20000], Bound: 0.37027767300605774, Entropy: 138.9703826904297, Temp: 2.6924843788146973, KL: 70.53178405761719, Loss: 0.014516090974211693, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11195/20000], Bound: 0.42154911160469055, Entropy: 138.74856567382812, Temp: 2.69246244430542, KL: 84.9208984375, Loss: 0.015947872772812843, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11196/20000], Bound: 0.38472476601600647, Entropy: 141.93894958496094, Temp: 2.692455291748047, KL: 72.94760131835938, Loss: 0.01774892956018448, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11197/20000], Bound: 0.37965503334999084, Entropy: 142.6161651611328, Temp: 2.6924426555633545, KL: 71.90318298339844, Loss: 0.01696116477251053, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11198/20000], Bound: 0.3578236997127533, Entropy: 142.19757080078125, Temp: 2.6924281120300293, KL: 66.93003845214844, Loss: 0.014676200225949287, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11199/20000], Bound: 0.3739338219165802, Entropy: 142.12399291992188, Temp: 2.6924173831939697, KL: 70.24160766601562, Loss: 0.0169928427785635, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11200/20000], Bound: 0.4101959466934204, Entropy: 141.96826171875, Temp: 2.6924028396606445, KL: 82.12136840820312, Loss: 0.014725455082952976, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11201/20000], Bound: 0.39627280831336975, Entropy: 140.16195678710938, Temp: 2.6924052238464355, KL: 77.09745788574219, Loss: 0.016329772770404816, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11202/20000], Bound: 0.3690209686756134, Entropy: 141.8801727294922, Temp: 2.692411422729492, KL: 70.59437561035156, Loss: 0.01373522263020277, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11203/20000], Bound: 0.3742537498474121, Entropy: 141.79660034179688, Temp: 2.692426919937134, KL: 70.8900146484375, Loss: 0.0159588735550642, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11204/20000], Bound: 0.3617439568042755, Entropy: 143.44949340820312, Temp: 2.69244122505188, KL: 67.81938171386719, Loss: 0.01506728958338499, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11205/20000], Bound: 0.3916873633861542, Entropy: 140.75721740722656, Temp: 2.692455530166626, KL: 74.59715270996094, Loss: 0.018463952466845512, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11206/20000], Bound: 0.3856818377971649, Entropy: 144.59019470214844, Temp: 2.692460775375366, KL: 74.12921142578125, Loss: 0.016071757301688194, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11207/20000], Bound: 0.37221935391426086, Entropy: 142.2509307861328, Temp: 2.6924679279327393, KL: 69.98527526855469, Loss: 0.01655900478363037, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11208/20000], Bound: 0.3699241578578949, Entropy: 140.89041137695312, Temp: 2.692471504211426, KL: 68.64970397949219, Loss: 0.01782415434718132, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11209/20000], Bound: 0.39688926935195923, Entropy: 142.252197265625, Temp: 2.6924643516540527, KL: 77.61395263671875, Loss: 0.0157098937779665, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11210/20000], Bound: 0.3919437825679779, Entropy: 142.43252563476562, Temp: 2.69246506690979, KL: 76.64871215820312, Loss: 0.014794101007282734, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11211/20000], Bound: 0.40071433782577515, Entropy: 139.468994140625, Temp: 2.692476511001587, KL: 78.29609680175781, Loss: 0.016551557928323746, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11212/20000], Bound: 0.3882353901863098, Entropy: 142.018310546875, Temp: 2.692491054534912, KL: 73.86672973632812, Loss: 0.01794261299073696, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11213/20000], Bound: 0.39464661478996277, Entropy: 140.11337280273438, Temp: 2.692497968673706, KL: 77.91072082519531, Loss: 0.013928537257015705, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11214/20000], Bound: 0.41415250301361084, Entropy: 140.63558959960938, Temp: 2.6925199031829834, KL: 81.06509399414062, Loss: 0.01891317591071129, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11215/20000], Bound: 0.37534359097480774, Entropy: 141.06019592285156, Temp: 2.692535638809204, KL: 71.10519409179688, Loss: 0.016140252351760864, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11216/20000], Bound: 0.37396693229675293, Entropy: 141.99256896972656, Temp: 2.692549467086792, KL: 70.64373779296875, Loss: 0.016264760866761208, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11217/20000], Bound: 0.36059707403182983, Entropy: 140.76611328125, Temp: 2.6925606727600098, KL: 66.78146362304688, Loss: 0.016396939754486084, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11218/20000], Bound: 0.40375763177871704, Entropy: 141.86988830566406, Temp: 2.6925649642944336, KL: 80.36679077148438, Loss: 0.0143932169303298, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11219/20000], Bound: 0.40021196007728577, Entropy: 142.34361267089844, Temp: 2.69258451461792, KL: 78.12641906738281, Loss: 0.016590172424912453, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11220/20000], Bound: 0.4026438295841217, Entropy: 140.97561645507812, Temp: 2.692605972290039, KL: 78.635986328125, Loss: 0.016989750787615776, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11221/20000], Bound: 0.38847753405570984, Entropy: 143.500732421875, Temp: 2.6926279067993164, KL: 74.68820190429688, Loss: 0.016549797728657722, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11222/20000], Bound: 0.38074472546577454, Entropy: 142.55970764160156, Temp: 2.692648410797119, KL: 72.37123107910156, Loss: 0.016678251326084137, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11223/20000], Bound: 0.38511067628860474, Entropy: 139.11920166015625, Temp: 2.6926655769348145, KL: 74.25465393066406, Loss: 0.015532071702182293, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11224/20000], Bound: 0.36617511510849, Entropy: 142.65512084960938, Temp: 2.692685842514038, KL: 67.153076171875, Loss: 0.018628746271133423, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11225/20000], Bound: 0.4001835882663727, Entropy: 141.34722900390625, Temp: 2.6926889419555664, KL: 78.5865478515625, Loss: 0.015721134841442108, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11226/20000], Bound: 0.4002701938152313, Entropy: 141.06649780273438, Temp: 2.692699909210205, KL: 77.20167541503906, Loss: 0.018340613692998886, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11227/20000], Bound: 0.3809174597263336, Entropy: 141.91464233398438, Temp: 2.692704916000366, KL: 73.05198669433594, Loss: 0.015507384203374386, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11228/20000], Bound: 0.37242406606674194, Entropy: 142.03713989257812, Temp: 2.692713975906372, KL: 71.15158081054688, Loss: 0.01450390089303255, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11229/20000], Bound: 0.38221096992492676, Entropy: 141.61331176757812, Temp: 2.6927287578582764, KL: 72.96781921386719, Loss: 0.01635904796421528, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11230/20000], Bound: 0.4022972881793976, Entropy: 141.0397186279297, Temp: 2.6927425861358643, KL: 78.47663879394531, Loss: 0.017094947397708893, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11231/20000], Bound: 0.41715455055236816, Entropy: 140.95823669433594, Temp: 2.6927568912506104, KL: 82.41610717773438, Loss: 0.01810397021472454, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11232/20000], Bound: 0.3792811930179596, Entropy: 141.083984375, Temp: 2.692770481109619, KL: 71.95257568359375, Loss: 0.01667194813489914, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11233/20000], Bound: 0.36537113785743713, Entropy: 141.08790588378906, Temp: 2.6927809715270996, KL: 69.76689147949219, Loss: 0.013353629037737846, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11234/20000], Bound: 0.39683905243873596, Entropy: 142.94253540039062, Temp: 2.6928012371063232, KL: 76.56710815429688, Loss: 0.017629427835345268, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11235/20000], Bound: 0.3749346137046814, Entropy: 140.21127319335938, Temp: 2.692817449569702, KL: 69.32431030273438, Loss: 0.01923164166510105, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11236/20000], Bound: 0.3852311074733734, Entropy: 139.26153564453125, Temp: 2.6928162574768066, KL: 75.06256103515625, Loss: 0.014098390936851501, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11237/20000], Bound: 0.39412301778793335, Entropy: 139.96087646484375, Temp: 2.692826986312866, KL: 76.51173400878906, Loss: 0.016242962330579758, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11238/20000], Bound: 0.4098109304904938, Entropy: 140.4468994140625, Temp: 2.692840814590454, KL: 81.02607727050781, Loss: 0.01654830388724804, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11239/20000], Bound: 0.39032799005508423, Entropy: 140.38021850585938, Temp: 2.6928598880767822, KL: 75.75633239746094, Loss: 0.015574493445456028, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11240/20000], Bound: 0.3865908086299896, Entropy: 142.3119354248047, Temp: 2.692883253097534, KL: 75.03852844238281, Loss: 0.014878956601023674, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11241/20000], Bound: 0.3997807502746582, Entropy: 140.00233459472656, Temp: 2.692913055419922, KL: 77.34062194824219, Loss: 0.01781434752047062, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11242/20000], Bound: 0.3884366750717163, Entropy: 139.60284423828125, Temp: 2.6929376125335693, KL: 74.63133239746094, Loss: 0.016636068001389503, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11243/20000], Bound: 0.4061247408390045, Entropy: 141.02459716796875, Temp: 2.692960500717163, KL: 80.28936767578125, Loss: 0.015857860445976257, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11244/20000], Bound: 0.3844384551048279, Entropy: 141.4878387451172, Temp: 2.6929898262023926, KL: 73.29031372070312, Loss: 0.016962599009275436, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11245/20000], Bound: 0.39118483662605286, Entropy: 140.7442626953125, Temp: 2.693013906478882, KL: 74.26913452148438, Loss: 0.018803831189870834, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11246/20000], Bound: 0.3837321400642395, Entropy: 139.42831420898438, Temp: 2.693026542663574, KL: 74.15499877929688, Loss: 0.014976590871810913, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11247/20000], Bound: 0.3639722168445587, Entropy: 140.03909301757812, Temp: 2.693045139312744, KL: 67.43656921386719, Loss: 0.01694873720407486, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11248/20000], Bound: 0.3908303380012512, Entropy: 140.9608154296875, Temp: 2.693054437637329, KL: 76.49143981933594, Loss: 0.014485074207186699, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11249/20000], Bound: 0.4023021161556244, Entropy: 141.5618133544922, Temp: 2.6930747032165527, KL: 78.24087524414062, Loss: 0.01753862202167511, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11250/20000], Bound: 0.376634806394577, Entropy: 141.22119140625, Temp: 2.6930925846099854, KL: 70.8197021484375, Loss: 0.01736331917345524, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11251/20000], Bound: 0.3651888966560364, Entropy: 142.09310913085938, Temp: 2.6931025981903076, KL: 68.31163024902344, Loss: 0.01596267707645893, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11252/20000], Bound: 0.4012325704097748, Entropy: 140.24896240234375, Temp: 2.6931092739105225, KL: 79.43446350097656, Loss: 0.0147309061139822, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11253/20000], Bound: 0.42029252648353577, Entropy: 142.267822265625, Temp: 2.6931285858154297, KL: 84.97018432617188, Loss: 0.01514799427241087, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11254/20000], Bound: 0.3824481666088104, Entropy: 142.72959899902344, Temp: 2.693162202835083, KL: 74.10812377929688, Loss: 0.01437340211123228, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11255/20000], Bound: 0.359140545129776, Entropy: 143.9805450439453, Temp: 2.693202495574951, KL: 66.50874328613281, Loss: 0.016149280592799187, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11256/20000], Bound: 0.3914209306240082, Entropy: 142.63169860839844, Temp: 2.6932339668273926, KL: 75.02848815917969, Loss: 0.01752464659512043, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11257/20000], Bound: 0.37421682476997375, Entropy: 141.9699249267578, Temp: 2.6932592391967773, KL: 70.00741577148438, Loss: 0.01758485659956932, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11258/20000], Bound: 0.38437026739120483, Entropy: 141.40878295898438, Temp: 2.6932742595672607, KL: 73.87156677246094, Loss: 0.015849249437451363, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11259/20000], Bound: 0.38581541180610657, Entropy: 142.825927734375, Temp: 2.693290948867798, KL: 74.72856140136719, Loss: 0.015038899146020412, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11260/20000], Bound: 0.4172499477863312, Entropy: 138.7422637939453, Temp: 2.6933135986328125, KL: 82.60562133789062, Loss: 0.017811985686421394, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11261/20000], Bound: 0.36353710293769836, Entropy: 140.91082763671875, Temp: 2.6933364868164062, KL: 68.94459533691406, Loss: 0.01392348762601614, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11262/20000], Bound: 0.39284107089042664, Entropy: 142.3683624267578, Temp: 2.6933646202087402, KL: 76.69171142578125, Loss: 0.01521309744566679, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11263/20000], Bound: 0.3908401131629944, Entropy: 142.2876434326172, Temp: 2.6933987140655518, KL: 76.01739501953125, Loss: 0.015373860485851765, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11264/20000], Bound: 0.3643665611743927, Entropy: 142.28932189941406, Temp: 2.693436861038208, KL: 68.38340759277344, Loss: 0.015400704927742481, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11265/20000], Bound: 0.4077993333339691, Entropy: 139.35018920898438, Temp: 2.6934714317321777, KL: 81.31753540039062, Loss: 0.014888705685734749, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11266/20000], Bound: 0.3717803359031677, Entropy: 139.71218872070312, Temp: 2.693516731262207, KL: 71.11453247070312, Loss: 0.014238678850233555, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11267/20000], Bound: 0.3971857726573944, Entropy: 140.21090698242188, Temp: 2.693565607070923, KL: 77.3902587890625, Loss: 0.01629907637834549, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11268/20000], Bound: 0.39995336532592773, Entropy: 140.26739501953125, Temp: 2.6936140060424805, KL: 78.34783935546875, Loss: 0.016046540811657906, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11269/20000], Bound: 0.3849278688430786, Entropy: 142.02102661132812, Temp: 2.693664073944092, KL: 73.57823181152344, Loss: 0.016698217019438744, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11270/20000], Bound: 0.3684861361980438, Entropy: 142.46278381347656, Temp: 2.6937084197998047, KL: 69.54643249511719, Loss: 0.015410001389682293, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11271/20000], Bound: 0.4060339033603668, Entropy: 140.9475860595703, Temp: 2.6937499046325684, KL: 78.85313415527344, Loss: 0.01848142221570015, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11272/20000], Bound: 0.3900987207889557, Entropy: 141.28768920898438, Temp: 2.6937828063964844, KL: 75.70408630371094, Loss: 0.015555605292320251, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11273/20000], Bound: 0.4052196443080902, Entropy: 141.78152465820312, Temp: 2.693819046020508, KL: 80.11878967285156, Loss: 0.015679655596613884, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11274/20000], Bound: 0.3918015956878662, Entropy: 141.56056213378906, Temp: 2.693861246109009, KL: 74.17060852050781, Loss: 0.01933007687330246, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11275/20000], Bound: 0.3818150460720062, Entropy: 141.09397888183594, Temp: 2.6938869953155518, KL: 74.0614013671875, Loss: 0.014126676134765148, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11276/20000], Bound: 0.35678619146347046, Entropy: 141.2955780029297, Temp: 2.6939213275909424, KL: 67.19400024414062, Loss: 0.01365977618843317, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11277/20000], Bound: 0.38885197043418884, Entropy: 139.91412353515625, Temp: 2.6939597129821777, KL: 73.83065795898438, Loss: 0.018356826156377792, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11278/20000], Bound: 0.38162797689437866, Entropy: 142.45018005371094, Temp: 2.693986177444458, KL: 72.06687927246094, Loss: 0.01772892288863659, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11279/20000], Bound: 0.38899746537208557, Entropy: 140.24461364746094, Temp: 2.694002866744995, KL: 74.23382568359375, Loss: 0.017687968909740448, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11280/20000], Bound: 0.39281415939331055, Entropy: 142.2318572998047, Temp: 2.6940135955810547, KL: 75.49884033203125, Loss: 0.01741875894367695, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11281/20000], Bound: 0.3904784917831421, Entropy: 140.64390563964844, Temp: 2.694020986557007, KL: 75.25601196289062, Loss: 0.016596149653196335, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11282/20000], Bound: 0.3813249468803406, Entropy: 138.34165954589844, Temp: 2.694028854370117, KL: 72.2440185546875, Loss: 0.017237717285752296, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11283/20000], Bound: 0.3694644570350647, Entropy: 140.29547119140625, Temp: 2.694031238555908, KL: 68.81120300292969, Loss: 0.0172934178262949, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11284/20000], Bound: 0.39257803559303284, Entropy: 140.00146484375, Temp: 2.6940255165100098, KL: 75.70347595214844, Loss: 0.01691017672419548, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11285/20000], Bound: 0.3642035722732544, Entropy: 141.19757080078125, Temp: 2.6940207481384277, KL: 66.65692138671875, Loss: 0.018524330109357834, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11286/20000], Bound: 0.3844486176967621, Entropy: 140.40553283691406, Temp: 2.6940009593963623, KL: 73.27735900878906, Loss: 0.0170009583234787, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11287/20000], Bound: 0.3952331840991974, Entropy: 138.31674194335938, Temp: 2.6939806938171387, KL: 75.64007568359375, Loss: 0.01847977377474308, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11288/20000], Bound: 0.4105719327926636, Entropy: 139.6443328857422, Temp: 2.693955659866333, KL: 80.74778747558594, Loss: 0.017503034323453903, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11289/20000], Bound: 0.37745288014411926, Entropy: 140.81785583496094, Temp: 2.693934917449951, KL: 71.96469116210938, Loss: 0.01568167470395565, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11290/20000], Bound: 0.380491703748703, Entropy: 142.69012451171875, Temp: 2.6939187049865723, KL: 73.0281982421875, Loss: 0.015334155410528183, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11291/20000], Bound: 0.37399211525917053, Entropy: 140.43951416015625, Temp: 2.693908929824829, KL: 71.79670715332031, Loss: 0.014149595983326435, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11292/20000], Bound: 0.38850298523902893, Entropy: 140.8107147216797, Temp: 2.6939094066619873, KL: 74.28900146484375, Loss: 0.017316309735178947, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11293/20000], Bound: 0.39127904176712036, Entropy: 142.41065979003906, Temp: 2.693906784057617, KL: 76.62948608398438, Loss: 0.014481822960078716, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11294/20000], Bound: 0.381783664226532, Entropy: 139.73538208007812, Temp: 2.6939163208007812, KL: 73.66656494140625, Loss: 0.014842920936644077, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11295/20000], Bound: 0.39911508560180664, Entropy: 139.78082275390625, Temp: 2.69393253326416, KL: 77.84355163574219, Loss: 0.016523344442248344, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11296/20000], Bound: 0.385743111371994, Entropy: 141.21546936035156, Temp: 2.693950891494751, KL: 73.31817626953125, Loss: 0.01762385293841362, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11297/20000], Bound: 0.39733612537384033, Entropy: 141.69044494628906, Temp: 2.693962574005127, KL: 76.33798217773438, Loss: 0.018338637426495552, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11298/20000], Bound: 0.41092026233673096, Entropy: 142.62689208984375, Temp: 2.69396710395813, KL: 81.63932800292969, Loss: 0.016043823212385178, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11299/20000], Bound: 0.37806057929992676, Entropy: 142.30210876464844, Temp: 2.6939806938171387, KL: 71.307861328125, Loss: 0.017225850373506546, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11300/20000], Bound: 0.3872106671333313, Entropy: 141.58810424804688, Temp: 2.6939876079559326, KL: 73.00544738769531, Loss: 0.018998602405190468, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11301/20000], Bound: 0.35729941725730896, Entropy: 142.05825805664062, Temp: 2.6939821243286133, KL: 65.93728637695312, Loss: 0.01625899039208889, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11302/20000], Bound: 0.3696182072162628, Entropy: 140.81109619140625, Temp: 2.6939711570739746, KL: 69.91337585449219, Loss: 0.01532854326069355, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11303/20000], Bound: 0.3961072862148285, Entropy: 141.4353485107422, Temp: 2.6939635276794434, KL: 77.67817687988281, Loss: 0.015176277607679367, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11304/20000], Bound: 0.39173609018325806, Entropy: 140.57546997070312, Temp: 2.6939663887023926, KL: 74.65364074707031, Loss: 0.018398704007267952, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11305/20000], Bound: 0.3827365040779114, Entropy: 141.61349487304688, Temp: 2.6939616203308105, KL: 73.25308227539062, Loss: 0.016123197972774506, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11306/20000], Bound: 0.3883276581764221, Entropy: 141.43954467773438, Temp: 2.6939587593078613, KL: 75.58822631835938, Loss: 0.014810259453952312, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11307/20000], Bound: 0.36642885208129883, Entropy: 140.4305877685547, Temp: 2.6939656734466553, KL: 70.42453002929688, Loss: 0.012699282728135586, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11308/20000], Bound: 0.40601739287376404, Entropy: 140.033203125, Temp: 2.693986654281616, KL: 78.43327331542969, Loss: 0.019253771752119064, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11309/20000], Bound: 0.4016771614551544, Entropy: 140.09962463378906, Temp: 2.693997383117676, KL: 78.29985046386719, Loss: 0.017092067748308182, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11310/20000], Bound: 0.39594796299934387, Entropy: 141.5540313720703, Temp: 2.6940090656280518, KL: 77.12541198730469, Loss: 0.016115251928567886, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11311/20000], Bound: 0.37804505228996277, Entropy: 141.41851806640625, Temp: 2.6940243244171143, KL: 71.90814208984375, Loss: 0.016103820875287056, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11312/20000], Bound: 0.38700589537620544, Entropy: 139.45802307128906, Temp: 2.6940386295318604, KL: 75.49777221679688, Loss: 0.014262502081692219, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11313/20000], Bound: 0.35259461402893066, Entropy: 141.99050903320312, Temp: 2.694063186645508, KL: 63.75917053222656, Loss: 0.017868317663669586, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11314/20000], Bound: 0.392948180437088, Entropy: 140.32919311523438, Temp: 2.6940701007843018, KL: 76.24423217773438, Loss: 0.01610909029841423, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11315/20000], Bound: 0.40776342153549194, Entropy: 140.68063354492188, Temp: 2.6940805912017822, KL: 79.70626831054688, Loss: 0.017865700647234917, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11316/20000], Bound: 0.369517058134079, Entropy: 141.89695739746094, Temp: 2.694089412689209, KL: 68.22108459472656, Loss: 0.01841684617102146, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11317/20000], Bound: 0.3731828033924103, Entropy: 140.31277465820312, Temp: 2.6940839290618896, KL: 69.14617919921875, Loss: 0.018640507012605667, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11318/20000], Bound: 0.4121256172657013, Entropy: 140.123779296875, Temp: 2.6940650939941406, KL: 81.70574951171875, Loss: 0.01659848727285862, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11319/20000], Bound: 0.3714883029460907, Entropy: 142.56179809570312, Temp: 2.6940553188323975, KL: 69.36003112792969, Loss: 0.01734510064125061, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11320/20000], Bound: 0.39404016733169556, Entropy: 140.65684509277344, Temp: 2.6940386295318604, KL: 75.98776245117188, Loss: 0.017181724309921265, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11321/20000], Bound: 0.4046924114227295, Entropy: 140.22291564941406, Temp: 2.6940228939056396, KL: 79.25387573242188, Loss: 0.016993878409266472, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11322/20000], Bound: 0.38154086470603943, Entropy: 141.77084350585938, Temp: 2.694011688232422, KL: 71.75749206542969, Loss: 0.01825653947889805, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11323/20000], Bound: 0.3770594894886017, Entropy: 141.10043334960938, Temp: 2.6939921379089355, KL: 71.82795715332031, Loss: 0.01572594977915287, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11324/20000], Bound: 0.4060942530632019, Entropy: 140.66876220703125, Temp: 2.693976640701294, KL: 79.27998352050781, Loss: 0.017724962905049324, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11325/20000], Bound: 0.4036966860294342, Entropy: 140.5615234375, Temp: 2.693962335586548, KL: 78.34893798828125, Loss: 0.018119754269719124, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11326/20000], Bound: 0.3749517798423767, Entropy: 140.98292541503906, Temp: 2.693946599960327, KL: 68.84683227539062, Loss: 0.02013527601957321, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11327/20000], Bound: 0.3987271189689636, Entropy: 140.89404296875, Temp: 2.693911552429199, KL: 76.54655456542969, Loss: 0.018716607242822647, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11328/20000], Bound: 0.37837857007980347, Entropy: 141.17161560058594, Temp: 2.6938729286193848, KL: 73.23814392089844, Loss: 0.013812239281833172, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11329/20000], Bound: 0.37456437945365906, Entropy: 141.37356567382812, Temp: 2.69385027885437, KL: 71.51467895507812, Loss: 0.01497672963887453, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11330/20000], Bound: 0.3511202931404114, Entropy: 141.85092163085938, Temp: 2.6938350200653076, KL: 64.15620422363281, Loss: 0.016370436176657677, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11331/20000], Bound: 0.38294246792793274, Entropy: 142.76756286621094, Temp: 2.6938130855560303, KL: 71.24282836914062, Loss: 0.019963955506682396, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11332/20000], Bound: 0.40425431728363037, Entropy: 142.82284545898438, Temp: 2.6937756538391113, KL: 77.99690246582031, Loss: 0.01908106729388237, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11333/20000], Bound: 0.3560180068016052, Entropy: 142.23736572265625, Temp: 2.693734645843506, KL: 65.96478271484375, Loss: 0.015541652217507362, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11334/20000], Bound: 0.39485007524490356, Entropy: 140.05694580078125, Temp: 2.693695068359375, KL: 75.91748046875, Loss: 0.017752399668097496, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11335/20000], Bound: 0.3899279236793518, Entropy: 141.8477020263672, Temp: 2.6936562061309814, KL: 71.94198608398438, Loss: 0.022444767877459526, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11336/20000], Bound: 0.347007691860199, Entropy: 143.96722412109375, Temp: 2.6935932636260986, KL: 62.5352783203125, Loss: 0.017267264425754547, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11337/20000], Bound: 0.3578766882419586, Entropy: 140.7719268798828, Temp: 2.6935229301452637, KL: 66.65657043457031, Loss: 0.01522017177194357, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11338/20000], Bound: 0.40219274163246155, Entropy: 141.90467834472656, Temp: 2.6934595108032227, KL: 79.17137145996094, Loss: 0.015754394233226776, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11339/20000], Bound: 0.3763745427131653, Entropy: 140.07601928710938, Temp: 2.693410634994507, KL: 71.57577514648438, Loss: 0.01582351140677929, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11340/20000], Bound: 0.406081885099411, Entropy: 140.6215057373047, Temp: 2.6933679580688477, KL: 79.91539001464844, Loss: 0.016532521694898605, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11341/20000], Bound: 0.39671778678894043, Entropy: 142.66580200195312, Temp: 2.6933352947235107, KL: 76.06452941894531, Loss: 0.018500788137316704, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11342/20000], Bound: 0.404109388589859, Entropy: 142.43833923339844, Temp: 2.6932995319366455, KL: 80.13984680175781, Loss: 0.015017885714769363, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11343/20000], Bound: 0.3709466755390167, Entropy: 140.7232666015625, Temp: 2.693279504776001, KL: 70.40103149414062, Loss: 0.01511967834085226, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11344/20000], Bound: 0.36904218792915344, Entropy: 141.90914916992188, Temp: 2.693265438079834, KL: 70.37876892089844, Loss: 0.01415441557765007, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11345/20000], Bound: 0.38198384642601013, Entropy: 140.74668884277344, Temp: 2.693260431289673, KL: 72.00730895996094, Loss: 0.018024813383817673, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11346/20000], Bound: 0.40672850608825684, Entropy: 141.43405151367188, Temp: 2.6932477951049805, KL: 78.928955078125, Loss: 0.01872304081916809, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11347/20000], Bound: 0.40295282006263733, Entropy: 142.9896697998047, Temp: 2.6932315826416016, KL: 76.711181640625, Loss: 0.020740646868944168, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11348/20000], Bound: 0.37703952193260193, Entropy: 142.81875610351562, Temp: 2.6932010650634766, KL: 71.66206359863281, Loss: 0.01601635478436947, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11349/20000], Bound: 0.4086046814918518, Entropy: 141.3907928466797, Temp: 2.693174362182617, KL: 80.56431579589844, Loss: 0.01673394814133644, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11350/20000], Bound: 0.3917843997478485, Entropy: 144.30116271972656, Temp: 2.693155288696289, KL: 76.28457641601562, Loss: 0.015390107408165932, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11351/20000], Bound: 0.38174158334732056, Entropy: 143.72032165527344, Temp: 2.693145990371704, KL: 73.27033996582031, Loss: 0.01554869581013918, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11352/20000], Bound: 0.3774125277996063, Entropy: 141.4531707763672, Temp: 2.6931416988372803, KL: 71.06576538085938, Loss: 0.01732204668223858, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11353/20000], Bound: 0.3634113669395447, Entropy: 141.9219207763672, Temp: 2.693132162094116, KL: 67.90538024902344, Loss: 0.015785250812768936, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11354/20000], Bound: 0.39456743001937866, Entropy: 143.0888214111328, Temp: 2.6931216716766357, KL: 76.65452575683594, Loss: 0.0162239708006382, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11355/20000], Bound: 0.38667672872543335, Entropy: 141.56747436523438, Temp: 2.6931164264678955, KL: 74.70875549316406, Loss: 0.015539970248937607, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11356/20000], Bound: 0.3709055185317993, Entropy: 141.12890625, Temp: 2.693117380142212, KL: 70.08811950683594, Loss: 0.01567745953798294, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11357/20000], Bound: 0.37664252519607544, Entropy: 140.35775756835938, Temp: 2.6931190490722656, KL: 72.48396301269531, Loss: 0.01427782978862524, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11358/20000], Bound: 0.38320958614349365, Entropy: 143.40090942382812, Temp: 2.693129539489746, KL: 71.89619445800781, Loss: 0.018889619037508965, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11359/20000], Bound: 0.3791864812374115, Entropy: 141.93922424316406, Temp: 2.693126916885376, KL: 71.27528381347656, Loss: 0.017881708219647408, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11360/20000], Bound: 0.3925095200538635, Entropy: 141.80931091308594, Temp: 2.6931161880493164, KL: 75.64663696289062, Loss: 0.016969820484519005, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11361/20000], Bound: 0.36785638332366943, Entropy: 142.1497344970703, Temp: 2.6931066513061523, KL: 69.06912231445312, Loss: 0.015959186479449272, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11362/20000], Bound: 0.40208813548088074, Entropy: 142.27224731445312, Temp: 2.693096399307251, KL: 77.76689147949219, Loss: 0.018300265073776245, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11363/20000], Bound: 0.3970341086387634, Entropy: 141.6631317138672, Temp: 2.6930830478668213, KL: 77.72972106933594, Loss: 0.01558073703199625, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11364/20000], Bound: 0.37957069277763367, Entropy: 141.56919860839844, Temp: 2.6930792331695557, KL: 71.62220764160156, Loss: 0.017443010583519936, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11365/20000], Bound: 0.3800022006034851, Entropy: 142.26507568359375, Temp: 2.6930699348449707, KL: 71.665283203125, Loss: 0.017594268545508385, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11366/20000], Bound: 0.3700391352176666, Entropy: 140.3529052734375, Temp: 2.6930551528930664, KL: 69.59939575195312, Loss: 0.01612612046301365, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11367/20000], Bound: 0.39989253878593445, Entropy: 142.5979766845703, Temp: 2.693039894104004, KL: 77.06114196777344, Loss: 0.01839614100754261, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11368/20000], Bound: 0.3847815990447998, Entropy: 140.6493682861328, Temp: 2.693020820617676, KL: 73.12213134765625, Loss: 0.01746036671102047, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11369/20000], Bound: 0.3732525408267975, Entropy: 142.27169799804688, Temp: 2.6929993629455566, KL: 69.40751647949219, Loss: 0.018184231594204903, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11370/20000], Bound: 0.3852725625038147, Entropy: 141.78225708007812, Temp: 2.6929686069488525, KL: 73.11859130859375, Loss: 0.017731651663780212, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11371/20000], Bound: 0.43330222368240356, Entropy: 141.29867553710938, Temp: 2.6929352283477783, KL: 85.24052429199219, Loss: 0.022124838083982468, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11372/20000], Bound: 0.3750390112400055, Entropy: 142.50765991210938, Temp: 2.6928906440734863, KL: 70.34396362304688, Loss: 0.01739451102912426, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11373/20000], Bound: 0.4039575457572937, Entropy: 139.2226104736328, Temp: 2.6928436756134033, KL: 79.53450012207031, Loss: 0.016052665188908577, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11374/20000], Bound: 0.3421291708946228, Entropy: 141.97427368164062, Temp: 2.6928091049194336, KL: 62.399566650390625, Loss: 0.015025647357106209, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11375/20000], Bound: 0.36792445182800293, Entropy: 140.64418029785156, Temp: 2.6927742958068848, KL: 67.77151489257812, Loss: 0.018401771783828735, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11376/20000], Bound: 0.38603654503822327, Entropy: 141.40643310546875, Temp: 2.6927294731140137, KL: 73.90829467773438, Loss: 0.016676250845193863, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11377/20000], Bound: 0.3985978066921234, Entropy: 141.4362030029297, Temp: 2.6926887035369873, KL: 77.23622131347656, Loss: 0.017353743314743042, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11378/20000], Bound: 0.393292099237442, Entropy: 140.86358642578125, Temp: 2.6926517486572266, KL: 76.068115234375, Loss: 0.016610493883490562, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11379/20000], Bound: 0.40525272488594055, Entropy: 141.98324584960938, Temp: 2.692620277404785, KL: 80.40824890136719, Loss: 0.015147924423217773, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11380/20000], Bound: 0.3388270437717438, Entropy: 141.74464416503906, Temp: 2.6926045417785645, KL: 60.72846984863281, Loss: 0.016452321782708168, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11381/20000], Bound: 0.37301647663116455, Entropy: 142.05740356445312, Temp: 2.6925787925720215, KL: 70.15719604492188, Loss: 0.016663575544953346, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11382/20000], Bound: 0.38881683349609375, Entropy: 142.600830078125, Temp: 2.692551851272583, KL: 74.41204833984375, Loss: 0.01724615879356861, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11383/20000], Bound: 0.38258129358291626, Entropy: 141.03884887695312, Temp: 2.6925251483917236, KL: 72.96255493164062, Loss: 0.016566263511776924, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11384/20000], Bound: 0.39679113030433655, Entropy: 142.05206298828125, Temp: 2.692500591278076, KL: 76.91120910644531, Loss: 0.016961293295025826, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11385/20000], Bound: 0.38910138607025146, Entropy: 143.70654296875, Temp: 2.692479372024536, KL: 76.38934326171875, Loss: 0.013728203251957893, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11386/20000], Bound: 0.3775007724761963, Entropy: 141.98294067382812, Temp: 2.6924757957458496, KL: 71.01182556152344, Loss: 0.017463913187384605, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11387/20000], Bound: 0.38710400462150574, Entropy: 141.9571075439453, Temp: 2.6924660205841064, KL: 76.27537536621094, Loss: 0.012856046669185162, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11388/20000], Bound: 0.4166346788406372, Entropy: 142.30621337890625, Temp: 2.692476511001587, KL: 83.00761413574219, Loss: 0.016708137467503548, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11389/20000], Bound: 0.3982740640640259, Entropy: 141.56381225585938, Temp: 2.6924936771392822, KL: 77.81317138671875, Loss: 0.016102135181427002, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11390/20000], Bound: 0.3925153315067291, Entropy: 140.79539489746094, Temp: 2.6925148963928223, KL: 76.00874328613281, Loss: 0.01629502698779106, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11391/20000], Bound: 0.3896367847919464, Entropy: 141.41522216796875, Temp: 2.692537546157837, KL: 74.34895324707031, Loss: 0.017808808013796806, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11392/20000], Bound: 0.35759568214416504, Entropy: 143.7919158935547, Temp: 2.6925528049468994, KL: 64.11798095703125, Loss: 0.019780632108449936, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11393/20000], Bound: 0.3916241228580475, Entropy: 140.91749572753906, Temp: 2.692542791366577, KL: 76.40151977539062, Loss: 0.015079524368047714, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11394/20000], Bound: 0.36757320165634155, Entropy: 140.16162109375, Temp: 2.6925430297851562, KL: 67.55555725097656, Loss: 0.018616028130054474, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11395/20000], Bound: 0.3905610144138336, Entropy: 140.14022827148438, Temp: 2.692528247833252, KL: 75.94773864746094, Loss: 0.01534270215779543, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11396/20000], Bound: 0.415739506483078, Entropy: 142.2176971435547, Temp: 2.6925227642059326, KL: 83.54412841796875, Loss: 0.015205876901745796, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11397/20000], Bound: 0.3839948773384094, Entropy: 140.4206085205078, Temp: 2.692532539367676, KL: 72.86708068847656, Loss: 0.01750519871711731, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11398/20000], Bound: 0.37689730525016785, Entropy: 141.3774871826172, Temp: 2.6925363540649414, KL: 69.21308898925781, Loss: 0.020482363179326057, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11399/20000], Bound: 0.39438992738723755, Entropy: 140.62481689453125, Temp: 2.6925177574157715, KL: 76.08021545410156, Loss: 0.01718739978969097, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11400/20000], Bound: 0.39543411135673523, Entropy: 140.42462158203125, Temp: 2.6925008296966553, KL: 78.35330200195312, Loss: 0.013538348488509655, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11401/20000], Bound: 0.38318634033203125, Entropy: 140.7430877685547, Temp: 2.6925032138824463, KL: 72.81686401367188, Loss: 0.017162425443530083, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11402/20000], Bound: 0.3792773485183716, Entropy: 142.72386169433594, Temp: 2.6925017833709717, KL: 71.33888244628906, Loss: 0.017807193100452423, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11403/20000], Bound: 0.412737637758255, Entropy: 142.354248046875, Temp: 2.692492961883545, KL: 81.79489135742188, Loss: 0.01676049269735813, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11404/20000], Bound: 0.39042091369628906, Entropy: 141.1403045654297, Temp: 2.692491292953491, KL: 75.10055541992188, Loss: 0.016839291900396347, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11405/20000], Bound: 0.3829563558101654, Entropy: 141.180419921875, Temp: 2.6924901008605957, KL: 74.17828369140625, Loss: 0.014510254375636578, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11406/20000], Bound: 0.39720815420150757, Entropy: 140.854736328125, Temp: 2.6924986839294434, KL: 77.04188537597656, Loss: 0.016947874799370766, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11407/20000], Bound: 0.3859877288341522, Entropy: 141.5110321044922, Temp: 2.69250750541687, KL: 73.36180114746094, Loss: 0.01766267977654934, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11408/20000], Bound: 0.39206454157829285, Entropy: 142.63113403320312, Temp: 2.6925106048583984, KL: 75.59872436523438, Loss: 0.016810288652777672, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11409/20000], Bound: 0.4085436463356018, Entropy: 141.02606201171875, Temp: 2.692513942718506, KL: 82.08377075195312, Loss: 0.013871300965547562, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11410/20000], Bound: 0.3759075999259949, Entropy: 141.49478149414062, Temp: 2.6925363540649414, KL: 70.69483947753906, Loss: 0.01720280572772026, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11411/20000], Bound: 0.37357908487319946, Entropy: 142.393310546875, Temp: 2.6925508975982666, KL: 70.21371459960938, Loss: 0.016857177019119263, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11412/20000], Bound: 0.3646800220012665, Entropy: 141.37120056152344, Temp: 2.6925597190856934, KL: 68.19219970703125, Loss: 0.015913063660264015, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11413/20000], Bound: 0.38735100626945496, Entropy: 142.31494140625, Temp: 2.692565441131592, KL: 74.35285949707031, Loss: 0.016560956835746765, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11414/20000], Bound: 0.34866413474082947, Entropy: 143.53720092773438, Temp: 2.6925711631774902, KL: 63.568450927734375, Loss: 0.016190847381949425, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11415/20000], Bound: 0.38829708099365234, Entropy: 142.77220153808594, Temp: 2.692568778991699, KL: 74.88414001464844, Loss: 0.016087472438812256, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11416/20000], Bound: 0.38163837790489197, Entropy: 141.6653594970703, Temp: 2.6925697326660156, KL: 70.87321472167969, Loss: 0.0199393592774868, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11417/20000], Bound: 0.3841679096221924, Entropy: 141.9309539794922, Temp: 2.6925532817840576, KL: 74.35751342773438, Loss: 0.014831062406301498, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11418/20000], Bound: 0.3876067101955414, Entropy: 142.7487335205078, Temp: 2.6925466060638428, KL: 73.33958435058594, Loss: 0.01858101412653923, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11419/20000], Bound: 0.37078437209129333, Entropy: 141.44764709472656, Temp: 2.692531108856201, KL: 69.66351318359375, Loss: 0.01639689691364765, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11420/20000], Bound: 0.3994053304195404, Entropy: 141.61264038085938, Temp: 2.692514419555664, KL: 77.70274353027344, Loss: 0.01693097874522209, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11421/20000], Bound: 0.393942654132843, Entropy: 140.50595092773438, Temp: 2.6925013065338135, KL: 76.71755981445312, Loss: 0.01575886458158493, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11422/20000], Bound: 0.365252822637558, Entropy: 143.39060974121094, Temp: 2.6924960613250732, KL: 67.13496398925781, Loss: 0.018176469951868057, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11423/20000], Bound: 0.39224979281425476, Entropy: 141.04478454589844, Temp: 2.6924779415130615, KL: 76.10881042480469, Loss: 0.015963852405548096, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11424/20000], Bound: 0.4157712459564209, Entropy: 142.801513671875, Temp: 2.6924664974212646, KL: 81.72015380859375, Loss: 0.018610388040542603, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11425/20000], Bound: 0.36536741256713867, Entropy: 141.9084014892578, Temp: 2.6924543380737305, KL: 67.98127746582031, Loss: 0.016664693132042885, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11426/20000], Bound: 0.401936411857605, Entropy: 142.75466918945312, Temp: 2.6924376487731934, KL: 78.661865234375, Loss: 0.016548072919249535, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11427/20000], Bound: 0.3721367418766022, Entropy: 142.61605834960938, Temp: 2.692427158355713, KL: 68.87832641601562, Loss: 0.018570542335510254, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11428/20000], Bound: 0.3709441125392914, Entropy: 141.14036560058594, Temp: 2.692404270172119, KL: 70.80705261230469, Loss: 0.014356754720211029, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11429/20000], Bound: 0.3772425353527069, Entropy: 142.61566162109375, Temp: 2.6923911571502686, KL: 70.81460571289062, Loss: 0.017691535875201225, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11430/20000], Bound: 0.41236284375190735, Entropy: 141.48483276367188, Temp: 2.6923716068267822, KL: 81.57608032226562, Loss: 0.016954679042100906, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11431/20000], Bound: 0.36895906925201416, Entropy: 142.89089965820312, Temp: 2.692359447479248, KL: 69.34085083007812, Loss: 0.016030026599764824, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11432/20000], Bound: 0.41166284680366516, Entropy: 142.6014862060547, Temp: 2.6923470497131348, KL: 80.3602294921875, Loss: 0.018818849697709084, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11433/20000], Bound: 0.3773111402988434, Entropy: 143.08062744140625, Temp: 2.692331552505493, KL: 70.263671875, Loss: 0.01875082589685917, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11434/20000], Bound: 0.3707449436187744, Entropy: 140.95787048339844, Temp: 2.692304849624634, KL: 69.14169311523438, Loss: 0.017343290150165558, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11435/20000], Bound: 0.3914898633956909, Entropy: 141.44346618652344, Temp: 2.6922731399536133, KL: 76.44139099121094, Loss: 0.014929565601050854, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11436/20000], Bound: 0.3896736204624176, Entropy: 141.62196350097656, Temp: 2.6922545433044434, KL: 75.23915100097656, Loss: 0.016173144802451134, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11437/20000], Bound: 0.3956436812877655, Entropy: 141.99522399902344, Temp: 2.6922409534454346, KL: 74.8760986328125, Loss: 0.020108332857489586, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11438/20000], Bound: 0.4089599847793579, Entropy: 139.9700469970703, Temp: 2.692214012145996, KL: 78.75494384765625, Loss: 0.020283129066228867, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11439/20000], Bound: 0.3551880717277527, Entropy: 140.947998046875, Temp: 2.6921777725219727, KL: 66.30438232421875, Loss: 0.014469127170741558, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11440/20000], Bound: 0.36298781633377075, Entropy: 143.45587158203125, Temp: 2.692147970199585, KL: 67.20423889160156, Loss: 0.016857853159308434, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11441/20000], Bound: 0.37605831027030945, Entropy: 142.9536895751953, Temp: 2.6921141147613525, KL: 71.91845703125, Loss: 0.015007116831839085, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11442/20000], Bound: 0.3715985417366028, Entropy: 141.28762817382812, Temp: 2.6920888423919678, KL: 71.3375244140625, Loss: 0.01371529046446085, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11443/20000], Bound: 0.37443089485168457, Entropy: 141.35159301757812, Temp: 2.692077159881592, KL: 68.93136596679688, Loss: 0.019687913358211517, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11444/20000], Bound: 0.37257927656173706, Entropy: 140.58041381835938, Temp: 2.6920480728149414, KL: 70.76840209960938, Loss: 0.015291995368897915, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11445/20000], Bound: 0.37405848503112793, Entropy: 141.29519653320312, Temp: 2.6920254230499268, KL: 71.44476318359375, Loss: 0.014821218326687813, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11446/20000], Bound: 0.3941558003425598, Entropy: 139.110595703125, Temp: 2.6920106410980225, KL: 75.47981262207031, Loss: 0.018169676885008812, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11447/20000], Bound: 0.3668268620967865, Entropy: 139.77516174316406, Temp: 2.6919920444488525, KL: 67.81253051757812, Loss: 0.01774182729423046, Learning Rate: 9.96987917088029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11448/20000], Bound: 0.37193891406059265, Entropy: 144.49871826171875, Temp: 2.6919643878936768, KL: 70.73660278320312, Loss: 0.015010684728622437, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11449/20000], Bound: 0.38631346821784973, Entropy: 141.76080322265625, Temp: 2.691944122314453, KL: 74.21681213378906, Loss: 0.016246002167463303, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11450/20000], Bound: 0.40251898765563965, Entropy: 140.75640869140625, Temp: 2.691927909851074, KL: 78.27151489257812, Loss: 0.017590802162885666, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11451/20000], Bound: 0.38868486881256104, Entropy: 141.68630981445312, Temp: 2.691912889480591, KL: 75.68000793457031, Loss: 0.014813651330769062, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11452/20000], Bound: 0.36624476313591003, Entropy: 142.2116241455078, Temp: 2.691909074783325, KL: 68.26361083984375, Loss: 0.016597138717770576, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11453/20000], Bound: 0.4041275978088379, Entropy: 142.31785583496094, Temp: 2.6919007301330566, KL: 77.81581115722656, Loss: 0.019329724833369255, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11454/20000], Bound: 0.38167911767959595, Entropy: 141.8661651611328, Temp: 2.6918842792510986, KL: 73.04301452636719, Loss: 0.015925848856568336, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11455/20000], Bound: 0.3699173033237457, Entropy: 141.21302795410156, Temp: 2.6918721199035645, KL: 69.118408203125, Loss: 0.01694539003074169, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11456/20000], Bound: 0.40391960740089417, Entropy: 141.513916015625, Temp: 2.6918554306030273, KL: 78.94056701660156, Loss: 0.017124561592936516, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11457/20000], Bound: 0.38833221793174744, Entropy: 141.20729064941406, Temp: 2.691842555999756, KL: 75.35871887207031, Loss: 0.015218250453472137, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11458/20000], Bound: 0.3855510652065277, Entropy: 145.0381317138672, Temp: 2.6918387413024902, KL: 73.52432250976562, Loss: 0.017118889838457108, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11459/20000], Bound: 0.364021360874176, Entropy: 141.26498413085938, Temp: 2.6918327808380127, KL: 68.2071533203125, Loss: 0.01553401630371809, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11460/20000], Bound: 0.36849233508110046, Entropy: 141.19808959960938, Temp: 2.6918272972106934, KL: 69.62400817871094, Loss: 0.015253418125212193, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11461/20000], Bound: 0.40464937686920166, Entropy: 140.18687438964844, Temp: 2.6918246746063232, KL: 77.53968811035156, Loss: 0.020131953060626984, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11462/20000], Bound: 0.3952684998512268, Entropy: 140.81866455078125, Temp: 2.69180965423584, KL: 75.92031860351562, Loss: 0.01795932464301586, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11463/20000], Bound: 0.39849162101745605, Entropy: 141.53372192382812, Temp: 2.6917920112609863, KL: 79.49807739257812, Loss: 0.01308527309447527, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11464/20000], Bound: 0.39614805579185486, Entropy: 140.23133850097656, Temp: 2.6917972564697266, KL: 77.49656677246094, Loss: 0.015513995662331581, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11465/20000], Bound: 0.35830241441726685, Entropy: 141.8657989501953, Temp: 2.691810131072998, KL: 66.57661437988281, Loss: 0.015576567500829697, Learning Rate: 9.96987917088029e-05\n",
      "Epoch [11466/20000], Bound: 0.40227818489074707, Entropy: 140.97813415527344, Temp: 2.691817045211792, KL: 76.191162109375, Loss: 0.02132047526538372, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11467/20000], Bound: 0.3721596598625183, Entropy: 143.105224609375, Temp: 2.691809892654419, KL: 70.41012573242188, Loss: 0.015732793137431145, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11468/20000], Bound: 0.38217705488204956, Entropy: 139.82791137695312, Temp: 2.6918041706085205, KL: 73.64202880859375, Loss: 0.015080244280397892, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11469/20000], Bound: 0.40442609786987305, Entropy: 141.9955291748047, Temp: 2.6918036937713623, KL: 78.97036743164062, Loss: 0.01735013723373413, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11470/20000], Bound: 0.38674023747444153, Entropy: 140.37213134765625, Temp: 2.6918041706085205, KL: 73.109619140625, Loss: 0.018532386049628258, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11471/20000], Bound: 0.35973700881004333, Entropy: 140.33538818359375, Temp: 2.691798210144043, KL: 67.31683349609375, Loss: 0.01494829636067152, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11472/20000], Bound: 0.372236043214798, Entropy: 140.10507202148438, Temp: 2.69179368019104, KL: 70.0589599609375, Loss: 0.01642545685172081, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11473/20000], Bound: 0.38063719868659973, Entropy: 142.6506805419922, Temp: 2.6917879581451416, KL: 72.12382507324219, Loss: 0.017072614282369614, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11474/20000], Bound: 0.40110406279563904, Entropy: 142.8914794921875, Temp: 2.6917800903320312, KL: 76.41024780273438, Loss: 0.020263152197003365, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11475/20000], Bound: 0.3993963301181793, Entropy: 141.01409912109375, Temp: 2.691763401031494, KL: 78.83895874023438, Loss: 0.014808136969804764, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11476/20000], Bound: 0.40744268894195557, Entropy: 140.85716247558594, Temp: 2.6917572021484375, KL: 78.75581359863281, Loss: 0.019429022446274757, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11477/20000], Bound: 0.38893893361091614, Entropy: 142.21592712402344, Temp: 2.6917459964752197, KL: 74.94195556640625, Loss: 0.016320986673235893, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11478/20000], Bound: 0.4074103832244873, Entropy: 141.3878631591797, Temp: 2.691737413406372, KL: 78.74014282226562, Loss: 0.019439978525042534, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11479/20000], Bound: 0.39055272936820984, Entropy: 141.83160400390625, Temp: 2.6917240619659424, KL: 76.75846862792969, Loss: 0.013824383728206158, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11480/20000], Bound: 0.39225947856903076, Entropy: 141.3768310546875, Temp: 2.691722869873047, KL: 75.4412841796875, Loss: 0.017201807349920273, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11481/20000], Bound: 0.37814241647720337, Entropy: 140.314208984375, Temp: 2.691720724105835, KL: 71.68980407714844, Loss: 0.016541428864002228, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11482/20000], Bound: 0.3533734381198883, Entropy: 140.09727478027344, Temp: 2.6917178630828857, KL: 64.16084289550781, Loss: 0.017508799210190773, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11483/20000], Bound: 0.3868752717971802, Entropy: 143.69021606445312, Temp: 2.6917061805725098, KL: 73.38214111328125, Loss: 0.0180984940379858, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11484/20000], Bound: 0.39573368430137634, Entropy: 140.49365234375, Temp: 2.691690444946289, KL: 77.00790405273438, Loss: 0.01619318127632141, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11485/20000], Bound: 0.411085307598114, Entropy: 141.26699829101562, Temp: 2.6916799545288086, KL: 80.39198303222656, Loss: 0.018428875133395195, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11486/20000], Bound: 0.39373958110809326, Entropy: 140.46302795410156, Temp: 2.691668748855591, KL: 76.25840759277344, Loss: 0.01649247109889984, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11487/20000], Bound: 0.37511497735977173, Entropy: 141.11160278320312, Temp: 2.6916606426239014, KL: 70.40089416503906, Loss: 0.017319314181804657, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11488/20000], Bound: 0.41003164649009705, Entropy: 142.028564453125, Temp: 2.6916487216949463, KL: 80.27693176269531, Loss: 0.01805109716951847, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11489/20000], Bound: 0.3979012966156006, Entropy: 141.4747772216797, Temp: 2.6916377544403076, KL: 75.40287780761719, Loss: 0.020365692675113678, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11490/20000], Bound: 0.36834976077079773, Entropy: 140.0301055908203, Temp: 2.691617012023926, KL: 69.86219787597656, Loss: 0.014733951538801193, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11491/20000], Bound: 0.42534691095352173, Entropy: 139.90774536132812, Temp: 2.6916017532348633, KL: 84.19346618652344, Loss: 0.019462475553154945, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11492/20000], Bound: 0.35166874527931213, Entropy: 142.0323486328125, Temp: 2.6915857791900635, KL: 62.64141845703125, Loss: 0.019451046362519264, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11493/20000], Bound: 0.3799842596054077, Entropy: 141.9244384765625, Temp: 2.6915550231933594, KL: 71.34840393066406, Loss: 0.018160760402679443, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11494/20000], Bound: 0.407945841550827, Entropy: 138.5073699951172, Temp: 2.6915206909179688, KL: 80.47840881347656, Loss: 0.016507934778928757, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11495/20000], Bound: 0.3892183005809784, Entropy: 140.2925567626953, Temp: 2.6914944648742676, KL: 75.89114379882812, Loss: 0.014707189053297043, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11496/20000], Bound: 0.3779197633266449, Entropy: 140.97496032714844, Temp: 2.6914782524108887, KL: 72.53858947753906, Loss: 0.014843473210930824, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11497/20000], Bound: 0.4098510146141052, Entropy: 143.58554077148438, Temp: 2.6914684772491455, KL: 81.95628356933594, Loss: 0.014828274957835674, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11498/20000], Bound: 0.35874009132385254, Entropy: 140.59744262695312, Temp: 2.6914703845977783, KL: 67.42001342773438, Loss: 0.014234787784516811, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11499/20000], Bound: 0.40266847610473633, Entropy: 141.88470458984375, Temp: 2.6914753913879395, KL: 79.88157653808594, Loss: 0.014678278006613255, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11500/20000], Bound: 0.3996783196926117, Entropy: 140.69158935546875, Temp: 2.6914899349212646, KL: 77.46734619140625, Loss: 0.017508959397673607, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11501/20000], Bound: 0.403743714094162, Entropy: 140.39181518554688, Temp: 2.691502571105957, KL: 78.06390380859375, Loss: 0.018651945516467094, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11502/20000], Bound: 0.3794546127319336, Entropy: 140.45236206054688, Temp: 2.6915102005004883, KL: 73.67369079589844, Loss: 0.013556783087551594, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11503/20000], Bound: 0.3614352345466614, Entropy: 143.14991760253906, Temp: 2.6915266513824463, KL: 66.70947265625, Loss: 0.016960375010967255, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11504/20000], Bound: 0.39656007289886475, Entropy: 142.1508331298828, Temp: 2.691535711288452, KL: 76.73094177246094, Loss: 0.01715998351573944, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11505/20000], Bound: 0.41676265001296997, Entropy: 141.0248565673828, Temp: 2.6915440559387207, KL: 80.82310485839844, Loss: 0.020828528329730034, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11506/20000], Bound: 0.3775376081466675, Entropy: 142.57752990722656, Temp: 2.6915431022644043, KL: 70.607421875, Loss: 0.018227247521281242, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11507/20000], Bound: 0.39280959963798523, Entropy: 139.94276428222656, Temp: 2.6915347576141357, KL: 76.796875, Loss: 0.014982335269451141, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11508/20000], Bound: 0.3674430847167969, Entropy: 140.4729461669922, Temp: 2.6915342807769775, KL: 69.31869506835938, Loss: 0.015264966525137424, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11509/20000], Bound: 0.3796044886112213, Entropy: 140.5632781982422, Temp: 2.691535472869873, KL: 72.46905517578125, Loss: 0.015875184908509254, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11510/20000], Bound: 0.3656136691570282, Entropy: 140.84622192382812, Temp: 2.691537857055664, KL: 69.00262451171875, Loss: 0.014889625832438469, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11511/20000], Bound: 0.41241708397865295, Entropy: 142.6552276611328, Temp: 2.691542625427246, KL: 81.11904907226562, Loss: 0.017825458198785782, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11512/20000], Bound: 0.37978044152259827, Entropy: 142.45150756835938, Temp: 2.6915476322174072, KL: 71.94236755371094, Loss: 0.01694801263511181, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11513/20000], Bound: 0.36509352922439575, Entropy: 142.06600952148438, Temp: 2.691549777984619, KL: 67.47821044921875, Loss: 0.01744834892451763, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11514/20000], Bound: 0.4156886637210846, Entropy: 142.1825714111328, Temp: 2.691545248031616, KL: 82.7027587890625, Loss: 0.01672893948853016, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11515/20000], Bound: 0.38673707842826843, Entropy: 142.016357421875, Temp: 2.6915462017059326, KL: 75.01455688476562, Loss: 0.014989776536822319, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11516/20000], Bound: 0.3886930048465729, Entropy: 142.15512084960938, Temp: 2.6915526390075684, KL: 74.50350952148438, Loss: 0.01700008288025856, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11517/20000], Bound: 0.39048337936401367, Entropy: 142.2271270751953, Temp: 2.6915581226348877, KL: 74.71318054199219, Loss: 0.01758437789976597, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11518/20000], Bound: 0.3789106011390686, Entropy: 141.58001708984375, Temp: 2.6915602684020996, KL: 71.78468322753906, Loss: 0.01677493005990982, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11519/20000], Bound: 0.3833051025867462, Entropy: 138.9299774169922, Temp: 2.6915605068206787, KL: 74.65786743164062, Loss: 0.01379834022372961, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11520/20000], Bound: 0.3984600007534027, Entropy: 140.93931579589844, Temp: 2.691570281982422, KL: 79.53504943847656, Loss: 0.012996713630855083, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11521/20000], Bound: 0.3901955783367157, Entropy: 141.06683349609375, Temp: 2.691594123840332, KL: 75.41581726074219, Loss: 0.016122756525874138, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11522/20000], Bound: 0.41426050662994385, Entropy: 140.48696899414062, Temp: 2.6916182041168213, KL: 81.83021545410156, Loss: 0.01754387840628624, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11523/20000], Bound: 0.4154464900493622, Entropy: 141.71923828125, Temp: 2.6916418075561523, KL: 81.22474670410156, Loss: 0.01933859847486019, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11524/20000], Bound: 0.36242377758026123, Entropy: 141.2087860107422, Temp: 2.691659450531006, KL: 65.95639038085938, Loss: 0.018877023831009865, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11525/20000], Bound: 0.3805689215660095, Entropy: 141.534912109375, Temp: 2.6916627883911133, KL: 72.06222534179688, Loss: 0.017149358987808228, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11526/20000], Bound: 0.3756062686443329, Entropy: 142.54644775390625, Temp: 2.6916630268096924, KL: 72.30624389648438, Loss: 0.01404169574379921, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11527/20000], Bound: 0.3756570816040039, Entropy: 143.7123260498047, Temp: 2.6916704177856445, KL: 71.30673217773438, Loss: 0.015925539657473564, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11528/20000], Bound: 0.3881825804710388, Entropy: 141.12527465820312, Temp: 2.6916775703430176, KL: 74.67706298828125, Loss: 0.01640166901051998, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11529/20000], Bound: 0.3895866572856903, Entropy: 142.8667755126953, Temp: 2.691685438156128, KL: 76.62277221679688, Loss: 0.013550307601690292, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11530/20000], Bound: 0.3671286404132843, Entropy: 140.70652770996094, Temp: 2.6917037963867188, KL: 68.4373779296875, Loss: 0.01673787273466587, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11531/20000], Bound: 0.3895515203475952, Entropy: 141.56480407714844, Temp: 2.6917166709899902, KL: 74.34515380859375, Loss: 0.017762314528226852, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11532/20000], Bound: 0.39113056659698486, Entropy: 140.60552978515625, Temp: 2.6917247772216797, KL: 74.43263244628906, Loss: 0.018459567800164223, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11533/20000], Bound: 0.403091162443161, Entropy: 141.55332946777344, Temp: 2.6917269229888916, KL: 80.55268859863281, Loss: 0.013668779283761978, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11534/20000], Bound: 0.399193674325943, Entropy: 140.8085174560547, Temp: 2.691741943359375, KL: 76.60446166992188, Loss: 0.018846796825528145, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11535/20000], Bound: 0.3888890743255615, Entropy: 142.3211212158203, Temp: 2.6917502880096436, KL: 72.3828125, Loss: 0.02104763872921467, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11536/20000], Bound: 0.409254252910614, Entropy: 140.15878295898438, Temp: 2.6917428970336914, KL: 80.32667541503906, Loss: 0.01752403751015663, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11537/20000], Bound: 0.3974132835865021, Entropy: 141.4540252685547, Temp: 2.691737651824951, KL: 76.50735473632812, Loss: 0.018046343699097633, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11538/20000], Bound: 0.3892054259777069, Entropy: 141.23004150390625, Temp: 2.691730260848999, KL: 74.51907348632812, Loss: 0.017251182347536087, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11539/20000], Bound: 0.3723469376564026, Entropy: 140.15919494628906, Temp: 2.6917216777801514, KL: 69.00776672363281, Loss: 0.018436329439282417, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11540/20000], Bound: 0.37459394335746765, Entropy: 142.4513702392578, Temp: 2.6917052268981934, KL: 71.28889465332031, Loss: 0.015392784960567951, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11541/20000], Bound: 0.35499951243400574, Entropy: 142.16043090820312, Temp: 2.69169282913208, KL: 64.36311340332031, Loss: 0.017973674461245537, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11542/20000], Bound: 0.39954420924186707, Entropy: 141.35499572753906, Temp: 2.6916708946228027, KL: 78.4130859375, Loss: 0.015679897740483284, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11543/20000], Bound: 0.4045015573501587, Entropy: 140.94297790527344, Temp: 2.691657304763794, KL: 78.57127380371094, Loss: 0.018132012337446213, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11544/20000], Bound: 0.38731464743614197, Entropy: 142.135986328125, Temp: 2.691642999649048, KL: 72.02932739257812, Loss: 0.020849017426371574, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11545/20000], Bound: 0.3936755359172821, Entropy: 143.27020263671875, Temp: 2.6916158199310303, KL: 75.67939758300781, Loss: 0.017532499507069588, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11546/20000], Bound: 0.39652934670448303, Entropy: 140.93978881835938, Temp: 2.691589593887329, KL: 76.23866271972656, Loss: 0.01805809885263443, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11547/20000], Bound: 0.41752588748931885, Entropy: 139.7703857421875, Temp: 2.691563129425049, KL: 84.48504638671875, Loss: 0.01445865724235773, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11548/20000], Bound: 0.36176833510398865, Entropy: 140.66229248046875, Temp: 2.6915526390075684, KL: 67.86637878417969, Loss: 0.014985472895205021, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11549/20000], Bound: 0.40644821524620056, Entropy: 140.1766357421875, Temp: 2.691544532775879, KL: 78.24101257324219, Loss: 0.01982840523123741, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11550/20000], Bound: 0.38878270983695984, Entropy: 140.91632080078125, Temp: 2.691530227661133, KL: 73.10658264160156, Loss: 0.019643645733594894, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11551/20000], Bound: 0.3683489263057709, Entropy: 140.93341064453125, Temp: 2.691507339477539, KL: 69.62857055664062, Loss: 0.015166576951742172, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11552/20000], Bound: 0.39257147908210754, Entropy: 139.83004760742188, Temp: 2.691488265991211, KL: 76.092041015625, Loss: 0.016161177307367325, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11553/20000], Bound: 0.4170342683792114, Entropy: 140.37696838378906, Temp: 2.691474437713623, KL: 81.86651611328125, Loss: 0.019043363630771637, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11554/20000], Bound: 0.3955138027667999, Entropy: 141.4205780029297, Temp: 2.6914591789245605, KL: 76.10874938964844, Loss: 0.017740651965141296, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11555/20000], Bound: 0.41065603494644165, Entropy: 141.21966552734375, Temp: 2.69144344329834, KL: 80.69342041015625, Loss: 0.017625534906983376, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11556/20000], Bound: 0.3848041892051697, Entropy: 140.49444580078125, Temp: 2.6914305686950684, KL: 72.80551147460938, Loss: 0.018047038465738297, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11557/20000], Bound: 0.38950875401496887, Entropy: 140.62478637695312, Temp: 2.691413640975952, KL: 74.59719848632812, Loss: 0.017268113791942596, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11558/20000], Bound: 0.3902062773704529, Entropy: 141.3103485107422, Temp: 2.691396951675415, KL: 75.51637268066406, Loss: 0.015939904376864433, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11559/20000], Bound: 0.39220938086509705, Entropy: 141.54946899414062, Temp: 2.691385269165039, KL: 77.30178833007812, Loss: 0.013714952394366264, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11560/20000], Bound: 0.3607463240623474, Entropy: 141.86257934570312, Temp: 2.6913857460021973, KL: 66.51657104492188, Loss: 0.016958028078079224, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11561/20000], Bound: 0.40499040484428406, Entropy: 140.4521484375, Temp: 2.691380739212036, KL: 80.57136535644531, Loss: 0.014685512520372868, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11562/20000], Bound: 0.3521847128868103, Entropy: 141.12115478515625, Temp: 2.6913862228393555, KL: 64.29426574707031, Loss: 0.016645187512040138, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11563/20000], Bound: 0.39340275526046753, Entropy: 140.353271484375, Temp: 2.691385269165039, KL: 76.38145446777344, Loss: 0.01607687957584858, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11564/20000], Bound: 0.3844468295574188, Entropy: 141.20697021484375, Temp: 2.691387414932251, KL: 73.13032531738281, Loss: 0.017250290140509605, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11565/20000], Bound: 0.40035951137542725, Entropy: 141.91043090820312, Temp: 2.6913869380950928, KL: 78.04913330078125, Loss: 0.016803381964564323, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11566/20000], Bound: 0.3734469413757324, Entropy: 142.0618438720703, Temp: 2.6913888454437256, KL: 71.15676879882812, Loss: 0.01502547599375248, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11567/20000], Bound: 0.37039685249328613, Entropy: 141.2373046875, Temp: 2.6913936138153076, KL: 69.69017028808594, Loss: 0.016133015975356102, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11568/20000], Bound: 0.3665980398654938, Entropy: 141.92958068847656, Temp: 2.691396951675415, KL: 68.74104309082031, Loss: 0.01589202880859375, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11569/20000], Bound: 0.36504805088043213, Entropy: 142.25839233398438, Temp: 2.691398859024048, KL: 68.85548400878906, Loss: 0.014864700846374035, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11570/20000], Bound: 0.3939834237098694, Entropy: 143.1020050048828, Temp: 2.6914031505584717, KL: 75.8072509765625, Loss: 0.017461545765399933, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11571/20000], Bound: 0.3702661693096161, Entropy: 142.63804626464844, Temp: 2.6914055347442627, KL: 70.65716552734375, Loss: 0.014267535880208015, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11572/20000], Bound: 0.4104020297527313, Entropy: 139.355712890625, Temp: 2.691413402557373, KL: 81.18748474121094, Loss: 0.016564838588237762, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11573/20000], Bound: 0.3994450867176056, Entropy: 140.62161254882812, Temp: 2.691425323486328, KL: 76.05329895019531, Loss: 0.020006561651825905, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11574/20000], Bound: 0.3802637755870819, Entropy: 144.64431762695312, Temp: 2.6914265155792236, KL: 72.27716064453125, Loss: 0.016584286466240883, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11575/20000], Bound: 0.3770943880081177, Entropy: 143.2631072998047, Temp: 2.6914267539978027, KL: 72.40591430664062, Loss: 0.014648311771452427, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11576/20000], Bound: 0.40806061029434204, Entropy: 140.38172912597656, Temp: 2.691432237625122, KL: 78.92625427246094, Loss: 0.01945463940501213, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11577/20000], Bound: 0.3720552921295166, Entropy: 140.7396240234375, Temp: 2.691431760787964, KL: 71.62403869628906, Loss: 0.013419105671346188, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11578/20000], Bound: 0.389108270406723, Entropy: 139.886962890625, Temp: 2.6914401054382324, KL: 76.37289428710938, Loss: 0.013751860707998276, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11579/20000], Bound: 0.39452484250068665, Entropy: 141.80325317382812, Temp: 2.691458225250244, KL: 75.09295654296875, Loss: 0.01908552087843418, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11580/20000], Bound: 0.38318705558776855, Entropy: 140.56021118164062, Temp: 2.691467523574829, KL: 73.76556396484375, Loss: 0.015391441993415356, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11581/20000], Bound: 0.43558457493782043, Entropy: 140.06190490722656, Temp: 2.6914799213409424, KL: 87.74441528320312, Loss: 0.0187872052192688, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11582/20000], Bound: 0.3856537640094757, Entropy: 140.9596710205078, Temp: 2.691492795944214, KL: 75.8753662109375, Loss: 0.012803858146071434, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11583/20000], Bound: 0.39738044142723083, Entropy: 141.7665252685547, Temp: 2.6915175914764404, KL: 77.34495544433594, Loss: 0.016470223665237427, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11584/20000], Bound: 0.3925142288208008, Entropy: 142.2584686279297, Temp: 2.691542863845825, KL: 77.55465698242188, Loss: 0.013413341715931892, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11585/20000], Bound: 0.4109448194503784, Entropy: 140.16580200195312, Temp: 2.691577911376953, KL: 81.88519287109375, Loss: 0.015575085766613483, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11586/20000], Bound: 0.3510938882827759, Entropy: 140.25802612304688, Temp: 2.691617727279663, KL: 63.9326171875, Loss: 0.016756555065512657, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11587/20000], Bound: 0.3850032687187195, Entropy: 139.6468505859375, Temp: 2.6916465759277344, KL: 74.05747985839844, Loss: 0.01583075523376465, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11588/20000], Bound: 0.3816818594932556, Entropy: 140.37257385253906, Temp: 2.6916751861572266, KL: 72.93988037109375, Loss: 0.016117028892040253, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11589/20000], Bound: 0.38144901394844055, Entropy: 142.42938232421875, Temp: 2.691702127456665, KL: 70.05599975585938, Loss: 0.02134908176958561, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11590/20000], Bound: 0.3820665776729584, Entropy: 140.54718017578125, Temp: 2.691709041595459, KL: 73.09564208984375, Loss: 0.016034899279475212, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11591/20000], Bound: 0.408829003572464, Entropy: 142.04444885253906, Temp: 2.6917166709899902, KL: 80.37904357910156, Loss: 0.017188463360071182, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11592/20000], Bound: 0.41236820816993713, Entropy: 141.295654296875, Temp: 2.691725730895996, KL: 80.95179748535156, Loss: 0.018110478296875954, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11593/20000], Bound: 0.39596301317214966, Entropy: 140.7560272216797, Temp: 2.6917335987091064, KL: 76.68077087402344, Loss: 0.016927119344472885, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11594/20000], Bound: 0.40674257278442383, Entropy: 140.0308380126953, Temp: 2.691741704940796, KL: 79.95808410644531, Loss: 0.01680486649274826, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11595/20000], Bound: 0.4083930552005768, Entropy: 140.88734436035156, Temp: 2.6917521953582764, KL: 80.72190856933594, Loss: 0.016308018937706947, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11596/20000], Bound: 0.38535594940185547, Entropy: 141.48690795898438, Temp: 2.6917667388916016, KL: 73.67582702636719, Loss: 0.016731375828385353, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11597/20000], Bound: 0.38877785205841064, Entropy: 142.24496459960938, Temp: 2.691779375076294, KL: 75.59815979003906, Loss: 0.015014906413853168, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11598/20000], Bound: 0.38052064180374146, Entropy: 140.98512268066406, Temp: 2.6917970180511475, KL: 72.93276977539062, Loss: 0.015507559292018414, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11599/20000], Bound: 0.38226318359375, Entropy: 140.60894775390625, Temp: 2.6918158531188965, KL: 71.20448303222656, Loss: 0.019654374569654465, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11600/20000], Bound: 0.3802469074726105, Entropy: 140.59600830078125, Temp: 2.691821575164795, KL: 72.69825744628906, Loss: 0.01579648070037365, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11601/20000], Bound: 0.3612309992313385, Entropy: 143.61285400390625, Temp: 2.691828489303589, KL: 66.11909484863281, Loss: 0.01795261539518833, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11602/20000], Bound: 0.36920788884162903, Entropy: 140.28147888183594, Temp: 2.6918256282806396, KL: 70.1229248046875, Loss: 0.014704344794154167, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11603/20000], Bound: 0.3682238757610321, Entropy: 140.30499267578125, Temp: 2.691826581954956, KL: 68.96258544921875, Loss: 0.0163403507322073, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11604/20000], Bound: 0.37148481607437134, Entropy: 140.58648681640625, Temp: 2.6918253898620605, KL: 69.98320007324219, Loss: 0.016168246045708656, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11605/20000], Bound: 0.4096878468990326, Entropy: 141.29043579101562, Temp: 2.6918230056762695, KL: 81.17108154296875, Loss: 0.0161992609500885, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11606/20000], Bound: 0.39253976941108704, Entropy: 139.41212463378906, Temp: 2.691826820373535, KL: 76.5770263671875, Loss: 0.015246205031871796, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11607/20000], Bound: 0.3882710039615631, Entropy: 143.0735626220703, Temp: 2.691836357116699, KL: 73.57090759277344, Loss: 0.018505776301026344, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11608/20000], Bound: 0.3915462791919708, Entropy: 140.2606658935547, Temp: 2.6918387413024902, KL: 73.92387390136719, Loss: 0.01963222771883011, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11609/20000], Bound: 0.3649379014968872, Entropy: 140.01927185058594, Temp: 2.6918318271636963, KL: 67.60853576660156, Loss: 0.01712670549750328, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11610/20000], Bound: 0.373126745223999, Entropy: 143.34593200683594, Temp: 2.691819429397583, KL: 70.76303100585938, Loss: 0.015590584836900234, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11611/20000], Bound: 0.3978344202041626, Entropy: 139.62664794921875, Temp: 2.691809892654419, KL: 77.22576904296875, Loss: 0.016944292932748795, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11612/20000], Bound: 0.38256505131721497, Entropy: 140.10829162597656, Temp: 2.691802501678467, KL: 72.93287658691406, Loss: 0.0166062880307436, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11613/20000], Bound: 0.37066322565078735, Entropy: 140.90237426757812, Temp: 2.6917948722839355, KL: 67.15980529785156, Loss: 0.020977405831217766, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11614/20000], Bound: 0.376144140958786, Entropy: 141.64419555664062, Temp: 2.691770315170288, KL: 72.31240844726562, Loss: 0.014318027533590794, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11615/20000], Bound: 0.3975677192211151, Entropy: 141.60247802734375, Temp: 2.6917543411254883, KL: 75.0543212890625, Loss: 0.02083052322268486, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11616/20000], Bound: 0.3945801258087158, Entropy: 141.77691650390625, Temp: 2.691727638244629, KL: 73.4234619140625, Loss: 0.02221929468214512, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11617/20000], Bound: 0.3918302357196808, Entropy: 139.1349334716797, Temp: 2.6916861534118652, KL: 75.48806762695312, Loss: 0.016880284994840622, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11618/20000], Bound: 0.3699643611907959, Entropy: 142.00450134277344, Temp: 2.6916487216949463, KL: 68.24440002441406, Loss: 0.018592072650790215, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11619/20000], Bound: 0.38399559259414673, Entropy: 141.19921875, Temp: 2.69160532951355, KL: 74.41265869140625, Loss: 0.014626589603722095, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11620/20000], Bound: 0.3980831801891327, Entropy: 142.02105712890625, Temp: 2.691572904586792, KL: 76.4686279296875, Loss: 0.018485503271222115, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11621/20000], Bound: 0.3939495384693146, Entropy: 141.72222900390625, Temp: 2.6915395259857178, KL: 75.66329956054688, Loss: 0.017711635679006577, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11622/20000], Bound: 0.36089980602264404, Entropy: 141.49066162109375, Temp: 2.691507339477539, KL: 67.26750183105469, Loss: 0.01564403437077999, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11623/20000], Bound: 0.37872251868247986, Entropy: 140.20571899414062, Temp: 2.6914772987365723, KL: 72.16310119628906, Loss: 0.015970522537827492, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11624/20000], Bound: 0.3891376852989197, Entropy: 140.06654357910156, Temp: 2.69145131111145, KL: 75.68424987792969, Loss: 0.015047270804643631, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11625/20000], Bound: 0.3912309408187866, Entropy: 141.1876678466797, Temp: 2.691433906555176, KL: 74.85858154296875, Loss: 0.01772048883140087, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11626/20000], Bound: 0.3795657157897949, Entropy: 143.2664794921875, Temp: 2.691415309906006, KL: 72.06361389160156, Loss: 0.016606539487838745, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11627/20000], Bound: 0.3852159380912781, Entropy: 140.22518920898438, Temp: 2.6913976669311523, KL: 73.87594604492188, Loss: 0.01628059521317482, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11628/20000], Bound: 0.34875014424324036, Entropy: 141.54501342773438, Temp: 2.691382646560669, KL: 63.8426513671875, Loss: 0.015717271715402603, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11629/20000], Bound: 0.3847816586494446, Entropy: 142.3709716796875, Temp: 2.691365957260132, KL: 73.6273193359375, Loss: 0.016507556661963463, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11630/20000], Bound: 0.37113723158836365, Entropy: 141.6920166015625, Temp: 2.6913509368896484, KL: 66.22236633300781, Loss: 0.022967051714658737, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11631/20000], Bound: 0.3625873327255249, Entropy: 140.22108459472656, Temp: 2.691312789916992, KL: 69.13053894042969, Loss: 0.013063210994005203, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11632/20000], Bound: 0.36673039197921753, Entropy: 140.5950927734375, Temp: 2.69128680229187, KL: 68.3004150390625, Loss: 0.016779376193881035, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11633/20000], Bound: 0.3953154385089874, Entropy: 141.90069580078125, Temp: 2.6912591457366943, KL: 76.65365600585938, Loss: 0.016617631539702415, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11634/20000], Bound: 0.38134071230888367, Entropy: 140.25247192382812, Temp: 2.6912362575531006, KL: 71.92776489257812, Loss: 0.01781010627746582, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11635/20000], Bound: 0.3683905005455017, Entropy: 140.67459106445312, Temp: 2.6912107467651367, KL: 69.54231262207031, Loss: 0.015346242114901543, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11636/20000], Bound: 0.3909187316894531, Entropy: 140.77842712402344, Temp: 2.6911888122558594, KL: 76.62495422363281, Loss: 0.014266317710280418, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11637/20000], Bound: 0.39491623640060425, Entropy: 140.8701171875, Temp: 2.691178321838379, KL: 76.14280700683594, Loss: 0.01734708435833454, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11638/20000], Bound: 0.37781909108161926, Entropy: 140.68994140625, Temp: 2.6911683082580566, KL: 73.02517700195312, Loss: 0.013882745057344437, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11639/20000], Bound: 0.4040466547012329, Entropy: 141.95448303222656, Temp: 2.691167116165161, KL: 79.82035827636719, Loss: 0.015553692355751991, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11640/20000], Bound: 0.42072218656539917, Entropy: 141.64849853515625, Temp: 2.6911730766296387, KL: 84.68231201171875, Loss: 0.015904616564512253, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11641/20000], Bound: 0.39315125346183777, Entropy: 141.44602966308594, Temp: 2.691187620162964, KL: 76.44822692871094, Loss: 0.015813367441296577, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11642/20000], Bound: 0.38046810030937195, Entropy: 138.35752868652344, Temp: 2.691204786300659, KL: 73.28887939453125, Loss: 0.014812355861067772, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11643/20000], Bound: 0.4060852825641632, Entropy: 139.65509033203125, Temp: 2.691225290298462, KL: 80.70736694335938, Loss: 0.015040925703942776, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11644/20000], Bound: 0.37743809819221497, Entropy: 141.9343719482422, Temp: 2.691253185272217, KL: 70.39442443847656, Loss: 0.01856747269630432, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11645/20000], Bound: 0.3901383578777313, Entropy: 140.95303344726562, Temp: 2.691269874572754, KL: 76.47048950195312, Loss: 0.01412906963378191, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11646/20000], Bound: 0.37878185510635376, Entropy: 142.61692810058594, Temp: 2.6912941932678223, KL: 71.82853698730469, Loss: 0.016622265800833702, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11647/20000], Bound: 0.3955950438976288, Entropy: 141.54405212402344, Temp: 2.691315174102783, KL: 75.159423828125, Loss: 0.0195475947111845, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11648/20000], Bound: 0.38404515385627747, Entropy: 141.37353515625, Temp: 2.6913256645202637, KL: 73.10118103027344, Loss: 0.017087114974856377, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11649/20000], Bound: 0.40511128306388855, Entropy: 141.63938903808594, Temp: 2.691333293914795, KL: 79.33154296875, Loss: 0.017055660486221313, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11650/20000], Bound: 0.3760732412338257, Entropy: 140.11181640625, Temp: 2.6913421154022217, KL: 70.79098510742188, Loss: 0.01710275560617447, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11651/20000], Bound: 0.3845438063144684, Entropy: 141.3383331298828, Temp: 2.6913468837738037, KL: 72.94068908691406, Loss: 0.017654595896601677, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11652/20000], Bound: 0.3728006184101105, Entropy: 141.89999389648438, Temp: 2.691347122192383, KL: 69.23797607421875, Loss: 0.018246609717607498, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11653/20000], Bound: 0.4065903425216675, Entropy: 141.50621032714844, Temp: 2.6913392543792725, KL: 80.75143432617188, Loss: 0.015241889283061028, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11654/20000], Bound: 0.39928916096687317, Entropy: 140.09347534179688, Temp: 2.691340923309326, KL: 78.82516479492188, Loss: 0.01477019302546978, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11655/20000], Bound: 0.3708638846874237, Entropy: 140.566162109375, Temp: 2.6913511753082275, KL: 69.37347412109375, Loss: 0.016968173906207085, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11656/20000], Bound: 0.36046960949897766, Entropy: 143.73458862304688, Temp: 2.691356658935547, KL: 66.81130981445312, Loss: 0.016265878453850746, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11657/20000], Bound: 0.42227903008461, Entropy: 140.62794494628906, Temp: 2.6913580894470215, KL: 85.85197448730469, Loss: 0.014622015878558159, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11658/20000], Bound: 0.389212429523468, Entropy: 141.46658325195312, Temp: 2.6913726329803467, KL: 75.37181091308594, Loss: 0.015667572617530823, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11659/20000], Bound: 0.37090417742729187, Entropy: 140.13369750976562, Temp: 2.691390037536621, KL: 69.67091369628906, Loss: 0.016437280923128128, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11660/20000], Bound: 0.40320947766304016, Entropy: 139.91830444335938, Temp: 2.69140362739563, KL: 77.80278015136719, Loss: 0.01883949525654316, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11661/20000], Bound: 0.39750462770462036, Entropy: 140.74928283691406, Temp: 2.6914114952087402, KL: 78.42153930664062, Loss: 0.014537501148879528, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11662/20000], Bound: 0.3630337715148926, Entropy: 140.94686889648438, Temp: 2.691427707672119, KL: 67.97244262695312, Loss: 0.015449358150362968, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11663/20000], Bound: 0.3585282862186432, Entropy: 140.69541931152344, Temp: 2.6914422512054443, KL: 66.39256286621094, Loss: 0.016033099964261055, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11664/20000], Bound: 0.39656928181648254, Entropy: 140.79254150390625, Temp: 2.6914525032043457, KL: 76.23214721679688, Loss: 0.018090900033712387, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11665/20000], Bound: 0.38105878233909607, Entropy: 142.41253662109375, Temp: 2.6914587020874023, KL: 72.68797302246094, Loss: 0.01624818705022335, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11666/20000], Bound: 0.3925403654575348, Entropy: 142.44139099121094, Temp: 2.69146466255188, KL: 75.38861083984375, Loss: 0.01745070330798626, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11667/20000], Bound: 0.3583682179450989, Entropy: 141.0953826904297, Temp: 2.6914684772491455, KL: 67.77330017089844, Loss: 0.013385029509663582, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11668/20000], Bound: 0.39132899045944214, Entropy: 141.06619262695312, Temp: 2.6914782524108887, KL: 74.62832641601562, Loss: 0.018202094361186028, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11669/20000], Bound: 0.39786073565483093, Entropy: 140.0639190673828, Temp: 2.6914827823638916, KL: 77.05899047851562, Loss: 0.017265476286411285, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11670/20000], Bound: 0.39139658212661743, Entropy: 141.0249786376953, Temp: 2.6914868354797363, KL: 75.76126098632812, Loss: 0.01613435707986355, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11671/20000], Bound: 0.37677305936813354, Entropy: 142.3221893310547, Temp: 2.691493272781372, KL: 71.89614868164062, Loss: 0.01542434561997652, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11672/20000], Bound: 0.36120450496673584, Entropy: 140.91102600097656, Temp: 2.6915013790130615, KL: 67.19490051269531, Loss: 0.01593792252242565, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11673/20000], Bound: 0.39138293266296387, Entropy: 140.84146118164062, Temp: 2.6915066242218018, KL: 74.8553466796875, Loss: 0.017810015007853508, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11674/20000], Bound: 0.358376681804657, Entropy: 143.03472900390625, Temp: 2.6915085315704346, KL: 67.52323913574219, Loss: 0.013854303397238255, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11675/20000], Bound: 0.3861728012561798, Entropy: 141.2896728515625, Temp: 2.691514730453491, KL: 73.75022888183594, Loss: 0.017032677307724953, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11676/20000], Bound: 0.38887539505958557, Entropy: 141.0566864013672, Temp: 2.691519260406494, KL: 75.90882873535156, Loss: 0.014488221146166325, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11677/20000], Bound: 0.40473365783691406, Entropy: 139.04937744140625, Temp: 2.691531181335449, KL: 79.31825256347656, Loss: 0.016872260719537735, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11678/20000], Bound: 0.38654619455337524, Entropy: 142.1949920654297, Temp: 2.691544532775879, KL: 73.75164794921875, Loss: 0.01723245158791542, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11679/20000], Bound: 0.4186561107635498, Entropy: 140.8939208984375, Temp: 2.691554546356201, KL: 83.96908569335938, Loss: 0.016058482229709625, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11680/20000], Bound: 0.3816039264202118, Entropy: 142.07199096679688, Temp: 2.6915717124938965, KL: 72.10136413574219, Loss: 0.017631826922297478, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11681/20000], Bound: 0.40425005555152893, Entropy: 140.00877380371094, Temp: 2.691582679748535, KL: 79.14408874511719, Loss: 0.016927413642406464, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11682/20000], Bound: 0.3931259512901306, Entropy: 142.27679443359375, Temp: 2.6915950775146484, KL: 76.05387878417969, Loss: 0.01653606817126274, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11683/20000], Bound: 0.36292725801467896, Entropy: 140.82794189453125, Temp: 2.691607713699341, KL: 66.91183471679688, Loss: 0.017365235835313797, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11684/20000], Bound: 0.39797163009643555, Entropy: 139.28836059570312, Temp: 2.6916122436523438, KL: 77.26557922363281, Loss: 0.016943998634815216, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11685/20000], Bound: 0.37651965022087097, Entropy: 140.66331481933594, Temp: 2.691617727279663, KL: 72.64109802246094, Loss: 0.013906370848417282, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11686/20000], Bound: 0.38195306062698364, Entropy: 141.1779022216797, Temp: 2.6916303634643555, KL: 73.381591796875, Loss: 0.01544190850108862, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11687/20000], Bound: 0.3961358070373535, Entropy: 140.9469451904297, Temp: 2.6916451454162598, KL: 77.41586303710938, Loss: 0.01565566658973694, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11688/20000], Bound: 0.40589597821235657, Entropy: 141.19964599609375, Temp: 2.6916635036468506, KL: 78.7540283203125, Loss: 0.018568729981780052, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11689/20000], Bound: 0.3590606451034546, Entropy: 140.84999084472656, Temp: 2.6916773319244385, KL: 67.24461364746094, Loss: 0.014729172922670841, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11690/20000], Bound: 0.36851009726524353, Entropy: 142.91395568847656, Temp: 2.6916913986206055, KL: 69.62312316894531, Loss: 0.015263276174664497, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11691/20000], Bound: 0.39532625675201416, Entropy: 141.88748168945312, Temp: 2.6917057037353516, KL: 76.146240234375, Loss: 0.01757042109966278, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11692/20000], Bound: 0.3936499357223511, Entropy: 140.97174072265625, Temp: 2.6917171478271484, KL: 76.16841125488281, Loss: 0.016611069440841675, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11693/20000], Bound: 0.3687762916088104, Entropy: 141.26913452148438, Temp: 2.6917288303375244, KL: 69.75558471679688, Loss: 0.015157977119088173, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11694/20000], Bound: 0.387744277715683, Entropy: 142.41880798339844, Temp: 2.691741466522217, KL: 74.759521484375, Loss: 0.016011299565434456, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11695/20000], Bound: 0.4085761308670044, Entropy: 139.4550323486328, Temp: 2.6917552947998047, KL: 81.85928344726562, Loss: 0.014297788962721825, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11696/20000], Bound: 0.38267770409584045, Entropy: 141.5377197265625, Temp: 2.691779851913452, KL: 73.90406799316406, Loss: 0.014862754382193089, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11697/20000], Bound: 0.4038325548171997, Entropy: 141.6492919921875, Temp: 2.691807746887207, KL: 78.68905639648438, Loss: 0.017542939633131027, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11698/20000], Bound: 0.41194817423820496, Entropy: 142.50820922851562, Temp: 2.6918327808380127, KL: 81.56051635742188, Loss: 0.016744671389460564, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11699/20000], Bound: 0.3865124583244324, Entropy: 140.11643981933594, Temp: 2.6918599605560303, KL: 71.5616455078125, Loss: 0.021284785121679306, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11700/20000], Bound: 0.3854091763496399, Entropy: 142.01251220703125, Temp: 2.691868305206299, KL: 72.69595336914062, Loss: 0.01858110912144184, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11701/20000], Bound: 0.38201767206192017, Entropy: 139.43226623535156, Temp: 2.691868543624878, KL: 73.07109069824219, Loss: 0.016055617481470108, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11702/20000], Bound: 0.3829551637172699, Entropy: 139.95413208007812, Temp: 2.6918704509735107, KL: 73.96133422851562, Loss: 0.014906651340425014, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11703/20000], Bound: 0.41042813658714294, Entropy: 142.23695373535156, Temp: 2.6918773651123047, KL: 79.56114196777344, Loss: 0.019605237990617752, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11704/20000], Bound: 0.3909038305282593, Entropy: 140.05419921875, Temp: 2.691877841949463, KL: 75.12391662597656, Loss: 0.017053304240107536, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11705/20000], Bound: 0.37571191787719727, Entropy: 141.9045867919922, Temp: 2.691877841949463, KL: 69.49403381347656, Loss: 0.019323531538248062, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11706/20000], Bound: 0.3764861524105072, Entropy: 140.6190948486328, Temp: 2.691866636276245, KL: 71.18363952636719, Loss: 0.01659802347421646, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11707/20000], Bound: 0.4011761546134949, Entropy: 140.17999267578125, Temp: 2.691854953765869, KL: 78.89382934570312, Loss: 0.015690522268414497, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11708/20000], Bound: 0.3806139826774597, Entropy: 142.20864868164062, Temp: 2.691850185394287, KL: 72.63310241699219, Loss: 0.01611475460231304, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11709/20000], Bound: 0.3845116198062897, Entropy: 140.53543090820312, Temp: 2.6918468475341797, KL: 73.45314025878906, Loss: 0.016689617186784744, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11710/20000], Bound: 0.38913798332214355, Entropy: 141.82611083984375, Temp: 2.691843271255493, KL: 75.84796142578125, Loss: 0.014747180975973606, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11711/20000], Bound: 0.3706112205982208, Entropy: 141.79452514648438, Temp: 2.691847324371338, KL: 68.68672180175781, Loss: 0.018114030361175537, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11712/20000], Bound: 0.3873598277568817, Entropy: 140.8260498046875, Temp: 2.691842794418335, KL: 74.00363159179688, Loss: 0.017207827419042587, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11713/20000], Bound: 0.39842677116394043, Entropy: 142.07327270507812, Temp: 2.6918370723724365, KL: 78.43971252441406, Loss: 0.015015901066362858, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11714/20000], Bound: 0.37295785546302795, Entropy: 140.72129821777344, Temp: 2.6918396949768066, KL: 70.43341064453125, Loss: 0.016113318502902985, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11715/20000], Bound: 0.38066935539245605, Entropy: 142.6536102294922, Temp: 2.6918413639068604, KL: 72.68341064453125, Loss: 0.016050972044467926, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11716/20000], Bound: 0.38516664505004883, Entropy: 140.96804809570312, Temp: 2.6918442249298096, KL: 74.31436157226562, Loss: 0.015443704091012478, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11717/20000], Bound: 0.3968532085418701, Entropy: 140.17306518554688, Temp: 2.6918506622314453, KL: 75.68020629882812, Loss: 0.019275778904557228, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11718/20000], Bound: 0.38125038146972656, Entropy: 141.24156188964844, Temp: 2.6918492317199707, KL: 74.42597961425781, Loss: 0.01312631368637085, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11719/20000], Bound: 0.3859017491340637, Entropy: 140.6147003173828, Temp: 2.691859483718872, KL: 73.19699096679688, Loss: 0.017916737124323845, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11720/20000], Bound: 0.3759276866912842, Entropy: 140.3140411376953, Temp: 2.691864252090454, KL: 70.5438232421875, Loss: 0.017488516867160797, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11721/20000], Bound: 0.39236781001091003, Entropy: 141.76422119140625, Temp: 2.691863536834717, KL: 76.16319274902344, Loss: 0.01592135615646839, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11722/20000], Bound: 0.37823376059532166, Entropy: 142.62554931640625, Temp: 2.691866636276245, KL: 71.0872802734375, Loss: 0.017710676416754723, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11723/20000], Bound: 0.38629060983657837, Entropy: 141.8207550048828, Temp: 2.691864252090454, KL: 75.2911376953125, Loss: 0.01423738431185484, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11724/20000], Bound: 0.3830246031284332, Entropy: 140.83824157714844, Temp: 2.6918704509735107, KL: 73.12760925292969, Loss: 0.016492623835802078, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11725/20000], Bound: 0.381460577249527, Entropy: 141.54104614257812, Temp: 2.69187593460083, KL: 74.32090759277344, Loss: 0.013434666208922863, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11726/20000], Bound: 0.3799351751804352, Entropy: 141.78836059570312, Temp: 2.6918911933898926, KL: 72.04312133789062, Loss: 0.016846779733896255, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11727/20000], Bound: 0.37681975960731506, Entropy: 140.741943359375, Temp: 2.6919031143188477, KL: 71.67898559570312, Loss: 0.015856308862566948, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11728/20000], Bound: 0.37316465377807617, Entropy: 139.48597717285156, Temp: 2.6919147968292236, KL: 71.04034423828125, Loss: 0.01509642694145441, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11729/20000], Bound: 0.3672369718551636, Entropy: 139.43606567382812, Temp: 2.6919283866882324, KL: 68.44793701171875, Loss: 0.01677706092596054, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11730/20000], Bound: 0.3857884705066681, Entropy: 141.82553100585938, Temp: 2.691936492919922, KL: 72.039306640625, Loss: 0.020006384700536728, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11731/20000], Bound: 0.3916741907596588, Entropy: 141.17811584472656, Temp: 2.691932201385498, KL: 75.92958068847656, Loss: 0.01597736030817032, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11732/20000], Bound: 0.38544565439224243, Entropy: 142.1435546875, Temp: 2.6919314861297607, KL: 73.94168090820312, Loss: 0.0162875484675169, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11733/20000], Bound: 0.3705635666847229, Entropy: 143.98097229003906, Temp: 2.691931962966919, KL: 69.34562683105469, Loss: 0.016865618526935577, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11734/20000], Bound: 0.355267196893692, Entropy: 140.9885711669922, Temp: 2.6919288635253906, KL: 65.57882690429688, Loss: 0.015855753794312477, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11735/20000], Bound: 0.39281851053237915, Entropy: 141.75205993652344, Temp: 2.691923141479492, KL: 74.65534973144531, Loss: 0.01896880567073822, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11736/20000], Bound: 0.40353238582611084, Entropy: 139.39219665527344, Temp: 2.691910982131958, KL: 79.12130737304688, Loss: 0.01657438464462757, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11737/20000], Bound: 0.3723859488964081, Entropy: 139.9864959716797, Temp: 2.691903829574585, KL: 69.35398864746094, Loss: 0.017815321683883667, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11738/20000], Bound: 0.38696151971817017, Entropy: 142.0483856201172, Temp: 2.691890239715576, KL: 73.81283569335938, Loss: 0.01734679937362671, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11739/20000], Bound: 0.39798206090927124, Entropy: 141.42294311523438, Temp: 2.69187593460083, KL: 77.07675170898438, Loss: 0.01730300858616829, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11740/20000], Bound: 0.36784589290618896, Entropy: 142.33187866210938, Temp: 2.6918628215789795, KL: 69.37821960449219, Loss: 0.015369360335171223, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11741/20000], Bound: 0.3861750364303589, Entropy: 141.60948181152344, Temp: 2.69185209274292, KL: 74.83479309082031, Loss: 0.015022370032966137, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11742/20000], Bound: 0.4185928404331207, Entropy: 140.41775512695312, Temp: 2.691848039627075, KL: 82.39140319824219, Loss: 0.018956372514367104, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11743/20000], Bound: 0.3746565878391266, Entropy: 141.49745178222656, Temp: 2.691842555999756, KL: 71.407470703125, Loss: 0.015207061544060707, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11744/20000], Bound: 0.3697797358036041, Entropy: 141.18650817871094, Temp: 2.691840648651123, KL: 68.69708251953125, Loss: 0.017655009403824806, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11745/20000], Bound: 0.376594603061676, Entropy: 141.4044647216797, Temp: 2.6918320655822754, KL: 72.3355712890625, Loss: 0.014515896327793598, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11746/20000], Bound: 0.36463648080825806, Entropy: 140.61952209472656, Temp: 2.6918303966522217, KL: 69.08250427246094, Loss: 0.014230651780962944, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11747/20000], Bound: 0.38613981008529663, Entropy: 140.89402770996094, Temp: 2.691833257675171, KL: 75.19584655761719, Loss: 0.014332471415400505, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11748/20000], Bound: 0.3946336507797241, Entropy: 143.23956298828125, Temp: 2.6918439865112305, KL: 72.73562622070312, Loss: 0.023527098819613457, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11749/20000], Bound: 0.3975798487663269, Entropy: 141.79164123535156, Temp: 2.691831350326538, KL: 77.275146484375, Loss: 0.016712667420506477, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11750/20000], Bound: 0.39097702503204346, Entropy: 142.06932067871094, Temp: 2.691821813583374, KL: 77.41213989257812, Loss: 0.012842349708080292, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11751/20000], Bound: 0.37700679898262024, Entropy: 140.73098754882812, Temp: 2.6918270587921143, KL: 68.88816833496094, Loss: 0.021139394491910934, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11752/20000], Bound: 0.3845798075199127, Entropy: 140.80783081054688, Temp: 2.691814661026001, KL: 72.88780212402344, Loss: 0.017776254564523697, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11753/20000], Bound: 0.34598690271377563, Entropy: 142.22705078125, Temp: 2.6917991638183594, KL: 62.758453369140625, Loss: 0.016318831592798233, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11754/20000], Bound: 0.3767443299293518, Entropy: 139.509765625, Temp: 2.691779136657715, KL: 71.07780456542969, Loss: 0.01693166047334671, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11755/20000], Bound: 0.3811952471733093, Entropy: 144.14195251464844, Temp: 2.691758155822754, KL: 70.88668823242188, Loss: 0.019670072942972183, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11756/20000], Bound: 0.359223335981369, Entropy: 140.67730712890625, Temp: 2.691727876663208, KL: 67.60557556152344, Loss: 0.014143765904009342, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11757/20000], Bound: 0.384589821100235, Entropy: 143.24819946289062, Temp: 2.691704511642456, KL: 73.81147766113281, Loss: 0.016064966097474098, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11758/20000], Bound: 0.3918174207210541, Entropy: 141.89683532714844, Temp: 2.691685199737549, KL: 75.81739807128906, Loss: 0.016261501237750053, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11759/20000], Bound: 0.398284375667572, Entropy: 140.6859130859375, Temp: 2.6916704177856445, KL: 77.19171142578125, Loss: 0.017254024744033813, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11760/20000], Bound: 0.36509037017822266, Entropy: 142.28298950195312, Temp: 2.691657066345215, KL: 68.640869140625, Loss: 0.015287763439118862, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11761/20000], Bound: 0.3755369782447815, Entropy: 142.15060424804688, Temp: 2.691645860671997, KL: 71.46826171875, Loss: 0.015561291016638279, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11762/20000], Bound: 0.3786605894565582, Entropy: 140.5155487060547, Temp: 2.6916377544403076, KL: 72.30245971679688, Loss: 0.015679912641644478, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11763/20000], Bound: 0.3934897482395172, Entropy: 143.41273498535156, Temp: 2.6916322708129883, KL: 76.50675964355469, Loss: 0.015894094482064247, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11764/20000], Bound: 0.38070815801620483, Entropy: 141.98599243164062, Temp: 2.691631555557251, KL: 73.02122497558594, Loss: 0.015442385338246822, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11765/20000], Bound: 0.37327757477760315, Entropy: 142.12307739257812, Temp: 2.691633939743042, KL: 70.72482299804688, Loss: 0.01574006676673889, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11766/20000], Bound: 0.39873504638671875, Entropy: 141.42166137695312, Temp: 2.691636800765991, KL: 77.37287902832031, Loss: 0.01716551184654236, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11767/20000], Bound: 0.3696289360523224, Entropy: 139.77001953125, Temp: 2.6916399002075195, KL: 69.09036254882812, Loss: 0.016843220219016075, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11768/20000], Bound: 0.3755415081977844, Entropy: 142.42100524902344, Temp: 2.691638946533203, KL: 71.94094848632812, Loss: 0.014685573056340218, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11769/20000], Bound: 0.38205406069755554, Entropy: 141.62234497070312, Temp: 2.691642999649048, KL: 71.42170715332031, Loss: 0.019137050956487656, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11770/20000], Bound: 0.4065711200237274, Entropy: 142.41310119628906, Temp: 2.6916372776031494, KL: 80.78202819824219, Loss: 0.015177570283412933, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11771/20000], Bound: 0.3714454472064972, Entropy: 140.85092163085938, Temp: 2.691640853881836, KL: 70.98529052734375, Loss: 0.014284348115324974, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11772/20000], Bound: 0.40088048577308655, Entropy: 141.11500549316406, Temp: 2.6916496753692627, KL: 79.05366516113281, Loss: 0.01522799301892519, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11773/20000], Bound: 0.39882078766822815, Entropy: 141.71218872070312, Temp: 2.6916651725769043, KL: 78.56636047363281, Loss: 0.0149960583075881, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11774/20000], Bound: 0.3753839433193207, Entropy: 141.98057556152344, Temp: 2.6916873455047607, KL: 70.60884094238281, Loss: 0.017076538875699043, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11775/20000], Bound: 0.3871334195137024, Entropy: 141.22409057617188, Temp: 2.6917035579681396, KL: 75.68902587890625, Loss: 0.013953117653727531, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11776/20000], Bound: 0.38858646154403687, Entropy: 141.19468688964844, Temp: 2.691727638244629, KL: 74.31663513183594, Loss: 0.01729091815650463, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11777/20000], Bound: 0.38929834961891174, Entropy: 142.1722869873047, Temp: 2.6917476654052734, KL: 76.13743591308594, Loss: 0.014295694418251514, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11778/20000], Bound: 0.42037758231163025, Entropy: 140.3079833984375, Temp: 2.691774368286133, KL: 81.0687255859375, Loss: 0.022427553310990334, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11779/20000], Bound: 0.4150850176811218, Entropy: 140.16490173339844, Temp: 2.6917850971221924, KL: 82.67922973632812, Loss: 0.016434069722890854, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11780/20000], Bound: 0.3828829526901245, Entropy: 141.52284240722656, Temp: 2.691800594329834, KL: 73.08152770996094, Loss: 0.01650131866335869, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11781/20000], Bound: 0.393378883600235, Entropy: 141.5487060546875, Temp: 2.691814661026001, KL: 74.78053283691406, Loss: 0.019041672348976135, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11782/20000], Bound: 0.378168523311615, Entropy: 141.00865173339844, Temp: 2.6918201446533203, KL: 69.26753234863281, Loss: 0.021055597811937332, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11783/20000], Bound: 0.3839070796966553, Entropy: 141.894775390625, Temp: 2.6918084621429443, KL: 74.037841796875, Loss: 0.015276987105607986, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11784/20000], Bound: 0.41165295243263245, Entropy: 140.63975524902344, Temp: 2.6918020248413086, KL: 80.82008361816406, Loss: 0.017953727394342422, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11785/20000], Bound: 0.3972190022468567, Entropy: 141.61289978027344, Temp: 2.6917965412139893, KL: 76.60487365722656, Loss: 0.01775885559618473, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11786/20000], Bound: 0.39343076944351196, Entropy: 141.39906311035156, Temp: 2.6917898654937744, KL: 77.4027099609375, Loss: 0.014199168421328068, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11787/20000], Bound: 0.38660094141960144, Entropy: 141.19248962402344, Temp: 2.69179368019104, KL: 75.16036987304688, Loss: 0.014647583477199078, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11788/20000], Bound: 0.39601731300354004, Entropy: 141.74514770507812, Temp: 2.6918039321899414, KL: 77.86607360839844, Loss: 0.014755919575691223, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11789/20000], Bound: 0.38920682668685913, Entropy: 139.58570861816406, Temp: 2.691821575164795, KL: 75.40512084960938, Loss: 0.015606936998665333, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11790/20000], Bound: 0.38340213894844055, Entropy: 140.41629028320312, Temp: 2.6918413639068604, KL: 73.70657348632812, Loss: 0.015620389021933079, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11791/20000], Bound: 0.39084741473197937, Entropy: 140.728759765625, Temp: 2.6918623447418213, KL: 75.48916625976562, Loss: 0.016344008967280388, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11792/20000], Bound: 0.39315661787986755, Entropy: 138.71896362304688, Temp: 2.691883087158203, KL: 77.54208374023438, Loss: 0.013791344128549099, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11793/20000], Bound: 0.378421425819397, Entropy: 140.9730987548828, Temp: 2.691912889480591, KL: 72.54295349121094, Loss: 0.015107659623026848, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11794/20000], Bound: 0.39628711342811584, Entropy: 142.16485595703125, Temp: 2.691943407058716, KL: 76.63909912109375, Loss: 0.017184482887387276, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11795/20000], Bound: 0.3826264441013336, Entropy: 142.03932189941406, Temp: 2.6919710636138916, KL: 72.66458129882812, Loss: 0.017139144241809845, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11796/20000], Bound: 0.3686075210571289, Entropy: 141.94699096679688, Temp: 2.691993474960327, KL: 69.27519226074219, Loss: 0.01596342958509922, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11797/20000], Bound: 0.4280450940132141, Entropy: 140.8738555908203, Temp: 2.6920127868652344, KL: 86.52587890625, Loss: 0.016686292365193367, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11798/20000], Bound: 0.3970870077610016, Entropy: 141.18312072753906, Temp: 2.69203782081604, KL: 77.01683044433594, Loss: 0.016923345625400543, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11799/20000], Bound: 0.39111003279685974, Entropy: 142.184814453125, Temp: 2.692061185836792, KL: 73.52674865722656, Loss: 0.02013382874429226, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11800/20000], Bound: 0.38699302077293396, Entropy: 142.76300048828125, Temp: 2.6920711994171143, KL: 71.8695068359375, Loss: 0.020974792540073395, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11801/20000], Bound: 0.3683748245239258, Entropy: 140.71041870117188, Temp: 2.692065477371216, KL: 69.148193359375, Loss: 0.016077188774943352, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11802/20000], Bound: 0.35928335785865784, Entropy: 144.55941772460938, Temp: 2.692058801651001, KL: 64.48747253417969, Loss: 0.019969096407294273, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11803/20000], Bound: 0.37636029720306396, Entropy: 142.93734741210938, Temp: 2.6920363903045654, KL: 71.49478149414062, Loss: 0.015954388305544853, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11804/20000], Bound: 0.36607426404953003, Entropy: 140.45852661132812, Temp: 2.692016839981079, KL: 68.2080078125, Loss: 0.016611596569418907, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11805/20000], Bound: 0.36697742342948914, Entropy: 143.11846923828125, Temp: 2.691995620727539, KL: 69.69088745117188, Loss: 0.014332291670143604, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11806/20000], Bound: 0.399068683385849, Entropy: 142.29869079589844, Temp: 2.6919808387756348, KL: 78.39588928222656, Loss: 0.015452670864760876, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11807/20000], Bound: 0.3558903634548187, Entropy: 141.16932678222656, Temp: 2.69197416305542, KL: 65.45527648925781, Loss: 0.016408387571573257, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11808/20000], Bound: 0.38932445645332336, Entropy: 141.6195068359375, Temp: 2.6919636726379395, KL: 73.78302001953125, Loss: 0.01868508569896221, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11809/20000], Bound: 0.3752816617488861, Entropy: 142.22393798828125, Temp: 2.6919472217559814, KL: 70.34884643554688, Loss: 0.0175070371478796, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11810/20000], Bound: 0.36034542322158813, Entropy: 141.3585205078125, Temp: 2.691927671432495, KL: 66.52850341796875, Loss: 0.016730738803744316, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11811/20000], Bound: 0.397318959236145, Entropy: 139.65933227539062, Temp: 2.6919050216674805, KL: 78.61741638183594, Loss: 0.0140767190605402, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11812/20000], Bound: 0.38515254855155945, Entropy: 140.80921936035156, Temp: 2.6918954849243164, KL: 72.05235290527344, Loss: 0.0196380652487278, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11813/20000], Bound: 0.3671545386314392, Entropy: 140.5089874267578, Temp: 2.6918764114379883, KL: 68.68925476074219, Loss: 0.016285009682178497, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11814/20000], Bound: 0.38842636346817017, Entropy: 140.2758331298828, Temp: 2.691857099533081, KL: 74.13760375976562, Loss: 0.017537666484713554, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11815/20000], Bound: 0.3894933760166168, Entropy: 140.9041748046875, Temp: 2.6918368339538574, KL: 73.02552795410156, Loss: 0.0201828945428133, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11816/20000], Bound: 0.3849433958530426, Entropy: 141.53282165527344, Temp: 2.691807270050049, KL: 73.31549072265625, Loss: 0.017178140580654144, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11817/20000], Bound: 0.36035534739494324, Entropy: 141.32550048828125, Temp: 2.6917786598205566, KL: 67.95648193359375, Loss: 0.014082326553761959, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11818/20000], Bound: 0.39976900815963745, Entropy: 142.26736450195312, Temp: 2.6917569637298584, KL: 78.65223693847656, Loss: 0.015360619872808456, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11819/20000], Bound: 0.37001556158065796, Entropy: 140.96469116210938, Temp: 2.691744327545166, KL: 70.67092895507812, Loss: 0.014112446457147598, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11820/20000], Bound: 0.37566718459129333, Entropy: 141.8435516357422, Temp: 2.6917388439178467, KL: 71.9288330078125, Loss: 0.014775938354432583, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11821/20000], Bound: 0.37716031074523926, Entropy: 141.0595703125, Temp: 2.6917383670806885, KL: 71.62313842773438, Loss: 0.016140449792146683, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11822/20000], Bound: 0.3866784870624542, Entropy: 141.9300537109375, Temp: 2.6917381286621094, KL: 74.0660400390625, Loss: 0.016721777617931366, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11823/20000], Bound: 0.3904288411140442, Entropy: 140.47454833984375, Temp: 2.691737651824951, KL: 76.35140991210938, Loss: 0.014513184316456318, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11824/20000], Bound: 0.3783092796802521, Entropy: 141.82086181640625, Temp: 2.6917455196380615, KL: 71.89155578613281, Loss: 0.016256147995591164, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11825/20000], Bound: 0.3911380171775818, Entropy: 141.82431030273438, Temp: 2.6917524337768555, KL: 74.44517517089844, Loss: 0.018440602347254753, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11826/20000], Bound: 0.35772013664245605, Entropy: 141.3380584716797, Temp: 2.6917531490325928, KL: 66.26057434082031, Loss: 0.015860509127378464, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11827/20000], Bound: 0.39999544620513916, Entropy: 141.93743896484375, Temp: 2.691751718521118, KL: 77.48291015625, Loss: 0.017657628282904625, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11828/20000], Bound: 0.39639168977737427, Entropy: 142.90481567382812, Temp: 2.691749095916748, KL: 75.253662109375, Loss: 0.01981358975172043, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11829/20000], Bound: 0.41371408104896545, Entropy: 140.4058074951172, Temp: 2.6917381286621094, KL: 82.92924499511719, Loss: 0.015195452608168125, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11830/20000], Bound: 0.3440709710121155, Entropy: 140.8550567626953, Temp: 2.6917381286621094, KL: 62.56524658203125, Loss: 0.015699120238423347, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11831/20000], Bound: 0.36884376406669617, Entropy: 141.64749145507812, Temp: 2.6917335987091064, KL: 69.70176696777344, Loss: 0.01529359444975853, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11832/20000], Bound: 0.3764347732067108, Entropy: 141.39564514160156, Temp: 2.6917312145233154, KL: 71.85491943359375, Loss: 0.015322508290410042, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11833/20000], Bound: 0.39706951379776, Entropy: 141.0895233154297, Temp: 2.6917319297790527, KL: 76.56617736816406, Loss: 0.017747942358255386, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11834/20000], Bound: 0.37602776288986206, Entropy: 139.3034210205078, Temp: 2.691730499267578, KL: 71.08340454101562, Loss: 0.01653851754963398, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11835/20000], Bound: 0.3811396062374115, Entropy: 140.71536254882812, Temp: 2.691727876663208, KL: 73.68606567382812, Loss: 0.014440021477639675, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11836/20000], Bound: 0.3681684732437134, Entropy: 143.23643493652344, Temp: 2.691732406616211, KL: 68.91889953613281, Loss: 0.016391539946198463, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11837/20000], Bound: 0.38351792097091675, Entropy: 141.20877075195312, Temp: 2.6917338371276855, KL: 72.48051452636719, Loss: 0.0179592315107584, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11838/20000], Bound: 0.38310113549232483, Entropy: 141.46424865722656, Temp: 2.691730260848999, KL: 72.80538940429688, Loss: 0.017131170257925987, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11839/20000], Bound: 0.3923135995864868, Entropy: 140.57261657714844, Temp: 2.6917245388031006, KL: 76.08099365234375, Loss: 0.016043080016970634, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11840/20000], Bound: 0.3721052408218384, Entropy: 139.57920837402344, Temp: 2.691722869873047, KL: 70.24957275390625, Loss: 0.01600145548582077, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11841/20000], Bound: 0.3938303291797638, Entropy: 142.25985717773438, Temp: 2.691720724105835, KL: 75.73236083984375, Loss: 0.017519788816571236, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11842/20000], Bound: 0.3917602300643921, Entropy: 142.01255798339844, Temp: 2.6917173862457275, KL: 76.15576171875, Loss: 0.015602084808051586, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11843/20000], Bound: 0.3865523338317871, Entropy: 142.3030242919922, Temp: 2.6917190551757812, KL: 72.54576110839844, Loss: 0.019477304071187973, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11844/20000], Bound: 0.38696739077568054, Entropy: 139.64419555664062, Temp: 2.691710948944092, KL: 74.55841064453125, Loss: 0.01596345193684101, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11845/20000], Bound: 0.3708195090293884, Entropy: 142.24208068847656, Temp: 2.6917059421539307, KL: 69.43600463867188, Loss: 0.016831373795866966, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11846/20000], Bound: 0.40462854504585266, Entropy: 141.17677307128906, Temp: 2.691697835922241, KL: 80.46067810058594, Loss: 0.014693338423967361, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11847/20000], Bound: 0.38489848375320435, Entropy: 141.8978729248047, Temp: 2.6917009353637695, KL: 74.51434326171875, Loss: 0.014925984665751457, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11848/20000], Bound: 0.39948299527168274, Entropy: 141.8142852783203, Temp: 2.691709280014038, KL: 79.48908996582031, Loss: 0.013647761195898056, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11849/20000], Bound: 0.37510600686073303, Entropy: 142.49391174316406, Temp: 2.6917295455932617, KL: 70.64018249511719, Loss: 0.01687059924006462, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11850/20000], Bound: 0.4254865050315857, Entropy: 140.0025177001953, Temp: 2.691744804382324, KL: 86.08990478515625, Loss: 0.01602138765156269, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11851/20000], Bound: 0.37652015686035156, Entropy: 142.04037475585938, Temp: 2.691767930984497, KL: 72.50161743164062, Loss: 0.014167162589728832, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11852/20000], Bound: 0.3740294873714447, Entropy: 140.4412841796875, Temp: 2.691795587539673, KL: 70.52706909179688, Loss: 0.01650839112699032, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11853/20000], Bound: 0.3919582962989807, Entropy: 139.6693572998047, Temp: 2.6918184757232666, KL: 76.8193359375, Loss: 0.014478549361228943, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11854/20000], Bound: 0.3572520315647125, Entropy: 139.92822265625, Temp: 2.691847801208496, KL: 65.11708068847656, Loss: 0.017742056399583817, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11855/20000], Bound: 0.36829259991645813, Entropy: 142.28306579589844, Temp: 2.6918652057647705, KL: 70.16497802734375, Loss: 0.014143552631139755, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11856/20000], Bound: 0.3831935524940491, Entropy: 139.59661865234375, Temp: 2.6918861865997314, KL: 74.2445068359375, Loss: 0.01450921967625618, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11857/20000], Bound: 0.3805704712867737, Entropy: 140.36532592773438, Temp: 2.6919119358062744, KL: 72.74468994140625, Loss: 0.015884697437286377, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11858/20000], Bound: 0.39835023880004883, Entropy: 142.4569091796875, Temp: 2.691936492919922, KL: 75.77220153808594, Loss: 0.019929422065615654, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11859/20000], Bound: 0.37916257977485657, Entropy: 141.49664306640625, Temp: 2.6919498443603516, KL: 72.26335144042969, Loss: 0.016024133190512657, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11860/20000], Bound: 0.3564494550228119, Entropy: 142.93923950195312, Temp: 2.691962480545044, KL: 65.74588012695312, Loss: 0.016158415004611015, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11861/20000], Bound: 0.4098598062992096, Entropy: 142.8660888671875, Temp: 2.691970109939575, KL: 81.35865783691406, Loss: 0.015948787331581116, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11862/20000], Bound: 0.37295305728912354, Entropy: 141.78195190429688, Temp: 2.691983699798584, KL: 71.39472961425781, Loss: 0.014326493255794048, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11863/20000], Bound: 0.38300076127052307, Entropy: 141.69384765625, Temp: 2.6920018196105957, KL: 73.47613525390625, Loss: 0.015833605080842972, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11864/20000], Bound: 0.3787976801395416, Entropy: 143.12847900390625, Temp: 2.6920201778411865, KL: 72.90542602539062, Loss: 0.014636831358075142, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11865/20000], Bound: 0.3679048418998718, Entropy: 142.78456115722656, Temp: 2.692042350769043, KL: 67.15805053710938, Loss: 0.019525524228811264, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11866/20000], Bound: 0.37964844703674316, Entropy: 142.33920288085938, Temp: 2.6920487880706787, KL: 72.78924560546875, Loss: 0.015308610163629055, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11867/20000], Bound: 0.3855215907096863, Entropy: 140.87110900878906, Temp: 2.6920580863952637, KL: 74.2528076171875, Loss: 0.015751849859952927, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11868/20000], Bound: 0.38641074299812317, Entropy: 141.58787536621094, Temp: 2.6920695304870605, KL: 73.17448425292969, Loss: 0.018235720694065094, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11869/20000], Bound: 0.3866981863975525, Entropy: 141.43240356445312, Temp: 2.6920740604400635, KL: 74.65864562988281, Loss: 0.01563485525548458, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11870/20000], Bound: 0.3701182007789612, Entropy: 140.8008270263672, Temp: 2.692081928253174, KL: 69.63053894042969, Loss: 0.016102084890007973, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11871/20000], Bound: 0.35782769322395325, Entropy: 142.7763214111328, Temp: 2.6920878887176514, KL: 66.56849670410156, Loss: 0.015346982516348362, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11872/20000], Bound: 0.3688857853412628, Entropy: 142.64700317382812, Temp: 2.6920926570892334, KL: 70.06709289550781, Loss: 0.014640307053923607, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11873/20000], Bound: 0.41888996958732605, Entropy: 141.5617218017578, Temp: 2.6921005249023438, KL: 84.35879516601562, Loss: 0.015473682433366776, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11874/20000], Bound: 0.4116484224796295, Entropy: 140.58665466308594, Temp: 2.692117691040039, KL: 81.04656982421875, Loss: 0.01753377728164196, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11875/20000], Bound: 0.4116862416267395, Entropy: 138.9589385986328, Temp: 2.6921348571777344, KL: 80.83375549316406, Loss: 0.017950473353266716, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11876/20000], Bound: 0.40404605865478516, Entropy: 141.0138702392578, Temp: 2.692150354385376, KL: 79.09669494628906, Loss: 0.01690777949988842, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11877/20000], Bound: 0.34625399112701416, Entropy: 141.8656463623047, Temp: 2.692166805267334, KL: 63.312042236328125, Loss: 0.015429714694619179, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11878/20000], Bound: 0.4008064568042755, Entropy: 141.25367736816406, Temp: 2.692178726196289, KL: 76.79682922363281, Loss: 0.019384022802114487, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11879/20000], Bound: 0.36931461095809937, Entropy: 142.09799194335938, Temp: 2.6921825408935547, KL: 69.15859985351562, Loss: 0.016554759815335274, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11880/20000], Bound: 0.3781813979148865, Entropy: 142.0857696533203, Temp: 2.692183017730713, KL: 73.22822570800781, Loss: 0.01370901521295309, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11881/20000], Bound: 0.4097948670387268, Entropy: 141.3564453125, Temp: 2.692192554473877, KL: 80.46725463867188, Loss: 0.017570318654179573, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11882/20000], Bound: 0.3892202079296112, Entropy: 140.38577270507812, Temp: 2.692202091217041, KL: 73.56842041015625, Loss: 0.01902901381254196, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11883/20000], Bound: 0.39079952239990234, Entropy: 142.292236328125, Temp: 2.6922028064727783, KL: 75.84884643554688, Loss: 0.01565314084291458, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11884/20000], Bound: 0.36687713861465454, Entropy: 141.27294921875, Temp: 2.6922078132629395, KL: 69.15521240234375, Loss: 0.015276215970516205, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11885/20000], Bound: 0.4104516804218292, Entropy: 142.1645050048828, Temp: 2.692213535308838, KL: 81.73121643066406, Loss: 0.015591342002153397, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11886/20000], Bound: 0.3828105628490448, Entropy: 142.12554931640625, Temp: 2.6922266483306885, KL: 72.42155456542969, Loss: 0.017691832035779953, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11887/20000], Bound: 0.3796687126159668, Entropy: 140.97862243652344, Temp: 2.6922342777252197, KL: 71.25724792480469, Loss: 0.018166374415159225, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11888/20000], Bound: 0.3956499993801117, Entropy: 141.20469665527344, Temp: 2.692234516143799, KL: 77.54293823242188, Loss: 0.015158910304307938, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11889/20000], Bound: 0.37838122248649597, Entropy: 142.933349609375, Temp: 2.6922414302825928, KL: 72.14765930175781, Loss: 0.015823321416974068, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11890/20000], Bound: 0.3811647295951843, Entropy: 143.8187255859375, Temp: 2.692249298095703, KL: 73.53611755371094, Loss: 0.014736942946910858, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11891/20000], Bound: 0.3685317933559418, Entropy: 139.62551879882812, Temp: 2.6922621726989746, KL: 70.00250244140625, Loss: 0.014574963599443436, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11892/20000], Bound: 0.37730544805526733, Entropy: 142.525390625, Temp: 2.692277431488037, KL: 71.05088806152344, Loss: 0.01728537678718567, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11893/20000], Bound: 0.35201171040534973, Entropy: 142.72894287109375, Temp: 2.6922874450683594, KL: 64.0743408203125, Loss: 0.01697077602148056, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11894/20000], Bound: 0.3775608241558075, Entropy: 141.59286499023438, Temp: 2.692288875579834, KL: 71.99440002441406, Loss: 0.015669653192162514, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11895/20000], Bound: 0.4327685534954071, Entropy: 142.0653533935547, Temp: 2.6922919750213623, KL: 86.58970642089844, Loss: 0.01930275931954384, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11896/20000], Bound: 0.3784884810447693, Entropy: 140.8717803955078, Temp: 2.6922945976257324, KL: 72.04988098144531, Loss: 0.01606271229684353, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11897/20000], Bound: 0.37481823563575745, Entropy: 140.4772491455078, Temp: 2.6922972202301025, KL: 71.51762390136719, Loss: 0.015092500485479832, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11898/20000], Bound: 0.37919121980667114, Entropy: 141.1221923828125, Temp: 2.692302942276001, KL: 71.02366638183594, Loss: 0.018344849348068237, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11899/20000], Bound: 0.399244487285614, Entropy: 140.85206604003906, Temp: 2.692301034927368, KL: 77.10946655273438, Loss: 0.017941968515515327, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11900/20000], Bound: 0.38559505343437195, Entropy: 142.00613403320312, Temp: 2.6922969818115234, KL: 73.36444091796875, Loss: 0.01744362898170948, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11901/20000], Bound: 0.38741669058799744, Entropy: 143.223388671875, Temp: 2.6922905445098877, KL: 75.49777221679688, Loss: 0.014467759057879448, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11902/20000], Bound: 0.37662404775619507, Entropy: 139.6283416748047, Temp: 2.6922924518585205, KL: 70.79348754882812, Loss: 0.017399761825799942, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11903/20000], Bound: 0.383071631193161, Entropy: 143.2726593017578, Temp: 2.6922898292541504, KL: 73.30964660644531, Loss: 0.01618359237909317, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11904/20000], Bound: 0.42379364371299744, Entropy: 141.00233459472656, Temp: 2.692288398742676, KL: 84.8671875, Loss: 0.01732819341123104, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11905/20000], Bound: 0.3453338146209717, Entropy: 144.09422302246094, Temp: 2.692291736602783, KL: 61.99089050292969, Loss: 0.017413971945643425, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11906/20000], Bound: 0.38098621368408203, Entropy: 141.152587890625, Temp: 2.69228458404541, KL: 73.04676818847656, Loss: 0.015550185926258564, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11907/20000], Bound: 0.3842731714248657, Entropy: 142.13504028320312, Temp: 2.6922807693481445, KL: 73.51828002929688, Loss: 0.016443802043795586, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11908/20000], Bound: 0.3801485002040863, Entropy: 141.70114135742188, Temp: 2.692277669906616, KL: 72.99285888671875, Loss: 0.015200662426650524, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11909/20000], Bound: 0.3560616374015808, Entropy: 142.14361572265625, Temp: 2.6922786235809326, KL: 66.70159912109375, Loss: 0.014184747822582722, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11910/20000], Bound: 0.38055887818336487, Entropy: 141.57839965820312, Temp: 2.6922829151153564, KL: 72.81741333007812, Loss: 0.015746699646115303, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11911/20000], Bound: 0.3740016520023346, Entropy: 143.39344787597656, Temp: 2.692288637161255, KL: 70.73771667480469, Loss: 0.01610647886991501, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11912/20000], Bound: 0.3712143301963806, Entropy: 142.77197265625, Temp: 2.692293405532837, KL: 70.01419067382812, Loss: 0.015971284359693527, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11913/20000], Bound: 0.37857189774513245, Entropy: 142.06886291503906, Temp: 2.6922974586486816, KL: 71.26780700683594, Loss: 0.01755981147289276, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11914/20000], Bound: 0.39720791578292847, Entropy: 143.32676696777344, Temp: 2.6922965049743652, KL: 78.10440063476562, Loss: 0.014972546137869358, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11915/20000], Bound: 0.36648687720298767, Entropy: 140.40386962890625, Temp: 2.692303419113159, KL: 69.12007141113281, Loss: 0.01513694878667593, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11916/20000], Bound: 0.36571478843688965, Entropy: 142.74913024902344, Temp: 2.6923110485076904, KL: 67.27406311035156, Loss: 0.01815943978726864, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11917/20000], Bound: 0.3858812749385834, Entropy: 144.01951599121094, Temp: 2.6923089027404785, KL: 74.06962585449219, Loss: 0.016288863494992256, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11918/20000], Bound: 0.3903459906578064, Entropy: 140.89747619628906, Temp: 2.692307949066162, KL: 75.32524108886719, Loss: 0.01637953892350197, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11919/20000], Bound: 0.3883208930492401, Entropy: 141.97598266601562, Temp: 2.6923089027404785, KL: 73.52606201171875, Loss: 0.018620144575834274, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11920/20000], Bound: 0.38678425550460815, Entropy: 141.53140258789062, Temp: 2.69230318069458, KL: 74.94972229003906, Loss: 0.015143043361604214, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11921/20000], Bound: 0.36793965101242065, Entropy: 142.68016052246094, Temp: 2.692303419113159, KL: 68.43095397949219, Loss: 0.017181675881147385, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11922/20000], Bound: 0.37947073578834534, Entropy: 142.57638549804688, Temp: 2.692298173904419, KL: 72.27806091308594, Loss: 0.01616497151553631, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11923/20000], Bound: 0.36698946356773376, Entropy: 143.5596923828125, Temp: 2.692294120788574, KL: 68.67391967773438, Loss: 0.016229908913373947, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11924/20000], Bound: 0.36788684129714966, Entropy: 141.82867431640625, Temp: 2.6922879219055176, KL: 70.06767272949219, Loss: 0.0141140753403306, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11925/20000], Bound: 0.3816174864768982, Entropy: 139.7800750732422, Temp: 2.6922881603240967, KL: 73.86375427246094, Loss: 0.014372093603014946, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11926/20000], Bound: 0.37729889154434204, Entropy: 142.62930297851562, Temp: 2.692295551300049, KL: 71.21376037597656, Loss: 0.016979573294520378, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11927/20000], Bound: 0.37578633427619934, Entropy: 140.09063720703125, Temp: 2.6922993659973145, KL: 71.79881286621094, Loss: 0.015085975639522076, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11928/20000], Bound: 0.3851798176765442, Entropy: 142.38568115234375, Temp: 2.6923062801361084, KL: 74.30436706542969, Loss: 0.015473687089979649, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11929/20000], Bound: 0.39615899324417114, Entropy: 141.75157165527344, Temp: 2.6923162937164307, KL: 76.46273803710938, Loss: 0.017445188015699387, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11930/20000], Bound: 0.3607556223869324, Entropy: 141.8215789794922, Temp: 2.692324161529541, KL: 67.60368347167969, Loss: 0.014950896613299847, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11931/20000], Bound: 0.3730981945991516, Entropy: 141.68702697753906, Temp: 2.6923327445983887, KL: 70.18815612792969, Loss: 0.01664746180176735, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11932/20000], Bound: 0.3818412125110626, Entropy: 142.7613067626953, Temp: 2.69233775138855, KL: 74.65826416015625, Loss: 0.013017356395721436, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11933/20000], Bound: 0.4000161588191986, Entropy: 141.1839599609375, Temp: 2.692354440689087, KL: 77.25595092773438, Loss: 0.018096312880516052, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11934/20000], Bound: 0.39076635241508484, Entropy: 139.80625915527344, Temp: 2.6923670768737793, KL: 75.99354553222656, Loss: 0.015367879532277584, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11935/20000], Bound: 0.4000524580478668, Entropy: 140.29025268554688, Temp: 2.692383289337158, KL: 78.95329284667969, Loss: 0.01496447529643774, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11936/20000], Bound: 0.3753095269203186, Entropy: 140.6847381591797, Temp: 2.692406415939331, KL: 71.41299438476562, Loss: 0.01554941851645708, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11937/20000], Bound: 0.40632471442222595, Entropy: 143.70054626464844, Temp: 2.6924290657043457, KL: 80.42085266113281, Loss: 0.015719497576355934, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11938/20000], Bound: 0.43468841910362244, Entropy: 142.62515258789062, Temp: 2.6924564838409424, KL: 87.74430847167969, Loss: 0.01827649585902691, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11939/20000], Bound: 0.4011407196521759, Entropy: 141.1466522216797, Temp: 2.6924843788146973, KL: 77.87033081054688, Loss: 0.017578095197677612, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11940/20000], Bound: 0.3718385100364685, Entropy: 140.84226989746094, Temp: 2.692509174346924, KL: 70.1588134765625, Loss: 0.01603519730269909, Learning Rate: 6.978915419616203e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11941/20000], Bound: 0.34659603238105774, Entropy: 143.2820281982422, Temp: 2.692530870437622, KL: 62.02992248535156, Loss: 0.017988160252571106, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11942/20000], Bound: 0.39571133255958557, Entropy: 142.65994262695312, Temp: 2.692538261413574, KL: 76.79493713378906, Loss: 0.016584668308496475, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11943/20000], Bound: 0.4020598232746124, Entropy: 141.84207153320312, Temp: 2.692546844482422, KL: 79.57164001464844, Loss: 0.01492808572947979, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11944/20000], Bound: 0.36645182967185974, Entropy: 140.98489379882812, Temp: 2.692563056945801, KL: 68.54116821289062, Loss: 0.01619572937488556, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11945/20000], Bound: 0.38026735186576843, Entropy: 141.4688262939453, Temp: 2.6925759315490723, KL: 73.00331115722656, Loss: 0.015247712843120098, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11946/20000], Bound: 0.3632560074329376, Entropy: 141.43601989746094, Temp: 2.6925909519195557, KL: 67.83163452148438, Loss: 0.015836557373404503, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11947/20000], Bound: 0.38435104489326477, Entropy: 141.85601806640625, Temp: 2.692603349685669, KL: 72.36680603027344, Loss: 0.018626928329467773, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11948/20000], Bound: 0.36092689633369446, Entropy: 143.067138671875, Temp: 2.6926069259643555, KL: 66.00125122070312, Loss: 0.018018167465925217, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11949/20000], Bound: 0.3829619586467743, Entropy: 140.47906494140625, Temp: 2.692600727081299, KL: 72.13565063476562, Loss: 0.01830737106502056, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11950/20000], Bound: 0.3838759660720825, Entropy: 141.6158905029297, Temp: 2.6925885677337646, KL: 73.13822937011719, Loss: 0.016938088461756706, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11951/20000], Bound: 0.3652113080024719, Entropy: 141.56407165527344, Temp: 2.6925761699676514, KL: 66.10435485839844, Loss: 0.020069027319550514, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11952/20000], Bound: 0.3951197862625122, Entropy: 141.01751708984375, Temp: 2.6925487518310547, KL: 76.48289489746094, Loss: 0.01683981530368328, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11953/20000], Bound: 0.39818084239959717, Entropy: 141.2555694580078, Temp: 2.6925251483917236, KL: 77.58660888671875, Loss: 0.016471806913614273, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11954/20000], Bound: 0.3880046010017395, Entropy: 142.5874786376953, Temp: 2.692506790161133, KL: 74.14251708984375, Loss: 0.017305413261055946, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11955/20000], Bound: 0.363081693649292, Entropy: 142.7698516845703, Temp: 2.692487955093384, KL: 67.72927856445312, Loss: 0.01593456044793129, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11956/20000], Bound: 0.3841562569141388, Entropy: 140.82379150390625, Temp: 2.692469358444214, KL: 73.41085815429688, Loss: 0.016581928357481956, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11957/20000], Bound: 0.42265766859054565, Entropy: 142.551513671875, Temp: 2.6924524307250977, KL: 85.54074096679688, Loss: 0.015429586172103882, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11958/20000], Bound: 0.37513914704322815, Entropy: 142.41001892089844, Temp: 2.692448139190674, KL: 71.76795959472656, Loss: 0.014799851924180984, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11959/20000], Bound: 0.37982824444770813, Entropy: 140.9752960205078, Temp: 2.692448377609253, KL: 72.58981323242188, Loss: 0.015778999775648117, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11960/20000], Bound: 0.3475292921066284, Entropy: 144.08663940429688, Temp: 2.692450761795044, KL: 64.92929077148438, Loss: 0.013081133365631104, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11961/20000], Bound: 0.39129120111465454, Entropy: 139.7704620361328, Temp: 2.692458152770996, KL: 76.50503540039062, Loss: 0.014704959467053413, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11962/20000], Bound: 0.4045998454093933, Entropy: 140.5546875, Temp: 2.6924726963043213, KL: 80.50408935546875, Loss: 0.014605199918150902, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11963/20000], Bound: 0.35708630084991455, Entropy: 141.68560791015625, Temp: 2.6924962997436523, KL: 66.17811584472656, Loss: 0.01569010131061077, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11964/20000], Bound: 0.39689067006111145, Entropy: 142.5746612548828, Temp: 2.6925153732299805, KL: 77.46342468261719, Loss: 0.015990693122148514, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11965/20000], Bound: 0.42386943101882935, Entropy: 142.098876953125, Temp: 2.6925368309020996, KL: 85.61160278320312, Loss: 0.015991980209946632, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11966/20000], Bound: 0.3988601267337799, Entropy: 141.5426788330078, Temp: 2.6925652027130127, KL: 77.23075866699219, Loss: 0.017507296055555344, Learning Rate: 6.978915419616203e-05\n",
      "Epoch [11967/20000], Bound: 0.3579310476779938, Entropy: 142.83428955078125, Temp: 2.692582607269287, KL: 66.56114196777344, Loss: 0.015418211929500103, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11968/20000], Bound: 0.39658406376838684, Entropy: 142.83265686035156, Temp: 2.6925976276397705, KL: 76.3006591796875, Loss: 0.017982222139835358, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11969/20000], Bound: 0.41005298495292664, Entropy: 140.70130920410156, Temp: 2.6926093101501465, KL: 81.62068176269531, Loss: 0.015577375888824463, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11970/20000], Bound: 0.4039534628391266, Entropy: 141.88157653808594, Temp: 2.6926252841949463, KL: 79.03411865234375, Loss: 0.01697729527950287, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11971/20000], Bound: 0.38805463910102844, Entropy: 142.820068359375, Temp: 2.692641258239746, KL: 75.05563354492188, Loss: 0.015638142824172974, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11972/20000], Bound: 0.360425740480423, Entropy: 140.04837036132812, Temp: 2.6926584243774414, KL: 67.39787292480469, Loss: 0.015163704752922058, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11973/20000], Bound: 0.37874311208724976, Entropy: 141.72471618652344, Temp: 2.692673921585083, KL: 72.71070861816406, Loss: 0.014975243248045444, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11974/20000], Bound: 0.37034156918525696, Entropy: 142.10540771484375, Temp: 2.6926910877227783, KL: 69.56391906738281, Loss: 0.016348866745829582, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11975/20000], Bound: 0.39145931601524353, Entropy: 142.0548858642578, Temp: 2.6927051544189453, KL: 75.61331176757812, Loss: 0.016454853117465973, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11976/20000], Bound: 0.4192892611026764, Entropy: 142.3616485595703, Temp: 2.692718982696533, KL: 84.46403503417969, Loss: 0.015512332320213318, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11977/20000], Bound: 0.38090085983276367, Entropy: 139.94068908691406, Temp: 2.6927382946014404, KL: 74.50959777832031, Loss: 0.012792222201824188, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11978/20000], Bound: 0.3857283592224121, Entropy: 142.68016052246094, Temp: 2.6927647590637207, KL: 72.81391906738281, Loss: 0.018541980534791946, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11979/20000], Bound: 0.3695763051509857, Entropy: 141.3827362060547, Temp: 2.6927835941314697, KL: 70.29624938964844, Loss: 0.01458536833524704, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11980/20000], Bound: 0.3526129722595215, Entropy: 144.43533325195312, Temp: 2.692803382873535, KL: 64.90887451171875, Loss: 0.01573476567864418, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11981/20000], Bound: 0.37869277596473694, Entropy: 141.49740600585938, Temp: 2.692819118499756, KL: 71.14512634277344, Loss: 0.017856581136584282, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11982/20000], Bound: 0.38052433729171753, Entropy: 138.92730712890625, Temp: 2.6928293704986572, KL: 74.58303833007812, Loss: 0.01245470717549324, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11983/20000], Bound: 0.3860514461994171, Entropy: 141.62628173828125, Temp: 2.692847967147827, KL: 74.42373657226562, Loss: 0.01572829857468605, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11984/20000], Bound: 0.4093533754348755, Entropy: 143.1490936279297, Temp: 2.692866802215576, KL: 79.37344360351562, Loss: 0.01936080865561962, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11985/20000], Bound: 0.3813176155090332, Entropy: 142.97874450683594, Temp: 2.692880392074585, KL: 74.36419677734375, Loss: 0.013287422247231007, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11986/20000], Bound: 0.38039228320121765, Entropy: 141.52854919433594, Temp: 2.6929001808166504, KL: 72.95101928710938, Loss: 0.015414769761264324, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11987/20000], Bound: 0.3947102427482605, Entropy: 142.2033233642578, Temp: 2.692920207977295, KL: 76.80720520019531, Loss: 0.016016749665141106, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11988/20000], Bound: 0.4062594175338745, Entropy: 141.3714599609375, Temp: 2.6929407119750977, KL: 80.14692687988281, Loss: 0.016197143122553825, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11989/20000], Bound: 0.3845328688621521, Entropy: 140.79356384277344, Temp: 2.692962884902954, KL: 74.05801391601562, Loss: 0.015587914735078812, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11990/20000], Bound: 0.3492690622806549, Entropy: 142.35987854003906, Temp: 2.6929850578308105, KL: 64.14277648925781, Loss: 0.015437809750437737, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11991/20000], Bound: 0.37802186608314514, Entropy: 141.4728240966797, Temp: 2.6930031776428223, KL: 72.48919677734375, Loss: 0.015003729611635208, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11992/20000], Bound: 0.3866354823112488, Entropy: 143.8928680419922, Temp: 2.6930224895477295, KL: 74.02658081054688, Loss: 0.016783375293016434, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11993/20000], Bound: 0.3982081413269043, Entropy: 140.83444213867188, Temp: 2.693039894104004, KL: 77.84883117675781, Loss: 0.016005080193281174, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11994/20000], Bound: 0.4036255180835724, Entropy: 140.7928924560547, Temp: 2.6930582523345947, KL: 77.97210693359375, Loss: 0.01877128891646862, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11995/20000], Bound: 0.3756391704082489, Entropy: 143.43589782714844, Temp: 2.6930718421936035, KL: 71.154541015625, Loss: 0.01621062494814396, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11996/20000], Bound: 0.3780193328857422, Entropy: 142.20126342773438, Temp: 2.6930837631225586, KL: 71.62864685058594, Loss: 0.016600795090198517, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11997/20000], Bound: 0.4006703794002533, Entropy: 142.9589385986328, Temp: 2.6930935382843018, KL: 76.79100036621094, Loss: 0.01932777650654316, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11998/20000], Bound: 0.3870626986026764, Entropy: 142.53616333007812, Temp: 2.6930975914001465, KL: 72.67083740234375, Loss: 0.01953241229057312, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [11999/20000], Bound: 0.3995520770549774, Entropy: 139.9728546142578, Temp: 2.693094253540039, KL: 77.45347595214844, Loss: 0.01748032122850418, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12000/20000], Bound: 0.37981462478637695, Entropy: 141.2696990966797, Temp: 2.6930909156799316, KL: 72.36354064941406, Loss: 0.01619749516248703, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12001/20000], Bound: 0.38473331928253174, Entropy: 142.121826171875, Temp: 2.6930880546569824, KL: 73.53909301757812, Loss: 0.016660714522004128, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12002/20000], Bound: 0.4054660201072693, Entropy: 141.39491271972656, Temp: 2.693085193634033, KL: 80.32196044921875, Loss: 0.015431835316121578, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12003/20000], Bound: 0.4210696816444397, Entropy: 138.46774291992188, Temp: 2.6930880546569824, KL: 83.91810607910156, Loss: 0.01754348911345005, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12004/20000], Bound: 0.36972135305404663, Entropy: 142.60372924804688, Temp: 2.6930928230285645, KL: 69.64100646972656, Loss: 0.015881260856986046, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12005/20000], Bound: 0.41626858711242676, Entropy: 140.4764404296875, Temp: 2.693096876144409, KL: 83.2890625, Loss: 0.015985185280442238, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12006/20000], Bound: 0.3962402641773224, Entropy: 140.86163330078125, Temp: 2.693105936050415, KL: 77.44709777832031, Loss: 0.015669571235775948, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12007/20000], Bound: 0.4039398431777954, Entropy: 142.55503845214844, Temp: 2.69311785697937, KL: 79.40928649902344, Loss: 0.016278138384222984, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12008/20000], Bound: 0.36405447125434875, Entropy: 141.26576232910156, Temp: 2.693131446838379, KL: 67.29339599609375, Loss: 0.017258327454328537, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12009/20000], Bound: 0.3924478590488434, Entropy: 140.9410858154297, Temp: 2.6931395530700684, KL: 74.21882629394531, Loss: 0.01958724483847618, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12010/20000], Bound: 0.38358646631240845, Entropy: 142.40048217773438, Temp: 2.6931402683258057, KL: 73.3245849609375, Loss: 0.016440851613879204, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12011/20000], Bound: 0.36092454195022583, Entropy: 143.35675048828125, Temp: 2.693140983581543, KL: 66.06124877929688, Loss: 0.017909271642565727, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12012/20000], Bound: 0.3682512938976288, Entropy: 142.22401428222656, Temp: 2.6931350231170654, KL: 69.47059631347656, Loss: 0.015422176569700241, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12013/20000], Bound: 0.3986138105392456, Entropy: 141.4046173095703, Temp: 2.6931304931640625, KL: 78.21162414550781, Loss: 0.015555808320641518, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12014/20000], Bound: 0.34285804629325867, Entropy: 143.6231689453125, Temp: 2.6931304931640625, KL: 62.6275634765625, Loss: 0.014975366182625294, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12015/20000], Bound: 0.39101994037628174, Entropy: 141.1337432861328, Temp: 2.693129062652588, KL: 76.41207885742188, Loss: 0.014736427925527096, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12016/20000], Bound: 0.36194005608558655, Entropy: 140.3615264892578, Temp: 2.6931331157684326, KL: 67.62905883789062, Loss: 0.015528700314462185, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12017/20000], Bound: 0.3746385872364044, Entropy: 141.51321411132812, Temp: 2.693136215209961, KL: 71.38427734375, Loss: 0.01525197271257639, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12018/20000], Bound: 0.3826484680175781, Entropy: 140.6029052734375, Temp: 2.693140983581543, KL: 74.45216369628906, Loss: 0.013842273503541946, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12019/20000], Bound: 0.37129050493240356, Entropy: 141.4586944580078, Temp: 2.6931514739990234, KL: 69.62605285644531, Loss: 0.016739415004849434, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12020/20000], Bound: 0.40253913402557373, Entropy: 143.24070739746094, Temp: 2.6931588649749756, KL: 80.0233154296875, Loss: 0.014361525885760784, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12021/20000], Bound: 0.3698318600654602, Entropy: 142.805908203125, Temp: 2.6931731700897217, KL: 68.81315612792969, Loss: 0.017477260902523994, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12022/20000], Bound: 0.4150528311729431, Entropy: 141.5441131591797, Temp: 2.6931817531585693, KL: 83.12106323242188, Loss: 0.01561086717993021, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12023/20000], Bound: 0.37891972064971924, Entropy: 141.99072265625, Temp: 2.6931958198547363, KL: 72.40956115722656, Loss: 0.01563364267349243, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12024/20000], Bound: 0.38169732689857483, Entropy: 142.4426727294922, Temp: 2.6932098865509033, KL: 73.14570617675781, Loss: 0.015756884589791298, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12025/20000], Bound: 0.3743501305580139, Entropy: 142.52999877929688, Temp: 2.6932239532470703, KL: 71.98674011230469, Loss: 0.013980801217257977, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12026/20000], Bound: 0.35176920890808105, Entropy: 144.08847045898438, Temp: 2.693241834640503, KL: 64.41917419433594, Loss: 0.01621217466890812, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12027/20000], Bound: 0.3734394311904907, Entropy: 140.52301025390625, Temp: 2.693254232406616, KL: 71.70431518554688, Loss: 0.014021456241607666, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12028/20000], Bound: 0.40576615929603577, Entropy: 142.03602600097656, Temp: 2.693270206451416, KL: 78.81526184082031, Loss: 0.018398037180304527, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12029/20000], Bound: 0.3539799451828003, Entropy: 143.09324645996094, Temp: 2.6932828426361084, KL: 64.45596313476562, Loss: 0.017284715548157692, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12030/20000], Bound: 0.39434027671813965, Entropy: 140.0765838623047, Temp: 2.693288564682007, KL: 77.28086853027344, Loss: 0.014938456937670708, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12031/20000], Bound: 0.4098879396915436, Entropy: 140.9206085205078, Temp: 2.693298816680908, KL: 81.58157348632812, Loss: 0.015565024688839912, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12032/20000], Bound: 0.38536036014556885, Entropy: 140.9680633544922, Temp: 2.6933135986328125, KL: 75.26544189453125, Loss: 0.013796500861644745, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12033/20000], Bound: 0.38615667819976807, Entropy: 142.22317504882812, Temp: 2.693333864212036, KL: 74.16000366210938, Loss: 0.016279347240924835, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12034/20000], Bound: 0.3830464780330658, Entropy: 141.66452026367188, Temp: 2.693352699279785, KL: 72.56143188476562, Loss: 0.01756860502064228, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12035/20000], Bound: 0.3948972225189209, Entropy: 142.18167114257812, Temp: 2.6933670043945312, KL: 76.13473510742188, Loss: 0.017371943220496178, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12036/20000], Bound: 0.3968294858932495, Entropy: 142.02613830566406, Temp: 2.6933791637420654, KL: 75.39741516113281, Loss: 0.019800951704382896, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12037/20000], Bound: 0.37633001804351807, Entropy: 140.90994262695312, Temp: 2.6933836936950684, KL: 70.18424987792969, Loss: 0.018382752314209938, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12038/20000], Bound: 0.3867936134338379, Entropy: 141.57504272460938, Temp: 2.6933820247650146, KL: 72.84017944335938, Loss: 0.019074568524956703, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12039/20000], Bound: 0.38731712102890015, Entropy: 141.2216796875, Temp: 2.6933748722076416, KL: 74.18986511230469, Loss: 0.016852542757987976, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12040/20000], Bound: 0.3867025673389435, Entropy: 142.76255798339844, Temp: 2.6933677196502686, KL: 74.72105407714844, Loss: 0.015533512458205223, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12041/20000], Bound: 0.402538001537323, Entropy: 141.2838897705078, Temp: 2.693364143371582, KL: 78.20274353027344, Loss: 0.01774287223815918, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12042/20000], Bound: 0.4027506411075592, Entropy: 142.7533721923828, Temp: 2.6933600902557373, KL: 77.9473876953125, Loss: 0.018334705382585526, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12043/20000], Bound: 0.381882905960083, Entropy: 142.25083923339844, Temp: 2.693354606628418, KL: 71.76234436035156, Loss: 0.018426064401865005, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12044/20000], Bound: 0.36212486028671265, Entropy: 142.8628387451172, Temp: 2.6933443546295166, KL: 68.78770446777344, Loss: 0.013476023450493813, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12045/20000], Bound: 0.3703027069568634, Entropy: 142.61968994140625, Temp: 2.693340301513672, KL: 69.10310363769531, Loss: 0.01718910038471222, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12046/20000], Bound: 0.4005659222602844, Entropy: 140.25222778320312, Temp: 2.6933329105377197, KL: 78.94612121582031, Loss: 0.015271355397999287, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12047/20000], Bound: 0.39966678619384766, Entropy: 141.54190063476562, Temp: 2.693331241607666, KL: 78.57746887207031, Loss: 0.015459297224879265, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12048/20000], Bound: 0.3833008408546448, Entropy: 139.51280212402344, Temp: 2.6933348178863525, KL: 72.84999084472656, Loss: 0.017169775441288948, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12049/20000], Bound: 0.3938544988632202, Entropy: 138.84523010253906, Temp: 2.693335771560669, KL: 75.61781311035156, Loss: 0.017760414630174637, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12050/20000], Bound: 0.37503790855407715, Entropy: 141.23907470703125, Temp: 2.6933350563049316, KL: 71.22795104980469, Loss: 0.01575639843940735, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12051/20000], Bound: 0.36657097935676575, Entropy: 142.96519470214844, Temp: 2.6933350563049316, KL: 67.67448425292969, Loss: 0.017873454838991165, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12052/20000], Bound: 0.4045429825782776, Entropy: 141.0623016357422, Temp: 2.6933295726776123, KL: 78.6259765625, Loss: 0.018069515004754066, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12053/20000], Bound: 0.397005558013916, Entropy: 143.15711975097656, Temp: 2.6933233737945557, KL: 77.6978759765625, Loss: 0.015626566484570503, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12054/20000], Bound: 0.3935657739639282, Entropy: 140.92735290527344, Temp: 2.693321704864502, KL: 76.99545288085938, Loss: 0.015044859610497952, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12055/20000], Bound: 0.4215250313282013, Entropy: 141.7989959716797, Temp: 2.6933250427246094, KL: 85.05250549316406, Loss: 0.01569974608719349, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12056/20000], Bound: 0.3808901906013489, Entropy: 141.37083435058594, Temp: 2.6933348178863525, KL: 71.63485717773438, Loss: 0.01812928356230259, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12057/20000], Bound: 0.3841824531555176, Entropy: 143.1403350830078, Temp: 2.6933388710021973, KL: 72.25723266601562, Loss: 0.018745403736829758, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12058/20000], Bound: 0.3816165626049042, Entropy: 141.13998413085938, Temp: 2.6933374404907227, KL: 73.39028930664062, Loss: 0.015260579995810986, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12059/20000], Bound: 0.37085071206092834, Entropy: 141.54324340820312, Temp: 2.693338632583618, KL: 70.61566162109375, Loss: 0.01467098481953144, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12060/20000], Bound: 0.40871259570121765, Entropy: 141.38015747070312, Temp: 2.693342685699463, KL: 80.56167602539062, Loss: 0.016800925135612488, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12061/20000], Bound: 0.40571263432502747, Entropy: 141.27880859375, Temp: 2.6933486461639404, KL: 79.48797607421875, Loss: 0.01712011732161045, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12062/20000], Bound: 0.36917129158973694, Entropy: 141.2042999267578, Temp: 2.6933555603027344, KL: 70.87654113769531, Loss: 0.013299265876412392, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12063/20000], Bound: 0.40854522585868835, Entropy: 142.0646209716797, Temp: 2.6933677196502686, KL: 81.19029235839844, Loss: 0.015540648251771927, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12064/20000], Bound: 0.39656496047973633, Entropy: 141.7744140625, Temp: 2.6933844089508057, KL: 78.34591674804688, Loss: 0.014182095415890217, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12065/20000], Bound: 0.3696020841598511, Entropy: 142.26702880859375, Temp: 2.693406581878662, KL: 68.94894409179688, Loss: 0.017105575650930405, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12066/20000], Bound: 0.39438411593437195, Entropy: 142.8934783935547, Temp: 2.693423271179199, KL: 77.30299377441406, Loss: 0.01492270827293396, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12067/20000], Bound: 0.3613942861557007, Entropy: 141.87559509277344, Temp: 2.693443536758423, KL: 66.95491027832031, Loss: 0.01649758778512478, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12068/20000], Bound: 0.3820984661579132, Entropy: 140.8451385498047, Temp: 2.6934585571289062, KL: 70.80322265625, Loss: 0.020323244854807854, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12069/20000], Bound: 0.3616451025009155, Entropy: 143.1184539794922, Temp: 2.693462610244751, KL: 66.91181945800781, Loss: 0.016708675771951675, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12070/20000], Bound: 0.3906267583370209, Entropy: 142.07125854492188, Temp: 2.69346284866333, KL: 75.41943359375, Loss: 0.01636834442615509, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12071/20000], Bound: 0.365837424993515, Entropy: 141.58206176757812, Temp: 2.6934640407562256, KL: 70.34025573730469, Loss: 0.012540278024971485, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12072/20000], Bound: 0.4012768864631653, Entropy: 139.0584716796875, Temp: 2.6934728622436523, KL: 78.44293212890625, Loss: 0.01659989543259144, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12073/20000], Bound: 0.360957533121109, Entropy: 142.7169647216797, Temp: 2.6934826374053955, KL: 68.01182556152344, Loss: 0.01430797390639782, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12074/20000], Bound: 0.41216927766799927, Entropy: 142.2743682861328, Temp: 2.6934938430786133, KL: 82.9005126953125, Loss: 0.014399079605937004, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12075/20000], Bound: 0.37577810883522034, Entropy: 142.2289276123047, Temp: 2.6935129165649414, KL: 71.58779907226562, Loss: 0.015484156087040901, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12076/20000], Bound: 0.37014687061309814, Entropy: 139.32513427734375, Temp: 2.6935312747955322, KL: 70.13705444335938, Loss: 0.015188917517662048, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12077/20000], Bound: 0.4020020365715027, Entropy: 141.88870239257812, Temp: 2.693549156188965, KL: 80.37388610839844, Loss: 0.013417527079582214, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12078/20000], Bound: 0.3983471393585205, Entropy: 143.81475830078125, Temp: 2.693575382232666, KL: 77.97634887695312, Loss: 0.015850206837058067, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12079/20000], Bound: 0.38103795051574707, Entropy: 143.69537353515625, Temp: 2.6936020851135254, KL: 73.10017395019531, Loss: 0.015490787103772163, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12080/20000], Bound: 0.39315134286880493, Entropy: 141.87803649902344, Temp: 2.6936283111572266, KL: 77.09449768066406, Loss: 0.014637577347457409, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12081/20000], Bound: 0.3891168236732483, Entropy: 142.37860107421875, Temp: 2.693657636642456, KL: 73.25111389160156, Loss: 0.019573893398046494, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12082/20000], Bound: 0.41184768080711365, Entropy: 139.76620483398438, Temp: 2.6936774253845215, KL: 82.66554260253906, Loss: 0.014656596817076206, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12083/20000], Bound: 0.3553658425807953, Entropy: 141.0951385498047, Temp: 2.6937029361724854, KL: 66.54939270019531, Loss: 0.014118514955043793, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12084/20000], Bound: 0.397501677274704, Entropy: 139.26881408691406, Temp: 2.69372820854187, KL: 79.06379699707031, Loss: 0.013368021696805954, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12085/20000], Bound: 0.37812191247940063, Entropy: 143.1688995361328, Temp: 2.693760395050049, KL: 68.79414367675781, Loss: 0.021922649815678596, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12086/20000], Bound: 0.40063712000846863, Entropy: 140.580810546875, Temp: 2.693774938583374, KL: 79.14167785644531, Loss: 0.014952294528484344, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12087/20000], Bound: 0.4092920422554016, Entropy: 140.9026336669922, Temp: 2.6937942504882812, KL: 81.30642700195312, Loss: 0.015747463330626488, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12088/20000], Bound: 0.4007578492164612, Entropy: 142.4486083984375, Temp: 2.6938164234161377, KL: 78.09031677246094, Loss: 0.016970843076705933, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12089/20000], Bound: 0.40136051177978516, Entropy: 143.42527770996094, Temp: 2.6938376426696777, KL: 77.71003723144531, Loss: 0.018010098487138748, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12090/20000], Bound: 0.3964230716228485, Entropy: 140.0751495361328, Temp: 2.693855047225952, KL: 77.62457275390625, Loss: 0.01544802263379097, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12091/20000], Bound: 0.377555787563324, Entropy: 141.0503692626953, Temp: 2.6938750743865967, KL: 71.74905395507812, Loss: 0.016136346384882927, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12092/20000], Bound: 0.385653555393219, Entropy: 142.80300903320312, Temp: 2.6938931941986084, KL: 73.32608032226562, Loss: 0.017560254782438278, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12093/20000], Bound: 0.3612438142299652, Entropy: 142.14559936523438, Temp: 2.6939070224761963, KL: 67.02909851074219, Loss: 0.016284866258502007, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12094/20000], Bound: 0.4042741656303406, Entropy: 140.850830078125, Temp: 2.6939167976379395, KL: 79.18565368652344, Loss: 0.016887055709958076, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12095/20000], Bound: 0.38100722432136536, Entropy: 140.5246124267578, Temp: 2.693927526473999, KL: 74.39256286621094, Loss: 0.013078548945486546, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12096/20000], Bound: 0.3936176598072052, Entropy: 141.84730529785156, Temp: 2.6939451694488525, KL: 75.39718627929688, Loss: 0.018045878037810326, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12097/20000], Bound: 0.3849314749240875, Entropy: 142.18174743652344, Temp: 2.6939585208892822, KL: 73.33625793457031, Loss: 0.017151879146695137, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12098/20000], Bound: 0.3612174391746521, Entropy: 141.36476135253906, Temp: 2.6939690113067627, KL: 68.47746276855469, Loss: 0.01358342356979847, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12099/20000], Bound: 0.4113718271255493, Entropy: 143.0607147216797, Temp: 2.6939826011657715, KL: 82.24575805664062, Loss: 0.015171926468610764, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12100/20000], Bound: 0.3769453167915344, Entropy: 140.59213256835938, Temp: 2.6940014362335205, KL: 70.72859191894531, Loss: 0.01770547218620777, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12101/20000], Bound: 0.3701322376728058, Entropy: 141.74916076660156, Temp: 2.694014549255371, KL: 70.31950378417969, Loss: 0.014846687205135822, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12102/20000], Bound: 0.38269469141960144, Entropy: 141.4921417236328, Temp: 2.694028854370117, KL: 73.7032470703125, Loss: 0.015265777707099915, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12103/20000], Bound: 0.4149358570575714, Entropy: 140.85044860839844, Temp: 2.694044351577759, KL: 82.12242126464844, Loss: 0.01740795187652111, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12104/20000], Bound: 0.3926813304424286, Entropy: 141.4942169189453, Temp: 2.6940600872039795, KL: 75.42391967773438, Loss: 0.017485685646533966, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12105/20000], Bound: 0.37035977840423584, Entropy: 141.9206085205078, Temp: 2.694072961807251, KL: 68.16143798828125, Loss: 0.018972694873809814, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12106/20000], Bound: 0.371651828289032, Entropy: 140.12278747558594, Temp: 2.6940767765045166, KL: 69.77394104003906, Loss: 0.016663696616888046, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12107/20000], Bound: 0.3859160542488098, Entropy: 140.757080078125, Temp: 2.694077968597412, KL: 74.37127685546875, Loss: 0.015763919800519943, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12108/20000], Bound: 0.37017592787742615, Entropy: 143.12667846679688, Temp: 2.6940815448760986, KL: 70.29910278320312, Loss: 0.014908220618963242, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12109/20000], Bound: 0.38967397809028625, Entropy: 141.52674865722656, Temp: 2.6940865516662598, KL: 74.29524230957031, Loss: 0.017942270264029503, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12110/20000], Bound: 0.390401154756546, Entropy: 142.33718872070312, Temp: 2.6940882205963135, KL: 71.07835388183594, Loss: 0.02430805191397667, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12111/20000], Bound: 0.39360862970352173, Entropy: 143.6225128173828, Temp: 2.6940717697143555, KL: 75.46495056152344, Loss: 0.017916297540068626, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12112/20000], Bound: 0.3932653069496155, Entropy: 142.10580444335938, Temp: 2.69405460357666, KL: 75.68013000488281, Loss: 0.0173291377723217, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12113/20000], Bound: 0.405378133058548, Entropy: 140.24777221679688, Temp: 2.6940383911132812, KL: 81.35598754882812, Loss: 0.013473957777023315, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12114/20000], Bound: 0.35691311955451965, Entropy: 141.5242919921875, Temp: 2.694033622741699, KL: 66.03062438964844, Loss: 0.015885746106505394, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12115/20000], Bound: 0.3904304504394531, Entropy: 141.67294311523438, Temp: 2.6940274238586426, KL: 74.80380249023438, Loss: 0.01740935817360878, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12116/20000], Bound: 0.38947033882141113, Entropy: 141.119873046875, Temp: 2.6940205097198486, KL: 73.3800048828125, Loss: 0.01952965371310711, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12117/20000], Bound: 0.4273461699485779, Entropy: 141.89111328125, Temp: 2.694007635116577, KL: 85.8856201171875, Loss: 0.017495570704340935, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12118/20000], Bound: 0.398958683013916, Entropy: 142.4618377685547, Temp: 2.6939992904663086, KL: 76.24124145507812, Loss: 0.01941167376935482, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12119/20000], Bound: 0.3739898204803467, Entropy: 141.41046142578125, Temp: 2.693986654281616, KL: 71.58155822753906, Loss: 0.014548365958034992, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12120/20000], Bound: 0.3692799508571625, Entropy: 141.20382690429688, Temp: 2.6939785480499268, KL: 68.90464782714844, Loss: 0.01702219247817993, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12121/20000], Bound: 0.36701473593711853, Entropy: 142.0154266357422, Temp: 2.6939680576324463, KL: 68.68452453613281, Loss: 0.016236908733844757, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12122/20000], Bound: 0.38112401962280273, Entropy: 142.41416931152344, Temp: 2.6939573287963867, KL: 72.71023559570312, Loss: 0.01626395620405674, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12123/20000], Bound: 0.39546987414360046, Entropy: 141.67970275878906, Temp: 2.6939477920532227, KL: 75.68170166015625, Loss: 0.018531961366534233, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12124/20000], Bound: 0.4006391763687134, Entropy: 141.5314178466797, Temp: 2.6939353942871094, KL: 77.62515258789062, Loss: 0.017769835889339447, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12125/20000], Bound: 0.3707105815410614, Entropy: 142.6197967529297, Temp: 2.6939237117767334, KL: 71.22703552246094, Loss: 0.013467293232679367, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12126/20000], Bound: 0.3708600699901581, Entropy: 140.27249145507812, Temp: 2.6939187049865723, KL: 70.72697448730469, Loss: 0.01447442639619112, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12127/20000], Bound: 0.3672504425048828, Entropy: 142.2123260498047, Temp: 2.6939175128936768, KL: 69.182861328125, Loss: 0.015435614623129368, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12128/20000], Bound: 0.3878695070743561, Entropy: 140.5652313232422, Temp: 2.6939167976379395, KL: 75.57438659667969, Loss: 0.01458704937249422, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12129/20000], Bound: 0.40651968121528625, Entropy: 141.41961669921875, Temp: 2.6939213275909424, KL: 78.8197021484375, Loss: 0.018815793097019196, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12130/20000], Bound: 0.40358439087867737, Entropy: 142.3244171142578, Temp: 2.693922758102417, KL: 79.68606567382812, Loss: 0.015575375407934189, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12131/20000], Bound: 0.3720604181289673, Entropy: 142.42332458496094, Temp: 2.6939287185668945, KL: 69.86882019042969, Loss: 0.016702843829989433, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12132/20000], Bound: 0.38694578409194946, Entropy: 142.2294464111328, Temp: 2.693932056427002, KL: 74.69056701660156, Loss: 0.015727100893855095, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12133/20000], Bound: 0.3810405731201172, Entropy: 142.3076171875, Temp: 2.693937301635742, KL: 73.16493225097656, Loss: 0.015375050716102123, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12134/20000], Bound: 0.37827202677726746, Entropy: 141.87884521484375, Temp: 2.6939444541931152, KL: 70.638671875, Loss: 0.01858062855899334, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12135/20000], Bound: 0.366912305355072, Entropy: 139.40208435058594, Temp: 2.6939449310302734, KL: 69.09236145019531, Loss: 0.015425856225192547, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12136/20000], Bound: 0.3946705758571625, Entropy: 141.84848022460938, Temp: 2.69394588470459, KL: 76.00645446777344, Loss: 0.017491258680820465, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12137/20000], Bound: 0.38917189836502075, Entropy: 141.7788848876953, Temp: 2.6939456462860107, KL: 74.88813781738281, Loss: 0.016567809507250786, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12138/20000], Bound: 0.37830814719200134, Entropy: 142.6489715576172, Temp: 2.69394588470459, KL: 71.41786193847656, Loss: 0.017153769731521606, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12139/20000], Bound: 0.38901829719543457, Entropy: 140.94473266601562, Temp: 2.693943500518799, KL: 74.82795715332031, Loss: 0.016596028581261635, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12140/20000], Bound: 0.36606472730636597, Entropy: 142.06546020507812, Temp: 2.693942070007324, KL: 68.40858459472656, Loss: 0.016249295324087143, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12141/20000], Bound: 0.38726022839546204, Entropy: 142.26866149902344, Temp: 2.693938970565796, KL: 74.21846008300781, Loss: 0.016773685812950134, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12142/20000], Bound: 0.3927314579486847, Entropy: 141.6896514892578, Temp: 2.6939358711242676, KL: 75.84703063964844, Loss: 0.01672666147351265, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12143/20000], Bound: 0.3999471068382263, Entropy: 141.0433349609375, Temp: 2.6939337253570557, KL: 77.17088317871094, Loss: 0.018230749294161797, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12144/20000], Bound: 0.39908599853515625, Entropy: 140.7679443359375, Temp: 2.693929433822632, KL: 77.93974304199219, Loss: 0.016328752040863037, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12145/20000], Bound: 0.40859025716781616, Entropy: 141.76693725585938, Temp: 2.6939280033111572, KL: 79.04521179199219, Loss: 0.019553158432245255, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12146/20000], Bound: 0.40216246247291565, Entropy: 141.76931762695312, Temp: 2.693922758102417, KL: 79.30218505859375, Loss: 0.015499668195843697, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12147/20000], Bound: 0.37558987736701965, Entropy: 142.6263885498047, Temp: 2.693922519683838, KL: 71.73800659179688, Loss: 0.01510867290198803, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12148/20000], Bound: 0.3837783932685852, Entropy: 142.3413543701172, Temp: 2.69392466545105, KL: 73.97860717773438, Loss: 0.015337392687797546, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12149/20000], Bound: 0.393790602684021, Entropy: 141.156005859375, Temp: 2.693929433822632, KL: 75.77081298828125, Loss: 0.0174468494951725, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12150/20000], Bound: 0.4141564965248108, Entropy: 142.32009887695312, Temp: 2.6939327716827393, KL: 83.812744140625, Loss: 0.013829746283590794, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12151/20000], Bound: 0.39436277747154236, Entropy: 141.003173828125, Temp: 2.69394588470459, KL: 76.301513671875, Loss: 0.016775084659457207, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12152/20000], Bound: 0.38795650005340576, Entropy: 140.2791748046875, Temp: 2.693958282470703, KL: 74.9541015625, Loss: 0.015785854309797287, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12153/20000], Bound: 0.40967240929603577, Entropy: 142.30442810058594, Temp: 2.693971872329712, KL: 80.73060607910156, Loss: 0.017030997201800346, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12154/20000], Bound: 0.4005531966686249, Entropy: 142.42547607421875, Temp: 2.693986177444458, KL: 77.24273681640625, Loss: 0.018432559445500374, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12155/20000], Bound: 0.39965012669563293, Entropy: 142.3161163330078, Temp: 2.6939961910247803, KL: 79.18696594238281, Loss: 0.014325680211186409, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12156/20000], Bound: 0.3835383355617523, Entropy: 142.28338623046875, Temp: 2.6940126419067383, KL: 73.3525390625, Loss: 0.016370797529816628, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12157/20000], Bound: 0.3828633725643158, Entropy: 142.29542541503906, Temp: 2.694027900695801, KL: 74.49900817871094, Loss: 0.013879667036235332, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12158/20000], Bound: 0.3891320824623108, Entropy: 142.0039825439453, Temp: 2.694047689437866, KL: 73.932373046875, Loss: 0.018320966511964798, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12159/20000], Bound: 0.39728787541389465, Entropy: 144.20811462402344, Temp: 2.694061756134033, KL: 77.13221740722656, Loss: 0.016838988289237022, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12160/20000], Bound: 0.40349283814430237, Entropy: 140.60174560546875, Temp: 2.694075107574463, KL: 77.65940856933594, Loss: 0.019287463277578354, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12161/20000], Bound: 0.38896337151527405, Entropy: 140.592041015625, Temp: 2.6940829753875732, KL: 75.72975158691406, Loss: 0.014893822371959686, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12162/20000], Bound: 0.39915984869003296, Entropy: 141.04428100585938, Temp: 2.694094657897949, KL: 76.65473937988281, Loss: 0.01875595562160015, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12163/20000], Bound: 0.4087451994419098, Entropy: 141.05136108398438, Temp: 2.694101333618164, KL: 81.34033203125, Loss: 0.015381877310574055, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12164/20000], Bound: 0.3695499002933502, Entropy: 141.70335388183594, Temp: 2.6941134929656982, KL: 67.31124877929688, Loss: 0.020122945308685303, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12165/20000], Bound: 0.36818423867225647, Entropy: 139.89002990722656, Temp: 2.6941134929656982, KL: 67.62396240234375, Loss: 0.018822170794010162, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12166/20000], Bound: 0.37588825821876526, Entropy: 141.76942443847656, Temp: 2.694105625152588, KL: 72.90052795410156, Loss: 0.013111741282045841, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12167/20000], Bound: 0.3900509178638458, Entropy: 141.95433044433594, Temp: 2.694105863571167, KL: 74.97177124023438, Loss: 0.016891831532120705, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12168/20000], Bound: 0.39644694328308105, Entropy: 139.50181579589844, Temp: 2.694105863571167, KL: 75.54373168945312, Loss: 0.01932547055184841, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12169/20000], Bound: 0.3741611838340759, Entropy: 141.04673767089844, Temp: 2.6941006183624268, KL: 70.50505065917969, Loss: 0.016638362780213356, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12170/20000], Bound: 0.4063413441181183, Entropy: 141.24119567871094, Temp: 2.694094181060791, KL: 80.14973449707031, Loss: 0.016249604523181915, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12171/20000], Bound: 0.39912649989128113, Entropy: 142.6907196044922, Temp: 2.694091796875, KL: 77.21435546875, Loss: 0.017698943614959717, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12172/20000], Bound: 0.4201444685459137, Entropy: 141.55955505371094, Temp: 2.6940886974334717, KL: 83.70223999023438, Loss: 0.017428219318389893, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12173/20000], Bound: 0.34929564595222473, Entropy: 141.54483032226562, Temp: 2.6940884590148926, KL: 63.22052001953125, Loss: 0.017171120271086693, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12174/20000], Bound: 0.36960098147392273, Entropy: 142.57583618164062, Temp: 2.694082021713257, KL: 69.38624572753906, Loss: 0.016298668459057808, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12175/20000], Bound: 0.38811635971069336, Entropy: 142.11575317382812, Temp: 2.694075107574463, KL: 73.42236328125, Loss: 0.01871645450592041, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12176/20000], Bound: 0.39269721508026123, Entropy: 138.80323791503906, Temp: 2.694063663482666, KL: 75.62007141113281, Loss: 0.01713036373257637, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12177/20000], Bound: 0.4043176770210266, Entropy: 140.44644165039062, Temp: 2.6940529346466064, KL: 79.83460998535156, Loss: 0.015708135440945625, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12178/20000], Bound: 0.3888368606567383, Entropy: 141.76812744140625, Temp: 2.6940479278564453, KL: 75.94224548339844, Loss: 0.014430458657443523, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12179/20000], Bound: 0.3762611746788025, Entropy: 140.98187255859375, Temp: 2.6940488815307617, KL: 72.1983642578125, Loss: 0.014613118022680283, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12180/20000], Bound: 0.40328875184059143, Entropy: 140.9364776611328, Temp: 2.6940534114837646, KL: 79.11988830566406, Loss: 0.016463514417409897, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12181/20000], Bound: 0.3673790395259857, Entropy: 142.4445343017578, Temp: 2.6940600872039795, KL: 70.1976318359375, Loss: 0.013621127232909203, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12182/20000], Bound: 0.3681877851486206, Entropy: 141.54641723632812, Temp: 2.694071054458618, KL: 68.53883361816406, Loss: 0.0171258095651865, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12183/20000], Bound: 0.3406311571598053, Entropy: 143.2889862060547, Temp: 2.694077253341675, KL: 60.67634582519531, Loss: 0.01747226156294346, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12184/20000], Bound: 0.3918011486530304, Entropy: 140.8630828857422, Temp: 2.694074869155884, KL: 75.08433532714844, Loss: 0.017635799944400787, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12185/20000], Bound: 0.376557856798172, Entropy: 142.4307861328125, Temp: 2.694070816040039, KL: 71.94155883789062, Loss: 0.015248162671923637, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12186/20000], Bound: 0.39286279678344727, Entropy: 140.3158721923828, Temp: 2.6940691471099854, KL: 76.70661926269531, Loss: 0.015204277820885181, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12187/20000], Bound: 0.38113081455230713, Entropy: 140.90631103515625, Temp: 2.6940720081329346, KL: 73.70484924316406, Loss: 0.014422685839235783, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12188/20000], Bound: 0.3830278217792511, Entropy: 140.48255920410156, Temp: 2.6940793991088867, KL: 73.28770446777344, Loss: 0.016216779127717018, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12189/20000], Bound: 0.4016117751598358, Entropy: 140.05844116210938, Temp: 2.6940865516662598, KL: 80.29507446289062, Loss: 0.013353807851672173, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12190/20000], Bound: 0.39694744348526, Entropy: 140.1418914794922, Temp: 2.6941027641296387, KL: 78.38011169433594, Loss: 0.01433627214282751, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12191/20000], Bound: 0.39404529333114624, Entropy: 143.17762756347656, Temp: 2.694124221801758, KL: 77.31776428222656, Loss: 0.014716955833137035, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12192/20000], Bound: 0.3829030990600586, Entropy: 140.43426513671875, Temp: 2.6941492557525635, KL: 72.30661010742188, Loss: 0.017971057444810867, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12193/20000], Bound: 0.41856589913368225, Entropy: 140.18069458007812, Temp: 2.6941678524017334, KL: 81.69406127929688, Loss: 0.02025889791548252, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12194/20000], Bound: 0.38930606842041016, Entropy: 141.39602661132812, Temp: 2.6941802501678467, KL: 74.62391662597656, Loss: 0.017133202403783798, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12195/20000], Bound: 0.3820291757583618, Entropy: 141.9801483154297, Temp: 2.694190263748169, KL: 72.320068359375, Loss: 0.017476310953497887, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12196/20000], Bound: 0.38435935974121094, Entropy: 141.1606903076172, Temp: 2.6941967010498047, KL: 74.59742736816406, Loss: 0.014504663646221161, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12197/20000], Bound: 0.387673020362854, Entropy: 140.57327270507812, Temp: 2.6942074298858643, KL: 76.2398681640625, Loss: 0.013248338364064693, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12198/20000], Bound: 0.38445883989334106, Entropy: 141.69754028320312, Temp: 2.694225549697876, KL: 74.63557434082031, Loss: 0.014487864449620247, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12199/20000], Bound: 0.42610782384872437, Entropy: 142.520751953125, Temp: 2.694246530532837, KL: 84.85052490234375, Loss: 0.018707461655139923, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12200/20000], Bound: 0.35725298523902893, Entropy: 140.36448669433594, Temp: 2.694265842437744, KL: 67.71890258789062, Loss: 0.012930729426443577, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12201/20000], Bound: 0.36926010251045227, Entropy: 143.11659240722656, Temp: 2.6942882537841797, KL: 68.72981262207031, Loss: 0.017338590696454048, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12202/20000], Bound: 0.36136120557785034, Entropy: 142.6724395751953, Temp: 2.6943044662475586, KL: 66.11334228515625, Loss: 0.01804857887327671, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12203/20000], Bound: 0.37809431552886963, Entropy: 142.78939819335938, Temp: 2.694312334060669, KL: 70.63963317871094, Loss: 0.018486695364117622, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12204/20000], Bound: 0.3977055251598358, Entropy: 141.848388671875, Temp: 2.6943140029907227, KL: 77.26544189453125, Loss: 0.016823802143335342, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12205/20000], Bound: 0.3715685307979584, Entropy: 141.27870178222656, Temp: 2.6943161487579346, KL: 69.98336791992188, Loss: 0.016232864931225777, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12206/20000], Bound: 0.40662601590156555, Entropy: 142.22642517089844, Temp: 2.69431734085083, KL: 80.52680969238281, Loss: 0.015710828825831413, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12207/20000], Bound: 0.3785662353038788, Entropy: 139.6512908935547, Temp: 2.6943230628967285, KL: 71.32551574707031, Loss: 0.017466288059949875, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12208/20000], Bound: 0.38731077313423157, Entropy: 140.715087890625, Temp: 2.6943249702453613, KL: 73.01185607910156, Loss: 0.019043708220124245, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12209/20000], Bound: 0.34196698665618896, Entropy: 140.9071807861328, Temp: 2.6943211555480957, KL: 61.88148498535156, Loss: 0.015915337949991226, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12210/20000], Bound: 0.37222784757614136, Entropy: 141.631591796875, Temp: 2.6943135261535645, KL: 69.85969543457031, Loss: 0.016811644658446312, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12211/20000], Bound: 0.3590805232524872, Entropy: 140.95053100585938, Temp: 2.6943044662475586, KL: 66.12867736816406, Loss: 0.016831684857606888, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12212/20000], Bound: 0.38156747817993164, Entropy: 141.5404815673828, Temp: 2.6942923069000244, KL: 72.42520141601562, Loss: 0.017033997923135757, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12213/20000], Bound: 0.38905030488967896, Entropy: 142.15003967285156, Temp: 2.694279432296753, KL: 75.30441284179688, Loss: 0.015732314437627792, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12214/20000], Bound: 0.3724375367164612, Entropy: 140.96925354003906, Temp: 2.694270610809326, KL: 68.93046569824219, Loss: 0.018646905198693275, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12215/20000], Bound: 0.38661620020866394, Entropy: 141.75233459472656, Temp: 2.694255828857422, KL: 74.62440490722656, Loss: 0.01567448116838932, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12216/20000], Bound: 0.37049806118011475, Entropy: 141.0768585205078, Temp: 2.694244623184204, KL: 71.02249145507812, Loss: 0.013737412169575691, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12217/20000], Bound: 0.3817380368709564, Entropy: 142.41921997070312, Temp: 2.694239616394043, KL: 74.19581604003906, Loss: 0.013839272782206535, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12218/20000], Bound: 0.37733587622642517, Entropy: 141.3008270263672, Temp: 2.694241523742676, KL: 72.57217407226562, Loss: 0.01449452992528677, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12219/20000], Bound: 0.35683414340019226, Entropy: 143.08863830566406, Temp: 2.694247007369995, KL: 66.69271850585938, Loss: 0.014617661945521832, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12220/20000], Bound: 0.36734309792518616, Entropy: 142.46630859375, Temp: 2.6942532062530518, KL: 68.13508605957031, Loss: 0.017431598156690598, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12221/20000], Bound: 0.3891896605491638, Entropy: 140.30296325683594, Temp: 2.6942543983459473, KL: 75.28718566894531, Loss: 0.01583973877131939, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12222/20000], Bound: 0.3632572889328003, Entropy: 141.71327209472656, Temp: 2.6942577362060547, KL: 68.11083984375, Loss: 0.01533224992454052, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12223/20000], Bound: 0.3834131062030792, Entropy: 140.20799255371094, Temp: 2.694260835647583, KL: 73.6695556640625, Loss: 0.015717243775725365, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12224/20000], Bound: 0.413957804441452, Entropy: 139.91754150390625, Temp: 2.694265604019165, KL: 81.72598266601562, Loss: 0.01759430207312107, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12225/20000], Bound: 0.3924740254878998, Entropy: 141.2486114501953, Temp: 2.6942708492279053, KL: 76.51248168945312, Loss: 0.015354282222688198, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12226/20000], Bound: 0.4057498872280121, Entropy: 140.26341247558594, Temp: 2.694279432296753, KL: 80.61978149414062, Loss: 0.015049843117594719, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12227/20000], Bound: 0.39347758889198303, Entropy: 141.90550231933594, Temp: 2.69429349899292, KL: 74.52154541015625, Loss: 0.019597399979829788, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12228/20000], Bound: 0.39362794160842896, Entropy: 142.12342834472656, Temp: 2.6942996978759766, KL: 77.67520141601562, Loss: 0.013827215880155563, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12229/20000], Bound: 0.39860230684280396, Entropy: 140.4530792236328, Temp: 2.6943130493164062, KL: 77.61285400390625, Loss: 0.016672668978571892, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12230/20000], Bound: 0.38345885276794434, Entropy: 142.2252960205078, Temp: 2.694326400756836, KL: 73.19297790527344, Loss: 0.016626890748739243, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12231/20000], Bound: 0.3763250708580017, Entropy: 142.04574584960938, Temp: 2.694338083267212, KL: 71.20780944824219, Loss: 0.016488024964928627, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12232/20000], Bound: 0.3811998963356018, Entropy: 141.88427734375, Temp: 2.694347858428955, KL: 71.86094665527344, Loss: 0.017884187400341034, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12233/20000], Bound: 0.38226765394210815, Entropy: 140.9044189453125, Temp: 2.694352626800537, KL: 73.52584838867188, Loss: 0.015368318185210228, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12234/20000], Bound: 0.396786630153656, Entropy: 140.7841033935547, Temp: 2.694359540939331, KL: 76.60722351074219, Loss: 0.017540637403726578, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12235/20000], Bound: 0.4058854281902313, Entropy: 141.880859375, Temp: 2.6943647861480713, KL: 79.81787109375, Loss: 0.01661432534456253, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12236/20000], Bound: 0.35420748591423035, Entropy: 141.40374755859375, Temp: 2.6943717002868652, KL: 65.70687866210938, Loss: 0.015088374726474285, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12237/20000], Bound: 0.383487731218338, Entropy: 141.90219116210938, Temp: 2.694377899169922, KL: 73.92683410644531, Loss: 0.01528108399361372, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12238/20000], Bound: 0.3918989300727844, Entropy: 141.43418884277344, Temp: 2.6943864822387695, KL: 76.35383605957031, Loss: 0.015336084179580212, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12239/20000], Bound: 0.41252538561820984, Entropy: 139.77685546875, Temp: 2.6943976879119873, KL: 80.90692138671875, Loss: 0.018309161067008972, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12240/20000], Bound: 0.4161393940448761, Entropy: 141.79351806640625, Temp: 2.6944074630737305, KL: 83.61235046386719, Loss: 0.015326821245253086, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12241/20000], Bound: 0.36250635981559753, Entropy: 140.9226531982422, Temp: 2.694422960281372, KL: 68.12582397460938, Loss: 0.01491303090006113, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12242/20000], Bound: 0.38310760259628296, Entropy: 140.29571533203125, Temp: 2.6944379806518555, KL: 74.72793579101562, Loss: 0.013590333051979542, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12243/20000], Bound: 0.3853673040866852, Entropy: 140.5711669921875, Temp: 2.694458484649658, KL: 74.9620361328125, Loss: 0.014374674297869205, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12244/20000], Bound: 0.4015914499759674, Entropy: 140.64407348632812, Temp: 2.6944823265075684, KL: 78.97297668457031, Loss: 0.015800323337316513, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12245/20000], Bound: 0.3712362051010132, Entropy: 142.6255645751953, Temp: 2.694507598876953, KL: 69.34306335449219, Loss: 0.017246674746274948, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12246/20000], Bound: 0.37185823917388916, Entropy: 141.10951232910156, Temp: 2.6945269107818604, KL: 70.71562194824219, Loss: 0.015029254369437695, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12247/20000], Bound: 0.37224116921424866, Entropy: 142.21531677246094, Temp: 2.6945462226867676, KL: 69.25335693359375, Loss: 0.0179456640034914, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12248/20000], Bound: 0.38891395926475525, Entropy: 140.5093536376953, Temp: 2.694558620452881, KL: 75.33503723144531, Loss: 0.015604083426296711, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12249/20000], Bound: 0.4070306718349457, Entropy: 141.4530487060547, Temp: 2.6945722103118896, KL: 79.29869079589844, Loss: 0.018218018114566803, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12250/20000], Bound: 0.3859860301017761, Entropy: 142.11395263671875, Temp: 2.6945836544036865, KL: 73.84036254882812, Loss: 0.016791576519608498, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12251/20000], Bound: 0.37958019971847534, Entropy: 141.36170959472656, Temp: 2.6945931911468506, KL: 73.74913024902344, Loss: 0.01351403072476387, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12252/20000], Bound: 0.3888823688030243, Entropy: 139.9059295654297, Temp: 2.694608449935913, KL: 73.80728149414062, Loss: 0.01842224784195423, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12253/20000], Bound: 0.36128318309783936, Entropy: 142.11337280273438, Temp: 2.6946182250976562, KL: 66.76776123046875, Loss: 0.016795765608549118, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12254/20000], Bound: 0.3874041736125946, Entropy: 141.0901336669922, Temp: 2.6946234703063965, KL: 72.6510009765625, Loss: 0.019766271114349365, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12255/20000], Bound: 0.39826419949531555, Entropy: 141.0373992919922, Temp: 2.694620370864868, KL: 78.45597839355469, Loss: 0.014925043098628521, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12256/20000], Bound: 0.3812461793422699, Entropy: 142.77853393554688, Temp: 2.6946232318878174, KL: 73.49333190917969, Loss: 0.014882304705679417, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12257/20000], Bound: 0.3647260069847107, Entropy: 141.5890350341797, Temp: 2.694629430770874, KL: 67.46705627441406, Loss: 0.01729917898774147, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12258/20000], Bound: 0.40268853306770325, Entropy: 139.50181579589844, Temp: 2.6946303844451904, KL: 78.97482299804688, Loss: 0.016405796632170677, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12259/20000], Bound: 0.3746764361858368, Entropy: 141.61863708496094, Temp: 2.694634199142456, KL: 69.53413391113281, Loss: 0.01871829852461815, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12260/20000], Bound: 0.354159951210022, Entropy: 143.7162628173828, Temp: 2.6946303844451904, KL: 65.64463806152344, Loss: 0.015181307680904865, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12261/20000], Bound: 0.34371039271354675, Entropy: 143.098388671875, Temp: 2.694626808166504, KL: 61.29530334472656, Loss: 0.01789180561900139, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12262/20000], Bound: 0.38099637627601624, Entropy: 140.24351501464844, Temp: 2.6946146488189697, KL: 73.95074462890625, Loss: 0.013899387791752815, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12263/20000], Bound: 0.39065930247306824, Entropy: 140.67538452148438, Temp: 2.6946096420288086, KL: 75.40394592285156, Loss: 0.016425494104623795, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12264/20000], Bound: 0.38613802194595337, Entropy: 141.63490295410156, Temp: 2.694606065750122, KL: 73.24125671386719, Loss: 0.0179856326431036, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12265/20000], Bound: 0.39671790599823, Entropy: 142.67630004882812, Temp: 2.6945996284484863, KL: 76.62528991699219, Loss: 0.017471585422754288, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12266/20000], Bound: 0.3840585947036743, Entropy: 141.2584686279297, Temp: 2.6945929527282715, KL: 71.34765625, Loss: 0.0203765407204628, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12267/20000], Bound: 0.3771226406097412, Entropy: 141.35984802246094, Temp: 2.69457745552063, KL: 71.81045532226562, Loss: 0.015797218307852745, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12268/20000], Bound: 0.3797541856765747, Entropy: 142.001708984375, Temp: 2.6945643424987793, KL: 72.70127868652344, Loss: 0.015551300719380379, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12269/20000], Bound: 0.42099177837371826, Entropy: 140.81732177734375, Temp: 2.694554328918457, KL: 84.09849548339844, Loss: 0.01718035340309143, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12270/20000], Bound: 0.3814680278301239, Entropy: 141.77139282226562, Temp: 2.6945483684539795, KL: 72.01554870605469, Loss: 0.017742902040481567, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12271/20000], Bound: 0.369935005903244, Entropy: 140.47503662109375, Temp: 2.6945393085479736, KL: 69.72648620605469, Loss: 0.015847448259592056, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12272/20000], Bound: 0.38121363520622253, Entropy: 141.65530395507812, Temp: 2.694531202316284, KL: 71.73995971679688, Loss: 0.01811755821108818, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12273/20000], Bound: 0.41805168986320496, Entropy: 140.0392608642578, Temp: 2.694519281387329, KL: 83.04827880859375, Loss: 0.017457691952586174, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12274/20000], Bound: 0.3770793378353119, Entropy: 139.99803161621094, Temp: 2.6945109367370605, KL: 70.85185241699219, Loss: 0.017552362754940987, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12275/20000], Bound: 0.42173120379447937, Entropy: 140.34873962402344, Temp: 2.6944997310638428, KL: 84.40948486328125, Loss: 0.01702416129410267, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12276/20000], Bound: 0.38979750871658325, Entropy: 142.98045349121094, Temp: 2.694493293762207, KL: 74.00177001953125, Loss: 0.01855754666030407, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12277/20000], Bound: 0.38270798325538635, Entropy: 140.15882873535156, Temp: 2.6944830417633057, KL: 73.91828918457031, Loss: 0.01487811654806137, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12278/20000], Bound: 0.37948077917099, Entropy: 140.46983337402344, Temp: 2.6944777965545654, KL: 71.61137390136719, Loss: 0.01742655411362648, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12279/20000], Bound: 0.37396475672721863, Entropy: 142.77618408203125, Temp: 2.694469928741455, KL: 70.48745727539062, Loss: 0.016569675877690315, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12280/20000], Bound: 0.3995596766471863, Entropy: 140.66233825683594, Temp: 2.6944615840911865, KL: 79.06863403320312, Loss: 0.014500342309474945, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12281/20000], Bound: 0.36390531063079834, Entropy: 142.20057678222656, Temp: 2.69446063041687, KL: 68.10581970214844, Loss: 0.015682492405176163, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12282/20000], Bound: 0.3788650631904602, Entropy: 143.42164611816406, Temp: 2.6944594383239746, KL: 71.82965087890625, Loss: 0.016691748052835464, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12283/20000], Bound: 0.3705485463142395, Entropy: 141.75335693359375, Temp: 2.6944568157196045, KL: 70.28462219238281, Loss: 0.015135279856622219, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12284/20000], Bound: 0.40961554646492004, Entropy: 142.52822875976562, Temp: 2.6944563388824463, KL: 80.11138916015625, Loss: 0.0181532371789217, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12285/20000], Bound: 0.4009392559528351, Entropy: 141.50009155273438, Temp: 2.6944549083709717, KL: 77.54493713378906, Loss: 0.018089383840560913, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12286/20000], Bound: 0.3689518868923187, Entropy: 140.9449920654297, Temp: 2.6944520473480225, KL: 70.66773986816406, Loss: 0.013581084087491035, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12287/20000], Bound: 0.3917867839336395, Entropy: 140.11610412597656, Temp: 2.6944546699523926, KL: 75.58795166015625, Loss: 0.016696806997060776, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12288/20000], Bound: 0.3796325922012329, Entropy: 141.7740936279297, Temp: 2.694457530975342, KL: 72.484375, Loss: 0.015887700021266937, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12289/20000], Bound: 0.36667004227638245, Entropy: 143.43777465820312, Temp: 2.6944611072540283, KL: 69.36314392089844, Loss: 0.014800259843468666, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12290/20000], Bound: 0.39642155170440674, Entropy: 139.70176696777344, Temp: 2.6944661140441895, KL: 77.36665344238281, Loss: 0.015931924805045128, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12291/20000], Bound: 0.3897106945514679, Entropy: 140.99908447265625, Temp: 2.6944737434387207, KL: 76.65744018554688, Loss: 0.01358216255903244, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12292/20000], Bound: 0.4037676453590393, Entropy: 140.3260955810547, Temp: 2.694488525390625, KL: 79.14599609375, Loss: 0.01668509654700756, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12293/20000], Bound: 0.37109389901161194, Entropy: 141.57191467285156, Temp: 2.6945035457611084, KL: 70.31648254394531, Loss: 0.015365030616521835, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12294/20000], Bound: 0.4032769799232483, Entropy: 140.73460388183594, Temp: 2.694518566131592, KL: 80.21253967285156, Loss: 0.014434099197387695, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12295/20000], Bound: 0.4288786053657532, Entropy: 140.33938598632812, Temp: 2.6945390701293945, KL: 85.5391845703125, Loss: 0.01902705803513527, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12296/20000], Bound: 0.3809646964073181, Entropy: 141.5696563720703, Temp: 2.6945576667785645, KL: 73.60942077636719, Loss: 0.014515210874378681, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12297/20000], Bound: 0.38866254687309265, Entropy: 142.33741760253906, Temp: 2.6945786476135254, KL: 73.30613708496094, Loss: 0.019232604652643204, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12298/20000], Bound: 0.3774072229862213, Entropy: 140.04209899902344, Temp: 2.694591522216797, KL: 72.26004028320312, Loss: 0.01511503104120493, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12299/20000], Bound: 0.3695131242275238, Entropy: 142.4890594482422, Temp: 2.6946053504943848, KL: 69.8712158203125, Loss: 0.01535665150731802, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12300/20000], Bound: 0.37679797410964966, Entropy: 142.9528045654297, Temp: 2.6946189403533936, KL: 72.44781494140625, Loss: 0.014441698789596558, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12301/20000], Bound: 0.39740151166915894, Entropy: 142.02688598632812, Temp: 2.6946351528167725, KL: 77.77639770507812, Loss: 0.015711627900600433, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12302/20000], Bound: 0.38110485672950745, Entropy: 138.97669982910156, Temp: 2.6946535110473633, KL: 72.45616149902344, Loss: 0.016731202602386475, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12303/20000], Bound: 0.4141485393047333, Entropy: 140.7440185546875, Temp: 2.694668769836426, KL: 82.11259460449219, Loss: 0.01698859967291355, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12304/20000], Bound: 0.369554340839386, Entropy: 140.92820739746094, Temp: 2.694685459136963, KL: 70.66253662109375, Loss: 0.013910762034356594, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12305/20000], Bound: 0.39671316742897034, Entropy: 140.93560791015625, Temp: 2.69470477104187, KL: 76.90138244628906, Loss: 0.016957666724920273, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12306/20000], Bound: 0.3672711253166199, Entropy: 141.12486267089844, Temp: 2.694722890853882, KL: 68.95014953613281, Loss: 0.015884948894381523, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12307/20000], Bound: 0.37989628314971924, Entropy: 142.86734008789062, Temp: 2.6947383880615234, KL: 71.48196411132812, Loss: 0.017891384661197662, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12308/20000], Bound: 0.403239369392395, Entropy: 140.7067108154297, Temp: 2.6947484016418457, KL: 77.98199462890625, Loss: 0.018554456532001495, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12309/20000], Bound: 0.3752663731575012, Entropy: 141.8492889404297, Temp: 2.6947548389434814, KL: 72.27595520019531, Loss: 0.013945686630904675, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12310/20000], Bound: 0.36806127429008484, Entropy: 142.7672882080078, Temp: 2.69476580619812, KL: 69.93309020996094, Loss: 0.014477543532848358, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12311/20000], Bound: 0.40265142917633057, Entropy: 141.42652893066406, Temp: 2.6947784423828125, KL: 78.72772216796875, Loss: 0.016845177859067917, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12312/20000], Bound: 0.3876156508922577, Entropy: 141.41363525390625, Temp: 2.694791316986084, KL: 74.26046752929688, Loss: 0.01689593493938446, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12313/20000], Bound: 0.3810542821884155, Entropy: 141.56752014160156, Temp: 2.6948022842407227, KL: 73.02664184570312, Loss: 0.01564686745405197, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12314/20000], Bound: 0.3841935992240906, Entropy: 139.40255737304688, Temp: 2.6948139667510986, KL: 74.26612854003906, Loss: 0.01503594871610403, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12315/20000], Bound: 0.3914559781551361, Entropy: 141.2040557861328, Temp: 2.6948280334472656, KL: 76.34983825683594, Loss: 0.01510639488697052, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12316/20000], Bound: 0.4105115532875061, Entropy: 140.27894592285156, Temp: 2.694844961166382, KL: 79.54875183105469, Loss: 0.019702918827533722, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12317/20000], Bound: 0.38865411281585693, Entropy: 139.5949249267578, Temp: 2.6948556900024414, KL: 76.19720458984375, Loss: 0.013866202905774117, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12318/20000], Bound: 0.39021816849708557, Entropy: 140.25877380371094, Temp: 2.6948726177215576, KL: 76.50947570800781, Loss: 0.014136731624603271, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12319/20000], Bound: 0.3957894742488861, Entropy: 141.92245483398438, Temp: 2.694894313812256, KL: 76.34992980957031, Loss: 0.017475750297307968, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12320/20000], Bound: 0.35146573185920715, Entropy: 142.04617309570312, Temp: 2.694912910461426, KL: 64.33273315429688, Loss: 0.01622822694480419, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12321/20000], Bound: 0.39666748046875, Entropy: 140.99436950683594, Temp: 2.6949260234832764, KL: 77.68644714355469, Loss: 0.015478139743208885, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12322/20000], Bound: 0.40255504846572876, Entropy: 140.09884643554688, Temp: 2.694941997528076, KL: 78.51303100585938, Loss: 0.01719174161553383, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12323/20000], Bound: 0.373973548412323, Entropy: 141.591064453125, Temp: 2.6949567794799805, KL: 71.10084533691406, Loss: 0.015440313145518303, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12324/20000], Bound: 0.3583540618419647, Entropy: 142.0021209716797, Temp: 2.6949715614318848, KL: 67.20561218261719, Loss: 0.014460832811892033, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12325/20000], Bound: 0.3744460940361023, Entropy: 141.0586395263672, Temp: 2.694986343383789, KL: 71.33258056640625, Loss: 0.015261792577803135, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12326/20000], Bound: 0.3746722936630249, Entropy: 139.30421447753906, Temp: 2.6950018405914307, KL: 70.35768127441406, Loss: 0.01719086989760399, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12327/20000], Bound: 0.39605507254600525, Entropy: 140.6299285888672, Temp: 2.6950125694274902, KL: 77.80328369140625, Loss: 0.014926140196621418, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12328/20000], Bound: 0.3624752163887024, Entropy: 140.1379852294922, Temp: 2.6950275897979736, KL: 65.73313903808594, Loss: 0.019340764731168747, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12329/20000], Bound: 0.3735274076461792, Entropy: 141.9853057861328, Temp: 2.69503116607666, KL: 70.13475036621094, Loss: 0.01699643023312092, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12330/20000], Bound: 0.41137123107910156, Entropy: 140.56443786621094, Temp: 2.6950316429138184, KL: 81.25471496582031, Loss: 0.0170219074934721, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12331/20000], Bound: 0.3734716475009918, Entropy: 143.341796875, Temp: 2.6950342655181885, KL: 71.03761291503906, Loss: 0.015291785821318626, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12332/20000], Bound: 0.37027469277381897, Entropy: 141.2162322998047, Temp: 2.695038318634033, KL: 70.05497741699219, Loss: 0.015421559102833271, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12333/20000], Bound: 0.3731396198272705, Entropy: 143.093994140625, Temp: 2.695042610168457, KL: 68.91975402832031, Loss: 0.019044799730181694, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12334/20000], Bound: 0.39833977818489075, Entropy: 141.59422302246094, Temp: 2.6950390338897705, KL: 76.44174194335938, Loss: 0.018707891926169395, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12335/20000], Bound: 0.35068756341934204, Entropy: 141.2884979248047, Temp: 2.6950318813323975, KL: 64.49822998046875, Loss: 0.015521762892603874, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12336/20000], Bound: 0.4145495295524597, Entropy: 141.0260772705078, Temp: 2.695023536682129, KL: 81.86625671386719, Loss: 0.017675528302788734, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12337/20000], Bound: 0.37777119874954224, Entropy: 140.8402862548828, Temp: 2.695017099380493, KL: 71.70826721191406, Loss: 0.016336938366293907, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12338/20000], Bound: 0.4080166518688202, Entropy: 140.46478271484375, Temp: 2.6950109004974365, KL: 79.2078857421875, Loss: 0.018940933048725128, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12339/20000], Bound: 0.3834076523780823, Entropy: 142.49156188964844, Temp: 2.695002555847168, KL: 74.44844055175781, Loss: 0.014276020228862762, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12340/20000], Bound: 0.3823711574077606, Entropy: 140.454345703125, Temp: 2.695000171661377, KL: 72.65858459472656, Loss: 0.017038945108652115, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12341/20000], Bound: 0.3798697292804718, Entropy: 143.86578369140625, Temp: 2.6949965953826904, KL: 72.481201171875, Loss: 0.01602538488805294, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12342/20000], Bound: 0.37642619013786316, Entropy: 141.19569396972656, Temp: 2.6949939727783203, KL: 70.7198486328125, Loss: 0.01745278388261795, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12343/20000], Bound: 0.37292808294296265, Entropy: 139.78378295898438, Temp: 2.694988250732422, KL: 70.11639404296875, Loss: 0.016712050884962082, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12344/20000], Bound: 0.39569053053855896, Entropy: 142.00314331054688, Temp: 2.6949808597564697, KL: 74.96066284179688, Loss: 0.01999979466199875, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12345/20000], Bound: 0.3736136257648468, Entropy: 141.01251220703125, Temp: 2.694967031478882, KL: 69.93873596191406, Loss: 0.017405381426215172, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12346/20000], Bound: 0.360932856798172, Entropy: 141.70614624023438, Temp: 2.694951057434082, KL: 66.69244384765625, Loss: 0.01675521396100521, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12347/20000], Bound: 0.38886210322380066, Entropy: 140.89125061035156, Temp: 2.6949329376220703, KL: 74.49037170410156, Loss: 0.017146632075309753, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12348/20000], Bound: 0.40003347396850586, Entropy: 141.12130737304688, Temp: 2.694915771484375, KL: 75.34782409667969, Loss: 0.021669914945960045, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12349/20000], Bound: 0.3923872411251068, Entropy: 139.11009216308594, Temp: 2.694889545440674, KL: 76.00117492675781, Loss: 0.016261721029877663, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12350/20000], Bound: 0.3480489253997803, Entropy: 141.815185546875, Temp: 2.6948673725128174, KL: 64.98092651367188, Loss: 0.013270866125822067, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12351/20000], Bound: 0.38611337542533875, Entropy: 139.26443481445312, Temp: 2.6948511600494385, KL: 73.121337890625, Loss: 0.018196871504187584, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12352/20000], Bound: 0.37589627504348755, Entropy: 140.6529083251953, Temp: 2.6948323249816895, KL: 71.34440612792969, Loss: 0.01601024903357029, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12353/20000], Bound: 0.39612099528312683, Entropy: 140.91014099121094, Temp: 2.6948156356811523, KL: 78.54588317871094, Loss: 0.013582495972514153, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12354/20000], Bound: 0.39508938789367676, Entropy: 141.42630004882812, Temp: 2.6948091983795166, KL: 75.78128051757812, Loss: 0.018146319314837456, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12355/20000], Bound: 0.34174901247024536, Entropy: 141.7716827392578, Temp: 2.694800853729248, KL: 62.4683837890625, Loss: 0.014718879014253616, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12356/20000], Bound: 0.4097868800163269, Entropy: 142.17588806152344, Temp: 2.6947922706604004, KL: 81.930908203125, Loss: 0.014876464381814003, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12357/20000], Bound: 0.3786463737487793, Entropy: 142.9206085205078, Temp: 2.694791793823242, KL: 72.10551452636719, Loss: 0.016065731644630432, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12358/20000], Bound: 0.3962065577507019, Entropy: 140.49513244628906, Temp: 2.694791555404663, KL: 77.82221984863281, Loss: 0.014971866272389889, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12359/20000], Bound: 0.37856417894363403, Entropy: 141.6195831298828, Temp: 2.694796562194824, KL: 71.0885009765625, Loss: 0.017908815294504166, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12360/20000], Bound: 0.3852688670158386, Entropy: 141.08018493652344, Temp: 2.6947970390319824, KL: 74.48019409179688, Loss: 0.015218825079500675, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12361/20000], Bound: 0.3579559326171875, Entropy: 143.3173828125, Temp: 2.694800615310669, KL: 64.712890625, Loss: 0.018877653405070305, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12362/20000], Bound: 0.3748857080936432, Entropy: 141.2320098876953, Temp: 2.6947944164276123, KL: 71.35185241699219, Loss: 0.015458147041499615, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12363/20000], Bound: 0.39319348335266113, Entropy: 140.4463653564453, Temp: 2.6947901248931885, KL: 76.16612243652344, Loss: 0.016394905745983124, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12364/20000], Bound: 0.3865249752998352, Entropy: 142.8238525390625, Temp: 2.6947877407073975, KL: 75.05262756347656, Loss: 0.01483556441962719, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12365/20000], Bound: 0.3639068007469177, Entropy: 140.96299743652344, Temp: 2.6947898864746094, KL: 67.13642883300781, Loss: 0.01748453639447689, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12366/20000], Bound: 0.3896533250808716, Entropy: 141.5640106201172, Temp: 2.694786787033081, KL: 74.49179077148438, Loss: 0.01757242903113365, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12367/20000], Bound: 0.41839274764060974, Entropy: 139.45098876953125, Temp: 2.694782018661499, KL: 84.00418090820312, Loss: 0.015880344435572624, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12368/20000], Bound: 0.4042750298976898, Entropy: 141.9746551513672, Temp: 2.6947836875915527, KL: 79.39564514160156, Loss: 0.01650654338300228, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12369/20000], Bound: 0.3791486918926239, Entropy: 142.73390197753906, Temp: 2.6947877407073975, KL: 71.55435180664062, Loss: 0.017357101663947105, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12370/20000], Bound: 0.3737582862377167, Entropy: 141.184814453125, Temp: 2.694788694381714, KL: 69.94816589355469, Loss: 0.01746322773396969, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12371/20000], Bound: 0.36295753717422485, Entropy: 141.24234008789062, Temp: 2.6947855949401855, KL: 67.70773315429688, Loss: 0.015927648171782494, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12372/20000], Bound: 0.3992730677127838, Entropy: 140.30967712402344, Temp: 2.694781541824341, KL: 77.90715026855469, Loss: 0.016500739380717278, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12373/20000], Bound: 0.35890015959739685, Entropy: 143.32852172851562, Temp: 2.694779634475708, KL: 67.38194274902344, Loss: 0.014415981248021126, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12374/20000], Bound: 0.3903546929359436, Entropy: 141.29055786132812, Temp: 2.694779872894287, KL: 76.58259582519531, Loss: 0.014074429869651794, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12375/20000], Bound: 0.3608645796775818, Entropy: 141.3346405029297, Temp: 2.69478702545166, KL: 65.45660400390625, Loss: 0.01901141181588173, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12376/20000], Bound: 0.3963148891925812, Entropy: 141.09207153320312, Temp: 2.694783926010132, KL: 77.96836853027344, Loss: 0.01476005557924509, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12377/20000], Bound: 0.393648624420166, Entropy: 141.87440490722656, Temp: 2.69478702545166, KL: 76.748779296875, Loss: 0.015562543645501137, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12378/20000], Bound: 0.39320042729377747, Entropy: 140.8068389892578, Temp: 2.694793224334717, KL: 76.61471557617188, Loss: 0.015566372312605381, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12379/20000], Bound: 0.3699710965156555, Entropy: 143.46066284179688, Temp: 2.6948022842407227, KL: 68.11964416503906, Loss: 0.018850065767765045, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12380/20000], Bound: 0.3860269784927368, Entropy: 142.10728454589844, Temp: 2.694802761077881, KL: 74.63916015625, Loss: 0.015333544462919235, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12381/20000], Bound: 0.4010016620159149, Entropy: 141.9444580078125, Temp: 2.6948063373565674, KL: 77.69010925292969, Loss: 0.017857803031802177, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12382/20000], Bound: 0.39243149757385254, Entropy: 140.49269104003906, Temp: 2.6948084831237793, KL: 74.39585876464844, Loss: 0.019263602793216705, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12383/20000], Bound: 0.39765554666519165, Entropy: 140.4696044921875, Temp: 2.6948044300079346, KL: 76.50732421875, Loss: 0.018207650631666183, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12384/20000], Bound: 0.38247591257095337, Entropy: 138.65284729003906, Temp: 2.694798469543457, KL: 73.38447570800781, Loss: 0.015746695920825005, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12385/20000], Bound: 0.35972821712493896, Entropy: 142.50613403320312, Temp: 2.6947946548461914, KL: 68.28712463378906, Loss: 0.013167518191039562, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12386/20000], Bound: 0.3845314681529999, Entropy: 139.35606384277344, Temp: 2.694796323776245, KL: 72.87979125976562, Loss: 0.017790252342820168, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12387/20000], Bound: 0.38313454389572144, Entropy: 139.4754638671875, Temp: 2.6947948932647705, KL: 72.57499694824219, Loss: 0.017602991312742233, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12388/20000], Bound: 0.38971713185310364, Entropy: 139.8862762451172, Temp: 2.6947903633117676, KL: 75.05961608886719, Loss: 0.016553601250052452, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12389/20000], Bound: 0.392389714717865, Entropy: 142.85452270507812, Temp: 2.69478702545166, KL: 76.72412109375, Loss: 0.014920701272785664, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12390/20000], Bound: 0.39041292667388916, Entropy: 142.4517059326172, Temp: 2.694788694381714, KL: 73.58792114257812, Loss: 0.019662588834762573, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12391/20000], Bound: 0.36240607500076294, Entropy: 140.31887817382812, Temp: 2.6947834491729736, KL: 69.225341796875, Loss: 0.012823464348912239, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12392/20000], Bound: 0.3682061433792114, Entropy: 141.11642456054688, Temp: 2.6947851181030273, KL: 70.05337524414062, Loss: 0.014330852776765823, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12393/20000], Bound: 0.37617436051368713, Entropy: 141.26254272460938, Temp: 2.6947896480560303, KL: 71.82342529296875, Loss: 0.015269284136593342, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12394/20000], Bound: 0.3957836627960205, Entropy: 140.9022674560547, Temp: 2.694795846939087, KL: 75.69343566894531, Loss: 0.018689749762415886, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12395/20000], Bound: 0.38652417063713074, Entropy: 142.34024047851562, Temp: 2.6947977542877197, KL: 74.67576599121094, Loss: 0.01553445216268301, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12396/20000], Bound: 0.38612204790115356, Entropy: 142.6903839111328, Temp: 2.6948020458221436, KL: 74.9442138671875, Loss: 0.014818945899605751, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12397/20000], Bound: 0.38979092240333557, Entropy: 141.48252868652344, Temp: 2.694809913635254, KL: 74.57398986816406, Loss: 0.017494922503829002, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12398/20000], Bound: 0.3619209825992584, Entropy: 143.26441955566406, Temp: 2.6948153972625732, KL: 68.22080993652344, Loss: 0.014434175565838814, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12399/20000], Bound: 0.38861751556396484, Entropy: 140.51846313476562, Temp: 2.6948227882385254, KL: 73.37161254882812, Loss: 0.019088635221123695, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12400/20000], Bound: 0.4167926013469696, Entropy: 142.2604522705078, Temp: 2.6948235034942627, KL: 81.95484924316406, Loss: 0.01877640001475811, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12401/20000], Bound: 0.38963478803634644, Entropy: 140.9938507080078, Temp: 2.6948227882385254, KL: 75.42462158203125, Loss: 0.015831930562853813, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12402/20000], Bound: 0.4141761362552643, Entropy: 141.2303924560547, Temp: 2.694824457168579, KL: 82.19108581542969, Loss: 0.016860205680131912, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12403/20000], Bound: 0.3732447922229767, Entropy: 141.20159912109375, Temp: 2.694828987121582, KL: 69.65864562988281, Loss: 0.01772812008857727, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12404/20000], Bound: 0.42789995670318604, Entropy: 140.21881103515625, Temp: 2.694828510284424, KL: 87.40165710449219, Loss: 0.015010714530944824, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12405/20000], Bound: 0.3999403119087219, Entropy: 142.02281188964844, Temp: 2.6948375701904297, KL: 77.44802856445312, Loss: 0.01772114261984825, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12406/20000], Bound: 0.3882880210876465, Entropy: 141.39137268066406, Temp: 2.6948444843292236, KL: 75.30987548828125, Loss: 0.015313827432692051, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12407/20000], Bound: 0.40477558970451355, Entropy: 140.3738250732422, Temp: 2.694854259490967, KL: 79.68429565429688, Loss: 0.016249820590019226, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12408/20000], Bound: 0.3628929555416107, Entropy: 141.3363800048828, Temp: 2.694866180419922, KL: 68.56829833984375, Loss: 0.01429782621562481, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12409/20000], Bound: 0.3679831326007843, Entropy: 141.61923217773438, Temp: 2.6948797702789307, KL: 70.12626647949219, Loss: 0.014078971929848194, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12410/20000], Bound: 0.3740037679672241, Entropy: 138.49598693847656, Temp: 2.6948957443237305, KL: 71.28802490234375, Loss: 0.015108553692698479, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12411/20000], Bound: 0.36752504110336304, Entropy: 141.1817169189453, Temp: 2.6949121952056885, KL: 68.37332153320312, Loss: 0.01709032617509365, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12412/20000], Bound: 0.41044291853904724, Entropy: 141.7272186279297, Temp: 2.6949234008789062, KL: 80.84407043457031, Loss: 0.017261924222111702, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12413/20000], Bound: 0.37213605642318726, Entropy: 142.02621459960938, Temp: 2.694934844970703, KL: 69.98237609863281, Loss: 0.016540395095944405, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12414/20000], Bound: 0.39925718307495117, Entropy: 142.41171264648438, Temp: 2.694943428039551, KL: 76.37825012207031, Loss: 0.019330203533172607, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12415/20000], Bound: 0.36753296852111816, Entropy: 140.7410888671875, Temp: 2.694946527481079, KL: 69.51431274414062, Loss: 0.014977860264480114, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12416/20000], Bound: 0.38658788800239563, Entropy: 142.4706573486328, Temp: 2.694950580596924, KL: 76.1634521484375, Loss: 0.012810232117772102, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12417/20000], Bound: 0.392679899930954, Entropy: 141.50694274902344, Temp: 2.6949639320373535, KL: 75.44004821777344, Loss: 0.017463184893131256, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12418/20000], Bound: 0.3975786566734314, Entropy: 141.6312255859375, Temp: 2.694974184036255, KL: 77.48526000976562, Loss: 0.01635255105793476, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12419/20000], Bound: 0.3891826868057251, Entropy: 141.31362915039062, Temp: 2.6949856281280518, KL: 75.99308776855469, Loss: 0.01453317143023014, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12420/20000], Bound: 0.39834466576576233, Entropy: 141.21510314941406, Temp: 2.6950016021728516, KL: 78.84945678710938, Loss: 0.014243277721107006, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12421/20000], Bound: 0.36151111125946045, Entropy: 141.47682189941406, Temp: 2.6950230598449707, KL: 66.10403442382812, Loss: 0.018149111419916153, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12422/20000], Bound: 0.3899678885936737, Entropy: 140.29554748535156, Temp: 2.695035457611084, KL: 75.48260498046875, Loss: 0.01590743288397789, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12423/20000], Bound: 0.3724117577075958, Entropy: 140.33294677734375, Temp: 2.6950485706329346, KL: 70.95283508300781, Loss: 0.014886975288391113, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12424/20000], Bound: 0.4059324860572815, Entropy: 141.74908447265625, Temp: 2.6950626373291016, KL: 80.04786682128906, Loss: 0.016220957040786743, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12425/20000], Bound: 0.38165563344955444, Entropy: 141.4746856689453, Temp: 2.6950790882110596, KL: 71.29316711425781, Loss: 0.01918821968138218, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12426/20000], Bound: 0.4271589517593384, Entropy: 139.4396209716797, Temp: 2.69508695602417, KL: 85.28982543945312, Loss: 0.01850546896457672, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12427/20000], Bound: 0.37486281991004944, Entropy: 141.4027099609375, Temp: 2.695094347000122, KL: 69.69271850585938, Loss: 0.01852661743760109, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12428/20000], Bound: 0.3860129714012146, Entropy: 141.22239685058594, Temp: 2.6950950622558594, KL: 72.98748779296875, Loss: 0.01839294284582138, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12429/20000], Bound: 0.35716739296913147, Entropy: 141.2216033935547, Temp: 2.6950912475585938, KL: 66.06999206542969, Loss: 0.015952514484524727, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12430/20000], Bound: 0.4065726101398468, Entropy: 139.8671112060547, Temp: 2.695085287094116, KL: 79.78240966796875, Loss: 0.017070220783352852, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12431/20000], Bound: 0.33558106422424316, Entropy: 142.8204803466797, Temp: 2.6950817108154297, KL: 61.395294189453125, Loss: 0.013592720031738281, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12432/20000], Bound: 0.39361995458602905, Entropy: 142.8475799560547, Temp: 2.6950795650482178, KL: 76.8900146484375, Loss: 0.01528770849108696, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12433/20000], Bound: 0.3986201584339142, Entropy: 141.80296325683594, Temp: 2.6950817108154297, KL: 77.14726257324219, Loss: 0.01755378395318985, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12434/20000], Bound: 0.35181325674057007, Entropy: 142.00111389160156, Temp: 2.695082664489746, KL: 65.65666198730469, Loss: 0.01395215280354023, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12435/20000], Bound: 0.3840160071849823, Entropy: 142.87503051757812, Temp: 2.6950857639312744, KL: 72.20086669921875, Loss: 0.018774351105093956, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12436/20000], Bound: 0.3684701919555664, Entropy: 144.26019287109375, Temp: 2.695082902908325, KL: 69.49574279785156, Loss: 0.015507138334214687, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12437/20000], Bound: 0.39986661076545715, Entropy: 140.55819702148438, Temp: 2.6950807571411133, KL: 79.04478454589844, Loss: 0.014720425009727478, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12438/20000], Bound: 0.3514001965522766, Entropy: 140.91024780273438, Temp: 2.695085287094116, KL: 64.22334289550781, Loss: 0.016398705542087555, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12439/20000], Bound: 0.39442020654678345, Entropy: 142.2091827392578, Temp: 2.695085287094116, KL: 77.62232971191406, Loss: 0.014366833493113518, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12440/20000], Bound: 0.38026559352874756, Entropy: 142.2185516357422, Temp: 2.695091724395752, KL: 72.41134643554688, Loss: 0.016367943957448006, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12441/20000], Bound: 0.3816376030445099, Entropy: 140.74713134765625, Temp: 2.6950972080230713, KL: 73.06051635742188, Loss: 0.015899846330285072, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12442/20000], Bound: 0.39223742485046387, Entropy: 141.9346466064453, Temp: 2.695103406906128, KL: 75.16474914550781, Loss: 0.0177337396889925, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12443/20000], Bound: 0.37015387415885925, Entropy: 143.741943359375, Temp: 2.6951069831848145, KL: 69.13209533691406, Loss: 0.017070474103093147, Learning Rate: 4.885240793731342e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12444/20000], Bound: 0.3956097662448883, Entropy: 140.7279510498047, Temp: 2.6951067447662354, KL: 76.96165466308594, Loss: 0.016244282945990562, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12445/20000], Bound: 0.3881779909133911, Entropy: 141.65054321289062, Temp: 2.695108652114868, KL: 73.35142517089844, Loss: 0.018890049308538437, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12446/20000], Bound: 0.3969784379005432, Entropy: 140.27325439453125, Temp: 2.6951050758361816, KL: 76.30999755859375, Loss: 0.01820431463420391, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12447/20000], Bound: 0.3944879174232483, Entropy: 139.99668884277344, Temp: 2.695099115371704, KL: 77.80145263671875, Loss: 0.01407170481979847, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12448/20000], Bound: 0.3695633113384247, Entropy: 142.12088012695312, Temp: 2.695101261138916, KL: 69.29820251464844, Loss: 0.01645039953291416, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12449/20000], Bound: 0.40753763914108276, Entropy: 141.63442993164062, Temp: 2.695101261138916, KL: 79.52066040039062, Loss: 0.018094155937433243, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12450/20000], Bound: 0.37621670961380005, Entropy: 140.65367126464844, Temp: 2.6951003074645996, KL: 71.1292724609375, Loss: 0.016582421958446503, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12451/20000], Bound: 0.38716787099838257, Entropy: 140.89439392089844, Temp: 2.6950979232788086, KL: 73.44332885742188, Loss: 0.018172111362218857, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12452/20000], Bound: 0.3839956223964691, Entropy: 140.07131958007812, Temp: 2.69509220123291, KL: 72.79379272460938, Loss: 0.01766335964202881, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12453/20000], Bound: 0.3504260182380676, Entropy: 141.75714111328125, Temp: 2.6950843334198, KL: 63.64598083496094, Loss: 0.016968851909041405, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12454/20000], Bound: 0.3676212728023529, Entropy: 139.96897888183594, Temp: 2.695071220397949, KL: 69.72550964355469, Loss: 0.014633608981966972, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12455/20000], Bound: 0.39956429600715637, Entropy: 141.26670837402344, Temp: 2.6950621604919434, KL: 77.85885620117188, Loss: 0.016753675416111946, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12456/20000], Bound: 0.3924458920955658, Entropy: 140.9853515625, Temp: 2.6950552463531494, KL: 75.18698120117188, Loss: 0.017805781215429306, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12457/20000], Bound: 0.3728736340999603, Entropy: 143.34751892089844, Temp: 2.695046901702881, KL: 68.82435607910156, Loss: 0.019080696627497673, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12458/20000], Bound: 0.3878134489059448, Entropy: 139.8571014404297, Temp: 2.6950314044952393, KL: 76.02470397949219, Loss: 0.013732140883803368, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12459/20000], Bound: 0.39785099029541016, Entropy: 141.6626434326172, Temp: 2.6950247287750244, KL: 76.50741577148438, Loss: 0.01831698790192604, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12460/20000], Bound: 0.39541900157928467, Entropy: 140.39450073242188, Temp: 2.6950161457061768, KL: 76.97174072265625, Loss: 0.016120119020342827, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12461/20000], Bound: 0.3769114911556244, Entropy: 142.8662109375, Temp: 2.6950104236602783, KL: 71.85905456542969, Loss: 0.015598167665302753, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12462/20000], Bound: 0.3767211437225342, Entropy: 142.17153930664062, Temp: 2.695006847381592, KL: 71.50480651855469, Loss: 0.016153864562511444, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12463/20000], Bound: 0.3907991051673889, Entropy: 139.2176971435547, Temp: 2.695003032684326, KL: 74.12147521972656, Loss: 0.018884576857089996, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12464/20000], Bound: 0.3773834705352783, Entropy: 141.1383819580078, Temp: 2.6949949264526367, KL: 71.22872924804688, Loss: 0.017019372433423996, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12465/20000], Bound: 0.376528263092041, Entropy: 141.10806274414062, Temp: 2.6949853897094727, KL: 71.43540954589844, Loss: 0.016179535537958145, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12466/20000], Bound: 0.3766355812549591, Entropy: 142.01856994628906, Temp: 2.694976568222046, KL: 71.62619018554688, Loss: 0.015882758423686028, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12467/20000], Bound: 0.3729211091995239, Entropy: 142.48252868652344, Temp: 2.6949689388275146, KL: 70.95071411132812, Loss: 0.015160324051976204, Learning Rate: 4.885240793731342e-05\n",
      "Epoch [12468/20000], Bound: 0.3904528021812439, Entropy: 141.65565490722656, Temp: 2.6949656009674072, KL: 76.15019226074219, Loss: 0.014931905083358288, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12469/20000], Bound: 0.3934445381164551, Entropy: 142.62384033203125, Temp: 2.6949658393859863, KL: 74.97740173339844, Loss: 0.018739184364676476, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12470/20000], Bound: 0.380691796541214, Entropy: 140.20555114746094, Temp: 2.694962978363037, KL: 72.107177734375, Loss: 0.017159704118967056, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12471/20000], Bound: 0.4156654179096222, Entropy: 141.8431854248047, Temp: 2.6949589252471924, KL: 81.96284484863281, Loss: 0.01812555082142353, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12472/20000], Bound: 0.3918724060058594, Entropy: 142.22561645507812, Temp: 2.694955348968506, KL: 75.717041015625, Loss: 0.016508668661117554, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12473/20000], Bound: 0.3910851776599884, Entropy: 141.43589782714844, Temp: 2.6949527263641357, KL: 74.99427795410156, Loss: 0.017420606687664986, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12474/20000], Bound: 0.38407403230667114, Entropy: 139.87940979003906, Temp: 2.6949493885040283, KL: 74.40164184570312, Loss: 0.014721386134624481, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12475/20000], Bound: 0.377581387758255, Entropy: 140.73915100097656, Temp: 2.6949493885040283, KL: 71.86140441894531, Loss: 0.015950867906212807, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12476/20000], Bound: 0.38385576009750366, Entropy: 141.19448852539062, Temp: 2.6949498653411865, KL: 73.59683227539062, Loss: 0.01609690859913826, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12477/20000], Bound: 0.3581104874610901, Entropy: 142.3757781982422, Temp: 2.694950580596924, KL: 66.19070434570312, Loss: 0.016217080876231194, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12478/20000], Bound: 0.3908829391002655, Entropy: 144.43162536621094, Temp: 2.6949496269226074, KL: 73.48643493652344, Loss: 0.020107977092266083, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12479/20000], Bound: 0.3970181345939636, Entropy: 140.33407592773438, Temp: 2.6949429512023926, KL: 77.32733154296875, Loss: 0.016337169334292412, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12480/20000], Bound: 0.40368837118148804, Entropy: 141.39974975585938, Temp: 2.6949384212493896, KL: 77.97067260742188, Loss: 0.018826253712177277, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12481/20000], Bound: 0.3873152732849121, Entropy: 140.908935546875, Temp: 2.694932222366333, KL: 75.11752319335938, Loss: 0.015144336968660355, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12482/20000], Bound: 0.3913489580154419, Entropy: 141.6874237060547, Temp: 2.6949291229248047, KL: 75.45372009277344, Loss: 0.016711652278900146, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12483/20000], Bound: 0.3942468464374542, Entropy: 141.38137817382812, Temp: 2.6949267387390137, KL: 77.21443176269531, Loss: 0.015027111396193504, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12484/20000], Bound: 0.4092005789279938, Entropy: 141.3789520263672, Temp: 2.69492769241333, KL: 80.87069702148438, Loss: 0.016516871750354767, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12485/20000], Bound: 0.3620370030403137, Entropy: 142.7395477294922, Temp: 2.6949310302734375, KL: 66.78807067871094, Loss: 0.01715392991900444, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12486/20000], Bound: 0.36191657185554504, Entropy: 143.12440490722656, Temp: 2.6949310302734375, KL: 66.63789367675781, Loss: 0.017369670793414116, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12487/20000], Bound: 0.3371005952358246, Entropy: 142.79234313964844, Temp: 2.694927215576172, KL: 61.37249755859375, Loss: 0.014399873092770576, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12488/20000], Bound: 0.381610244512558, Entropy: 142.38951110839844, Temp: 2.694923162460327, KL: 73.43278503417969, Loss: 0.015192922204732895, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12489/20000], Bound: 0.38517022132873535, Entropy: 142.20155334472656, Temp: 2.6949214935302734, KL: 73.66142272949219, Loss: 0.016685819253325462, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12490/20000], Bound: 0.410116970539093, Entropy: 141.1764678955078, Temp: 2.6949195861816406, KL: 81.68055725097656, Loss: 0.015527267009019852, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12491/20000], Bound: 0.35747602581977844, Entropy: 141.3112030029297, Temp: 2.6949222087860107, KL: 65.21315002441406, Loss: 0.017701169475913048, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12492/20000], Bound: 0.38312262296676636, Entropy: 141.4140167236328, Temp: 2.6949195861816406, KL: 73.15367126464844, Loss: 0.016523990780115128, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12493/20000], Bound: 0.3957287073135376, Entropy: 140.10569763183594, Temp: 2.694917678833008, KL: 76.83456420898438, Loss: 0.016543464735150337, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12494/20000], Bound: 0.3712712526321411, Entropy: 140.51077270507812, Temp: 2.6949164867401123, KL: 70.770263671875, Loss: 0.014620471745729446, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12495/20000], Bound: 0.3721582591533661, Entropy: 139.72154235839844, Temp: 2.694917678833008, KL: 70.85224914550781, Loss: 0.014938103966414928, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12496/20000], Bound: 0.39749619364738464, Entropy: 141.24798583984375, Temp: 2.694920301437378, KL: 78.00131225585938, Loss: 0.015349256806075573, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12497/20000], Bound: 0.3938894271850586, Entropy: 140.49063110351562, Temp: 2.6949257850646973, KL: 76.17654418945312, Loss: 0.016757238656282425, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12498/20000], Bound: 0.40945425629615784, Entropy: 142.6101531982422, Temp: 2.6949310302734375, KL: 80.44068908691406, Loss: 0.017456652596592903, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12499/20000], Bound: 0.3999507427215576, Entropy: 141.59628295898438, Temp: 2.694936752319336, KL: 78.43817138671875, Loss: 0.01589079387485981, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12500/20000], Bound: 0.3768090307712555, Entropy: 141.95364379882812, Temp: 2.694944143295288, KL: 72.74540710449219, Loss: 0.01389848068356514, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12501/20000], Bound: 0.38246676325798035, Entropy: 141.78585815429688, Temp: 2.6949548721313477, KL: 71.19062805175781, Loss: 0.01981351524591446, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12502/20000], Bound: 0.36730775237083435, Entropy: 140.29376220703125, Temp: 2.694958448410034, KL: 69.32359313964844, Loss: 0.015213299542665482, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12503/20000], Bound: 0.3957294225692749, Entropy: 138.17575073242188, Temp: 2.6949622631073, KL: 77.58543395996094, Loss: 0.015151191502809525, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12504/20000], Bound: 0.4073932468891144, Entropy: 142.5281982421875, Temp: 2.6949691772460938, KL: 82.95980834960938, Loss: 0.011631579138338566, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12505/20000], Bound: 0.3830494284629822, Entropy: 142.54965209960938, Temp: 2.6949856281280518, KL: 72.81108093261719, Loss: 0.01712077111005783, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12506/20000], Bound: 0.3647069036960602, Entropy: 140.91241455078125, Temp: 2.6949992179870605, KL: 68.75444030761719, Loss: 0.014903426170349121, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12507/20000], Bound: 0.3588354289531708, Entropy: 140.48646545410156, Temp: 2.6950125694274902, KL: 67.14634704589844, Loss: 0.014821353368461132, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12508/20000], Bound: 0.3685281574726105, Entropy: 140.9356231689453, Temp: 2.6950254440307617, KL: 69.0029296875, Loss: 0.016451505944132805, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12509/20000], Bound: 0.40274152159690857, Entropy: 140.79933166503906, Temp: 2.695035457611084, KL: 78.75152587890625, Loss: 0.016853515058755875, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12510/20000], Bound: 0.38888540863990784, Entropy: 143.11549377441406, Temp: 2.6950454711914062, KL: 75.47401428222656, Loss: 0.015335376374423504, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12511/20000], Bound: 0.3764649033546448, Entropy: 139.72247314453125, Temp: 2.695056915283203, KL: 71.60017395019531, Loss: 0.015840699896216393, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12512/20000], Bound: 0.41101589798927307, Entropy: 141.2084197998047, Temp: 2.6950676441192627, KL: 82.04336547851562, Loss: 0.015359730459749699, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12513/20000], Bound: 0.34650376439094543, Entropy: 142.5321502685547, Temp: 2.6950817108154297, KL: 62.32421875, Loss: 0.017410771921277046, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12514/20000], Bound: 0.38432565331459045, Entropy: 141.98959350585938, Temp: 2.695089101791382, KL: 73.1068115234375, Loss: 0.017260553315281868, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12515/20000], Bound: 0.3659649193286896, Entropy: 142.421630859375, Temp: 2.695094347000122, KL: 68.14535522460938, Loss: 0.01669434830546379, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12516/20000], Bound: 0.40320494771003723, Entropy: 141.9457550048828, Temp: 2.6950974464416504, KL: 80.33662414550781, Loss: 0.014170238748192787, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12517/20000], Bound: 0.3737071454524994, Entropy: 141.35533142089844, Temp: 2.69510555267334, KL: 69.81935119628906, Loss: 0.017677592113614082, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12518/20000], Bound: 0.3835442066192627, Entropy: 141.999755859375, Temp: 2.6951100826263428, KL: 72.93367004394531, Loss: 0.0171608068048954, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12519/20000], Bound: 0.36430636048316956, Entropy: 140.1654510498047, Temp: 2.695112705230713, KL: 67.98594665527344, Loss: 0.016120178624987602, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12520/20000], Bound: 0.392177551984787, Entropy: 140.16534423828125, Temp: 2.6951141357421875, KL: 74.73515319824219, Loss: 0.018498122692108154, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12521/20000], Bound: 0.4083940386772156, Entropy: 140.19493103027344, Temp: 2.695112466812134, KL: 81.1044921875, Loss: 0.015634141862392426, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12522/20000], Bound: 0.38501811027526855, Entropy: 142.6908416748047, Temp: 2.695114850997925, KL: 74.77688598632812, Loss: 0.01453603245317936, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12523/20000], Bound: 0.3807069659233093, Entropy: 139.76800537109375, Temp: 2.695120334625244, KL: 72.27000427246094, Loss: 0.016867125406861305, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12524/20000], Bound: 0.3656176030635834, Entropy: 142.33824157714844, Temp: 2.695124387741089, KL: 68.96099853515625, Loss: 0.014999017119407654, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12525/20000], Bound: 0.40518757700920105, Entropy: 140.93113708496094, Temp: 2.6951286792755127, KL: 76.8963623046875, Loss: 0.021653851494193077, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12526/20000], Bound: 0.37834030389785767, Entropy: 141.73814392089844, Temp: 2.6951258182525635, KL: 70.01666259765625, Loss: 0.019780242815613747, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12527/20000], Bound: 0.38259148597717285, Entropy: 141.06472778320312, Temp: 2.6951169967651367, KL: 73.63041687011719, Loss: 0.015355457551777363, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12528/20000], Bound: 0.3639511466026306, Entropy: 140.7199249267578, Temp: 2.69511079788208, KL: 68.81881713867188, Loss: 0.014388898387551308, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12529/20000], Bound: 0.39590200781822205, Entropy: 139.5210418701172, Temp: 2.6951069831848145, KL: 77.88670349121094, Loss: 0.014688395895063877, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12530/20000], Bound: 0.3830530345439911, Entropy: 142.29930114746094, Temp: 2.6951076984405518, KL: 71.73692321777344, Loss: 0.019116565585136414, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12531/20000], Bound: 0.37461724877357483, Entropy: 143.08506774902344, Temp: 2.695103645324707, KL: 71.19966125488281, Loss: 0.015600408427417278, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12532/20000], Bound: 0.3912661373615265, Entropy: 141.10899353027344, Temp: 2.695100784301758, KL: 77.01631164550781, Loss: 0.013769214041531086, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12533/20000], Bound: 0.3632940948009491, Entropy: 141.4774627685547, Temp: 2.695103406906128, KL: 68.61686706542969, Loss: 0.01441955752670765, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12534/20000], Bound: 0.3913238048553467, Entropy: 141.7509307861328, Temp: 2.6951076984405518, KL: 75.83053588867188, Loss: 0.016000520437955856, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12535/20000], Bound: 0.39903074502944946, Entropy: 142.87486267089844, Temp: 2.695112705230713, KL: 77.40753173828125, Loss: 0.017297355458140373, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12536/20000], Bound: 0.4179154634475708, Entropy: 142.4142608642578, Temp: 2.695117473602295, KL: 81.84075927734375, Loss: 0.01962701603770256, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12537/20000], Bound: 0.3730331361293793, Entropy: 142.64120483398438, Temp: 2.6951193809509277, KL: 69.45602416992188, Loss: 0.0179939903318882, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12538/20000], Bound: 0.3797217607498169, Entropy: 142.0546417236328, Temp: 2.695117473602295, KL: 72.21852111816406, Loss: 0.016434509307146072, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12539/20000], Bound: 0.39656195044517517, Entropy: 141.33949279785156, Temp: 2.695115566253662, KL: 76.82041931152344, Loss: 0.017028773203492165, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12540/20000], Bound: 0.38185030221939087, Entropy: 141.24961853027344, Temp: 2.6951136589050293, KL: 70.86567687988281, Loss: 0.020086189731955528, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12541/20000], Bound: 0.40443193912506104, Entropy: 143.81822204589844, Temp: 2.69510555267334, KL: 79.37899780273438, Loss: 0.016627851873636246, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12542/20000], Bound: 0.37508177757263184, Entropy: 141.14500427246094, Temp: 2.6951000690460205, KL: 71.95756530761719, Loss: 0.014441367238759995, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12543/20000], Bound: 0.3833394944667816, Entropy: 142.98858642578125, Temp: 2.6950979232788086, KL: 73.80020141601562, Loss: 0.015442848205566406, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12544/20000], Bound: 0.3769949674606323, Entropy: 141.4962921142578, Temp: 2.6950974464416504, KL: 72.36416625976562, Loss: 0.014706392772495747, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12545/20000], Bound: 0.3915989398956299, Entropy: 142.3681640625, Temp: 2.6950995922088623, KL: 75.6341552734375, Loss: 0.016514725983142853, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12546/20000], Bound: 0.3736591041088104, Entropy: 140.473388671875, Temp: 2.6951019763946533, KL: 71.24723815917969, Loss: 0.015002996660768986, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12547/20000], Bound: 0.39702531695365906, Entropy: 140.8310546875, Temp: 2.695106029510498, KL: 78.012451171875, Loss: 0.015071677044034004, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12548/20000], Bound: 0.39216217398643494, Entropy: 140.68260192871094, Temp: 2.695113182067871, KL: 76.59097290039062, Loss: 0.015046835877001286, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12549/20000], Bound: 0.3687324821949005, Entropy: 141.01480102539062, Temp: 2.695122718811035, KL: 70.0927734375, Loss: 0.014538105577230453, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12550/20000], Bound: 0.3839956223964691, Entropy: 141.11114501953125, Temp: 2.6951334476470947, KL: 72.24366760253906, Loss: 0.018684327602386475, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12551/20000], Bound: 0.39183908700942993, Entropy: 140.83274841308594, Temp: 2.695139169692993, KL: 76.51918029785156, Loss: 0.015004093758761883, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12552/20000], Bound: 0.38797128200531006, Entropy: 141.76097106933594, Temp: 2.695147752761841, KL: 72.26507568359375, Loss: 0.020793667063117027, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12553/20000], Bound: 0.3695090413093567, Entropy: 141.84056091308594, Temp: 2.69514799118042, KL: 71.10377502441406, Loss: 0.013072412461042404, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12554/20000], Bound: 0.37747853994369507, Entropy: 141.3371124267578, Temp: 2.69515323638916, KL: 71.29464721679688, Loss: 0.016949135810136795, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12555/20000], Bound: 0.39725202322006226, Entropy: 140.80821228027344, Temp: 2.6951563358306885, KL: 76.3209228515625, Loss: 0.01833484321832657, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12556/20000], Bound: 0.3696701228618622, Entropy: 140.8874053955078, Temp: 2.695157289505005, KL: 70.81661987304688, Loss: 0.013690274208784103, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12557/20000], Bound: 0.3985813558101654, Entropy: 140.02012634277344, Temp: 2.6951613426208496, KL: 78.67506408691406, Loss: 0.014698808081448078, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12558/20000], Bound: 0.4002237319946289, Entropy: 142.5013885498047, Temp: 2.695169687271118, KL: 77.96977233886719, Loss: 0.016912739723920822, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12559/20000], Bound: 0.386079877614975, Entropy: 142.0468292236328, Temp: 2.6951775550842285, KL: 74.32847595214844, Loss: 0.015942061319947243, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12560/20000], Bound: 0.39706215262413025, Entropy: 142.38735961914062, Temp: 2.695186138153076, KL: 75.84814453125, Loss: 0.01910785771906376, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12561/20000], Bound: 0.4044523537158966, Entropy: 140.69232177734375, Temp: 2.695190191268921, KL: 79.3343505859375, Loss: 0.016722869127988815, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12562/20000], Bound: 0.3497456908226013, Entropy: 143.09934997558594, Temp: 2.6951956748962402, KL: 64.68638610839844, Loss: 0.01469002291560173, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12563/20000], Bound: 0.38487759232521057, Entropy: 140.13827514648438, Temp: 2.6952004432678223, KL: 74.60749816894531, Loss: 0.014775259420275688, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12564/20000], Bound: 0.4043068587779999, Entropy: 139.48460388183594, Temp: 2.6952075958251953, KL: 79.41426086425781, Loss: 0.016494020819664, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12565/20000], Bound: 0.3845697045326233, Entropy: 140.62733459472656, Temp: 2.695216178894043, KL: 74.247802734375, Loss: 0.015276594087481499, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12566/20000], Bound: 0.36551040410995483, Entropy: 143.38560485839844, Temp: 2.695225715637207, KL: 68.33512878417969, Loss: 0.016104692593216896, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12567/20000], Bound: 0.3924391269683838, Entropy: 143.25979614257812, Temp: 2.6952333450317383, KL: 75.98051452636719, Loss: 0.016331570222973824, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12568/20000], Bound: 0.3725438117980957, Entropy: 142.36083984375, Temp: 2.6952412128448486, KL: 70.53286743164062, Loss: 0.015737732872366905, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12569/20000], Bound: 0.3758082985877991, Entropy: 141.87652587890625, Temp: 2.695248603820801, KL: 69.58189392089844, Loss: 0.019236620515584946, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12570/20000], Bound: 0.43093061447143555, Entropy: 140.21640014648438, Temp: 2.695249557495117, KL: 87.1534423828125, Loss: 0.017225105315446854, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12571/20000], Bound: 0.3897334039211273, Entropy: 141.94711303710938, Temp: 2.695253610610962, KL: 74.75248718261719, Loss: 0.017136462032794952, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12572/20000], Bound: 0.3832414448261261, Entropy: 142.55421447753906, Temp: 2.695256471633911, KL: 73.13621520996094, Loss: 0.016623305156826973, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12573/20000], Bound: 0.3695816099643707, Entropy: 140.19314575195312, Temp: 2.695258855819702, KL: 69.25540161132812, Loss: 0.01654067449271679, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12574/20000], Bound: 0.3709199130535126, Entropy: 141.001708984375, Temp: 2.6952595710754395, KL: 69.42247009277344, Loss: 0.016937941312789917, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12575/20000], Bound: 0.3903025686740875, Entropy: 141.93109130859375, Temp: 2.6952579021453857, KL: 74.95675659179688, Loss: 0.01706703007221222, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12576/20000], Bound: 0.40530139207839966, Entropy: 140.26144409179688, Temp: 2.695255994796753, KL: 79.96946716308594, Loss: 0.01601722277700901, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12577/20000], Bound: 0.36626991629600525, Entropy: 141.88377380371094, Temp: 2.6952571868896484, KL: 68.05813598632812, Loss: 0.017017638310790062, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12578/20000], Bound: 0.39409714937210083, Entropy: 140.38694763183594, Temp: 2.695255756378174, KL: 75.71133422851562, Loss: 0.01773693971335888, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12579/20000], Bound: 0.3965882360935211, Entropy: 140.399658203125, Temp: 2.6952528953552246, KL: 75.55250549316406, Loss: 0.019396619871258736, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12580/20000], Bound: 0.37589967250823975, Entropy: 141.69888305664062, Temp: 2.695246934890747, KL: 69.34584045410156, Loss: 0.019723212346434593, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12581/20000], Bound: 0.38652899861335754, Entropy: 142.4901885986328, Temp: 2.695234775543213, KL: 75.52326965332031, Loss: 0.013968941755592823, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12582/20000], Bound: 0.37398242950439453, Entropy: 142.6040802001953, Temp: 2.6952285766601562, KL: 70.96922302246094, Loss: 0.015691595152020454, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12583/20000], Bound: 0.40993183851242065, Entropy: 141.615966796875, Temp: 2.695223093032837, KL: 80.9619140625, Loss: 0.01676005870103836, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12584/20000], Bound: 0.40527427196502686, Entropy: 141.3248748779297, Temp: 2.6952202320098877, KL: 79.09248352050781, Loss: 0.017628703266382217, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12585/20000], Bound: 0.40348273515701294, Entropy: 141.7276153564453, Temp: 2.6952176094055176, KL: 78.5498046875, Loss: 0.017640409991145134, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12586/20000], Bound: 0.38859036564826965, Entropy: 140.27764892578125, Temp: 2.6952149868011475, KL: 73.62115478515625, Loss: 0.018614210188388824, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12587/20000], Bound: 0.39700841903686523, Entropy: 141.11996459960938, Temp: 2.695209503173828, KL: 78.45042419433594, Loss: 0.014250943437218666, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12588/20000], Bound: 0.3986750543117523, Entropy: 141.77513122558594, Temp: 2.695209503173828, KL: 77.31060791015625, Loss: 0.017282145097851753, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12589/20000], Bound: 0.37556084990501404, Entropy: 141.2511749267578, Temp: 2.695209503173828, KL: 71.76411437988281, Loss: 0.01505624782294035, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12590/20000], Bound: 0.3788318932056427, Entropy: 142.24485778808594, Temp: 2.6952109336853027, KL: 71.5556640625, Loss: 0.017188675701618195, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12591/20000], Bound: 0.4085136651992798, Entropy: 139.85818481445312, Temp: 2.6952106952667236, KL: 80.78399658203125, Loss: 0.016296591609716415, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12592/20000], Bound: 0.3553815484046936, Entropy: 140.47999572753906, Temp: 2.6952128410339355, KL: 66.66754150390625, Loss: 0.013919678516685963, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12593/20000], Bound: 0.38467493653297424, Entropy: 140.89822387695312, Temp: 2.6952171325683594, KL: 75.80239868164062, Loss: 0.012449360452592373, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12594/20000], Bound: 0.3874238431453705, Entropy: 141.640380859375, Temp: 2.69522762298584, KL: 72.80528259277344, Loss: 0.01949544996023178, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12595/20000], Bound: 0.3697811961174011, Entropy: 141.52137756347656, Temp: 2.695232391357422, KL: 69.56634521484375, Loss: 0.016069015488028526, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12596/20000], Bound: 0.3695193827152252, Entropy: 141.17196655273438, Temp: 2.6952359676361084, KL: 71.10903930664062, Loss: 0.013068925589323044, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12597/20000], Bound: 0.40855035185813904, Entropy: 140.47763061523438, Temp: 2.6952435970306396, KL: 81.50857543945312, Loss: 0.014973249286413193, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12598/20000], Bound: 0.3555203378200531, Entropy: 144.0929412841797, Temp: 2.6952555179595947, KL: 66.41787719726562, Loss: 0.014454974792897701, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12599/20000], Bound: 0.4181053340435028, Entropy: 140.29403686523438, Temp: 2.6952669620513916, KL: 83.76359558105469, Loss: 0.016169099137187004, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12600/20000], Bound: 0.378911554813385, Entropy: 141.75209045410156, Temp: 2.6952810287475586, KL: 72.68817138671875, Loss: 0.015130959451198578, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12601/20000], Bound: 0.39685431122779846, Entropy: 142.38555908203125, Temp: 2.695295572280884, KL: 76.263427734375, Loss: 0.01822427473962307, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12602/20000], Bound: 0.4049738645553589, Entropy: 140.2100372314453, Temp: 2.6953067779541016, KL: 79.93592834472656, Loss: 0.01589784026145935, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12603/20000], Bound: 0.39459046721458435, Entropy: 140.29925537109375, Temp: 2.695319890975952, KL: 78.1324462890625, Loss: 0.013516087085008621, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12604/20000], Bound: 0.3840072453022003, Entropy: 140.26565551757812, Temp: 2.6953377723693848, KL: 75.61834716796875, Loss: 0.012432009913027287, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12605/20000], Bound: 0.41809237003326416, Entropy: 139.6632843017578, Temp: 2.6953606605529785, KL: 83.5711669921875, Loss: 0.016519766300916672, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12606/20000], Bound: 0.3647920489311218, Entropy: 141.25909423828125, Temp: 2.6953842639923096, KL: 68.572021484375, Loss: 0.015289667062461376, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12607/20000], Bound: 0.4210387170314789, Entropy: 141.2010955810547, Temp: 2.6954057216644287, KL: 83.89155578613281, Loss: 0.01760031469166279, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12608/20000], Bound: 0.40478408336639404, Entropy: 139.13140869140625, Temp: 2.6954267024993896, KL: 77.64878845214844, Loss: 0.02003624103963375, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12609/20000], Bound: 0.4001978933811188, Entropy: 140.0721893310547, Temp: 2.695441246032715, KL: 78.67852783203125, Loss: 0.015586410649120808, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12610/20000], Bound: 0.39778271317481995, Entropy: 141.00523376464844, Temp: 2.6954574584960938, KL: 77.74897766113281, Loss: 0.015980279073119164, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12611/20000], Bound: 0.38718381524086, Entropy: 141.79571533203125, Temp: 2.695474147796631, KL: 75.00355529785156, Loss: 0.01528973039239645, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12612/20000], Bound: 0.39264947175979614, Entropy: 140.82420349121094, Temp: 2.6954915523529053, KL: 76.90824890136719, Loss: 0.014727913774549961, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12613/20000], Bound: 0.3863852322101593, Entropy: 140.93341064453125, Temp: 2.6955108642578125, KL: 75.20198059082031, Loss: 0.014489936642348766, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12614/20000], Bound: 0.40584230422973633, Entropy: 139.83004760742188, Temp: 2.6955320835113525, KL: 78.58561706542969, Loss: 0.018887965008616447, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12615/20000], Bound: 0.38581544160842896, Entropy: 140.51327514648438, Temp: 2.6955487728118896, KL: 73.17375183105469, Loss: 0.017944471910595894, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12616/20000], Bound: 0.3924065828323364, Entropy: 141.72613525390625, Temp: 2.695561408996582, KL: 75.8736572265625, Loss: 0.016515137627720833, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12617/20000], Bound: 0.43025529384613037, Entropy: 142.97335815429688, Temp: 2.695573568344116, KL: 86.29428100585938, Loss: 0.018432173877954483, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12618/20000], Bound: 0.3780602216720581, Entropy: 142.6565399169922, Temp: 2.6955854892730713, KL: 70.27517700195312, Loss: 0.019154397770762444, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12619/20000], Bound: 0.38608577847480774, Entropy: 141.1651611328125, Temp: 2.6955907344818115, KL: 74.24201965332031, Loss: 0.016109390184283257, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12620/20000], Bound: 0.37494122982025146, Entropy: 142.35618591308594, Temp: 2.69559645652771, KL: 72.54127502441406, Loss: 0.013288392685353756, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12621/20000], Bound: 0.38586732745170593, Entropy: 141.23269653320312, Temp: 2.695606231689453, KL: 73.76765441894531, Loss: 0.016871366649866104, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12622/20000], Bound: 0.39577043056488037, Entropy: 141.43899536132812, Temp: 2.6956145763397217, KL: 76.23155212402344, Loss: 0.017691519111394882, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12623/20000], Bound: 0.37635305523872375, Entropy: 141.09552001953125, Temp: 2.6956210136413574, KL: 71.97752380371094, Loss: 0.015086056664586067, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12624/20000], Bound: 0.39876189827919006, Entropy: 140.2194061279297, Temp: 2.6956284046173096, KL: 75.8736572265625, Loss: 0.019999278709292412, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12625/20000], Bound: 0.39931678771972656, Entropy: 139.5858917236328, Temp: 2.6956305503845215, KL: 78.48069763183594, Loss: 0.015469331294298172, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12626/20000], Bound: 0.35713791847229004, Entropy: 143.58485412597656, Temp: 2.6956353187561035, KL: 66.78132629394531, Loss: 0.01462190318852663, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12627/20000], Bound: 0.38773491978645325, Entropy: 142.16690063476562, Temp: 2.6956405639648438, KL: 74.56614685058594, Loss: 0.016401145607233047, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12628/20000], Bound: 0.3765808343887329, Entropy: 141.27789306640625, Temp: 2.695645570755005, KL: 71.70671081542969, Loss: 0.015709972009062767, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12629/20000], Bound: 0.39148616790771484, Entropy: 142.24388122558594, Temp: 2.695650815963745, KL: 75.98968505859375, Loss: 0.015798939391970634, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12630/20000], Bound: 0.3791835904121399, Entropy: 142.20635986328125, Temp: 2.695657730102539, KL: 70.05934143066406, Loss: 0.020155970007181168, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12631/20000], Bound: 0.39988085627555847, Entropy: 141.2593536376953, Temp: 2.6956567764282227, KL: 78.72868347167969, Loss: 0.01532066985964775, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12632/20000], Bound: 0.39399224519729614, Entropy: 141.73519897460938, Temp: 2.6956591606140137, KL: 77.27192687988281, Loss: 0.014788546599447727, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12633/20000], Bound: 0.38494181632995605, Entropy: 142.25006103515625, Temp: 2.695665121078491, KL: 74.92204284667969, Loss: 0.014230879954993725, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12634/20000], Bound: 0.3985247015953064, Entropy: 140.7792510986328, Temp: 2.695674419403076, KL: 77.21922302246094, Loss: 0.017373288050293922, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12635/20000], Bound: 0.3823978006839752, Entropy: 141.17726135253906, Temp: 2.6956827640533447, KL: 73.49980163574219, Loss: 0.015498785302042961, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12636/20000], Bound: 0.39803043007850647, Entropy: 141.0727081298828, Temp: 2.6956915855407715, KL: 79.30833435058594, Loss: 0.01322654914110899, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12637/20000], Bound: 0.3923729360103607, Entropy: 140.42860412597656, Temp: 2.695706605911255, KL: 73.99830627441406, Loss: 0.01997656561434269, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12638/20000], Bound: 0.4200536906719208, Entropy: 139.37962341308594, Temp: 2.6957144737243652, KL: 84.93450927734375, Loss: 0.01510862447321415, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12639/20000], Bound: 0.3951973617076874, Entropy: 141.50230407714844, Temp: 2.6957273483276367, KL: 76.29914855957031, Loss: 0.017253125086426735, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12640/20000], Bound: 0.3912235200405121, Entropy: 141.6693115234375, Temp: 2.6957385540008545, KL: 75.21235656738281, Loss: 0.017098532989621162, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12641/20000], Bound: 0.3619929552078247, Entropy: 140.32318115234375, Temp: 2.6957480907440186, KL: 68.09396362304688, Loss: 0.014714791439473629, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12642/20000], Bound: 0.3994254171848297, Entropy: 141.32339477539062, Temp: 2.6957578659057617, KL: 78.3157958984375, Loss: 0.01583634875714779, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12643/20000], Bound: 0.3648427128791809, Entropy: 143.02151489257812, Temp: 2.6957693099975586, KL: 68.10469055175781, Loss: 0.01618615910410881, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12644/20000], Bound: 0.394385427236557, Entropy: 141.9757537841797, Temp: 2.6957781314849854, KL: 75.59822082519531, Loss: 0.0181091520935297, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12645/20000], Bound: 0.37212204933166504, Entropy: 142.91790771484375, Temp: 2.695784091949463, KL: 69.31025695800781, Loss: 0.017786506563425064, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12646/20000], Bound: 0.3915969431400299, Entropy: 141.7268524169922, Temp: 2.695786237716675, KL: 76.86003112792969, Loss: 0.014246329665184021, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12647/20000], Bound: 0.38149482011795044, Entropy: 141.8921661376953, Temp: 2.6957926750183105, KL: 72.92988586425781, Loss: 0.016071703284978867, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12648/20000], Bound: 0.39018481969833374, Entropy: 141.24346923828125, Temp: 2.695798873901367, KL: 72.63624572753906, Loss: 0.021311836317181587, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12649/20000], Bound: 0.3878055214881897, Entropy: 139.97984313964844, Temp: 2.695796489715576, KL: 74.73655700683594, Loss: 0.016124766319990158, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12650/20000], Bound: 0.38274940848350525, Entropy: 139.8008575439453, Temp: 2.6957955360412598, KL: 71.382080078125, Loss: 0.019616752862930298, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12651/20000], Bound: 0.3949066698551178, Entropy: 143.2441864013672, Temp: 2.695789098739624, KL: 76.75340270996094, Loss: 0.016251955181360245, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12652/20000], Bound: 0.3983367383480072, Entropy: 140.21461486816406, Temp: 2.695784568786621, KL: 78.50910949707031, Loss: 0.014878453686833382, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12653/20000], Bound: 0.3691057860851288, Entropy: 142.94703674316406, Temp: 2.695784568786621, KL: 69.51034545898438, Loss: 0.015821004286408424, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12654/20000], Bound: 0.40614280104637146, Entropy: 140.57431030273438, Temp: 2.695784330368042, KL: 79.79922485351562, Loss: 0.016806675121188164, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12655/20000], Bound: 0.38309910893440247, Entropy: 140.93161010742188, Temp: 2.6957855224609375, KL: 72.91134643554688, Loss: 0.016968416050076485, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12656/20000], Bound: 0.4026205837726593, Entropy: 141.54861450195312, Temp: 2.6957855224609375, KL: 79.69110107421875, Loss: 0.015051281079649925, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12657/20000], Bound: 0.3727623224258423, Entropy: 140.62611389160156, Temp: 2.6957895755767822, KL: 68.7952880859375, Loss: 0.019081011414527893, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12658/20000], Bound: 0.3852638602256775, Entropy: 141.5375518798828, Temp: 2.6957879066467285, KL: 75.09904479980469, Loss: 0.014077628962695599, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12659/20000], Bound: 0.37010374665260315, Entropy: 143.24842834472656, Temp: 2.6957905292510986, KL: 70.75492858886719, Loss: 0.01403937116265297, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12660/20000], Bound: 0.4080598056316376, Entropy: 141.70779418945312, Temp: 2.695796012878418, KL: 81.17433166503906, Loss: 0.015325208194553852, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12661/20000], Bound: 0.38677698373794556, Entropy: 140.06166076660156, Temp: 2.6958048343658447, KL: 74.35221862792969, Loss: 0.016280751675367355, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12662/20000], Bound: 0.37929558753967285, Entropy: 141.0124969482422, Temp: 2.6958134174346924, KL: 72.79644775390625, Loss: 0.01514044962823391, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12663/20000], Bound: 0.3856908082962036, Entropy: 140.33810424804688, Temp: 2.6958227157592773, KL: 75.32940673828125, Loss: 0.013881293125450611, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12664/20000], Bound: 0.3838980495929718, Entropy: 141.8023223876953, Temp: 2.695836067199707, KL: 73.59364318847656, Loss: 0.01613357849419117, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12665/20000], Bound: 0.38632696866989136, Entropy: 139.1925048828125, Temp: 2.6958484649658203, KL: 75.00267028808594, Loss: 0.014831348322331905, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12666/20000], Bound: 0.4118662178516388, Entropy: 141.44480895996094, Temp: 2.6958625316619873, KL: 81.43846130371094, Loss: 0.016967611387372017, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12667/20000], Bound: 0.38685253262519836, Entropy: 140.22422790527344, Temp: 2.6958770751953125, KL: 75.26094055175781, Loss: 0.014636904932558537, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12668/20000], Bound: 0.3938169479370117, Entropy: 140.95419311523438, Temp: 2.6958935260772705, KL: 75.87696838378906, Loss: 0.017282303422689438, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12669/20000], Bound: 0.3940679430961609, Entropy: 142.2998809814453, Temp: 2.6959078311920166, KL: 76.92323303222656, Loss: 0.015479174442589283, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12670/20000], Bound: 0.368231862783432, Entropy: 139.6519317626953, Temp: 2.695923089981079, KL: 69.66862487792969, Loss: 0.015067892149090767, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12671/20000], Bound: 0.38954681158065796, Entropy: 142.06277465820312, Temp: 2.6959378719329834, KL: 75.77671813964844, Loss: 0.015141649171710014, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12672/20000], Bound: 0.38610145449638367, Entropy: 139.82269287109375, Temp: 2.6959543228149414, KL: 74.84202575683594, Loss: 0.01500841323286295, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12673/20000], Bound: 0.38118135929107666, Entropy: 140.98802185058594, Temp: 2.6959712505340576, KL: 74.28096008300781, Loss: 0.013399304822087288, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12674/20000], Bound: 0.35169175267219543, Entropy: 143.7296600341797, Temp: 2.6959917545318604, KL: 65.53752136230469, Loss: 0.014117795042693615, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12675/20000], Bound: 0.35056382417678833, Entropy: 142.578369140625, Temp: 2.6960113048553467, KL: 63.80824279785156, Loss: 0.016745001077651978, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12676/20000], Bound: 0.38703957200050354, Entropy: 140.2342987060547, Temp: 2.6960256099700928, KL: 74.59335327148438, Loss: 0.015977658331394196, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12677/20000], Bound: 0.38855138421058655, Entropy: 142.54281616210938, Temp: 2.6960394382476807, KL: 74.11283874511719, Loss: 0.017688065767288208, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12678/20000], Bound: 0.3924623131752014, Entropy: 140.27198791503906, Temp: 2.6960501670837402, KL: 76.81857299804688, Loss: 0.014797687530517578, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12679/20000], Bound: 0.3787417411804199, Entropy: 140.59239196777344, Temp: 2.696063280105591, KL: 72.76220703125, Loss: 0.01490993332117796, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12680/20000], Bound: 0.39618048071861267, Entropy: 140.46270751953125, Temp: 2.696077585220337, KL: 77.37814331054688, Loss: 0.01579423062503338, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12681/20000], Bound: 0.39924156665802, Entropy: 142.20152282714844, Temp: 2.6960926055908203, KL: 77.27734375, Loss: 0.017664248123764992, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12682/20000], Bound: 0.38397663831710815, Entropy: 140.2938232421875, Temp: 2.696105480194092, KL: 72.79257202148438, Loss: 0.017663946375250816, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12683/20000], Bound: 0.38971441984176636, Entropy: 140.71951293945312, Temp: 2.6961147785186768, KL: 76.05178833007812, Loss: 0.014724320732057095, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12684/20000], Bound: 0.387185275554657, Entropy: 141.956298828125, Temp: 2.696126699447632, KL: 74.49893188476562, Loss: 0.0162325669080019, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12685/20000], Bound: 0.3865331709384918, Entropy: 142.8717803955078, Temp: 2.696138381958008, KL: 75.13841247558594, Loss: 0.01469393726438284, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12686/20000], Bound: 0.38679641485214233, Entropy: 142.74703979492188, Temp: 2.6961517333984375, KL: 73.91551208496094, Loss: 0.017104312777519226, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12687/20000], Bound: 0.36851540207862854, Entropy: 141.14369201660156, Temp: 2.6961631774902344, KL: 69.40638732910156, Loss: 0.01570562832057476, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12688/20000], Bound: 0.40106868743896484, Entropy: 140.59315490722656, Temp: 2.6961734294891357, KL: 77.44828796386719, Loss: 0.018356172367930412, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12689/20000], Bound: 0.3385918438434601, Entropy: 141.57257080078125, Temp: 2.696180820465088, KL: 61.30674743652344, Loss: 0.015283577144145966, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12690/20000], Bound: 0.39572083950042725, Entropy: 141.05262756347656, Temp: 2.696185350418091, KL: 76.7198486328125, Loss: 0.016764018684625626, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12691/20000], Bound: 0.38876983523368835, Entropy: 141.48411560058594, Temp: 2.696190118789673, KL: 76.55572509765625, Loss: 0.013277663849294186, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12692/20000], Bound: 0.39180469512939453, Entropy: 140.24569702148438, Temp: 2.696200370788574, KL: 76.34304809570312, Loss: 0.015322436578571796, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12693/20000], Bound: 0.40871939063072205, Entropy: 139.8830108642578, Temp: 2.69621205329895, KL: 81.57762145996094, Loss: 0.014950292184948921, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12694/20000], Bound: 0.4355725944042206, Entropy: 138.98451232910156, Temp: 2.696227550506592, KL: 88.18843078613281, Loss: 0.018011104315519333, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12695/20000], Bound: 0.38104456663131714, Entropy: 142.7742156982422, Temp: 2.6962435245513916, KL: 72.66328430175781, Loss: 0.016328459605574608, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12696/20000], Bound: 0.39376139640808105, Entropy: 142.0553741455078, Temp: 2.6962578296661377, KL: 75.65444946289062, Loss: 0.017667917534708977, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12697/20000], Bound: 0.3839571475982666, Entropy: 140.24139404296875, Temp: 2.6962695121765137, KL: 73.63636779785156, Loss: 0.016090068966150284, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12698/20000], Bound: 0.38723114132881165, Entropy: 140.6824951171875, Temp: 2.6962807178497314, KL: 73.53202819824219, Loss: 0.018051857128739357, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12699/20000], Bound: 0.38557446002960205, Entropy: 141.38043212890625, Temp: 2.6962881088256836, KL: 75.2025146484375, Loss: 0.014058389700949192, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12700/20000], Bound: 0.37046581506729126, Entropy: 141.47418212890625, Temp: 2.6962993144989014, KL: 70.27778625488281, Loss: 0.01512001920491457, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12701/20000], Bound: 0.37446537613868713, Entropy: 143.95269775390625, Temp: 2.69631028175354, KL: 71.256591796875, Loss: 0.015424503944814205, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12702/20000], Bound: 0.3820069134235382, Entropy: 142.20884704589844, Temp: 2.6963212490081787, KL: 73.15324401855469, Loss: 0.01593722030520439, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12703/20000], Bound: 0.39901503920555115, Entropy: 141.09901428222656, Temp: 2.696331739425659, KL: 77.64149475097656, Loss: 0.016866428777575493, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12704/20000], Bound: 0.37896728515625, Entropy: 142.5505828857422, Temp: 2.6963419914245605, KL: 73.10389709472656, Loss: 0.014399444684386253, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12705/20000], Bound: 0.427360475063324, Entropy: 139.538330078125, Temp: 2.6963541507720947, KL: 85.84187316894531, Loss: 0.017611434683203697, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12706/20000], Bound: 0.3807373046875, Entropy: 141.0184783935547, Temp: 2.6963672637939453, KL: 72.50308227539062, Loss: 0.016461817547678947, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12707/20000], Bound: 0.36892515420913696, Entropy: 139.96124267578125, Temp: 2.696378707885742, KL: 68.35359191894531, Loss: 0.017875606194138527, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12708/20000], Bound: 0.3989318907260895, Entropy: 139.8595733642578, Temp: 2.696385145187378, KL: 77.64186096191406, Loss: 0.01682046800851822, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12709/20000], Bound: 0.40134352445602417, Entropy: 142.27919006347656, Temp: 2.6963915824890137, KL: 78.51301574707031, Loss: 0.016535673290491104, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12710/20000], Bound: 0.394695520401001, Entropy: 143.31683349609375, Temp: 2.696398973464966, KL: 77.98286437988281, Loss: 0.01386245433241129, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12711/20000], Bound: 0.38055771589279175, Entropy: 142.4103546142578, Temp: 2.696410894393921, KL: 73.34309387207031, Loss: 0.014808207750320435, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12712/20000], Bound: 0.3927149474620819, Entropy: 140.36436462402344, Temp: 2.6964242458343506, KL: 75.61355590820312, Loss: 0.017173755913972855, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12713/20000], Bound: 0.42308348417282104, Entropy: 141.6192626953125, Temp: 2.6964359283447266, KL: 84.77070617675781, Loss: 0.017147419974207878, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12714/20000], Bound: 0.38714635372161865, Entropy: 141.33917236328125, Temp: 2.696448564529419, KL: 74.27328491210938, Loss: 0.01663285493850708, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12715/20000], Bound: 0.38760992884635925, Entropy: 141.90060424804688, Temp: 2.696460008621216, KL: 75.99354553222656, Loss: 0.013694091700017452, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12716/20000], Bound: 0.4018956422805786, Entropy: 142.23240661621094, Temp: 2.6964752674102783, KL: 78.45036315917969, Loss: 0.016957981511950493, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12717/20000], Bound: 0.3771986961364746, Entropy: 143.24574279785156, Temp: 2.6964900493621826, KL: 70.30429077148438, Loss: 0.01864728331565857, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12718/20000], Bound: 0.3792312741279602, Entropy: 140.49639892578125, Temp: 2.6964986324310303, KL: 71.45498657226562, Loss: 0.017599651589989662, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12719/20000], Bound: 0.4076400101184845, Entropy: 141.7340545654297, Temp: 2.6965041160583496, KL: 79.37471008300781, Loss: 0.018435532227158546, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12720/20000], Bound: 0.3927549123764038, Entropy: 141.80892944335938, Temp: 2.696507692337036, KL: 76.1463623046875, Loss: 0.016208382323384285, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12721/20000], Bound: 0.39519453048706055, Entropy: 139.25775146484375, Temp: 2.69651198387146, KL: 75.20492553710938, Loss: 0.019287796691060066, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12722/20000], Bound: 0.3633328080177307, Entropy: 142.18324279785156, Temp: 2.696512222290039, KL: 67.67518615722656, Loss: 0.016197754070162773, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12723/20000], Bound: 0.40006715059280396, Entropy: 138.50057983398438, Temp: 2.6965110301971436, KL: 77.41410827636719, Loss: 0.017869746312499046, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12724/20000], Bound: 0.3766508996486664, Entropy: 141.1023406982422, Temp: 2.6965086460113525, KL: 70.8035888671875, Loss: 0.017429454252123833, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12725/20000], Bound: 0.3620617985725403, Entropy: 143.24598693847656, Temp: 2.6965043544769287, KL: 68.43475341796875, Loss: 0.01412502396851778, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12726/20000], Bound: 0.3677771985530853, Entropy: 141.26416015625, Temp: 2.696502447128296, KL: 67.31349182128906, Loss: 0.01920040138065815, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12727/20000], Bound: 0.38361138105392456, Entropy: 139.30128479003906, Temp: 2.6964945793151855, KL: 73.88966369628906, Loss: 0.015436198562383652, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12728/20000], Bound: 0.3781469762325287, Entropy: 142.2323455810547, Temp: 2.696489095687866, KL: 72.24990844726562, Loss: 0.015545851550996304, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12729/20000], Bound: 0.4019395112991333, Entropy: 141.22760009765625, Temp: 2.6964855194091797, KL: 78.61399841308594, Loss: 0.016678903251886368, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12730/20000], Bound: 0.404554158449173, Entropy: 139.56040954589844, Temp: 2.6964831352233887, KL: 79.43386840820312, Loss: 0.016607897356152534, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12731/20000], Bound: 0.38716647028923035, Entropy: 141.09353637695312, Temp: 2.6964826583862305, KL: 73.64971923828125, Loss: 0.017800331115722656, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12732/20000], Bound: 0.39334869384765625, Entropy: 139.8658447265625, Temp: 2.6964802742004395, KL: 75.44355773925781, Loss: 0.017835475504398346, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12733/20000], Bound: 0.42239123582839966, Entropy: 142.06573486328125, Temp: 2.696476459503174, KL: 82.7711181640625, Loss: 0.02046046406030655, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12734/20000], Bound: 0.3974515199661255, Entropy: 143.01742553710938, Temp: 2.696469783782959, KL: 78.09510803222656, Loss: 0.01516642328351736, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12735/20000], Bound: 0.391456663608551, Entropy: 141.0999755859375, Temp: 2.696467399597168, KL: 75.2769775390625, Loss: 0.017112232744693756, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12736/20000], Bound: 0.38442179560661316, Entropy: 142.03436279296875, Temp: 2.696464776992798, KL: 73.31706237792969, Loss: 0.016934363171458244, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12737/20000], Bound: 0.38181039690971375, Entropy: 142.5848846435547, Temp: 2.6964614391326904, KL: 72.60806274414062, Loss: 0.016843823716044426, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12738/20000], Bound: 0.39337456226348877, Entropy: 141.2108612060547, Temp: 2.696457624435425, KL: 75.41519165039062, Loss: 0.017902009189128876, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12739/20000], Bound: 0.3810196816921234, Entropy: 140.68443298339844, Temp: 2.6964523792266846, KL: 74.04396057128906, Loss: 0.013756747357547283, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12740/20000], Bound: 0.39702948927879333, Entropy: 141.26242065429688, Temp: 2.6964521408081055, KL: 78.26356506347656, Loss: 0.014622037298977375, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12741/20000], Bound: 0.3779379725456238, Entropy: 141.2426300048828, Temp: 2.6964564323425293, KL: 71.29556274414062, Loss: 0.01720353774726391, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12742/20000], Bound: 0.376711905002594, Entropy: 141.63751220703125, Temp: 2.696458339691162, KL: 71.21965026855469, Loss: 0.016690075397491455, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12743/20000], Bound: 0.41634654998779297, Entropy: 142.6903533935547, Temp: 2.6964590549468994, KL: 82.54927062988281, Loss: 0.017438452690839767, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12744/20000], Bound: 0.3967302441596985, Entropy: 142.0917510986328, Temp: 2.6964609622955322, KL: 76.60910034179688, Loss: 0.01752566546201706, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12745/20000], Bound: 0.40148448944091797, Entropy: 140.46804809570312, Temp: 2.6964616775512695, KL: 77.01933288574219, Loss: 0.019383996725082397, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12746/20000], Bound: 0.37205666303634644, Entropy: 141.22190856933594, Temp: 2.6964590549468994, KL: 69.0924072265625, Loss: 0.018160982057452202, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12747/20000], Bound: 0.4019903242588043, Entropy: 141.09109497070312, Temp: 2.6964526176452637, KL: 78.85533142089844, Loss: 0.01625918783247471, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12748/20000], Bound: 0.40523138642311096, Entropy: 139.4022216796875, Temp: 2.696449041366577, KL: 80.36866760253906, Loss: 0.01525041926652193, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12749/20000], Bound: 0.4061417281627655, Entropy: 140.08558654785156, Temp: 2.6964497566223145, KL: 79.90101623535156, Loss: 0.01662401668727398, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12750/20000], Bound: 0.3622133731842041, Entropy: 142.97866821289062, Temp: 2.6964519023895264, KL: 68.92306518554688, Loss: 0.01329827681183815, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12751/20000], Bound: 0.4055890142917633, Entropy: 142.1111297607422, Temp: 2.6964573860168457, KL: 78.13909912109375, Loss: 0.019583646208047867, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12752/20000], Bound: 0.377589613199234, Entropy: 140.96853637695312, Temp: 2.6964590549468994, KL: 70.3046875, Loss: 0.018854929134249687, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12753/20000], Bound: 0.3764716386795044, Entropy: 141.11929321289062, Temp: 2.696455717086792, KL: 72.30963134765625, Loss: 0.01454083900898695, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12754/20000], Bound: 0.3856516182422638, Entropy: 140.19309997558594, Temp: 2.696455240249634, KL: 74.37373352050781, Loss: 0.01563849113881588, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12755/20000], Bound: 0.37334877252578735, Entropy: 144.42152404785156, Temp: 2.6964564323425293, KL: 67.74641418457031, Loss: 0.021341733634471893, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12756/20000], Bound: 0.35456931591033936, Entropy: 140.87962341308594, Temp: 2.6964480876922607, KL: 66.91993713378906, Loss: 0.013041786849498749, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12757/20000], Bound: 0.3861711323261261, Entropy: 140.84681701660156, Temp: 2.696443796157837, KL: 76.06922912597656, Loss: 0.01277514174580574, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12758/20000], Bound: 0.36332353949546814, Entropy: 141.9940185546875, Temp: 2.696446418762207, KL: 67.95747375488281, Loss: 0.01566893607378006, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12759/20000], Bound: 0.38346245884895325, Entropy: 140.49679565429688, Temp: 2.696448564529419, KL: 73.61148071289062, Loss: 0.015871437266469002, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12760/20000], Bound: 0.35779625177383423, Entropy: 141.8773193359375, Temp: 2.696451187133789, KL: 66.47465515136719, Loss: 0.015538599342107773, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12761/20000], Bound: 0.4154965877532959, Entropy: 139.59556579589844, Temp: 2.6964526176452637, KL: 84.27378845214844, Loss: 0.013760415837168694, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12762/20000], Bound: 0.3808879554271698, Entropy: 140.47557067871094, Temp: 2.6964616775512695, KL: 73.319580078125, Loss: 0.01502938847988844, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12763/20000], Bound: 0.3764193654060364, Entropy: 140.59332275390625, Temp: 2.696471929550171, KL: 71.92764282226562, Loss: 0.015221452340483665, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12764/20000], Bound: 0.41365665197372437, Entropy: 140.0101318359375, Temp: 2.6964824199676514, KL: 81.59912109375, Loss: 0.017682896926999092, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12765/20000], Bound: 0.39846619963645935, Entropy: 140.03643798828125, Temp: 2.6964926719665527, KL: 76.85572814941406, Loss: 0.018022801727056503, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12766/20000], Bound: 0.38248196244239807, Entropy: 141.34747314453125, Temp: 2.696500539779663, KL: 74.33271789550781, Loss: 0.014007092453539371, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12767/20000], Bound: 0.37555840611457825, Entropy: 141.0445556640625, Temp: 2.696511745452881, KL: 71.01406860351562, Loss: 0.016457263380289078, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12768/20000], Bound: 0.38058677315711975, Entropy: 141.29966735839844, Temp: 2.6965208053588867, KL: 72.80680847167969, Loss: 0.015819227322936058, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12769/20000], Bound: 0.37055447697639465, Entropy: 140.91574096679688, Temp: 2.6965298652648926, KL: 70.67721557617188, Loss: 0.014428239315748215, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12770/20000], Bound: 0.3777334988117218, Entropy: 141.69857788085938, Temp: 2.696540355682373, KL: 72.51344299316406, Loss: 0.014836822636425495, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12771/20000], Bound: 0.3888155221939087, Entropy: 140.13319396972656, Temp: 2.69655179977417, KL: 73.15663146972656, Loss: 0.019608836621046066, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12772/20000], Bound: 0.39576849341392517, Entropy: 141.53549194335938, Temp: 2.6965575218200684, KL: 76.807373046875, Loss: 0.01663138158619404, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12773/20000], Bound: 0.35732001066207886, Entropy: 141.29800415039062, Temp: 2.696563243865967, KL: 65.52540588378906, Loss: 0.017052458599209785, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12774/20000], Bound: 0.3654502034187317, Entropy: 141.42782592773438, Temp: 2.6965646743774414, KL: 67.44677734375, Loss: 0.01773085817694664, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12775/20000], Bound: 0.4086221158504486, Entropy: 139.64451599121094, Temp: 2.6965622901916504, KL: 80.65235900878906, Loss: 0.016615433618426323, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12776/20000], Bound: 0.3819815218448639, Entropy: 141.00892639160156, Temp: 2.6965620517730713, KL: 72.89547729492188, Loss: 0.0164036825299263, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12777/20000], Bound: 0.36230868101119995, Entropy: 140.761962890625, Temp: 2.696561574935913, KL: 66.85951232910156, Loss: 0.01717526838183403, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12778/20000], Bound: 0.3947986960411072, Entropy: 141.90318298339844, Temp: 2.6965582370758057, KL: 76.93452453613281, Loss: 0.015864424407482147, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12779/20000], Bound: 0.38167712092399597, Entropy: 142.18017578125, Temp: 2.696556806564331, KL: 72.42868041992188, Loss: 0.01710568554699421, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12780/20000], Bound: 0.3773847222328186, Entropy: 140.47604370117188, Temp: 2.69655442237854, KL: 71.63053894042969, Loss: 0.01628786511719227, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12781/20000], Bound: 0.38063734769821167, Entropy: 141.8753662109375, Temp: 2.69655179977417, KL: 73.57484436035156, Loss: 0.014422537758946419, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12782/20000], Bound: 0.3842446208000183, Entropy: 142.8232421875, Temp: 2.6965527534484863, KL: 74.74862670898438, Loss: 0.014185183681547642, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12783/20000], Bound: 0.3845500946044922, Entropy: 140.23670959472656, Temp: 2.6965575218200684, KL: 73.30360412597656, Loss: 0.017029257491230965, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12784/20000], Bound: 0.3932410776615143, Entropy: 140.4088592529297, Temp: 2.696560859680176, KL: 77.03114318847656, Loss: 0.014833697117865086, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12785/20000], Bound: 0.38789254426956177, Entropy: 141.8699188232422, Temp: 2.6965675354003906, KL: 72.59234619140625, Loss: 0.020154820755124092, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12786/20000], Bound: 0.37933701276779175, Entropy: 141.45513916015625, Temp: 2.6965675354003906, KL: 72.27229309082031, Loss: 0.01614134944975376, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12787/20000], Bound: 0.36037278175354004, Entropy: 140.69393920898438, Temp: 2.6965675354003906, KL: 66.828125, Loss: 0.016223758459091187, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12788/20000], Bound: 0.3891143798828125, Entropy: 140.2010955810547, Temp: 2.696565866470337, KL: 76.20445251464844, Loss: 0.014119814150035381, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12789/20000], Bound: 0.3913733661174774, Entropy: 141.38156127929688, Temp: 2.696568727493286, KL: 75.68930053710938, Loss: 0.016303278505802155, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12790/20000], Bound: 0.3594617545604706, Entropy: 140.4717559814453, Temp: 2.6965723037719727, KL: 66.99351501464844, Loss: 0.015442874282598495, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12791/20000], Bound: 0.4008415937423706, Entropy: 140.37762451171875, Temp: 2.6965749263763428, KL: 79.34895324707031, Loss: 0.014710175804793835, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12792/20000], Bound: 0.3884013593196869, Entropy: 141.91265869140625, Temp: 2.6965818405151367, KL: 75.50750732421875, Loss: 0.015025430358946323, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12793/20000], Bound: 0.3602026700973511, Entropy: 140.2069549560547, Temp: 2.6965909004211426, KL: 68.08197021484375, Loss: 0.013810445554554462, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12794/20000], Bound: 0.3716944456100464, Entropy: 139.0771942138672, Temp: 2.696601629257202, KL: 70.58377075195312, Loss: 0.0152050219476223, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12795/20000], Bound: 0.36760684847831726, Entropy: 140.707763671875, Temp: 2.6966123580932617, KL: 70.47703552246094, Loss: 0.013245711103081703, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12796/20000], Bound: 0.369606614112854, Entropy: 140.95571899414062, Temp: 2.6966261863708496, KL: 70.06625366210938, Loss: 0.01506134495139122, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12797/20000], Bound: 0.403209924697876, Entropy: 141.41122436523438, Temp: 2.6966397762298584, KL: 78.74923706054688, Loss: 0.01713312417268753, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12798/20000], Bound: 0.3827289938926697, Entropy: 141.3028564453125, Temp: 2.696652412414551, KL: 72.6004638671875, Loss: 0.01735321432352066, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12799/20000], Bound: 0.38074374198913574, Entropy: 142.4237823486328, Temp: 2.696662187576294, KL: 72.64303588867188, Loss: 0.01620832458138466, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12800/20000], Bound: 0.371255099773407, Entropy: 141.31866455078125, Temp: 2.6966710090637207, KL: 70.46241760253906, Loss: 0.015198159031569958, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12801/20000], Bound: 0.41111961007118225, Entropy: 142.10960388183594, Temp: 2.6966803073883057, KL: 81.70930480957031, Loss: 0.01605500467121601, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12802/20000], Bound: 0.39004629850387573, Entropy: 140.58401489257812, Temp: 2.6966915130615234, KL: 75.71076965332031, Loss: 0.015542649663984776, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12803/20000], Bound: 0.3633904755115509, Entropy: 141.79200744628906, Temp: 2.6967036724090576, KL: 66.88911437988281, Loss: 0.017686884850263596, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12804/20000], Bound: 0.3931760787963867, Entropy: 142.56532287597656, Temp: 2.6967105865478516, KL: 76.13517761230469, Loss: 0.01646094024181366, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12805/20000], Bound: 0.3854169547557831, Entropy: 140.17140197753906, Temp: 2.6967177391052246, KL: 73.22531127929688, Loss: 0.017643524333834648, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12806/20000], Bound: 0.3575282096862793, Entropy: 142.34359741210938, Temp: 2.6967222690582275, KL: 66.64701843261719, Loss: 0.015082032419741154, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12807/20000], Bound: 0.3866055905818939, Entropy: 140.35751342773438, Temp: 2.696726083755493, KL: 74.69212341308594, Loss: 0.015566243790090084, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12808/20000], Bound: 0.3846322298049927, Entropy: 142.5202178955078, Temp: 2.6967315673828125, KL: 73.77320861816406, Loss: 0.016204381361603737, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12809/20000], Bound: 0.37394386529922485, Entropy: 141.9254913330078, Temp: 2.6967368125915527, KL: 68.82048034667969, Loss: 0.0196679774671793, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12810/20000], Bound: 0.39723676443099976, Entropy: 140.10194396972656, Temp: 2.696735143661499, KL: 76.79656982421875, Loss: 0.01745874620974064, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12811/20000], Bound: 0.40207037329673767, Entropy: 139.834228515625, Temp: 2.696732759475708, KL: 78.17277526855469, Loss: 0.017571834847331047, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12812/20000], Bound: 0.3595602214336395, Entropy: 141.11685180664062, Temp: 2.696730613708496, KL: 66.0948486328125, Loss: 0.017161568626761436, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12813/20000], Bound: 0.40830880403518677, Entropy: 141.4201202392578, Temp: 2.6967251300811768, KL: 79.987548828125, Loss: 0.017674719914793968, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12814/20000], Bound: 0.3800269067287445, Entropy: 141.13734436035156, Temp: 2.6967201232910156, KL: 73.55119323730469, Loss: 0.01414081547409296, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12815/20000], Bound: 0.36630281805992126, Entropy: 143.40576171875, Temp: 2.6967196464538574, KL: 68.79046630859375, Loss: 0.015688210725784302, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12816/20000], Bound: 0.416927307844162, Entropy: 140.19680786132812, Temp: 2.696718692779541, KL: 82.99714660644531, Loss: 0.016939226537942886, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12817/20000], Bound: 0.3459949195384979, Entropy: 142.7350616455078, Temp: 2.6967198848724365, KL: 63.02769470214844, Loss: 0.015856968238949776, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12818/20000], Bound: 0.3719947636127472, Entropy: 142.95204162597656, Temp: 2.696718692779541, KL: 68.70510864257812, Loss: 0.018848249688744545, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12819/20000], Bound: 0.3743228018283844, Entropy: 140.09759521484375, Temp: 2.6967122554779053, KL: 69.78327941894531, Loss: 0.018083937466144562, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12820/20000], Bound: 0.4249209761619568, Entropy: 139.3139190673828, Temp: 2.696702480316162, KL: 84.97988891601562, Loss: 0.0178135447204113, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12821/20000], Bound: 0.37341588735580444, Entropy: 139.78062438964844, Temp: 2.696695566177368, KL: 69.73843383789062, Loss: 0.0176854245364666, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12822/20000], Bound: 0.37629202008247375, Entropy: 141.16592407226562, Temp: 2.696686029434204, KL: 70.48802185058594, Loss: 0.01782473549246788, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12823/20000], Bound: 0.39035338163375854, Entropy: 142.1806182861328, Temp: 2.696674108505249, KL: 74.94654846191406, Loss: 0.01712636463344097, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12824/20000], Bound: 0.40108102560043335, Entropy: 139.85491943359375, Temp: 2.6966631412506104, KL: 76.77108764648438, Loss: 0.019623098894953728, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12825/20000], Bound: 0.3831043243408203, Entropy: 140.59925842285156, Temp: 2.6966490745544434, KL: 73.57199096679688, Loss: 0.015753719955682755, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12826/20000], Bound: 0.3816471993923187, Entropy: 140.16799926757812, Temp: 2.6966373920440674, KL: 72.78895568847656, Loss: 0.016422299668192863, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12827/20000], Bound: 0.4069121778011322, Entropy: 139.82275390625, Temp: 2.696626901626587, KL: 80.72900390625, Loss: 0.01551976427435875, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12828/20000], Bound: 0.3768961429595947, Entropy: 139.19749450683594, Temp: 2.6966211795806885, KL: 72.12593078613281, Loss: 0.01510927639901638, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12829/20000], Bound: 0.3917657136917114, Entropy: 140.9443359375, Temp: 2.696617603302002, KL: 77.64776611328125, Loss: 0.01288609579205513, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12830/20000], Bound: 0.3836787939071655, Entropy: 141.03451538085938, Temp: 2.6966214179992676, KL: 71.17900085449219, Loss: 0.020499689504504204, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12831/20000], Bound: 0.3934025764465332, Entropy: 141.56692504882812, Temp: 2.696617603302002, KL: 74.6064453125, Loss: 0.01941828988492489, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12832/20000], Bound: 0.3914470374584198, Entropy: 140.197265625, Temp: 2.69661021232605, KL: 76.74624633789062, Loss: 0.014384031295776367, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12833/20000], Bound: 0.37694263458251953, Entropy: 139.93287658691406, Temp: 2.6966073513031006, KL: 71.73735046386719, Loss: 0.015854470431804657, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12834/20000], Bound: 0.36790093779563904, Entropy: 140.27830505371094, Temp: 2.6966054439544678, KL: 69.2669677734375, Loss: 0.015644164755940437, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12835/20000], Bound: 0.3891007900238037, Entropy: 141.6123809814453, Temp: 2.696603775024414, KL: 75.62905883789062, Loss: 0.015179729089140892, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12836/20000], Bound: 0.38576793670654297, Entropy: 141.73245239257812, Temp: 2.6966047286987305, KL: 73.32388305664062, Loss: 0.017649343237280846, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12837/20000], Bound: 0.4172148108482361, Entropy: 140.88583374023438, Temp: 2.696603775024414, KL: 83.77580261230469, Loss: 0.015656938776373863, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12838/20000], Bound: 0.3667212128639221, Entropy: 142.162841796875, Temp: 2.6966071128845215, KL: 68.57403564453125, Loss: 0.016308467835187912, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12839/20000], Bound: 0.37554606795310974, Entropy: 139.79769897460938, Temp: 2.6966090202331543, KL: 72.95854187011719, Loss: 0.012846116907894611, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12840/20000], Bound: 0.40716397762298584, Entropy: 142.59823608398438, Temp: 2.6966161727905273, KL: 80.80476379394531, Loss: 0.015519576147198677, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12841/20000], Bound: 0.3855127692222595, Entropy: 141.5502471923828, Temp: 2.6966264247894287, KL: 73.83482360839844, Loss: 0.01656433939933777, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12842/20000], Bound: 0.3794764578342438, Entropy: 142.08535766601562, Temp: 2.6966354846954346, KL: 72.70416259765625, Loss: 0.015415825881063938, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12843/20000], Bound: 0.40773510932922363, Entropy: 140.75209045410156, Temp: 2.6966450214385986, KL: 80.42607116699219, Loss: 0.016540590673685074, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12844/20000], Bound: 0.3742421567440033, Entropy: 140.68724060058594, Temp: 2.696655750274658, KL: 72.36701965332031, Loss: 0.013250011019408703, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12845/20000], Bound: 0.3915839195251465, Entropy: 141.91629028320312, Temp: 2.696669816970825, KL: 72.86674499511719, Loss: 0.02165229059755802, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12846/20000], Bound: 0.3622575104236603, Entropy: 143.313720703125, Temp: 2.696674346923828, KL: 67.23428344726562, Loss: 0.01645446941256523, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12847/20000], Bound: 0.3851999044418335, Entropy: 142.70619201660156, Temp: 2.69667649269104, KL: 73.95277404785156, Loss: 0.01617722027003765, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12848/20000], Bound: 0.4081159830093384, Entropy: 140.7626190185547, Temp: 2.696678876876831, KL: 82.28350830078125, Loss: 0.013309578411281109, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12849/20000], Bound: 0.4046039879322052, Entropy: 140.67588806152344, Temp: 2.696688652038574, KL: 80.64234924316406, Loss: 0.014396929182112217, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12850/20000], Bound: 0.398955374956131, Entropy: 140.1508026123047, Temp: 2.696702480316162, KL: 77.93556213378906, Loss: 0.016291923820972443, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12851/20000], Bound: 0.37437036633491516, Entropy: 139.7456512451172, Temp: 2.696716785430908, KL: 70.24238586425781, Loss: 0.017258012667298317, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12852/20000], Bound: 0.3770184814929962, Entropy: 140.65106201171875, Temp: 2.6967275142669678, KL: 69.3775634765625, Loss: 0.020271243527531624, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12853/20000], Bound: 0.4047996699810028, Entropy: 141.2869873046875, Temp: 2.6967296600341797, KL: 81.25578308105469, Loss: 0.013368717394769192, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12854/20000], Bound: 0.37374982237815857, Entropy: 141.38768005371094, Temp: 2.6967389583587646, KL: 71.60101318359375, Loss: 0.014409608207643032, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12855/20000], Bound: 0.37413570284843445, Entropy: 140.51052856445312, Temp: 2.696749687194824, KL: 70.73907470703125, Loss: 0.016212670132517815, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12856/20000], Bound: 0.4072619378566742, Entropy: 140.76434326171875, Temp: 2.696758985519409, KL: 78.82766723632812, Loss: 0.01924137771129608, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12857/20000], Bound: 0.4044090211391449, Entropy: 139.16456604003906, Temp: 2.6967649459838867, KL: 79.54373168945312, Loss: 0.01632641814649105, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12858/20000], Bound: 0.40468618273735046, Entropy: 141.10520935058594, Temp: 2.6967720985412598, KL: 77.91148376464844, Loss: 0.019506702199578285, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12859/20000], Bound: 0.391446977853775, Entropy: 141.0504150390625, Temp: 2.696775197982788, KL: 75.49494934082031, Loss: 0.016705624759197235, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12860/20000], Bound: 0.40280023217201233, Entropy: 140.6493377685547, Temp: 2.6967782974243164, KL: 78.859375, Loss: 0.016703302040696144, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12861/20000], Bound: 0.37421470880508423, Entropy: 140.54991149902344, Temp: 2.696782112121582, KL: 72.03086853027344, Loss: 0.013859907165169716, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12862/20000], Bound: 0.3831753432750702, Entropy: 142.10360717773438, Temp: 2.696789264678955, KL: 72.72113037109375, Loss: 0.017370760440826416, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12863/20000], Bound: 0.3961493968963623, Entropy: 140.5389862060547, Temp: 2.696794271469116, KL: 76.49327087402344, Loss: 0.01742483489215374, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12864/20000], Bound: 0.4137561321258545, Entropy: 140.8508758544922, Temp: 2.696798086166382, KL: 82.46287536621094, Loss: 0.016140691936016083, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12865/20000], Bound: 0.40329670906066895, Entropy: 140.35162353515625, Temp: 2.6968047618865967, KL: 79.36758422851562, Loss: 0.01603638008236885, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12866/20000], Bound: 0.3548294007778168, Entropy: 139.77354431152344, Temp: 2.6968131065368652, KL: 65.19929504394531, Loss: 0.016369422897696495, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12867/20000], Bound: 0.3724243938922882, Entropy: 142.76181030273438, Temp: 2.6968183517456055, KL: 71.49765014648438, Loss: 0.013899058103561401, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12868/20000], Bound: 0.41221359372138977, Entropy: 142.6497039794922, Temp: 2.6968259811401367, KL: 83.09715270996094, Loss: 0.01409745030105114, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12869/20000], Bound: 0.407898873090744, Entropy: 141.0099334716797, Temp: 2.6968395709991455, KL: 81.67277526855469, Loss: 0.014322536066174507, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12870/20000], Bound: 0.3766682744026184, Entropy: 141.0801239013672, Temp: 2.696857452392578, KL: 72.53556823730469, Loss: 0.014230425469577312, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12871/20000], Bound: 0.4036937355995178, Entropy: 140.46665954589844, Temp: 2.6968770027160645, KL: 80.59403991699219, Loss: 0.013983392156660557, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12872/20000], Bound: 0.40208500623703003, Entropy: 139.77488708496094, Temp: 2.6969001293182373, KL: 78.44834899902344, Loss: 0.01707061566412449, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12873/20000], Bound: 0.38126587867736816, Entropy: 142.02786254882812, Temp: 2.6969218254089355, KL: 72.11534118652344, Loss: 0.01746896654367447, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12874/20000], Bound: 0.4135682284832001, Entropy: 141.4462432861328, Temp: 2.69693922996521, KL: 83.5155029296875, Loss: 0.014084935188293457, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12875/20000], Bound: 0.37719517946243286, Entropy: 142.62799072265625, Temp: 2.6969614028930664, KL: 71.58961486816406, Loss: 0.0162661075592041, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12876/20000], Bound: 0.36480438709259033, Entropy: 143.14112854003906, Temp: 2.696981191635132, KL: 67.64810180664062, Loss: 0.01702207140624523, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12877/20000], Bound: 0.382588267326355, Entropy: 142.00701904296875, Temp: 2.6969964504241943, KL: 72.23086547851562, Loss: 0.017965659499168396, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12878/20000], Bound: 0.3889181613922119, Entropy: 140.3899383544922, Temp: 2.697007179260254, KL: 76.34397888183594, Loss: 0.013759104534983635, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12879/20000], Bound: 0.38249489665031433, Entropy: 142.7209014892578, Temp: 2.697021961212158, KL: 72.60264587402344, Loss: 0.017226431518793106, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12880/20000], Bound: 0.39547765254974365, Entropy: 142.79258728027344, Temp: 2.697033643722534, KL: 77.29920959472656, Loss: 0.015564745292067528, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12881/20000], Bound: 0.3607102930545807, Entropy: 139.781005859375, Temp: 2.6970467567443848, KL: 68.14395141601562, Loss: 0.013963810168206692, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12882/20000], Bound: 0.39737239480018616, Entropy: 140.54864501953125, Temp: 2.6970608234405518, KL: 75.44424438476562, Loss: 0.02004333771765232, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12883/20000], Bound: 0.3890787661075592, Entropy: 141.8935546875, Temp: 2.697068691253662, KL: 74.60334777832031, Loss: 0.017073770985007286, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12884/20000], Bound: 0.40622568130493164, Entropy: 140.65167236328125, Temp: 2.6970746517181396, KL: 80.92031860351562, Loss: 0.014787483029067516, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12885/20000], Bound: 0.39778515696525574, Entropy: 142.2753448486328, Temp: 2.69708514213562, KL: 78.65750122070312, Loss: 0.01431346870958805, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12886/20000], Bound: 0.40174400806427, Entropy: 141.72523498535156, Temp: 2.697099447250366, KL: 78.26446533203125, Loss: 0.017224837094545364, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12887/20000], Bound: 0.3510238230228424, Entropy: 142.13253784179688, Temp: 2.697112560272217, KL: 62.47654724121094, Loss: 0.019457586109638214, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12888/20000], Bound: 0.39111727476119995, Entropy: 142.1365203857422, Temp: 2.6971161365509033, KL: 74.44441223144531, Loss: 0.018476776778697968, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12889/20000], Bound: 0.39961156249046326, Entropy: 141.29071044921875, Temp: 2.6971166133880615, KL: 76.57948303222656, Loss: 0.01917143352329731, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12890/20000], Bound: 0.3833703100681305, Entropy: 140.5818328857422, Temp: 2.6971137523651123, KL: 73.97676086425781, Loss: 0.015150684863328934, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12891/20000], Bound: 0.3743271231651306, Entropy: 141.7785186767578, Temp: 2.697113275527954, KL: 70.36729431152344, Loss: 0.017006633803248405, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12892/20000], Bound: 0.384766548871994, Entropy: 140.5924530029297, Temp: 2.697111129760742, KL: 73.678466796875, Loss: 0.016455847769975662, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12893/20000], Bound: 0.43852663040161133, Entropy: 141.13084411621094, Temp: 2.6971089839935303, KL: 88.64016723632812, Loss: 0.018908096477389336, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12894/20000], Bound: 0.40590137243270874, Entropy: 140.77513122558594, Temp: 2.697108030319214, KL: 77.61386108398438, Loss: 0.020737022161483765, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12895/20000], Bound: 0.3954766094684601, Entropy: 139.12161254882812, Temp: 2.6971018314361572, KL: 76.65402221679688, Loss: 0.016760913655161858, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12896/20000], Bound: 0.3837849497795105, Entropy: 140.64759826660156, Temp: 2.697096586227417, KL: 71.860107421875, Loss: 0.01929769292473793, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12897/20000], Bound: 0.3935084640979767, Entropy: 143.05723571777344, Temp: 2.697087049484253, KL: 75.42205810546875, Loss: 0.017968012019991875, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12898/20000], Bound: 0.38941165804862976, Entropy: 139.7572479248047, Temp: 2.6970767974853516, KL: 75.11495971679688, Loss: 0.016306087374687195, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12899/20000], Bound: 0.3840806186199188, Entropy: 140.12596130371094, Temp: 2.697068214416504, KL: 74.168212890625, Loss: 0.015177813358604908, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12900/20000], Bound: 0.40193045139312744, Entropy: 141.03916931152344, Temp: 2.6970624923706055, KL: 78.11433410644531, Loss: 0.017605939880013466, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12901/20000], Bound: 0.3882334530353546, Entropy: 141.36599731445312, Temp: 2.6970572471618652, KL: 75.42231750488281, Loss: 0.015096952207386494, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12902/20000], Bound: 0.421012818813324, Entropy: 139.70787048339844, Temp: 2.6970551013946533, KL: 83.02638244628906, Loss: 0.019207336008548737, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12903/20000], Bound: 0.37847280502319336, Entropy: 142.4257354736328, Temp: 2.697051763534546, KL: 70.47618103027344, Loss: 0.01901322230696678, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12904/20000], Bound: 0.4033832848072052, Entropy: 138.5026092529297, Temp: 2.6970438957214355, KL: 78.66026306152344, Loss: 0.017398079857230186, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12905/20000], Bound: 0.33462581038475037, Entropy: 142.78460693359375, Temp: 2.6970369815826416, KL: 61.13374328613281, Loss: 0.013611157424747944, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12906/20000], Bound: 0.406729519367218, Entropy: 141.85765075683594, Temp: 2.697031259536743, KL: 79.97297668457031, Loss: 0.016823910176753998, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12907/20000], Bound: 0.39305785298347473, Entropy: 142.0000762939453, Temp: 2.6970274448394775, KL: 76.438720703125, Loss: 0.015836603939533234, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12908/20000], Bound: 0.379361093044281, Entropy: 141.89324951171875, Temp: 2.697026014328003, KL: 72.44775390625, Loss: 0.01583291031420231, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12909/20000], Bound: 0.3625021278858185, Entropy: 140.708251953125, Temp: 2.6970252990722656, KL: 68.77471923828125, Loss: 0.013729092665016651, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12910/20000], Bound: 0.34579548239707947, Entropy: 141.37583923339844, Temp: 2.6970274448394775, KL: 62.587005615234375, Loss: 0.016574237495660782, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12911/20000], Bound: 0.3807830214500427, Entropy: 141.47760009765625, Temp: 2.697025775909424, KL: 71.35923767089844, Loss: 0.018612559884786606, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12912/20000], Bound: 0.3844962418079376, Entropy: 140.01063537597656, Temp: 2.6970202922821045, KL: 73.58476257324219, Loss: 0.016483018174767494, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12913/20000], Bound: 0.3926948606967926, Entropy: 141.3024139404297, Temp: 2.6970150470733643, KL: 76.74212646484375, Loss: 0.015075919218361378, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12914/20000], Bound: 0.36953482031822205, Entropy: 142.78208923339844, Temp: 2.6970133781433105, KL: 68.80793762207031, Loss: 0.017359554767608643, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12915/20000], Bound: 0.39446771144866943, Entropy: 139.3067169189453, Temp: 2.6970090866088867, KL: 76.87452697753906, Loss: 0.01579894684255123, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12916/20000], Bound: 0.3847358822822571, Entropy: 142.69725036621094, Temp: 2.697007417678833, KL: 73.986083984375, Loss: 0.015868091955780983, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12917/20000], Bound: 0.3651218116283417, Entropy: 141.36534118652344, Temp: 2.697006940841675, KL: 68.39540100097656, Loss: 0.015803227201104164, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12918/20000], Bound: 0.4044511020183563, Entropy: 140.8755340576172, Temp: 2.6970059871673584, KL: 78.9090576171875, Loss: 0.017528872936964035, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12919/20000], Bound: 0.40457168221473694, Entropy: 139.62014770507812, Temp: 2.697005033493042, KL: 80.62472534179688, Loss: 0.014415111392736435, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12920/20000], Bound: 0.3849709630012512, Entropy: 143.25985717773438, Temp: 2.697009563446045, KL: 72.87083435058594, Loss: 0.018062492832541466, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12921/20000], Bound: 0.3846481740474701, Entropy: 138.6493682861328, Temp: 2.6970107555389404, KL: 73.88047790527344, Loss: 0.01601659692823887, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12922/20000], Bound: 0.39850524067878723, Entropy: 139.80413818359375, Temp: 2.6970126628875732, KL: 77.87466430664062, Loss: 0.01616004854440689, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12923/20000], Bound: 0.3637324273586273, Entropy: 140.6114501953125, Temp: 2.697016477584839, KL: 67.99493408203125, Loss: 0.01581793837249279, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12924/20000], Bound: 0.4047047197818756, Entropy: 140.351806640625, Temp: 2.69701886177063, KL: 79.39263916015625, Loss: 0.016773320734500885, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12925/20000], Bound: 0.38078778982162476, Entropy: 141.60365295410156, Temp: 2.6970224380493164, KL: 72.87065124511719, Loss: 0.015813101083040237, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12926/20000], Bound: 0.3554881811141968, Entropy: 140.08480834960938, Temp: 2.697026252746582, KL: 67.74960327148438, Loss: 0.011983578093349934, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12927/20000], Bound: 0.37445712089538574, Entropy: 141.21343994140625, Temp: 2.697035074234009, KL: 71.77639770507812, Loss: 0.014462723396718502, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12928/20000], Bound: 0.38127824664115906, Entropy: 142.78564453125, Temp: 2.6970455646514893, KL: 72.95646667480469, Loss: 0.015917282551527023, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12929/20000], Bound: 0.40187889337539673, Entropy: 140.03062438964844, Temp: 2.6970555782318115, KL: 78.79875183105469, Loss: 0.01630854234099388, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12930/20000], Bound: 0.37225499749183655, Entropy: 142.46678161621094, Temp: 2.697066307067871, KL: 69.02836608886719, Loss: 0.018389279022812843, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12931/20000], Bound: 0.4006258249282837, Entropy: 141.07691955566406, Temp: 2.6970715522766113, KL: 77.92356872558594, Loss: 0.017238762229681015, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12932/20000], Bound: 0.38256922364234924, Entropy: 140.78700256347656, Temp: 2.6970765590667725, KL: 73.48332214355469, Loss: 0.015634216368198395, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12933/20000], Bound: 0.38605526089668274, Entropy: 141.58436584472656, Temp: 2.69708251953125, KL: 73.89436340332031, Loss: 0.01675102673470974, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12934/20000], Bound: 0.4246230125427246, Entropy: 139.1437225341797, Temp: 2.697087287902832, KL: 85.53216552734375, Loss: 0.0166233591735363, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12935/20000], Bound: 0.3785788118839264, Entropy: 141.8874053955078, Temp: 2.6970951557159424, KL: 71.11471557617188, Loss: 0.017886407673358917, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12936/20000], Bound: 0.3963792622089386, Entropy: 143.09054565429688, Temp: 2.697098970413208, KL: 75.90289306640625, Loss: 0.018648182973265648, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12937/20000], Bound: 0.3799021244049072, Entropy: 141.19276428222656, Temp: 2.6970999240875244, KL: 71.05097961425781, Loss: 0.018712583929300308, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12938/20000], Bound: 0.3506831228733063, Entropy: 140.663330078125, Temp: 2.697096347808838, KL: 65.14918518066406, Loss: 0.014327747747302055, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12939/20000], Bound: 0.3793024718761444, Entropy: 140.0871124267578, Temp: 2.697093963623047, KL: 73.1063232421875, Loss: 0.014581263065338135, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12940/20000], Bound: 0.37987688183784485, Entropy: 142.3923797607422, Temp: 2.697094678878784, KL: 70.72378540039062, Loss: 0.019305627793073654, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12941/20000], Bound: 0.39997097849845886, Entropy: 139.19737243652344, Temp: 2.697089672088623, KL: 78.16880798339844, Loss: 0.01642300933599472, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12942/20000], Bound: 0.385515958070755, Entropy: 141.1366729736328, Temp: 2.697086811065674, KL: 73.75289916992188, Loss: 0.01672201044857502, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12943/20000], Bound: 0.368306964635849, Entropy: 139.2703094482422, Temp: 2.6970839500427246, KL: 70.33741760253906, Loss: 0.013877409510314465, Learning Rate: 3.419668555611939e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12944/20000], Bound: 0.36221081018447876, Entropy: 141.82296752929688, Temp: 2.697084426879883, KL: 67.98025512695312, Loss: 0.015050259418785572, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12945/20000], Bound: 0.38453662395477295, Entropy: 139.41552734375, Temp: 2.69708514213562, KL: 74.89253234863281, Loss: 0.014080964960157871, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12946/20000], Bound: 0.37568140029907227, Entropy: 142.6010284423828, Temp: 2.6970901489257812, KL: 71.06474304199219, Loss: 0.016433607786893845, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12947/20000], Bound: 0.3994424045085907, Entropy: 139.32325744628906, Temp: 2.6970937252044678, KL: 78.19189453125, Loss: 0.016088850796222687, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12948/20000], Bound: 0.373398095369339, Entropy: 140.72964477539062, Temp: 2.697098970413208, KL: 71.60812377929688, Loss: 0.014213014394044876, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12949/20000], Bound: 0.37840571999549866, Entropy: 140.12200927734375, Temp: 2.6971065998077393, KL: 70.6129150390625, Loss: 0.018724290654063225, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12950/20000], Bound: 0.35195595026016235, Entropy: 143.54847717285156, Temp: 2.6971089839935303, KL: 65.05085754394531, Loss: 0.015164714306592941, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12951/20000], Bound: 0.37739747762680054, Entropy: 142.61917114257812, Temp: 2.697110652923584, KL: 70.89617919921875, Loss: 0.01766080968081951, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12952/20000], Bound: 0.3613215386867523, Entropy: 141.41128540039062, Temp: 2.6971092224121094, KL: 67.70245361328125, Loss: 0.015101420693099499, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12953/20000], Bound: 0.3904738128185272, Entropy: 140.75643920898438, Temp: 2.697108268737793, KL: 75.22622680664062, Loss: 0.016677260398864746, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12954/20000], Bound: 0.369659423828125, Entropy: 140.32981872558594, Temp: 2.6971077919006348, KL: 71.43769836425781, Loss: 0.012550870887935162, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12955/20000], Bound: 0.3967933654785156, Entropy: 139.71478271484375, Temp: 2.697112560272217, KL: 73.89665222167969, Loss: 0.022594766691327095, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12956/20000], Bound: 0.3689359426498413, Entropy: 141.6950225830078, Temp: 2.6971077919006348, KL: 68.52177429199219, Loss: 0.017574969679117203, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12957/20000], Bound: 0.41347581148147583, Entropy: 140.61204528808594, Temp: 2.6970999240875244, KL: 81.61528015136719, Loss: 0.01755746454000473, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12958/20000], Bound: 0.381921648979187, Entropy: 141.2638702392578, Temp: 2.6970937252044678, KL: 73.19735717773438, Loss: 0.015816543251276016, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12959/20000], Bound: 0.40921488404273987, Entropy: 141.96539306640625, Temp: 2.6970889568328857, KL: 80.53474426269531, Loss: 0.017170218750834465, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12960/20000], Bound: 0.40254682302474976, Entropy: 142.1988983154297, Temp: 2.6970858573913574, KL: 79.48980712890625, Loss: 0.015397269278764725, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12961/20000], Bound: 0.3609362840652466, Entropy: 141.48887634277344, Temp: 2.6970860958099365, KL: 68.01638793945312, Loss: 0.01431838795542717, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12962/20000], Bound: 0.3689923882484436, Entropy: 140.36570739746094, Temp: 2.6970882415771484, KL: 70.3052978515625, Loss: 0.01429818570613861, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12963/20000], Bound: 0.381448894739151, Entropy: 140.9424591064453, Temp: 2.697092294692993, KL: 73.77632141113281, Loss: 0.014489390887320042, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12964/20000], Bound: 0.4062011241912842, Entropy: 139.95242309570312, Temp: 2.697099208831787, KL: 79.86456298828125, Loss: 0.016731299459934235, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12965/20000], Bound: 0.3520512282848358, Entropy: 142.3747100830078, Temp: 2.6971070766448975, KL: 63.729034423828125, Loss: 0.01766420714557171, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12966/20000], Bound: 0.4000779688358307, Entropy: 141.1177978515625, Temp: 2.6971089839935303, KL: 79.35992431640625, Loss: 0.014274056069552898, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12967/20000], Bound: 0.4025430679321289, Entropy: 140.03868103027344, Temp: 2.697115898132324, KL: 77.83889770507812, Loss: 0.018456032499670982, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12968/20000], Bound: 0.41446200013160706, Entropy: 142.3214111328125, Temp: 2.697120428085327, KL: 81.99862670898438, Loss: 0.017402496188879013, Learning Rate: 3.419668555611939e-05\n",
      "Epoch [12969/20000], Bound: 0.3789244294166565, Entropy: 141.09620666503906, Temp: 2.6971240043640137, KL: 71.10800170898438, Loss: 0.01808387041091919, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12970/20000], Bound: 0.39733952283859253, Entropy: 139.8870391845703, Temp: 2.69712495803833, KL: 76.1231689453125, Loss: 0.01876717247068882, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12971/20000], Bound: 0.37241509556770325, Entropy: 141.84559631347656, Temp: 2.6971237659454346, KL: 68.08834838867188, Loss: 0.020217165350914, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12972/20000], Bound: 0.3798658847808838, Entropy: 140.6685028076172, Temp: 2.697117567062378, KL: 71.24299621582031, Loss: 0.018337365239858627, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12973/20000], Bound: 0.4002923369407654, Entropy: 141.6833953857422, Temp: 2.6971092224121094, KL: 77.80043029785156, Loss: 0.0172833614051342, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12974/20000], Bound: 0.38709738850593567, Entropy: 141.1510009765625, Temp: 2.6971018314361572, KL: 74.29859924316406, Loss: 0.016565296798944473, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12975/20000], Bound: 0.36368823051452637, Entropy: 141.06736755371094, Temp: 2.6970951557159424, KL: 67.37043762207031, Loss: 0.016953179612755775, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12976/20000], Bound: 0.38926127552986145, Entropy: 139.29644775390625, Temp: 2.697087526321411, KL: 73.84719848632812, Loss: 0.01857481151819229, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12977/20000], Bound: 0.3894794285297394, Entropy: 141.58395385742188, Temp: 2.697078227996826, KL: 74.88346862792969, Loss: 0.01677207089960575, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12978/20000], Bound: 0.4003005623817444, Entropy: 141.35472106933594, Temp: 2.6970696449279785, KL: 78.47836303710938, Loss: 0.016030751168727875, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12979/20000], Bound: 0.4085509479045868, Entropy: 141.8448944091797, Temp: 2.697063684463501, KL: 81.75151062011719, Loss: 0.014543168246746063, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12980/20000], Bound: 0.3911513090133667, Entropy: 140.7051544189453, Temp: 2.6970622539520264, KL: 74.21978759765625, Loss: 0.01891126111149788, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12981/20000], Bound: 0.4048076868057251, Entropy: 140.47792053222656, Temp: 2.6970582008361816, KL: 79.58769226074219, Loss: 0.01646931655704975, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12982/20000], Bound: 0.3827989399433136, Entropy: 142.78652954101562, Temp: 2.6970558166503906, KL: 73.77606201171875, Loss: 0.015214824117720127, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12983/20000], Bound: 0.39330482482910156, Entropy: 142.47776794433594, Temp: 2.6970553398132324, KL: 76.6834716796875, Loss: 0.015518023632466793, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12984/20000], Bound: 0.3678516745567322, Entropy: 143.1052703857422, Temp: 2.697056293487549, KL: 68.7593994140625, Loss: 0.01656290516257286, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12985/20000], Bound: 0.4002615213394165, Entropy: 139.73106384277344, Temp: 2.6970560550689697, KL: 78.05097961425781, Loss: 0.01680135540664196, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12986/20000], Bound: 0.3736598789691925, Entropy: 141.9720916748047, Temp: 2.697056531906128, KL: 70.35626220703125, Loss: 0.016672328114509583, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12987/20000], Bound: 0.38576263189315796, Entropy: 143.18267822265625, Temp: 2.6970558166503906, KL: 74.79550170898438, Loss: 0.014922117814421654, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12988/20000], Bound: 0.38685843348503113, Entropy: 142.66506958007812, Temp: 2.6970572471618652, KL: 74.99739074707031, Loss: 0.015140167437493801, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12989/20000], Bound: 0.36338743567466736, Entropy: 141.9534149169922, Temp: 2.6970603466033936, KL: 68.62326049804688, Loss: 0.014472946524620056, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12990/20000], Bound: 0.38089755177497864, Entropy: 140.61727905273438, Temp: 2.697064161300659, KL: 73.04629516601562, Loss: 0.01554669626057148, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12991/20000], Bound: 0.38461846113204956, Entropy: 140.50830078125, Temp: 2.697068452835083, KL: 74.63273620605469, Loss: 0.01460651122033596, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12992/20000], Bound: 0.370582640171051, Entropy: 141.372802734375, Temp: 2.6970746517181396, KL: 70.79043579101562, Loss: 0.014238011091947556, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12993/20000], Bound: 0.35281091928482056, Entropy: 141.40283203125, Temp: 2.697082042694092, KL: 66.25810241699219, Loss: 0.013366826809942722, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12994/20000], Bound: 0.363136351108551, Entropy: 141.12176513671875, Temp: 2.6970903873443604, KL: 69.2760009765625, Loss: 0.013131837360560894, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12995/20000], Bound: 0.40602555871009827, Entropy: 140.65077209472656, Temp: 2.6971006393432617, KL: 81.24842834472656, Loss: 0.014068152755498886, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12996/20000], Bound: 0.3811853528022766, Entropy: 141.07191467285156, Temp: 2.6971142292022705, KL: 73.79597473144531, Loss: 0.014311753213405609, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12997/20000], Bound: 0.38595929741859436, Entropy: 140.90386962890625, Temp: 2.6971287727355957, KL: 72.57907104492188, Loss: 0.019137892872095108, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12998/20000], Bound: 0.3969734013080597, Entropy: 139.6432342529297, Temp: 2.697138547897339, KL: 76.13224792480469, Loss: 0.018549419939517975, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [12999/20000], Bound: 0.4106960892677307, Entropy: 141.68748474121094, Temp: 2.697145938873291, KL: 81.38545227050781, Loss: 0.016422903165221214, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13000/20000], Bound: 0.3723607659339905, Entropy: 143.01759338378906, Temp: 2.6971542835235596, KL: 70.14698791503906, Loss: 0.016372300684452057, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13001/20000], Bound: 0.37302619218826294, Entropy: 142.8328094482422, Temp: 2.6971609592437744, KL: 71.21220397949219, Loss: 0.014750288799405098, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13002/20000], Bound: 0.38557204604148865, Entropy: 142.50328063964844, Temp: 2.6971681118011475, KL: 71.72523498535156, Loss: 0.020511912181973457, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13003/20000], Bound: 0.42253798246383667, Entropy: 141.70809936523438, Temp: 2.6971700191497803, KL: 85.67459106445312, Loss: 0.015168579295277596, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13004/20000], Bound: 0.402213990688324, Entropy: 140.36766052246094, Temp: 2.6971755027770996, KL: 78.25932312011719, Loss: 0.01749504916369915, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13005/20000], Bound: 0.3879287838935852, Entropy: 140.73416137695312, Temp: 2.6971805095672607, KL: 74.49270629882812, Loss: 0.01665632054209709, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13006/20000], Bound: 0.38144198060035706, Entropy: 142.80880737304688, Temp: 2.6971850395202637, KL: 73.42796325683594, Loss: 0.015132340602576733, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13007/20000], Bound: 0.38673654198646545, Entropy: 141.2550506591797, Temp: 2.697190284729004, KL: 72.6905517578125, Loss: 0.01935185119509697, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13008/20000], Bound: 0.38498786091804504, Entropy: 140.70669555664062, Temp: 2.6971917152404785, KL: 74.52581787109375, Loss: 0.015005156397819519, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13009/20000], Bound: 0.3946697413921356, Entropy: 141.17828369140625, Temp: 2.697195053100586, KL: 76.22860717773438, Loss: 0.017108669504523277, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13010/20000], Bound: 0.37566328048706055, Entropy: 141.37010192871094, Temp: 2.697197437286377, KL: 71.19731140136719, Loss: 0.01617911458015442, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13011/20000], Bound: 0.3885723352432251, Entropy: 141.22584533691406, Temp: 2.697199821472168, KL: 73.37832641601562, Loss: 0.019071141257882118, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13012/20000], Bound: 0.37628090381622314, Entropy: 141.6927947998047, Temp: 2.6971988677978516, KL: 71.595458984375, Loss: 0.015769923105835915, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13013/20000], Bound: 0.36821234226226807, Entropy: 140.01914978027344, Temp: 2.6971983909606934, KL: 68.56724548339844, Loss: 0.01711009070277214, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13014/20000], Bound: 0.38812288641929626, Entropy: 140.1239013671875, Temp: 2.6971962451934814, KL: 74.99211120605469, Loss: 0.015835871919989586, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13015/20000], Bound: 0.36149707436561584, Entropy: 141.3722686767578, Temp: 2.697195053100586, KL: 66.49067687988281, Loss: 0.017440032213926315, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13016/20000], Bound: 0.405758798122406, Entropy: 139.92660522460938, Temp: 2.6971914768218994, KL: 80.64329528808594, Loss: 0.01504251454025507, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13017/20000], Bound: 0.3807840943336487, Entropy: 140.2815399169922, Temp: 2.6971912384033203, KL: 72.09123229980469, Loss: 0.01725747250020504, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13018/20000], Bound: 0.37678757309913635, Entropy: 139.8134002685547, Temp: 2.697190046310425, KL: 72.46958923339844, Loss: 0.014419407583773136, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13019/20000], Bound: 0.36012735962867737, Entropy: 142.05526733398438, Temp: 2.697190761566162, KL: 67.43287658691406, Loss: 0.014979579485952854, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13020/20000], Bound: 0.39830872416496277, Entropy: 139.30426025390625, Temp: 2.6971914768218994, KL: 79.09860229492188, Loss: 0.013784794136881828, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13021/20000], Bound: 0.419929563999176, Entropy: 141.01182556152344, Temp: 2.6971964836120605, KL: 84.69949340820312, Loss: 0.015491005033254623, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13022/20000], Bound: 0.3662987947463989, Entropy: 141.787841796875, Temp: 2.697204351425171, KL: 67.98445129394531, Loss: 0.017184188589453697, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13023/20000], Bound: 0.3634392321109772, Entropy: 142.3779754638672, Temp: 2.697209358215332, KL: 66.952880859375, Loss: 0.017597798258066177, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13024/20000], Bound: 0.3846646547317505, Entropy: 140.49417114257812, Temp: 2.6972110271453857, KL: 74.18269348144531, Loss: 0.015467088669538498, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13025/20000], Bound: 0.4009527564048767, Entropy: 141.3980255126953, Temp: 2.697214126586914, KL: 78.26679992675781, Loss: 0.016784382984042168, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13026/20000], Bound: 0.37706229090690613, Entropy: 140.1074981689453, Temp: 2.6972172260284424, KL: 71.01036071777344, Loss: 0.017271187156438828, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13027/20000], Bound: 0.4258902668952942, Entropy: 142.1767120361328, Temp: 2.697218418121338, KL: 85.2030029296875, Loss: 0.01796123757958412, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13028/20000], Bound: 0.37296411395072937, Entropy: 143.0253448486328, Temp: 2.69722056388855, KL: 70.9356689453125, Loss: 0.015230545774102211, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13029/20000], Bound: 0.40719813108444214, Entropy: 140.61830139160156, Temp: 2.697223424911499, KL: 78.05970764160156, Loss: 0.02063373290002346, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13030/20000], Bound: 0.4029622972011566, Entropy: 139.51695251464844, Temp: 2.6972222328186035, KL: 80.36640930175781, Loss: 0.01400377694517374, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13031/20000], Bound: 0.386003702878952, Entropy: 142.66847229003906, Temp: 2.697225332260132, KL: 74.59503173828125, Loss: 0.015425586141645908, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13032/20000], Bound: 0.3827606439590454, Entropy: 139.22824096679688, Temp: 2.6972296237945557, KL: 74.541015625, Loss: 0.013777785003185272, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13033/20000], Bound: 0.38954535126686096, Entropy: 140.2903289794922, Temp: 2.6972365379333496, KL: 76.51335144042969, Loss: 0.013787866570055485, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13034/20000], Bound: 0.4208802580833435, Entropy: 139.93678283691406, Temp: 2.6972463130950928, KL: 85.85102844238281, Loss: 0.013897678814828396, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13035/20000], Bound: 0.3791913688182831, Entropy: 140.4980926513672, Temp: 2.6972601413726807, KL: 72.73811340332031, Loss: 0.015205945819616318, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13036/20000], Bound: 0.37258827686309814, Entropy: 142.84652709960938, Temp: 2.6972739696502686, KL: 70.9066162109375, Loss: 0.015085647813975811, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13037/20000], Bound: 0.3716331422328949, Entropy: 142.2433624267578, Temp: 2.6972873210906982, KL: 69.97625732421875, Loss: 0.016304630786180496, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13038/20000], Bound: 0.3697073757648468, Entropy: 142.35133361816406, Temp: 2.697298765182495, KL: 69.552001953125, Loss: 0.016073541715741158, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13039/20000], Bound: 0.35862863063812256, Entropy: 142.02093505859375, Temp: 2.6973085403442383, KL: 67.1474609375, Loss: 0.014730176888406277, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13040/20000], Bound: 0.4310266375541687, Entropy: 139.49851989746094, Temp: 2.6973178386688232, KL: 87.57273864746094, Loss: 0.016527391970157623, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13041/20000], Bound: 0.3576289117336273, Entropy: 141.4154510498047, Temp: 2.697329044342041, KL: 66.37271118164062, Loss: 0.015647463500499725, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13042/20000], Bound: 0.36089026927948, Entropy: 142.6291046142578, Temp: 2.697338342666626, KL: 66.85299682617188, Loss: 0.01645306870341301, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13043/20000], Bound: 0.36710724234580994, Entropy: 141.84854125976562, Temp: 2.69734525680542, KL: 68.54483032226562, Loss: 0.016571275889873505, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13044/20000], Bound: 0.403928279876709, Entropy: 140.10964965820312, Temp: 2.697350263595581, KL: 78.33775329589844, Loss: 0.0183011032640934, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13045/20000], Bound: 0.40777480602264404, Entropy: 142.39537048339844, Temp: 2.6973538398742676, KL: 81.446533203125, Loss: 0.014678406529128551, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13046/20000], Bound: 0.3938544988632202, Entropy: 141.6055450439453, Temp: 2.6973607540130615, KL: 76.72210693359375, Loss: 0.01574966311454773, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13047/20000], Bound: 0.4113793969154358, Entropy: 139.1529083251953, Temp: 2.6973683834075928, KL: 81.14231872558594, Loss: 0.017259085550904274, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13048/20000], Bound: 0.3679836094379425, Entropy: 140.42782592773438, Temp: 2.697375774383545, KL: 69.23710632324219, Loss: 0.015749342739582062, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13049/20000], Bound: 0.35218730568885803, Entropy: 141.3009796142578, Temp: 2.6973824501037598, KL: 63.98899841308594, Loss: 0.01725418120622635, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13050/20000], Bound: 0.376810759305954, Entropy: 142.16151428222656, Temp: 2.697385549545288, KL: 72.59700012207031, Loss: 0.014197352342307568, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13051/20000], Bound: 0.38483452796936035, Entropy: 141.056396484375, Temp: 2.69739031791687, KL: 74.59426879882812, Loss: 0.014797412790358067, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13052/20000], Bound: 0.40535253286361694, Entropy: 141.52740478515625, Temp: 2.697396755218506, KL: 80.39149475097656, Loss: 0.01528551522642374, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13053/20000], Bound: 0.3762134909629822, Entropy: 140.47747802734375, Temp: 2.6974053382873535, KL: 70.99057006835938, Loss: 0.016857031732797623, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13054/20000], Bound: 0.39310145378112793, Entropy: 142.8134002685547, Temp: 2.6974120140075684, KL: 77.49395751953125, Loss: 0.013908118940889835, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13055/20000], Bound: 0.3723697364330292, Entropy: 141.81610107421875, Temp: 2.6974215507507324, KL: 70.68913269042969, Loss: 0.01537431962788105, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13056/20000], Bound: 0.4094673991203308, Entropy: 140.5937042236328, Temp: 2.6974306106567383, KL: 81.37762451171875, Loss: 0.015752527862787247, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13057/20000], Bound: 0.35902106761932373, Entropy: 143.000732421875, Temp: 2.697441339492798, KL: 65.41262817382812, Loss: 0.018150869756937027, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13058/20000], Bound: 0.3862648606300354, Entropy: 140.99525451660156, Temp: 2.6974472999572754, KL: 73.16310119628906, Loss: 0.0182229895144701, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13059/20000], Bound: 0.36911630630493164, Entropy: 140.66653442382812, Temp: 2.697450637817383, KL: 70.33697509765625, Loss: 0.014307945966720581, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13060/20000], Bound: 0.4273374378681183, Entropy: 140.3790283203125, Temp: 2.6974551677703857, KL: 87.08294677734375, Loss: 0.015310131944715977, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13061/20000], Bound: 0.3856704831123352, Entropy: 142.877197265625, Temp: 2.6974635124206543, KL: 75.34539794921875, Loss: 0.013856956735253334, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13062/20000], Bound: 0.3986734449863434, Entropy: 138.08956909179688, Temp: 2.6974740028381348, KL: 79.16561889648438, Loss: 0.013864278793334961, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13063/20000], Bound: 0.3901329040527344, Entropy: 141.14761352539062, Temp: 2.6974875926971436, KL: 75.34561157226562, Loss: 0.016274135559797287, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13064/20000], Bound: 0.392395555973053, Entropy: 142.9728240966797, Temp: 2.697500228881836, KL: 75.94577026367188, Loss: 0.01639358513057232, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13065/20000], Bound: 0.4137565791606903, Entropy: 138.3866424560547, Temp: 2.697512149810791, KL: 83.43919372558594, Loss: 0.014339055866003036, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13066/20000], Bound: 0.4020484983921051, Entropy: 141.0471954345703, Temp: 2.6975274085998535, KL: 75.98178100585938, Loss: 0.02162845991551876, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13067/20000], Bound: 0.3660699725151062, Entropy: 140.51063537597656, Temp: 2.697535991668701, KL: 68.34208679199219, Loss: 0.01640368439257145, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13068/20000], Bound: 0.3888877034187317, Entropy: 139.7479248046875, Temp: 2.697542667388916, KL: 73.20156860351562, Loss: 0.019572602584958076, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13069/20000], Bound: 0.38741183280944824, Entropy: 141.3459014892578, Temp: 2.697545051574707, KL: 74.61280822753906, Loss: 0.01615702547132969, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13070/20000], Bound: 0.37470415234565735, Entropy: 139.84719848632812, Temp: 2.6975479125976562, KL: 73.26148986816406, Loss: 0.01184594351798296, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13071/20000], Bound: 0.3983417749404907, Entropy: 142.06605529785156, Temp: 2.6975553035736084, KL: 76.61405944824219, Loss: 0.018412070348858833, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13072/20000], Bound: 0.37009578943252563, Entropy: 140.14378356933594, Temp: 2.6975605487823486, KL: 70.86537170410156, Loss: 0.013846296817064285, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13073/20000], Bound: 0.39802324771881104, Entropy: 141.0395050048828, Temp: 2.6975674629211426, KL: 77.96902465820312, Loss: 0.01572549156844616, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13074/20000], Bound: 0.37436920404434204, Entropy: 141.2030487060547, Temp: 2.697575569152832, KL: 71.22262573242188, Loss: 0.015447323210537434, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13075/20000], Bound: 0.36629778146743774, Entropy: 142.47439575195312, Temp: 2.6975834369659424, KL: 68.55172729492188, Loss: 0.01613505370914936, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13076/20000], Bound: 0.40662336349487305, Entropy: 141.21922302246094, Temp: 2.697589635848999, KL: 79.90760803222656, Loss: 0.016891593113541603, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13077/20000], Bound: 0.429362952709198, Entropy: 140.64755249023438, Temp: 2.6975960731506348, KL: 86.87666320800781, Loss: 0.01686030812561512, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13078/20000], Bound: 0.38860926032066345, Entropy: 141.244384765625, Temp: 2.697604179382324, KL: 74.88482666015625, Loss: 0.016302146017551422, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13079/20000], Bound: 0.36794793605804443, Entropy: 141.79257202148438, Temp: 2.6976120471954346, KL: 68.39859008789062, Loss: 0.017286639660596848, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13080/20000], Bound: 0.395488977432251, Entropy: 142.47889709472656, Temp: 2.6976170539855957, KL: 77.45695495605469, Loss: 0.015284349210560322, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13081/20000], Bound: 0.4033162295818329, Entropy: 141.99395751953125, Temp: 2.6976237297058105, KL: 80.35115051269531, Loss: 0.01423253957182169, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13082/20000], Bound: 0.37937644124031067, Entropy: 140.24356079101562, Temp: 2.6976335048675537, KL: 71.82650756835938, Loss: 0.016997957602143288, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13083/20000], Bound: 0.3869340717792511, Entropy: 141.17013549804688, Temp: 2.697641134262085, KL: 75.12174987792969, Loss: 0.01495613344013691, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13084/20000], Bound: 0.37993666529655457, Entropy: 141.82057189941406, Temp: 2.6976499557495117, KL: 71.15684509277344, Loss: 0.01853913441300392, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13085/20000], Bound: 0.3939812183380127, Entropy: 142.24827575683594, Temp: 2.697655439376831, KL: 75.31094360351562, Loss: 0.018437277525663376, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13086/20000], Bound: 0.4080234169960022, Entropy: 139.89759826660156, Temp: 2.6976583003997803, KL: 80.78379821777344, Loss: 0.016048841178417206, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13087/20000], Bound: 0.37730899453163147, Entropy: 142.35325622558594, Temp: 2.697662830352783, KL: 69.54368591308594, Loss: 0.020124811679124832, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13088/20000], Bound: 0.3966790437698364, Entropy: 140.17056274414062, Temp: 2.697662353515625, KL: 77.04667663574219, Loss: 0.016697635874152184, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13089/20000], Bound: 0.41957637667655945, Entropy: 141.27374267578125, Temp: 2.697662115097046, KL: 84.8309326171875, Loss: 0.015052112750709057, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13090/20000], Bound: 0.38831382989883423, Entropy: 140.4760284423828, Temp: 2.6976659297943115, KL: 75.24510192871094, Loss: 0.01547479722648859, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13091/20000], Bound: 0.37201640009880066, Entropy: 141.73158264160156, Temp: 2.6976704597473145, KL: 70.63534545898438, Loss: 0.015289001166820526, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13092/20000], Bound: 0.3927710950374603, Entropy: 141.5452880859375, Temp: 2.6976754665374756, KL: 76.30593872070312, Loss: 0.015932517126202583, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13093/20000], Bound: 0.3771533668041229, Entropy: 140.5294952392578, Temp: 2.697680950164795, KL: 71.48466491699219, Loss: 0.016444452106952667, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13094/20000], Bound: 0.3750936985015869, Entropy: 140.12525939941406, Temp: 2.697685480117798, KL: 70.35929870605469, Loss: 0.017433471977710724, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13095/20000], Bound: 0.39228159189224243, Entropy: 141.4975128173828, Temp: 2.6976876258850098, KL: 76.30015563964844, Loss: 0.015676377341151237, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13096/20000], Bound: 0.3893505334854126, Entropy: 139.42832946777344, Temp: 2.6976912021636963, KL: 74.45773315429688, Loss: 0.01749669387936592, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13097/20000], Bound: 0.38371655344963074, Entropy: 139.69639587402344, Temp: 2.697693347930908, KL: 75.07246398925781, Loss: 0.013311638496816158, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13098/20000], Bound: 0.3995820879936218, Entropy: 140.7793426513672, Temp: 2.6976993083953857, KL: 77.91705322265625, Loss: 0.016681235283613205, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13099/20000], Bound: 0.3792605400085449, Entropy: 144.22303771972656, Temp: 2.697705030441284, KL: 71.48121643066406, Loss: 0.017576515674591064, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13100/20000], Bound: 0.41258519887924194, Entropy: 139.86561584472656, Temp: 2.6977086067199707, KL: 81.68910217285156, Loss: 0.016926102340221405, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13101/20000], Bound: 0.3919294774532318, Entropy: 140.84375, Temp: 2.6977128982543945, KL: 74.70285034179688, Loss: 0.018445217981934547, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13102/20000], Bound: 0.3891162574291229, Entropy: 141.621826171875, Temp: 2.6977150440216064, KL: 75.36566162109375, Loss: 0.0156870074570179, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13103/20000], Bound: 0.3845781981945038, Entropy: 141.58058166503906, Temp: 2.6977181434631348, KL: 73.55628967285156, Loss: 0.016586171463131905, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13104/20000], Bound: 0.3704510033130646, Entropy: 141.52528381347656, Temp: 2.697720766067505, KL: 70.45834350585938, Loss: 0.014789707027375698, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13105/20000], Bound: 0.3679892420768738, Entropy: 141.9955291748047, Temp: 2.6977243423461914, KL: 69.32145690917969, Loss: 0.015598793514072895, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13106/20000], Bound: 0.35260334610939026, Entropy: 140.6789093017578, Temp: 2.6977274417877197, KL: 65.67816162109375, Loss: 0.014340023510158062, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13107/20000], Bound: 0.39357906579971313, Entropy: 140.54733276367188, Temp: 2.697730779647827, KL: 77.55802917480469, Loss: 0.01405345182865858, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13108/20000], Bound: 0.37191614508628845, Entropy: 141.65444946289062, Temp: 2.697737216949463, KL: 69.54972839355469, Loss: 0.017248596996068954, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13109/20000], Bound: 0.41976073384284973, Entropy: 139.95046997070312, Temp: 2.6977412700653076, KL: 84.422119140625, Loss: 0.01591545157134533, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13110/20000], Bound: 0.37272533774375916, Entropy: 140.93804931640625, Temp: 2.6977477073669434, KL: 68.94706726074219, Loss: 0.018794188275933266, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13111/20000], Bound: 0.3986927270889282, Entropy: 141.35816955566406, Temp: 2.6977498531341553, KL: 78.69776916503906, Loss: 0.014744969084858894, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13112/20000], Bound: 0.39933648705482483, Entropy: 141.86672973632812, Temp: 2.6977548599243164, KL: 77.13023376464844, Loss: 0.018004748970270157, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13113/20000], Bound: 0.36447596549987793, Entropy: 141.25816345214844, Temp: 2.697758436203003, KL: 66.85313415527344, Loss: 0.01832919754087925, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13114/20000], Bound: 0.37360137701034546, Entropy: 140.6202392578125, Temp: 2.697758197784424, KL: 70.5010986328125, Loss: 0.01637856848537922, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13115/20000], Bound: 0.40256428718566895, Entropy: 142.68731689453125, Temp: 2.6977572441101074, KL: 76.50251770019531, Loss: 0.020950565114617348, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13116/20000], Bound: 0.3885347247123718, Entropy: 140.95498657226562, Temp: 2.6977522373199463, KL: 76.25440979003906, Loss: 0.013724714517593384, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13117/20000], Bound: 0.36217787861824036, Entropy: 141.28465270996094, Temp: 2.697751522064209, KL: 67.32586669921875, Loss: 0.016251329332590103, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13118/20000], Bound: 0.3747669458389282, Entropy: 141.27867126464844, Temp: 2.697749376296997, KL: 71.70022583007812, Loss: 0.014774979092180729, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13119/20000], Bound: 0.35586413741111755, Entropy: 141.81216430664062, Temp: 2.697749137878418, KL: 66.55410766601562, Loss: 0.014400173909962177, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13120/20000], Bound: 0.36722084879875183, Entropy: 143.3826141357422, Temp: 2.697749137878418, KL: 69.13240051269531, Loss: 0.015545165166258812, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13121/20000], Bound: 0.39092350006103516, Entropy: 140.7206573486328, Temp: 2.697749376296997, KL: 74.25108337402344, Loss: 0.01873505488038063, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13122/20000], Bound: 0.3879780173301697, Entropy: 140.3454132080078, Temp: 2.6977474689483643, KL: 75.15238952636719, Loss: 0.015465470030903816, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13123/20000], Bound: 0.38250404596328735, Entropy: 141.63238525390625, Temp: 2.697746992111206, KL: 72.20501708984375, Loss: 0.017974477261304855, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13124/20000], Bound: 0.4014710783958435, Entropy: 140.91897583007812, Temp: 2.697744846343994, KL: 77.56532287597656, Loss: 0.018376026302576065, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13125/20000], Bound: 0.3805432915687561, Entropy: 141.2779998779297, Temp: 2.6977415084838867, KL: 72.65138244628906, Loss: 0.01609482616186142, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13126/20000], Bound: 0.38925081491470337, Entropy: 141.21392822265625, Temp: 2.6977386474609375, KL: 74.66561889648438, Loss: 0.017057690769433975, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13127/20000], Bound: 0.37188175320625305, Entropy: 141.6602020263672, Temp: 2.697735548019409, KL: 70.89227294921875, Loss: 0.014742087572813034, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13128/20000], Bound: 0.3697768449783325, Entropy: 139.70249938964844, Temp: 2.6977343559265137, KL: 70.16984558105469, Loss: 0.014968588016927242, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13129/20000], Bound: 0.40157225728034973, Entropy: 141.7493896484375, Temp: 2.6977338790893555, KL: 78.56907653808594, Loss: 0.016571469604969025, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13130/20000], Bound: 0.3823394477367401, Entropy: 139.81517028808594, Temp: 2.6977345943450928, KL: 73.15336608886719, Loss: 0.016128234565258026, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13131/20000], Bound: 0.41587942838668823, Entropy: 139.33177185058594, Temp: 2.697735548019409, KL: 83.09771728515625, Loss: 0.016171468421816826, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13132/20000], Bound: 0.36545395851135254, Entropy: 141.1813201904297, Temp: 2.6977386474609375, KL: 69.33341979980469, Loss: 0.014244682155549526, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13133/20000], Bound: 0.3909769058227539, Entropy: 140.23297119140625, Temp: 2.6977427005767822, KL: 74.56092834472656, Loss: 0.0181898046284914, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13134/20000], Bound: 0.3829464316368103, Entropy: 143.7263641357422, Temp: 2.697744846343994, KL: 74.29579162597656, Loss: 0.014337246306240559, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13135/20000], Bound: 0.3768223822116852, Entropy: 140.00428771972656, Temp: 2.697749137878418, KL: 71.93659973144531, Loss: 0.015430922619998455, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13136/20000], Bound: 0.39150315523147583, Entropy: 140.49099731445312, Temp: 2.69775390625, KL: 75.48843383789062, Loss: 0.01675727777183056, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13137/20000], Bound: 0.41253960132598877, Entropy: 140.2819061279297, Temp: 2.697758197784424, KL: 82.62870788574219, Loss: 0.01515957061201334, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13138/20000], Bound: 0.3612571060657501, Entropy: 138.53260803222656, Temp: 2.697765350341797, KL: 68.43508911132812, Loss: 0.013715259730815887, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13139/20000], Bound: 0.39321646094322205, Entropy: 138.98789978027344, Temp: 2.6977734565734863, KL: 74.47358703613281, Loss: 0.01957250013947487, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13140/20000], Bound: 0.36985066533088684, Entropy: 141.6044921875, Temp: 2.69777774810791, KL: 68.33282470703125, Loss: 0.018412601202726364, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13141/20000], Bound: 0.3793247938156128, Entropy: 140.61212158203125, Temp: 2.6977784633636475, KL: 72.29405212402344, Loss: 0.016104986891150475, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13142/20000], Bound: 0.38050395250320435, Entropy: 142.09591674804688, Temp: 2.6977789402008057, KL: 73.24079895019531, Loss: 0.014981692656874657, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13143/20000], Bound: 0.381888747215271, Entropy: 138.9110870361328, Temp: 2.6977810859680176, KL: 73.98703002929688, Loss: 0.01434148009866476, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13144/20000], Bound: 0.39423203468322754, Entropy: 140.15670776367188, Temp: 2.6977851390838623, KL: 75.52293395996094, Loss: 0.01818261295557022, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13145/20000], Bound: 0.3730120360851288, Entropy: 141.16062927246094, Temp: 2.6977877616882324, KL: 71.71931457519531, Loss: 0.013808473013341427, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13146/20000], Bound: 0.3722234070301056, Entropy: 141.5924530029297, Temp: 2.6977922916412354, KL: 68.05369567871094, Loss: 0.020184364169836044, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13147/20000], Bound: 0.3901766538619995, Entropy: 140.91049194335938, Temp: 2.697791337966919, KL: 75.17098999023438, Loss: 0.016624372452497482, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13148/20000], Bound: 0.3575463891029358, Entropy: 143.81793212890625, Temp: 2.6977903842926025, KL: 66.43060302734375, Loss: 0.015500874258577824, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13149/20000], Bound: 0.38481736183166504, Entropy: 141.10601806640625, Temp: 2.697788953781128, KL: 73.53175354003906, Loss: 0.016761204227805138, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13150/20000], Bound: 0.4322453439235687, Entropy: 138.66929626464844, Temp: 2.6977875232696533, KL: 86.79562377929688, Loss: 0.018678469583392143, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13151/20000], Bound: 0.38803598284721375, Entropy: 141.0150909423828, Temp: 2.697786569595337, KL: 75.58612060546875, Loss: 0.014693362638354301, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13152/20000], Bound: 0.36202335357666016, Entropy: 140.82763671875, Temp: 2.6977880001068115, KL: 67.51675415039062, Loss: 0.015817152336239815, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13153/20000], Bound: 0.3849974274635315, Entropy: 140.60675048828125, Temp: 2.6977884769439697, KL: 74.48068237304688, Loss: 0.015099589712917805, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13154/20000], Bound: 0.38031870126724243, Entropy: 139.27072143554688, Temp: 2.6977908611297607, KL: 72.42318725585938, Loss: 0.016397861763834953, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13155/20000], Bound: 0.36634835600852966, Entropy: 141.4083709716797, Temp: 2.6977925300598145, KL: 69.10302734375, Loss: 0.015141505748033524, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13156/20000], Bound: 0.37405925989151, Entropy: 138.85107421875, Temp: 2.6977946758270264, KL: 71.18571472167969, Loss: 0.01535305567085743, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13157/20000], Bound: 0.3720846474170685, Entropy: 142.21316528320312, Temp: 2.6977970600128174, KL: 70.25994873046875, Loss: 0.0160219706594944, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13158/20000], Bound: 0.38803187012672424, Entropy: 140.81797790527344, Temp: 2.69779896736145, KL: 75.17919921875, Loss: 0.015445408411324024, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13159/20000], Bound: 0.39406684041023254, Entropy: 141.85498046875, Temp: 2.6978023052215576, KL: 75.19677734375, Loss: 0.018696943297982216, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13160/20000], Bound: 0.369041383266449, Entropy: 141.9093475341797, Temp: 2.697803258895874, KL: 68.76695251464844, Loss: 0.01718134433031082, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13161/20000], Bound: 0.39845871925354004, Entropy: 141.7972869873047, Temp: 2.6978023052215576, KL: 74.744384765625, Loss: 0.021943790838122368, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13162/20000], Bound: 0.37279748916625977, Entropy: 142.14913940429688, Temp: 2.6977953910827637, KL: 70.4375, Loss: 0.016070464625954628, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13163/20000], Bound: 0.4103635549545288, Entropy: 141.38461303710938, Temp: 2.697789192199707, KL: 82.18496704101562, Loss: 0.014761602506041527, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13164/20000], Bound: 0.3805098831653595, Entropy: 141.9853057861328, Temp: 2.697787284851074, KL: 72.97210693359375, Loss: 0.01548290066421032, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13165/20000], Bound: 0.35492178797721863, Entropy: 141.51731872558594, Temp: 2.697786331176758, KL: 66.3995361328125, Loss: 0.01419966109097004, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13166/20000], Bound: 0.3756884038448334, Entropy: 140.3847198486328, Temp: 2.697786331176758, KL: 71.41880798339844, Loss: 0.015786930918693542, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13167/20000], Bound: 0.39664149284362793, Entropy: 141.25936889648438, Temp: 2.697786808013916, KL: 77.43548583984375, Loss: 0.01595759578049183, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13168/20000], Bound: 0.36220645904541016, Entropy: 144.35357666015625, Temp: 2.6977884769439697, KL: 66.89067077636719, Loss: 0.017073124647140503, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13169/20000], Bound: 0.38548582792282104, Entropy: 142.38771057128906, Temp: 2.6977877616882324, KL: 72.88388061523438, Loss: 0.018322592601180077, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13170/20000], Bound: 0.39462020993232727, Entropy: 140.07359313964844, Temp: 2.697784900665283, KL: 75.74745178222656, Loss: 0.017978781834244728, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13171/20000], Bound: 0.3895045816898346, Entropy: 141.61676025390625, Temp: 2.6977810859680176, KL: 75.94694519042969, Loss: 0.014821040444076061, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13172/20000], Bound: 0.37249380350112915, Entropy: 140.6722869873047, Temp: 2.697780132293701, KL: 68.42582702636719, Loss: 0.019637826830148697, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13173/20000], Bound: 0.3890141248703003, Entropy: 141.03277587890625, Temp: 2.697774648666382, KL: 73.6024169921875, Loss: 0.01890009082853794, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13174/20000], Bound: 0.38924160599708557, Entropy: 139.83543395996094, Temp: 2.6977670192718506, KL: 73.55288696289062, Loss: 0.019115300849080086, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13175/20000], Bound: 0.37456315755844116, Entropy: 141.59814453125, Temp: 2.6977572441101074, KL: 71.6697998046875, Loss: 0.014723182655870914, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13176/20000], Bound: 0.3608268201351166, Entropy: 140.52821350097656, Temp: 2.6977498531341553, KL: 68.10020446777344, Loss: 0.014111528173089027, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13177/20000], Bound: 0.3674836754798889, Entropy: 141.82211303710938, Temp: 2.697744846343994, KL: 69.25405883789062, Loss: 0.01545787788927555, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13178/20000], Bound: 0.3736223578453064, Entropy: 142.27981567382812, Temp: 2.697740077972412, KL: 70.51580810546875, Loss: 0.016362283378839493, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13179/20000], Bound: 0.3788294792175293, Entropy: 139.6503448486328, Temp: 2.697735548019409, KL: 72.96676635742188, Loss: 0.014592894352972507, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13180/20000], Bound: 0.3920619785785675, Entropy: 141.42388916015625, Temp: 2.697732925415039, KL: 74.656494140625, Loss: 0.0186034943908453, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13181/20000], Bound: 0.37020358443260193, Entropy: 140.353515625, Temp: 2.6977286338806152, KL: 69.76956176757812, Loss: 0.01593570038676262, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13182/20000], Bound: 0.4035526514053345, Entropy: 139.99365234375, Temp: 2.6977248191833496, KL: 78.98844909667969, Loss: 0.01689033769071102, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13183/20000], Bound: 0.38955092430114746, Entropy: 140.8426971435547, Temp: 2.6977219581604004, KL: 75.10342407226562, Loss: 0.016409045085310936, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13184/20000], Bound: 0.37571367621421814, Entropy: 142.60824584960938, Temp: 2.6977195739746094, KL: 69.87588500976562, Loss: 0.01865948550403118, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13185/20000], Bound: 0.40391185879707336, Entropy: 142.7431182861328, Temp: 2.697714328765869, KL: 79.02047729492188, Loss: 0.017030004411935806, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13186/20000], Bound: 0.3973953127861023, Entropy: 142.82492065429688, Temp: 2.6977100372314453, KL: 77.84988403320312, Loss: 0.015602665953338146, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13187/20000], Bound: 0.3686942756175995, Entropy: 140.0193328857422, Temp: 2.6977081298828125, KL: 70.66757202148438, Loss: 0.013475022278726101, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13188/20000], Bound: 0.4207104444503784, Entropy: 140.57894897460938, Temp: 2.697709083557129, KL: 84.71640014648438, Loss: 0.015909595414996147, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13189/20000], Bound: 0.4152836501598358, Entropy: 140.7719268798828, Temp: 2.6977128982543945, KL: 82.28704833984375, Loss: 0.017337432131171227, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13190/20000], Bound: 0.4105204641819, Entropy: 139.62290954589844, Temp: 2.6977169513702393, KL: 81.30903625488281, Loss: 0.016472160816192627, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13191/20000], Bound: 0.35379505157470703, Entropy: 143.05618286132812, Temp: 2.6977224349975586, KL: 65.14459228515625, Loss: 0.01594327948987484, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13192/20000], Bound: 0.3878297805786133, Entropy: 141.9102020263672, Temp: 2.697726011276245, KL: 74.59156799316406, Loss: 0.016424376517534256, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13193/20000], Bound: 0.3887822926044464, Entropy: 140.6413116455078, Temp: 2.6977293491363525, KL: 75.07493591308594, Loss: 0.016044791787862778, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13194/20000], Bound: 0.3710615038871765, Entropy: 140.6941680908203, Temp: 2.697733163833618, KL: 69.84840393066406, Loss: 0.016242889687418938, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13195/20000], Bound: 0.383205771446228, Entropy: 141.33261108398438, Temp: 2.6977360248565674, KL: 74.60139465332031, Loss: 0.013910223729908466, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13196/20000], Bound: 0.3873522877693176, Entropy: 141.61521911621094, Temp: 2.697741746902466, KL: 74.83351135253906, Loss: 0.01571759395301342, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13197/20000], Bound: 0.37969970703125, Entropy: 141.6885986328125, Temp: 2.6977477073669434, KL: 72.46502685546875, Loss: 0.015988465398550034, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13198/20000], Bound: 0.38372576236724854, Entropy: 138.84950256347656, Temp: 2.697753429412842, KL: 73.434326171875, Loss: 0.01635330729186535, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13199/20000], Bound: 0.3784157633781433, Entropy: 142.29905700683594, Temp: 2.697758674621582, KL: 72.39741516113281, Loss: 0.015427276492118835, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13200/20000], Bound: 0.3766498863697052, Entropy: 141.10350036621094, Temp: 2.6977643966674805, KL: 72.0843505859375, Loss: 0.015065259300172329, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13201/20000], Bound: 0.40447989106178284, Entropy: 142.52978515625, Temp: 2.697770595550537, KL: 79.66107177734375, Loss: 0.01615852862596512, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13202/20000], Bound: 0.38942286372184753, Entropy: 141.7602081298828, Temp: 2.69777774810791, KL: 76.65980529785156, Loss: 0.013455438427627087, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13203/20000], Bound: 0.37625280022621155, Entropy: 142.01937866210938, Temp: 2.6977882385253906, KL: 71.54354858398438, Loss: 0.015856267884373665, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13204/20000], Bound: 0.3819998800754547, Entropy: 142.73133850097656, Temp: 2.6977977752685547, KL: 74.84701538085938, Loss: 0.012807443737983704, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13205/20000], Bound: 0.36987701058387756, Entropy: 143.222900390625, Temp: 2.697810649871826, KL: 69.50724792480469, Loss: 0.01625015400350094, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13206/20000], Bound: 0.36397668719291687, Entropy: 141.59933471679688, Temp: 2.697821617126465, KL: 69.39335632324219, Loss: 0.013360349461436272, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13207/20000], Bound: 0.38250070810317993, Entropy: 143.7014923095703, Temp: 2.697833776473999, KL: 72.85362243652344, Loss: 0.01677127741277218, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13208/20000], Bound: 0.38082173466682434, Entropy: 140.74229431152344, Temp: 2.6978442668914795, KL: 72.83012390136719, Loss: 0.015913689509034157, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13209/20000], Bound: 0.3948464095592499, Entropy: 140.92291259765625, Temp: 2.6978540420532227, KL: 77.00286865234375, Loss: 0.01577644795179367, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13210/20000], Bound: 0.3982073962688446, Entropy: 140.33445739746094, Temp: 2.697864294052124, KL: 78.98905944824219, Loss: 0.013939275406301022, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13211/20000], Bound: 0.3794950246810913, Entropy: 141.39036560058594, Temp: 2.6978776454925537, KL: 69.59068298339844, Loss: 0.021207110956311226, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13212/20000], Bound: 0.38389989733695984, Entropy: 141.62872314453125, Temp: 2.697883367538452, KL: 73.73759460449219, Loss: 0.01588614471256733, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13213/20000], Bound: 0.39927342534065247, Entropy: 142.5316925048828, Temp: 2.6978890895843506, KL: 77.38694763183594, Loss: 0.017495445907115936, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13214/20000], Bound: 0.41415539383888245, Entropy: 141.77024841308594, Temp: 2.6978940963745117, KL: 83.52455139160156, Loss: 0.014409811235964298, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13215/20000], Bound: 0.3880005180835724, Entropy: 141.79359436035156, Temp: 2.6979026794433594, KL: 75.37091064453125, Loss: 0.015074096620082855, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13216/20000], Bound: 0.3864630460739136, Entropy: 144.52658081054688, Temp: 2.6979124546051025, KL: 74.47566223144531, Loss: 0.015901407226920128, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13217/20000], Bound: 0.3715038299560547, Entropy: 140.6149139404297, Temp: 2.6979219913482666, KL: 68.08912658691406, Loss: 0.019738774746656418, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13218/20000], Bound: 0.3884466886520386, Entropy: 141.2951202392578, Temp: 2.6979258060455322, KL: 75.73626708984375, Loss: 0.01463902648538351, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13219/20000], Bound: 0.4063071310520172, Entropy: 142.0753173828125, Temp: 2.6979315280914307, KL: 79.61372375488281, Loss: 0.017263635993003845, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13220/20000], Bound: 0.38157424330711365, Entropy: 141.24476623535156, Temp: 2.69793701171875, KL: 72.90277099609375, Loss: 0.016183536499738693, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13221/20000], Bound: 0.35569334030151367, Entropy: 142.97833251953125, Temp: 2.6979422569274902, KL: 66.68077087402344, Loss: 0.01407859567552805, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13222/20000], Bound: 0.3981642723083496, Entropy: 138.84381103515625, Temp: 2.6979479789733887, KL: 77.83099365234375, Loss: 0.01606263406574726, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13223/20000], Bound: 0.41747963428497314, Entropy: 139.19076538085938, Temp: 2.6979544162750244, KL: 83.23446655273438, Loss: 0.016825266182422638, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13224/20000], Bound: 0.39597180485725403, Entropy: 141.23080444335938, Temp: 2.6979620456695557, KL: 78.12840270996094, Loss: 0.014307960867881775, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13225/20000], Bound: 0.36033207178115845, Entropy: 140.7129669189453, Temp: 2.697972059249878, KL: 68.00579833984375, Loss: 0.01403063628822565, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13226/20000], Bound: 0.4061751961708069, Entropy: 141.5160675048828, Temp: 2.6979825496673584, KL: 80.41908264160156, Loss: 0.015698155388236046, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13227/20000], Bound: 0.40881186723709106, Entropy: 138.8063507080078, Temp: 2.6979939937591553, KL: 81.59458923339844, Loss: 0.014990021474659443, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13228/20000], Bound: 0.3777760863304138, Entropy: 142.62193298339844, Temp: 2.698007822036743, KL: 72.20083618164062, Loss: 0.015452198684215546, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13229/20000], Bound: 0.38140425086021423, Entropy: 140.57080078125, Temp: 2.6980209350585938, KL: 72.61431884765625, Loss: 0.016627585515379906, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13230/20000], Bound: 0.38975006341934204, Entropy: 142.09251403808594, Temp: 2.6980323791503906, KL: 72.75028991699219, Loss: 0.02088085375726223, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13231/20000], Bound: 0.35461440682411194, Entropy: 142.54006958007812, Temp: 2.698037624359131, KL: 64.28549194335938, Loss: 0.01796060800552368, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13232/20000], Bound: 0.37155500054359436, Entropy: 141.18836975097656, Temp: 2.6980388164520264, KL: 70.83258056640625, Loss: 0.014682459644973278, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13233/20000], Bound: 0.4008377194404602, Entropy: 140.9374237060547, Temp: 2.6980409622192383, KL: 77.58723449707031, Loss: 0.017988327890634537, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13234/20000], Bound: 0.3792664408683777, Entropy: 140.7207794189453, Temp: 2.698042392730713, KL: 73.23493957519531, Loss: 0.014332403428852558, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13235/20000], Bound: 0.3918801248073578, Entropy: 141.7244110107422, Temp: 2.6980457305908203, KL: 77.30876159667969, Loss: 0.013591906987130642, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13236/20000], Bound: 0.3737970292568207, Entropy: 140.97462463378906, Temp: 2.6980526447296143, KL: 69.66947937011719, Loss: 0.01802595518529415, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13237/20000], Bound: 0.3897610604763031, Entropy: 141.53347778320312, Temp: 2.6980559825897217, KL: 76.08168029785156, Loss: 0.014713325537741184, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13238/20000], Bound: 0.3713286817073822, Entropy: 140.79400634765625, Temp: 2.69806170463562, KL: 69.8831787109375, Loss: 0.016322394832968712, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13239/20000], Bound: 0.3797268271446228, Entropy: 143.91085815429688, Temp: 2.698065996170044, KL: 71.78530883789062, Loss: 0.01726541668176651, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13240/20000], Bound: 0.37549853324890137, Entropy: 142.43460083007812, Temp: 2.698068618774414, KL: 71.90338134765625, Loss: 0.014790345914661884, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13241/20000], Bound: 0.36587613821029663, Entropy: 143.35330200195312, Temp: 2.6980724334716797, KL: 68.55619812011719, Loss: 0.015909332782030106, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13242/20000], Bound: 0.3668827414512634, Entropy: 142.24691772460938, Temp: 2.698075532913208, KL: 70.06475830078125, Loss: 0.013642306439578533, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13243/20000], Bound: 0.3973146975040436, Entropy: 142.21096801757812, Temp: 2.69808030128479, KL: 76.98695373535156, Loss: 0.01716124638915062, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13244/20000], Bound: 0.40485525131225586, Entropy: 140.7211151123047, Temp: 2.698084831237793, KL: 81.13340759277344, Loss: 0.013641652651131153, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13245/20000], Bound: 0.40413036942481995, Entropy: 142.0016632080078, Temp: 2.6980931758880615, KL: 78.14634704589844, Loss: 0.01877487637102604, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13246/20000], Bound: 0.4263650178909302, Entropy: 141.0817108154297, Temp: 2.698099374771118, KL: 86.49273681640625, Loss: 0.015853140503168106, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13247/20000], Bound: 0.3792535662651062, Entropy: 140.63661193847656, Temp: 2.698108434677124, KL: 72.55134582519531, Loss: 0.015592972747981548, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13248/20000], Bound: 0.388594388961792, Entropy: 144.11756896972656, Temp: 2.6981170177459717, KL: 75.27230834960938, Loss: 0.015580756589770317, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13249/20000], Bound: 0.3857371509075165, Entropy: 141.28611755371094, Temp: 2.6981260776519775, KL: 73.38883972167969, Loss: 0.017525291070342064, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13250/20000], Bound: 0.3930073380470276, Entropy: 139.5653533935547, Temp: 2.6981329917907715, KL: 77.24897766113281, Loss: 0.014318220317363739, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13251/20000], Bound: 0.3824782073497772, Entropy: 140.79296875, Temp: 2.6981422901153564, KL: 74.28971862792969, Loss: 0.01410058606415987, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13252/20000], Bound: 0.347017765045166, Entropy: 142.12156677246094, Temp: 2.698153495788574, KL: 63.97625732421875, Loss: 0.014631388708949089, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13253/20000], Bound: 0.3798002600669861, Entropy: 141.53961181640625, Temp: 2.6981632709503174, KL: 72.0325927734375, Loss: 0.016847293823957443, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13254/20000], Bound: 0.411501407623291, Entropy: 140.90077209472656, Temp: 2.6981711387634277, KL: 80.29344177246094, Loss: 0.018908798694610596, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13255/20000], Bound: 0.3786414861679077, Entropy: 140.99595642089844, Temp: 2.6981773376464844, KL: 70.52833557128906, Loss: 0.01901520974934101, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13256/20000], Bound: 0.3691367506980896, Entropy: 142.067626953125, Temp: 2.698179244995117, KL: 69.91636657714844, Loss: 0.015104529447853565, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13257/20000], Bound: 0.4104582369327545, Entropy: 140.95765686035156, Temp: 2.6981818675994873, KL: 81.41696166992188, Loss: 0.01624220423400402, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13258/20000], Bound: 0.367399126291275, Entropy: 140.267822265625, Temp: 2.698185682296753, KL: 69.96910095214844, Loss: 0.014092009514570236, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13259/20000], Bound: 0.3815351724624634, Entropy: 141.5867919921875, Temp: 2.6981911659240723, KL: 71.61761474609375, Loss: 0.01854630559682846, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13260/20000], Bound: 0.39135804772377014, Entropy: 141.1973114013672, Temp: 2.6981935501098633, KL: 74.33641052246094, Loss: 0.01881713978946209, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13261/20000], Bound: 0.38212549686431885, Entropy: 142.0748748779297, Temp: 2.698193073272705, KL: 72.03602600097656, Loss: 0.01808788813650608, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13262/20000], Bound: 0.39736899733543396, Entropy: 143.41793823242188, Temp: 2.698190689086914, KL: 76.3818359375, Loss: 0.018313445150852203, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13263/20000], Bound: 0.3830048143863678, Entropy: 143.57568359375, Temp: 2.6981871128082275, KL: 73.1888427734375, Loss: 0.016424130648374557, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13264/20000], Bound: 0.3581041693687439, Entropy: 144.9826202392578, Temp: 2.69818377494812, KL: 65.58917236328125, Loss: 0.0173525158315897, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13265/20000], Bound: 0.43664130568504333, Entropy: 139.97547912597656, Temp: 2.6981780529022217, KL: 87.44319152832031, Loss: 0.020037483423948288, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13266/20000], Bound: 0.37815019488334656, Entropy: 139.38339233398438, Temp: 2.698171854019165, KL: 72.3623046875, Loss: 0.015354165807366371, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13267/20000], Bound: 0.37031352519989014, Entropy: 141.86819458007812, Temp: 2.698167562484741, KL: 70.92221069335938, Loss: 0.013861380517482758, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13268/20000], Bound: 0.4118683338165283, Entropy: 140.26583862304688, Temp: 2.6981656551361084, KL: 82.28852844238281, Loss: 0.015417533926665783, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13269/20000], Bound: 0.39437299966812134, Entropy: 141.93276977539062, Temp: 2.698167085647583, KL: 77.56857299804688, Loss: 0.014472234062850475, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13270/20000], Bound: 0.38802939653396606, Entropy: 139.17930603027344, Temp: 2.6981711387634277, KL: 76.38238525390625, Loss: 0.013217992149293423, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13271/20000], Bound: 0.3904268741607666, Entropy: 141.628662109375, Temp: 2.698178768157959, KL: 76.10589599609375, Loss: 0.015031459741294384, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13272/20000], Bound: 0.4037606418132782, Entropy: 142.0063018798828, Temp: 2.698188066482544, KL: 79.83970642089844, Loss: 0.015432733111083508, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13273/20000], Bound: 0.41692274808883667, Entropy: 139.21072387695312, Temp: 2.6981983184814453, KL: 83.92349243164062, Loss: 0.015235957689583302, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13274/20000], Bound: 0.3874014914035797, Entropy: 140.49452209472656, Temp: 2.698211431503296, KL: 73.90777587890625, Loss: 0.01746402122080326, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13275/20000], Bound: 0.3936355412006378, Entropy: 141.25497436523438, Temp: 2.6982219219207764, KL: 76.95344543457031, Loss: 0.015209706500172615, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13276/20000], Bound: 0.38033345341682434, Entropy: 141.68994140625, Temp: 2.6982336044311523, KL: 72.63543701171875, Loss: 0.01601625233888626, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13277/20000], Bound: 0.4046572744846344, Entropy: 141.9603729248047, Temp: 2.698244333267212, KL: 81.27294921875, Loss: 0.013274921104311943, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13278/20000], Bound: 0.3760682940483093, Entropy: 141.48524475097656, Temp: 2.698258876800537, KL: 71.24784851074219, Loss: 0.016309984028339386, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13279/20000], Bound: 0.37683042883872986, Entropy: 139.78060913085938, Temp: 2.6982715129852295, KL: 73.13655090332031, Loss: 0.013216241262853146, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13280/20000], Bound: 0.38091614842414856, Entropy: 139.798095703125, Temp: 2.698286533355713, KL: 74.22418212890625, Loss: 0.013385037891566753, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13281/20000], Bound: 0.37941664457321167, Entropy: 140.9630584716797, Temp: 2.69830322265625, KL: 72.46963500976562, Loss: 0.015833303332328796, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13282/20000], Bound: 0.38969382643699646, Entropy: 142.67591857910156, Temp: 2.6983187198638916, KL: 73.90501403808594, Loss: 0.018712788820266724, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13283/20000], Bound: 0.4093343913555145, Entropy: 142.614501953125, Temp: 2.6983301639556885, KL: 80.18199157714844, Loss: 0.017903264611959457, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13284/20000], Bound: 0.3966023623943329, Entropy: 140.4921417236328, Temp: 2.69834041595459, KL: 76.92210388183594, Loss: 0.016892854124307632, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13285/20000], Bound: 0.40949925780296326, Entropy: 140.49298095703125, Temp: 2.698349952697754, KL: 79.60540771484375, Loss: 0.019064094871282578, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13286/20000], Bound: 0.37636393308639526, Entropy: 141.5450439453125, Temp: 2.698357105255127, KL: 70.60029602050781, Loss: 0.01766812801361084, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13287/20000], Bound: 0.42482179403305054, Entropy: 139.90101623535156, Temp: 2.69836163520813, KL: 86.99388122558594, Loss: 0.014043133705854416, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13288/20000], Bound: 0.42209896445274353, Entropy: 141.75326538085938, Temp: 2.6983706951141357, KL: 84.62165832519531, Loss: 0.01688343472778797, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13289/20000], Bound: 0.35865941643714905, Entropy: 143.64535522460938, Temp: 2.698380947113037, KL: 65.083740234375, Loss: 0.01857871189713478, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13290/20000], Bound: 0.3788914978504181, Entropy: 140.5262908935547, Temp: 2.6983859539031982, KL: 71.05744934082031, Loss: 0.018169941380620003, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13291/20000], Bound: 0.39688727259635925, Entropy: 141.10174560546875, Temp: 2.698387861251831, KL: 77.69345092773438, Loss: 0.015620386227965355, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13292/20000], Bound: 0.3463398218154907, Entropy: 143.02963256835938, Temp: 2.6983914375305176, KL: 63.40049743652344, Loss: 0.01535370759665966, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13293/20000], Bound: 0.3880545496940613, Entropy: 142.32630920410156, Temp: 2.6983935832977295, KL: 74.3900146484375, Loss: 0.016925666481256485, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13294/20000], Bound: 0.4007570743560791, Entropy: 141.489501953125, Temp: 2.698395252227783, KL: 77.771484375, Loss: 0.017605692148208618, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13295/20000], Bound: 0.3857851028442383, Entropy: 141.8863983154297, Temp: 2.6983962059020996, KL: 74.73860168457031, Loss: 0.01505245640873909, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13296/20000], Bound: 0.3854403793811798, Entropy: 139.67323303222656, Temp: 2.698399066925049, KL: 72.35818481445312, Loss: 0.019277162849903107, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13297/20000], Bound: 0.3733302056789398, Entropy: 142.0592803955078, Temp: 2.6983981132507324, KL: 70.04249572753906, Loss: 0.017089787870645523, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13298/20000], Bound: 0.3691299557685852, Entropy: 141.3948974609375, Temp: 2.6983959674835205, KL: 70.54127502441406, Loss: 0.01394486054778099, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13299/20000], Bound: 0.37970006465911865, Entropy: 139.94032287597656, Temp: 2.6983959674835205, KL: 72.40475463867188, Loss: 0.016105996444821358, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13300/20000], Bound: 0.3857131600379944, Entropy: 142.5777130126953, Temp: 2.6983959674835205, KL: 74.44444274902344, Loss: 0.01555866189301014, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13301/20000], Bound: 0.3988894820213318, Entropy: 138.94293212890625, Temp: 2.698396921157837, KL: 78.26214599609375, Loss: 0.01566714607179165, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13302/20000], Bound: 0.38328152894973755, Entropy: 140.3853759765625, Temp: 2.698399782180786, KL: 73.85395812988281, Loss: 0.015342382714152336, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13303/20000], Bound: 0.36813026666641235, Entropy: 142.93946838378906, Temp: 2.6984035968780518, KL: 70.38763427734375, Loss: 0.013703003525733948, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13304/20000], Bound: 0.3756268322467804, Entropy: 143.12660217285156, Temp: 2.6984095573425293, KL: 70.41220092773438, Loss: 0.017624713480472565, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13305/20000], Bound: 0.4003964364528656, Entropy: 140.6243133544922, Temp: 2.6984126567840576, KL: 79.10012817382812, Loss: 0.014944955706596375, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13306/20000], Bound: 0.41300973296165466, Entropy: 140.68296813964844, Temp: 2.698418140411377, KL: 82.91183471679688, Loss: 0.014906506054103374, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13307/20000], Bound: 0.4071560502052307, Entropy: 140.73513793945312, Temp: 2.6984267234802246, KL: 82.12800598144531, Loss: 0.01308250892907381, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13308/20000], Bound: 0.37908869981765747, Entropy: 138.94935607910156, Temp: 2.698439836502075, KL: 71.35470581054688, Loss: 0.01772499829530716, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13309/20000], Bound: 0.380228728055954, Entropy: 141.32522583007812, Temp: 2.6984498500823975, KL: 72.68281555175781, Loss: 0.015874234959483147, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13310/20000], Bound: 0.39002498984336853, Entropy: 141.52645874023438, Temp: 2.6984591484069824, KL: 75.15022277832031, Loss: 0.016586558893322945, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13311/20000], Bound: 0.3588973879814148, Entropy: 141.97422790527344, Temp: 2.698467969894409, KL: 67.03561401367188, Loss: 0.015086299739778042, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13312/20000], Bound: 0.4067497253417969, Entropy: 140.83306884765625, Temp: 2.6984755992889404, KL: 80.05491638183594, Loss: 0.01669793203473091, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13313/20000], Bound: 0.38436394929885864, Entropy: 140.88441467285156, Temp: 2.69848370552063, KL: 74.36203002929688, Loss: 0.014984499663114548, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13314/20000], Bound: 0.3837141692638397, Entropy: 141.38601684570312, Temp: 2.6984925270080566, KL: 72.48823547363281, Loss: 0.018106607720255852, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13315/20000], Bound: 0.3885484039783478, Entropy: 140.67105102539062, Temp: 2.6984987258911133, KL: 75.95892333984375, Loss: 0.01428721658885479, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13316/20000], Bound: 0.3657527565956116, Entropy: 140.56968688964844, Temp: 2.6985068321228027, KL: 68.72073364257812, Loss: 0.015543188899755478, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13317/20000], Bound: 0.3878088593482971, Entropy: 141.26272583007812, Temp: 2.698514223098755, KL: 74.54547119140625, Loss: 0.016505615785717964, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13318/20000], Bound: 0.3672276735305786, Entropy: 143.11611938476562, Temp: 2.6985208988189697, KL: 67.97735595703125, Loss: 0.017695240676403046, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13319/20000], Bound: 0.3944220542907715, Entropy: 140.57131958007812, Temp: 2.6985244750976562, KL: 76.83256530761719, Loss: 0.01586645096540451, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13320/20000], Bound: 0.37659192085266113, Entropy: 140.139404296875, Temp: 2.69852876663208, KL: 72.24740600585938, Loss: 0.014739076606929302, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13321/20000], Bound: 0.3688466250896454, Entropy: 141.13584899902344, Temp: 2.6985344886779785, KL: 69.39418029785156, Loss: 0.01592215895652771, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13322/20000], Bound: 0.387506902217865, Entropy: 141.6646270751953, Temp: 2.6985390186309814, KL: 73.75218200683594, Loss: 0.017812244594097137, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13323/20000], Bound: 0.3943786323070526, Entropy: 141.52639770507812, Temp: 2.6985416412353516, KL: 76.23490905761719, Loss: 0.016950249671936035, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13324/20000], Bound: 0.35166090726852417, Entropy: 141.7001190185547, Temp: 2.6985440254211426, KL: 65.27999877929688, Loss: 0.014599094167351723, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13325/20000], Bound: 0.39636754989624023, Entropy: 137.74615478515625, Temp: 2.6985466480255127, KL: 76.253173828125, Loss: 0.018005453050136566, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13326/20000], Bound: 0.39805006980895996, Entropy: 141.45249938964844, Temp: 2.698547840118408, KL: 76.42738342285156, Loss: 0.018606457859277725, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13327/20000], Bound: 0.39970144629478455, Entropy: 140.88980102539062, Temp: 2.698547124862671, KL: 78.67439270019531, Loss: 0.015352011658251286, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13328/20000], Bound: 0.347921222448349, Entropy: 142.79603576660156, Temp: 2.6985485553741455, KL: 64.76350402832031, Loss: 0.013637666590511799, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13329/20000], Bound: 0.41267621517181396, Entropy: 141.44676208496094, Temp: 2.6985511779785156, KL: 83.25698852539062, Loss: 0.014081024564802647, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13330/20000], Bound: 0.38886401057243347, Entropy: 141.8939666748047, Temp: 2.6985578536987305, KL: 73.76152038574219, Loss: 0.01853034272789955, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13331/20000], Bound: 0.38424208760261536, Entropy: 140.83505249023438, Temp: 2.698561906814575, KL: 73.47102355957031, Loss: 0.016570504754781723, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13332/20000], Bound: 0.3920382261276245, Entropy: 138.92355346679688, Temp: 2.6985652446746826, KL: 75.51394653320312, Loss: 0.017008908092975616, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13333/20000], Bound: 0.3942340016365051, Entropy: 140.41754150390625, Temp: 2.698568105697632, KL: 76.48458862304688, Loss: 0.01640881784260273, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13334/20000], Bound: 0.37824028730392456, Entropy: 141.94236755371094, Temp: 2.69857120513916, KL: 72.31367492675781, Loss: 0.01549589354544878, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13335/20000], Bound: 0.37514203786849976, Entropy: 141.36300659179688, Temp: 2.6985747814178467, KL: 71.48118591308594, Loss: 0.015387553721666336, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13336/20000], Bound: 0.34150567650794983, Entropy: 143.80499267578125, Temp: 2.6985788345336914, KL: 62.56059265136719, Loss: 0.014451369643211365, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13337/20000], Bound: 0.33880099654197693, Entropy: 142.94969177246094, Temp: 2.698582172393799, KL: 61.75639343261719, Loss: 0.014572184532880783, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13338/20000], Bound: 0.39335018396377563, Entropy: 141.48764038085938, Temp: 2.69858455657959, KL: 77.94364929199219, Loss: 0.013222817331552505, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13339/20000], Bound: 0.4013211131095886, Entropy: 141.73980712890625, Temp: 2.6985912322998047, KL: 79.12409973144531, Loss: 0.015412804670631886, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13340/20000], Bound: 0.3774033188819885, Entropy: 141.6937713623047, Temp: 2.698599100112915, KL: 73.21296691894531, Loss: 0.013383259065449238, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13341/20000], Bound: 0.3873591125011444, Entropy: 141.54649353027344, Temp: 2.6986095905303955, KL: 76.10029602050781, Loss: 0.013382230885326862, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13342/20000], Bound: 0.3928467333316803, Entropy: 142.27056884765625, Temp: 2.698622941970825, KL: 75.9256591796875, Loss: 0.016687428578734398, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13343/20000], Bound: 0.39014163613319397, Entropy: 140.87106323242188, Temp: 2.6986351013183594, KL: 75.91244506835938, Loss: 0.015239319764077663, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13344/20000], Bound: 0.39085978269577026, Entropy: 140.95558166503906, Temp: 2.698647975921631, KL: 75.49234008789062, Loss: 0.016408167779445648, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13345/20000], Bound: 0.37297430634498596, Entropy: 143.06788635253906, Temp: 2.698659896850586, KL: 70.58851623535156, Loss: 0.01589156500995159, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13346/20000], Bound: 0.4131607413291931, Entropy: 140.09414672851562, Temp: 2.6986706256866455, KL: 82.17538452148438, Loss: 0.016358722001314163, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13347/20000], Bound: 0.3633297085762024, Entropy: 139.7373809814453, Temp: 2.6986820697784424, KL: 67.38505554199219, Loss: 0.016750426962971687, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13348/20000], Bound: 0.3641965687274933, Entropy: 141.60023498535156, Temp: 2.69869065284729, KL: 66.66804504394531, Loss: 0.01853237673640251, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13349/20000], Bound: 0.3709586262702942, Entropy: 142.1435546875, Temp: 2.6986947059631348, KL: 67.95333862304688, Loss: 0.019707435742020607, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13350/20000], Bound: 0.37643033266067505, Entropy: 142.939697265625, Temp: 2.6986935138702393, KL: 71.66400146484375, Loss: 0.01573539711534977, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13351/20000], Bound: 0.3838464617729187, Entropy: 139.98191833496094, Temp: 2.698692798614502, KL: 73.56346130371094, Loss: 0.016187334433197975, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13352/20000], Bound: 0.37986481189727783, Entropy: 139.74078369140625, Temp: 2.698692560195923, KL: 73.76179504394531, Loss: 0.013682509772479534, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13353/20000], Bound: 0.4152892827987671, Entropy: 140.8452911376953, Temp: 2.698695182800293, KL: 81.41407775878906, Loss: 0.018968315795063972, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13354/20000], Bound: 0.3821307122707367, Entropy: 140.00588989257812, Temp: 2.6986966133117676, KL: 74.0810546875, Loss: 0.0143058430403471, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13355/20000], Bound: 0.41313865780830383, Entropy: 142.24716186523438, Temp: 2.698700189590454, KL: 80.88909912109375, Loss: 0.01872974820435047, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13356/20000], Bound: 0.3900451958179474, Entropy: 142.13275146484375, Temp: 2.698702335357666, KL: 75.83357238769531, Loss: 0.015333683229982853, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13357/20000], Bound: 0.3803369402885437, Entropy: 139.14930725097656, Temp: 2.6987061500549316, KL: 71.38383483886719, Loss: 0.0183411817997694, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13358/20000], Bound: 0.3496324419975281, Entropy: 142.3870391845703, Temp: 2.698706865310669, KL: 64.28288269042969, Loss: 0.015405937097966671, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13359/20000], Bound: 0.35555171966552734, Entropy: 142.8837432861328, Temp: 2.69870662689209, KL: 66.54496765136719, Loss: 0.014263121411204338, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13360/20000], Bound: 0.36898937821388245, Entropy: 143.09698486328125, Temp: 2.698707342147827, KL: 69.64273071289062, Loss: 0.015538275241851807, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13361/20000], Bound: 0.4111460745334625, Entropy: 140.12771606445312, Temp: 2.6987080574035645, KL: 79.84504699707031, Loss: 0.01954548992216587, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13362/20000], Bound: 0.3709341585636139, Entropy: 140.58999633789062, Temp: 2.69870662689209, KL: 70.19874572753906, Loss: 0.015534440986812115, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13363/20000], Bound: 0.3792993426322937, Entropy: 141.4909210205078, Temp: 2.6987059116363525, KL: 71.85565185546875, Loss: 0.016911674290895462, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13364/20000], Bound: 0.3851824998855591, Entropy: 144.96141052246094, Temp: 2.6987040042877197, KL: 75.465087890625, Loss: 0.013384141959249973, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13365/20000], Bound: 0.3909235894680023, Entropy: 143.44583129882812, Temp: 2.6987061500549316, KL: 75.76490783691406, Loss: 0.01593843847513199, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13366/20000], Bound: 0.3917883336544037, Entropy: 142.05862426757812, Temp: 2.698709011077881, KL: 75.82078552246094, Loss: 0.016305577009916306, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13367/20000], Bound: 0.39786481857299805, Entropy: 141.53851318359375, Temp: 2.698712110519409, KL: 75.9124755859375, Loss: 0.019460082054138184, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13368/20000], Bound: 0.35920125246047974, Entropy: 139.666015625, Temp: 2.6987123489379883, KL: 66.75920104980469, Loss: 0.01575823500752449, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13369/20000], Bound: 0.38973376154899597, Entropy: 141.42266845703125, Temp: 2.698711633682251, KL: 74.43263244628906, Loss: 0.017760178074240685, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13370/20000], Bound: 0.3840123414993286, Entropy: 140.7918701171875, Temp: 2.6987099647521973, KL: 74.12156677246094, Loss: 0.015242735855281353, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13371/20000], Bound: 0.3887890577316284, Entropy: 140.1670379638672, Temp: 2.698709726333618, KL: 74.93028259277344, Loss: 0.016325540840625763, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13372/20000], Bound: 0.3848959505558014, Entropy: 140.38987731933594, Temp: 2.6987099647521973, KL: 72.0904541015625, Loss: 0.019481999799609184, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13373/20000], Bound: 0.3832022547721863, Entropy: 141.37591552734375, Temp: 2.6987063884735107, KL: 73.18351745605469, Loss: 0.016544736921787262, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13374/20000], Bound: 0.39286309480667114, Entropy: 139.3241424560547, Temp: 2.6987032890319824, KL: 76.11737060546875, Loss: 0.016341889277100563, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13375/20000], Bound: 0.4208281934261322, Entropy: 141.53746032714844, Temp: 2.6987006664276123, KL: 83.74420166015625, Loss: 0.017789127305150032, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13376/20000], Bound: 0.3727189004421234, Entropy: 143.6503448486328, Temp: 2.698699474334717, KL: 70.18675231933594, Loss: 0.016500910744071007, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13377/20000], Bound: 0.36911720037460327, Entropy: 140.50877380371094, Temp: 2.698697328567505, KL: 70.35708618164062, Loss: 0.014282074756920338, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13378/20000], Bound: 0.4087757468223572, Entropy: 141.15542602539062, Temp: 2.698697090148926, KL: 80.6756591796875, Loss: 0.016680074855685234, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13379/20000], Bound: 0.37588638067245483, Entropy: 141.13580322265625, Temp: 2.6986982822418213, KL: 72.22602844238281, Loss: 0.014404539950191975, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13380/20000], Bound: 0.3852977752685547, Entropy: 142.057373046875, Temp: 2.6987011432647705, KL: 72.681640625, Loss: 0.01860332116484642, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13381/20000], Bound: 0.3779003322124481, Entropy: 140.77259826660156, Temp: 2.6987011432647705, KL: 71.91835021972656, Loss: 0.01604798436164856, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13382/20000], Bound: 0.3629637658596039, Entropy: 140.16407775878906, Temp: 2.6987011432647705, KL: 68.13111877441406, Loss: 0.015177045948803425, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13383/20000], Bound: 0.3686720132827759, Entropy: 142.06781005859375, Temp: 2.6987013816833496, KL: 69.89120483398438, Loss: 0.014910666272044182, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13384/20000], Bound: 0.3923143148422241, Entropy: 142.3526153564453, Temp: 2.698702573776245, KL: 74.2783203125, Loss: 0.019449900835752487, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13385/20000], Bound: 0.39028194546699524, Entropy: 140.93128967285156, Temp: 2.698700189590454, KL: 76.00175476074219, Loss: 0.01515070628374815, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13386/20000], Bound: 0.3686114549636841, Entropy: 142.88204956054688, Temp: 2.698700189590454, KL: 69.58023071289062, Loss: 0.015454928390681744, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13387/20000], Bound: 0.4140191078186035, Entropy: 141.60623168945312, Temp: 2.6987006664276123, KL: 80.18836975097656, Loss: 0.020523449406027794, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13388/20000], Bound: 0.3742905557155609, Entropy: 143.60720825195312, Temp: 2.698697566986084, KL: 70.51077270507812, Loss: 0.016734156757593155, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13389/20000], Bound: 0.3570452630519867, Entropy: 140.49851989746094, Temp: 2.6986939907073975, KL: 66.82228088378906, Loss: 0.01452233549207449, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13390/20000], Bound: 0.3508438467979431, Entropy: 142.21731567382812, Temp: 2.6986913681030273, KL: 63.80204772949219, Loss: 0.016918480396270752, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13391/20000], Bound: 0.396867960691452, Entropy: 142.48814392089844, Temp: 2.698686361312866, KL: 77.15817260742188, Loss: 0.016604477539658546, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13392/20000], Bound: 0.39114025235176086, Entropy: 142.29739379882812, Temp: 2.6986823081970215, KL: 74.51936340332031, Loss: 0.01836375519633293, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13393/20000], Bound: 0.37563183903694153, Entropy: 139.80540466308594, Temp: 2.698676824569702, KL: 71.84445190429688, Loss: 0.01497584767639637, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13394/20000], Bound: 0.3854310214519501, Entropy: 142.44073486328125, Temp: 2.6986732482910156, KL: 74.17442321777344, Loss: 0.01590922847390175, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13395/20000], Bound: 0.4085537791252136, Entropy: 141.46031188964844, Temp: 2.6986706256866455, KL: 81.1865234375, Loss: 0.015609335154294968, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13396/20000], Bound: 0.3900340497493744, Entropy: 140.49520874023438, Temp: 2.6986706256866455, KL: 75.51240539550781, Loss: 0.015922395512461662, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13397/20000], Bound: 0.3978809416294098, Entropy: 140.24874877929688, Temp: 2.698671817779541, KL: 77.19740295410156, Loss: 0.01708793267607689, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13398/20000], Bound: 0.3598855435848236, Entropy: 143.39599609375, Temp: 2.6986730098724365, KL: 68.42930603027344, Loss: 0.01301945187151432, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13399/20000], Bound: 0.37060871720314026, Entropy: 142.27310180664062, Temp: 2.698676586151123, KL: 69.36607360839844, Loss: 0.01690494269132614, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13400/20000], Bound: 0.40007469058036804, Entropy: 141.3580780029297, Temp: 2.6986782550811768, KL: 75.77821350097656, Loss: 0.020924989134073257, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13401/20000], Bound: 0.4101802706718445, Entropy: 141.4942169189453, Temp: 2.6986756324768066, KL: 80.61404418945312, Loss: 0.017579397186636925, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13402/20000], Bound: 0.37782952189445496, Entropy: 141.36813354492188, Temp: 2.6986734867095947, KL: 72.42367553710938, Loss: 0.015073697082698345, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13403/20000], Bound: 0.39797645807266235, Entropy: 142.44952392578125, Temp: 2.6986725330352783, KL: 77.71977233886719, Loss: 0.01617259718477726, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13404/20000], Bound: 0.396323025226593, Entropy: 138.8621368408203, Temp: 2.6986730098724365, KL: 75.66676330566406, Loss: 0.019068686291575432, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13405/20000], Bound: 0.3814764618873596, Entropy: 141.65817260742188, Temp: 2.6986708641052246, KL: 73.02241516113281, Loss: 0.01591581478714943, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13406/20000], Bound: 0.38041162490844727, Entropy: 140.27696228027344, Temp: 2.698669672012329, KL: 73.39936828613281, Loss: 0.014646568335592747, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13407/20000], Bound: 0.38499969244003296, Entropy: 140.4033203125, Temp: 2.6986703872680664, KL: 73.63499450683594, Loss: 0.016675958409905434, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13408/20000], Bound: 0.40753018856048584, Entropy: 141.55726623535156, Temp: 2.6986706256866455, KL: 79.26011657714844, Loss: 0.01860741712152958, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13409/20000], Bound: 0.38433873653411865, Entropy: 141.4915771484375, Temp: 2.698669910430908, KL: 73.0650634765625, Loss: 0.017375634983181953, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13410/20000], Bound: 0.41182777285575867, Entropy: 140.9877471923828, Temp: 2.6986680030822754, KL: 80.3013916015625, Loss: 0.019081948325037956, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13411/20000], Bound: 0.3660896122455597, Entropy: 144.238037109375, Temp: 2.698664903640747, KL: 68.87211608886719, Loss: 0.015440733171999454, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13412/20000], Bound: 0.4002465605735779, Entropy: 140.078857421875, Temp: 2.698662281036377, KL: 78.04733276367188, Loss: 0.016815485432744026, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13413/20000], Bound: 0.3663603663444519, Entropy: 143.3670654296875, Temp: 2.698660373687744, KL: 68.05892944335938, Loss: 0.0170894842594862, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13414/20000], Bound: 0.4172613322734833, Entropy: 142.2336883544922, Temp: 2.6986567974090576, KL: 82.58720397949219, Loss: 0.017908543348312378, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13415/20000], Bound: 0.36265888810157776, Entropy: 140.87388610839844, Temp: 2.6986539363861084, KL: 67.58512878417969, Loss: 0.016029004007577896, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13416/20000], Bound: 0.37768206000328064, Entropy: 142.45529174804688, Temp: 2.698650360107422, KL: 71.36672973632812, Loss: 0.016953108832240105, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13417/20000], Bound: 0.3895874321460724, Entropy: 140.41639709472656, Temp: 2.698646068572998, KL: 76.515869140625, Loss: 0.013820411637425423, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13418/20000], Bound: 0.3786957561969757, Entropy: 141.89324951171875, Temp: 2.69864559173584, KL: 73.25241088867188, Loss: 0.01400057040154934, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13419/20000], Bound: 0.39809975028038025, Entropy: 139.33226013183594, Temp: 2.698647975921631, KL: 78.27149963378906, Loss: 0.015217911452054977, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13420/20000], Bound: 0.3900778591632843, Entropy: 142.4550018310547, Temp: 2.6986522674560547, KL: 76.12486267089844, Loss: 0.014811237342655659, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13421/20000], Bound: 0.3628377616405487, Entropy: 143.51702880859375, Temp: 2.6986584663391113, KL: 68.62535095214844, Loss: 0.014195148833096027, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13422/20000], Bound: 0.3864031136035919, Entropy: 143.2791748046875, Temp: 2.6986653804779053, KL: 74.24148559570312, Loss: 0.016309790313243866, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13423/20000], Bound: 0.4232700765132904, Entropy: 140.9381561279297, Temp: 2.69867205619812, KL: 85.35098266601562, Loss: 0.016203653067350388, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13424/20000], Bound: 0.38490164279937744, Entropy: 142.2753448486328, Temp: 2.6986806392669678, KL: 74.48980712890625, Loss: 0.015039416961371899, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13425/20000], Bound: 0.3917596638202667, Entropy: 140.0395050048828, Temp: 2.698690176010132, KL: 75.17013549804688, Loss: 0.01749528758227825, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13426/20000], Bound: 0.40395587682724, Entropy: 141.90432739257812, Temp: 2.698698043823242, KL: 80.17926025390625, Loss: 0.014917249791324139, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13427/20000], Bound: 0.3789375126361847, Entropy: 141.73216247558594, Temp: 2.6987080574035645, KL: 71.17990112304688, Loss: 0.017970211803913116, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13428/20000], Bound: 0.3918011784553528, Entropy: 142.12327575683594, Temp: 2.6987149715423584, KL: 75.65345764160156, Loss: 0.016622666269540787, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13429/20000], Bound: 0.3728691339492798, Entropy: 141.8131561279297, Temp: 2.698721408843994, KL: 71.02633666992188, Loss: 0.015025143511593342, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13430/20000], Bound: 0.4083947241306305, Entropy: 140.44830322265625, Temp: 2.698728084564209, KL: 79.83427429199219, Loss: 0.018026508390903473, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13431/20000], Bound: 0.3796340227127075, Entropy: 140.3723602294922, Temp: 2.6987335681915283, KL: 70.98391723632812, Loss: 0.018706027418375015, Learning Rate: 2.3937679889283573e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13432/20000], Bound: 0.39140090346336365, Entropy: 140.593994140625, Temp: 2.6987357139587402, KL: 75.50994873046875, Loss: 0.016670795157551765, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13433/20000], Bound: 0.37657588720321655, Entropy: 142.3732452392578, Temp: 2.698737621307373, KL: 71.37916564941406, Loss: 0.01634102873504162, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13434/20000], Bound: 0.3851574659347534, Entropy: 141.81890869140625, Temp: 2.6987390518188477, KL: 72.93928527832031, Loss: 0.01805061101913452, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13435/20000], Bound: 0.39495545625686646, Entropy: 140.06814575195312, Temp: 2.6987383365631104, KL: 76.73114013671875, Loss: 0.016348212957382202, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13436/20000], Bound: 0.371092826128006, Entropy: 142.53955078125, Temp: 2.6987385749816895, KL: 69.17134094238281, Loss: 0.017522023990750313, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13437/20000], Bound: 0.3735370337963104, Entropy: 142.31825256347656, Temp: 2.6987366676330566, KL: 70.66064453125, Loss: 0.016056900843977928, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13438/20000], Bound: 0.40668338537216187, Entropy: 140.7115936279297, Temp: 2.6987345218658447, KL: 79.96864318847656, Loss: 0.016823476180434227, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13439/20000], Bound: 0.4089726507663727, Entropy: 142.3866424560547, Temp: 2.6987335681915283, KL: 77.9140625, Loss: 0.02190694957971573, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13440/20000], Bound: 0.3999418020248413, Entropy: 142.1887664794922, Temp: 2.69872784614563, KL: 78.54292297363281, Loss: 0.015729881823062897, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13441/20000], Bound: 0.38867825269699097, Entropy: 141.20054626464844, Temp: 2.6987245082855225, KL: 73.63456726074219, Loss: 0.018666233867406845, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13442/20000], Bound: 0.3739265501499176, Entropy: 141.78155517578125, Temp: 2.6987192630767822, KL: 72.02496337890625, Loss: 0.013735711574554443, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13443/20000], Bound: 0.39252644777297974, Entropy: 141.7169189453125, Temp: 2.698716878890991, KL: 77.31913757324219, Loss: 0.013931888155639172, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13444/20000], Bound: 0.38215553760528564, Entropy: 142.65501403808594, Temp: 2.698718547821045, KL: 72.90434265136719, Loss: 0.016499504446983337, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13445/20000], Bound: 0.3806614279747009, Entropy: 140.7196044921875, Temp: 2.6987197399139404, KL: 72.11361694335938, Loss: 0.017163043841719627, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13446/20000], Bound: 0.41909727454185486, Entropy: 141.2578125, Temp: 2.6987195014953613, KL: 83.23727416992188, Loss: 0.01774505525827408, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13447/20000], Bound: 0.3686388432979584, Entropy: 141.48233032226562, Temp: 2.6987199783325195, KL: 68.00601196289062, Loss: 0.01838613674044609, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13448/20000], Bound: 0.39351168274879456, Entropy: 138.91574096679688, Temp: 2.698716878890991, KL: 77.39427185058594, Loss: 0.014330222271382809, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13449/20000], Bound: 0.4055318236351013, Entropy: 140.63470458984375, Temp: 2.6987173557281494, KL: 78.41000366210938, Loss: 0.019070353358983994, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13450/20000], Bound: 0.37650227546691895, Entropy: 141.43907165527344, Temp: 2.698716163635254, KL: 71.75953674316406, Loss: 0.015596901066601276, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13451/20000], Bound: 0.39861977100372314, Entropy: 142.8737030029297, Temp: 2.6987154483795166, KL: 77.56207275390625, Loss: 0.016818970441818237, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13452/20000], Bound: 0.39150601625442505, Entropy: 141.83363342285156, Temp: 2.6987152099609375, KL: 76.01548767089844, Loss: 0.015791181474924088, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13453/20000], Bound: 0.40885791182518005, Entropy: 140.0258331298828, Temp: 2.698716163635254, KL: 80.70260620117188, Loss: 0.01667623780667782, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13454/20000], Bound: 0.39180678129196167, Entropy: 141.59518432617188, Temp: 2.698718547821045, KL: 75.78106689453125, Loss: 0.01638929918408394, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13455/20000], Bound: 0.4012727439403534, Entropy: 142.29617309570312, Temp: 2.698720932006836, KL: 77.49932861328125, Loss: 0.018397709354758263, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13456/20000], Bound: 0.401695191860199, Entropy: 141.8625946044922, Temp: 2.6987218856811523, KL: 78.40330505371094, Loss: 0.016956310719251633, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13457/20000], Bound: 0.3815951645374298, Entropy: 140.87966918945312, Temp: 2.698723316192627, KL: 73.25845336914062, Loss: 0.015542685985565186, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13458/20000], Bound: 0.40965786576271057, Entropy: 142.14273071289062, Temp: 2.6987252235412598, KL: 80.44166564941406, Loss: 0.017606958746910095, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13459/20000], Bound: 0.38168075680732727, Entropy: 141.47088623046875, Temp: 2.6987271308898926, KL: 72.75138854980469, Loss: 0.016528110951185226, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13460/20000], Bound: 0.3682311773300171, Entropy: 141.17005920410156, Temp: 2.6987287998199463, KL: 69.25422668457031, Loss: 0.015858901664614677, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13461/20000], Bound: 0.4106803834438324, Entropy: 140.7103729248047, Temp: 2.6987297534942627, KL: 82.65055847167969, Loss: 0.014086885377764702, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13462/20000], Bound: 0.38569289445877075, Entropy: 140.24142456054688, Temp: 2.698735475540161, KL: 74.29489135742188, Loss: 0.015827955678105354, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13463/20000], Bound: 0.392863005399704, Entropy: 142.1286163330078, Temp: 2.6987411975860596, KL: 76.9249267578125, Loss: 0.01484599057585001, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13464/20000], Bound: 0.39904817938804626, Entropy: 142.58592224121094, Temp: 2.698748826980591, KL: 77.81234741210938, Loss: 0.016591396182775497, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13465/20000], Bound: 0.3828486502170563, Entropy: 139.51614379882812, Temp: 2.698756217956543, KL: 72.056640625, Loss: 0.018442774191498756, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13466/20000], Bound: 0.37926870584487915, Entropy: 143.0128936767578, Temp: 2.698760509490967, KL: 72.61679077148438, Loss: 0.01548558659851551, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13467/20000], Bound: 0.3972424566745758, Entropy: 141.40066528320312, Temp: 2.698765277862549, KL: 78.07501220703125, Loss: 0.015112161636352539, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13468/20000], Bound: 0.39103010296821594, Entropy: 141.240478515625, Temp: 2.6987719535827637, KL: 74.40435791015625, Loss: 0.018517663702368736, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13469/20000], Bound: 0.42198261618614197, Entropy: 140.47406005859375, Temp: 2.6987757682800293, KL: 82.07420349121094, Loss: 0.021541232243180275, Learning Rate: 2.3937679889283573e-05\n",
      "Epoch [13470/20000], Bound: 0.3898121118545532, Entropy: 137.93728637695312, Temp: 2.69877552986145, KL: 75.42303466796875, Loss: 0.015968389809131622, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13471/20000], Bound: 0.4128777086734772, Entropy: 140.96102905273438, Temp: 2.6987760066986084, KL: 81.87678527832031, Loss: 0.01675393246114254, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13472/20000], Bound: 0.3940771222114563, Entropy: 141.57542419433594, Temp: 2.698777437210083, KL: 76.11332702636719, Loss: 0.01701289415359497, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13473/20000], Bound: 0.39577311277389526, Entropy: 141.53053283691406, Temp: 2.6987788677215576, KL: 77.08094787597656, Loss: 0.0161481574177742, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13474/20000], Bound: 0.3994762599468231, Entropy: 138.91238403320312, Temp: 2.6987805366516113, KL: 78.26722717285156, Loss: 0.0159846730530262, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13475/20000], Bound: 0.42625996470451355, Entropy: 139.00564575195312, Temp: 2.6987833976745605, KL: 83.27149963378906, Loss: 0.021768901497125626, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13476/20000], Bound: 0.3937340974807739, Entropy: 143.336669921875, Temp: 2.6987833976745605, KL: 75.00430297851562, Loss: 0.018880225718021393, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13477/20000], Bound: 0.3806789517402649, Entropy: 141.8227081298828, Temp: 2.6987814903259277, KL: 72.65840148925781, Loss: 0.01616358757019043, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13478/20000], Bound: 0.3754742741584778, Entropy: 140.6214599609375, Temp: 2.698780059814453, KL: 70.98631286621094, Loss: 0.016482839360833168, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13479/20000], Bound: 0.3799242079257965, Entropy: 140.85032653808594, Temp: 2.6987781524658203, KL: 72.56646728515625, Loss: 0.015929684042930603, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13480/20000], Bound: 0.382707417011261, Entropy: 141.02415466308594, Temp: 2.6987767219543457, KL: 72.00837707519531, Loss: 0.018456462770700455, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13481/20000], Bound: 0.36760473251342773, Entropy: 141.55685424804688, Temp: 2.6987736225128174, KL: 69.06153869628906, Loss: 0.015886681154370308, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13482/20000], Bound: 0.37830212712287903, Entropy: 142.52774047851562, Temp: 2.698770523071289, KL: 71.16647338867188, Loss: 0.01765608787536621, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13483/20000], Bound: 0.38445836305618286, Entropy: 141.49644470214844, Temp: 2.6987667083740234, KL: 73.29319763183594, Loss: 0.01701824925839901, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13484/20000], Bound: 0.38589730858802795, Entropy: 141.3891143798828, Temp: 2.6987624168395996, KL: 75.27323913574219, Loss: 0.014125959947705269, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13485/20000], Bound: 0.3852423429489136, Entropy: 141.97299194335938, Temp: 2.698760509490967, KL: 73.62789916992188, Loss: 0.016820743680000305, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13486/20000], Bound: 0.36338984966278076, Entropy: 141.4442596435547, Temp: 2.698758840560913, KL: 67.58570861816406, Loss: 0.01641068607568741, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13487/20000], Bound: 0.38283881545066833, Entropy: 140.2458953857422, Temp: 2.698756217956543, KL: 73.2689208984375, Loss: 0.016191475093364716, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13488/20000], Bound: 0.34929391741752625, Entropy: 138.87220764160156, Temp: 2.698754072189331, KL: 64.06829833984375, Loss: 0.015630265697836876, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13489/20000], Bound: 0.3822646737098694, Entropy: 142.53343200683594, Temp: 2.698751211166382, KL: 72.15257263183594, Loss: 0.017951231449842453, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13490/20000], Bound: 0.39498528838157654, Entropy: 140.99717712402344, Temp: 2.698747158050537, KL: 78.02201843261719, Loss: 0.013973001390695572, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13491/20000], Bound: 0.37837398052215576, Entropy: 141.54493713378906, Temp: 2.6987462043762207, KL: 72.40225219726562, Loss: 0.015404747799038887, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13492/20000], Bound: 0.3969626724720001, Entropy: 141.1329803466797, Temp: 2.6987462043762207, KL: 76.46978759765625, Loss: 0.017932385206222534, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13493/20000], Bound: 0.39440470933914185, Entropy: 141.22265625, Temp: 2.6987452507019043, KL: 75.07334899902344, Loss: 0.01911843940615654, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13494/20000], Bound: 0.3779172897338867, Entropy: 139.21905517578125, Temp: 2.698742628097534, KL: 72.04171752929688, Loss: 0.015828846022486687, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13495/20000], Bound: 0.3606286644935608, Entropy: 140.32879638671875, Temp: 2.698740243911743, KL: 68.57211303710938, Loss: 0.013142244890332222, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13496/20000], Bound: 0.38745349645614624, Entropy: 141.57032775878906, Temp: 2.698740005493164, KL: 75.98118591308594, Loss: 0.013655352406203747, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13497/20000], Bound: 0.37972399592399597, Entropy: 142.27076721191406, Temp: 2.698742389678955, KL: 72.56443786621094, Loss: 0.015826014801859856, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13498/20000], Bound: 0.3882884681224823, Entropy: 142.24876403808594, Temp: 2.698744773864746, KL: 75.98480224609375, Loss: 0.014100839383900166, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13499/20000], Bound: 0.38214609026908875, Entropy: 140.54226684570312, Temp: 2.69874906539917, KL: 71.36701965332031, Loss: 0.01934291608631611, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13500/20000], Bound: 0.38417983055114746, Entropy: 141.78981018066406, Temp: 2.6987502574920654, KL: 74.15766906738281, Loss: 0.01526640634983778, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13501/20000], Bound: 0.3799404501914978, Entropy: 139.23614501953125, Temp: 2.6987526416778564, KL: 73.70346069335938, Loss: 0.013831643387675285, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13502/20000], Bound: 0.34595078229904175, Entropy: 142.95120239257812, Temp: 2.698756694793701, KL: 61.98689270019531, Loss: 0.01777671091258526, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13503/20000], Bound: 0.37573927640914917, Entropy: 140.29254150390625, Temp: 2.6987571716308594, KL: 72.47378540039062, Loss: 0.013867780566215515, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13504/20000], Bound: 0.4037606716156006, Entropy: 141.10838317871094, Temp: 2.6987595558166504, KL: 79.11944580078125, Loss: 0.016773145645856857, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13505/20000], Bound: 0.3981480300426483, Entropy: 142.91368103027344, Temp: 2.6987624168395996, KL: 76.50689697265625, Loss: 0.018514903262257576, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13506/20000], Bound: 0.4066355526447296, Entropy: 140.68045043945312, Temp: 2.698763608932495, KL: 80.16145324707031, Loss: 0.016439899802207947, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13507/20000], Bound: 0.41270843148231506, Entropy: 140.3454132080078, Temp: 2.698765993118286, KL: 82.73114013671875, Loss: 0.015075795352458954, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13508/20000], Bound: 0.39457252621650696, Entropy: 141.4947509765625, Temp: 2.69877028465271, KL: 76.35162353515625, Loss: 0.016842180863022804, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13509/20000], Bound: 0.39400964975357056, Entropy: 141.6405029296875, Temp: 2.6987743377685547, KL: 77.17045593261719, Loss: 0.015017442405223846, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13510/20000], Bound: 0.40114617347717285, Entropy: 141.2213134765625, Temp: 2.698779582977295, KL: 77.76243591308594, Loss: 0.017840879037976265, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13511/20000], Bound: 0.3949005603790283, Entropy: 142.08631896972656, Temp: 2.6987838745117188, KL: 77.67491149902344, Loss: 0.014570076018571854, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13512/20000], Bound: 0.37078431248664856, Entropy: 141.0539093017578, Temp: 2.698789596557617, KL: 71.32838439941406, Loss: 0.013363067060709, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13513/20000], Bound: 0.3813773989677429, Entropy: 139.46145629882812, Temp: 2.6987969875335693, KL: 73.43539428710938, Loss: 0.015098702162504196, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13514/20000], Bound: 0.37101826071739197, Entropy: 142.31651306152344, Temp: 2.6988046169281006, KL: 69.62489318847656, Loss: 0.0166428554803133, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13515/20000], Bound: 0.37134069204330444, Entropy: 143.20542907714844, Temp: 2.698810577392578, KL: 71.01091003417969, Loss: 0.014245516620576382, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13516/20000], Bound: 0.37205424904823303, Entropy: 142.51885986328125, Temp: 2.698817253112793, KL: 69.68336486816406, Loss: 0.01708253100514412, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13517/20000], Bound: 0.4243462085723877, Entropy: 139.70777893066406, Temp: 2.698822259902954, KL: 85.69068908691406, Loss: 0.016191016882658005, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13518/20000], Bound: 0.36393260955810547, Entropy: 141.88243103027344, Temp: 2.69882869720459, KL: 68.68595886230469, Loss: 0.01465668622404337, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13519/20000], Bound: 0.3656372129917145, Entropy: 141.12046813964844, Temp: 2.6988351345062256, KL: 69.22483825683594, Loss: 0.014551326632499695, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13520/20000], Bound: 0.3704869747161865, Entropy: 142.5785675048828, Temp: 2.6988415718078613, KL: 69.49945068359375, Loss: 0.016594871878623962, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13521/20000], Bound: 0.39865556359291077, Entropy: 140.39881896972656, Temp: 2.6988465785980225, KL: 75.17660522460938, Loss: 0.021259324625134468, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13522/20000], Bound: 0.3943518102169037, Entropy: 141.99867248535156, Temp: 2.698847770690918, KL: 76.58909606933594, Loss: 0.016282260417938232, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13523/20000], Bound: 0.38532811403274536, Entropy: 140.1202850341797, Temp: 2.6988492012023926, KL: 72.52522277832031, Loss: 0.018910661339759827, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13524/20000], Bound: 0.37518784403800964, Entropy: 142.4562530517578, Temp: 2.6988484859466553, KL: 71.64764404296875, Loss: 0.01510587614029646, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13525/20000], Bound: 0.3807547390460968, Entropy: 140.389892578125, Temp: 2.6988489627838135, KL: 70.88177490234375, Loss: 0.019496258348226547, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13526/20000], Bound: 0.3934406042098999, Entropy: 140.75665283203125, Temp: 2.6988461017608643, KL: 75.14262390136719, Loss: 0.018464215099811554, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13527/20000], Bound: 0.38686078786849976, Entropy: 141.9084930419922, Temp: 2.6988425254821777, KL: 75.46540832519531, Loss: 0.014291297644376755, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13528/20000], Bound: 0.3974621593952179, Entropy: 141.90969848632812, Temp: 2.6988413333892822, KL: 76.62525939941406, Loss: 0.01791946217417717, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13529/20000], Bound: 0.41545718908309937, Entropy: 139.8271484375, Temp: 2.6988391876220703, KL: 81.81452941894531, Loss: 0.0183225367218256, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13530/20000], Bound: 0.41460564732551575, Entropy: 140.93258666992188, Temp: 2.6988372802734375, KL: 81.1666259765625, Loss: 0.019042713567614555, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13531/20000], Bound: 0.3796452283859253, Entropy: 143.99427795410156, Temp: 2.6988348960876465, KL: 72.91206359863281, Loss: 0.01514063123613596, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13532/20000], Bound: 0.3919176757335663, Entropy: 139.81597900390625, Temp: 2.698833465576172, KL: 76.26397705078125, Loss: 0.015556124970316887, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13533/20000], Bound: 0.391488254070282, Entropy: 140.99664306640625, Temp: 2.6988332271575928, KL: 74.49449157714844, Loss: 0.01860053651034832, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13534/20000], Bound: 0.38860395550727844, Entropy: 140.35511779785156, Temp: 2.698831558227539, KL: 74.82145690917969, Loss: 0.016427937895059586, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13535/20000], Bound: 0.3777255415916443, Entropy: 142.29725646972656, Temp: 2.6988301277160645, KL: 72.25856018066406, Loss: 0.015325544402003288, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13536/20000], Bound: 0.3977205455303192, Entropy: 142.6707763671875, Temp: 2.698829412460327, KL: 78.21144104003906, Loss: 0.015122628770768642, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13537/20000], Bound: 0.37982895970344543, Entropy: 140.9217071533203, Temp: 2.6988306045532227, KL: 69.87246704101562, Loss: 0.020870212465524673, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13538/20000], Bound: 0.3646760880947113, Entropy: 142.30006408691406, Temp: 2.6988277435302734, KL: 68.00672912597656, Loss: 0.01630428433418274, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13539/20000], Bound: 0.381752073764801, Entropy: 143.04434204101562, Temp: 2.698824405670166, KL: 72.56402587890625, Loss: 0.016914332285523415, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13540/20000], Bound: 0.39594414830207825, Entropy: 140.8509063720703, Temp: 2.6988208293914795, KL: 76.669677734375, Loss: 0.017004234716296196, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13541/20000], Bound: 0.41036802530288696, Entropy: 139.324951171875, Temp: 2.698817729949951, KL: 79.76953125, Loss: 0.019250528886914253, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13542/20000], Bound: 0.4003114402294159, Entropy: 141.73831176757812, Temp: 2.6988136768341064, KL: 78.14851379394531, Loss: 0.016665279865264893, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13543/20000], Bound: 0.37534576654434204, Entropy: 138.2811279296875, Temp: 2.698810577392578, KL: 71.02700805664062, Loss: 0.0163393784314394, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13544/20000], Bound: 0.3762678802013397, Entropy: 142.26548767089844, Temp: 2.69880747795105, KL: 72.51911926269531, Loss: 0.014065590687096119, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13545/20000], Bound: 0.41089481115341187, Entropy: 139.61380004882812, Temp: 2.6988062858581543, KL: 81.34675598144531, Loss: 0.01662340760231018, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13546/20000], Bound: 0.38322576880455017, Entropy: 138.837646484375, Temp: 2.6988062858581543, KL: 74.876220703125, Loss: 0.013422238640487194, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13547/20000], Bound: 0.38596174120903015, Entropy: 141.792724609375, Temp: 2.6988089084625244, KL: 72.46502685546875, Loss: 0.01936386525630951, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13548/20000], Bound: 0.37364640831947327, Entropy: 143.03387451171875, Temp: 2.6988086700439453, KL: 71.217529296875, Loss: 0.015083822421729565, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13549/20000], Bound: 0.38385483622550964, Entropy: 142.52256774902344, Temp: 2.6988091468811035, KL: 73.0484619140625, Loss: 0.017146972939372063, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13550/20000], Bound: 0.37599119544029236, Entropy: 143.72451782226562, Temp: 2.6988089084625244, KL: 71.76058959960938, Loss: 0.015323631465435028, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13551/20000], Bound: 0.3756939768791199, Entropy: 140.45692443847656, Temp: 2.6988091468811035, KL: 71.408447265625, Loss: 0.01581786386668682, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13552/20000], Bound: 0.37168988585472107, Entropy: 142.06297302246094, Temp: 2.6988096237182617, KL: 70.92909240722656, Loss: 0.014581776224076748, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13553/20000], Bound: 0.3803674876689911, Entropy: 139.64198303222656, Temp: 2.6988112926483154, KL: 72.28082275390625, Loss: 0.016696548089385033, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13554/20000], Bound: 0.3744529187679291, Entropy: 142.53700256347656, Temp: 2.698812246322632, KL: 70.9381103515625, Loss: 0.016029559075832367, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13555/20000], Bound: 0.36706772446632385, Entropy: 141.84547424316406, Temp: 2.698812961578369, KL: 69.11749267578125, Loss: 0.015500988811254501, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13556/20000], Bound: 0.406715452671051, Entropy: 142.42724609375, Temp: 2.6988134384155273, KL: 79.95982360839844, Loss: 0.016858471557497978, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13557/20000], Bound: 0.3872317969799042, Entropy: 142.15505981445312, Temp: 2.698814868927002, KL: 72.4359130859375, Loss: 0.020104316994547844, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13558/20000], Bound: 0.39021411538124084, Entropy: 140.27044677734375, Temp: 2.698812961578369, KL: 75.70649719238281, Loss: 0.015661941841244698, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13559/20000], Bound: 0.4010489881038666, Entropy: 140.18589782714844, Temp: 2.698812246322632, KL: 78.7542724609375, Loss: 0.015949975699186325, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13560/20000], Bound: 0.3794191777706146, Entropy: 141.25283813476562, Temp: 2.698812484741211, KL: 72.95790100097656, Loss: 0.014934546314179897, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13561/20000], Bound: 0.3983241319656372, Entropy: 142.12982177734375, Temp: 2.6988141536712646, KL: 78.10476684570312, Loss: 0.015651866793632507, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13562/20000], Bound: 0.40619149804115295, Entropy: 141.79966735839844, Temp: 2.6988167762756348, KL: 79.15736389160156, Loss: 0.018053526058793068, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13563/20000], Bound: 0.3666377365589142, Entropy: 141.95277404785156, Temp: 2.6988189220428467, KL: 69.40199279785156, Loss: 0.014748063869774342, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13564/20000], Bound: 0.36690619587898254, Entropy: 142.69850158691406, Temp: 2.6988213062286377, KL: 69.66395568847656, Loss: 0.014403769746422768, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13565/20000], Bound: 0.4014248549938202, Entropy: 139.8802490234375, Temp: 2.698824405670166, KL: 78.59788513183594, Loss: 0.016447436064481735, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13566/20000], Bound: 0.39306390285491943, Entropy: 140.4451141357422, Temp: 2.6988279819488525, KL: 75.12368774414062, Loss: 0.01829354092478752, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13567/20000], Bound: 0.3589298129081726, Entropy: 140.17095947265625, Temp: 2.6988298892974854, KL: 66.82792663574219, Loss: 0.015490765683352947, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13568/20000], Bound: 0.38373374938964844, Entropy: 143.36485290527344, Temp: 2.698831558227539, KL: 72.86080932617188, Loss: 0.0174296572804451, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13569/20000], Bound: 0.37941646575927734, Entropy: 139.9485626220703, Temp: 2.6988320350646973, KL: 73.66471862792969, Loss: 0.013623771257698536, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13570/20000], Bound: 0.4239918887615204, Entropy: 138.70614624023438, Temp: 2.6988346576690674, KL: 84.91194152832031, Loss: 0.01743132993578911, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13571/20000], Bound: 0.3994162082672119, Entropy: 139.58460998535156, Temp: 2.698837995529175, KL: 79.17745971679688, Loss: 0.014265798032283783, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13572/20000], Bound: 0.3778197765350342, Entropy: 141.10031127929688, Temp: 2.698843240737915, KL: 71.79743957519531, Loss: 0.016230227425694466, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13573/20000], Bound: 0.38998517394065857, Entropy: 141.6678466796875, Temp: 2.698848009109497, KL: 75.62330627441406, Loss: 0.01569201983511448, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13574/20000], Bound: 0.38597196340560913, Entropy: 140.84593200683594, Temp: 2.698853015899658, KL: 74.76054382324219, Loss: 0.015116987749934196, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13575/20000], Bound: 0.3612401485443115, Entropy: 141.87820434570312, Temp: 2.6988589763641357, KL: 68.683349609375, Loss: 0.013255774974822998, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13576/20000], Bound: 0.40062281489372253, Entropy: 141.4532012939453, Temp: 2.6988656520843506, KL: 78.35946655273438, Loss: 0.016446705907583237, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13577/20000], Bound: 0.38013166189193726, Entropy: 140.44097900390625, Temp: 2.6988725662231445, KL: 72.67906188964844, Loss: 0.015832984820008278, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13578/20000], Bound: 0.39431750774383545, Entropy: 140.95411682128906, Temp: 2.6988792419433594, KL: 76.9736328125, Loss: 0.015551389195024967, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13579/20000], Bound: 0.3994665741920471, Entropy: 142.00906372070312, Temp: 2.6988863945007324, KL: 78.50881958007812, Loss: 0.01553279347717762, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13580/20000], Bound: 0.3858835995197296, Entropy: 141.8548583984375, Temp: 2.6988942623138428, KL: 72.63471984863281, Loss: 0.01900796964764595, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13581/20000], Bound: 0.40982532501220703, Entropy: 141.69422912597656, Temp: 2.698899030685425, KL: 81.16188049316406, Loss: 0.016368072479963303, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13582/20000], Bound: 0.3819422125816345, Entropy: 142.34046936035156, Temp: 2.698904514312744, KL: 73.93989562988281, Loss: 0.014468138106167316, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13583/20000], Bound: 0.3857666850090027, Entropy: 140.56948852539062, Temp: 2.698911428451538, KL: 75.13504028320312, Loss: 0.014312888495624065, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13584/20000], Bound: 0.38327229022979736, Entropy: 142.73387145996094, Temp: 2.6989190578460693, KL: 73.46162414550781, Loss: 0.01606903225183487, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13585/20000], Bound: 0.3918377757072449, Entropy: 140.33450317382812, Temp: 2.6989264488220215, KL: 76.51080322265625, Loss: 0.015056277625262737, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13586/20000], Bound: 0.3895566165447235, Entropy: 142.05712890625, Temp: 2.698934555053711, KL: 74.91038513183594, Loss: 0.016780873760581017, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13587/20000], Bound: 0.36481279134750366, Entropy: 141.84552001953125, Temp: 2.698941469192505, KL: 67.4697265625, Loss: 0.017371609807014465, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13588/20000], Bound: 0.3838173449039459, Entropy: 139.7827606201172, Temp: 2.698946475982666, KL: 74.99809265136719, Loss: 0.013516121543943882, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13589/20000], Bound: 0.3551938533782959, Entropy: 143.811279296875, Temp: 2.698953151702881, KL: 65.32504272460938, Loss: 0.016340097412467003, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13590/20000], Bound: 0.380391001701355, Entropy: 141.5393524169922, Temp: 2.698958158493042, KL: 71.80867004394531, Loss: 0.017585106194019318, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13591/20000], Bound: 0.4112875759601593, Entropy: 140.5936279296875, Temp: 2.6989612579345703, KL: 80.10667419433594, Loss: 0.01914253458380699, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13592/20000], Bound: 0.39113378524780273, Entropy: 140.6481170654297, Temp: 2.698962926864624, KL: 76.72816467285156, Loss: 0.014270712621510029, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13593/20000], Bound: 0.39182403683662415, Entropy: 140.26229858398438, Temp: 2.6989667415618896, KL: 74.88851928710938, Loss: 0.018054531887173653, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13594/20000], Bound: 0.3711279630661011, Entropy: 140.40533447265625, Temp: 2.6989691257476807, KL: 69.51089477539062, Loss: 0.01691335253417492, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13595/20000], Bound: 0.3929220736026764, Entropy: 141.5839385986328, Temp: 2.698970317840576, KL: 76.88706970214844, Loss: 0.01495065912604332, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13596/20000], Bound: 0.38707590103149414, Entropy: 142.46267700195312, Temp: 2.6989729404449463, KL: 73.30888366699219, Loss: 0.018403980880975723, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13597/20000], Bound: 0.40790900588035583, Entropy: 140.00584411621094, Temp: 2.6989738941192627, KL: 79.8436279296875, Loss: 0.01774052530527115, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13598/20000], Bound: 0.33954527974128723, Entropy: 141.93038940429688, Temp: 2.698974370956421, KL: 62.33183288574219, Loss: 0.01388513296842575, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13599/20000], Bound: 0.4233502447605133, Entropy: 140.3993377685547, Temp: 2.6989753246307373, KL: 84.81564331054688, Loss: 0.01724465750157833, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13600/20000], Bound: 0.3772689700126648, Entropy: 142.14047241210938, Temp: 2.69897723197937, KL: 70.79502868652344, Loss: 0.01779455877840519, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13601/20000], Bound: 0.3979836106300354, Entropy: 140.81988525390625, Temp: 2.698977470397949, KL: 78.93913269042969, Loss: 0.013920590281486511, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13602/20000], Bound: 0.37798839807510376, Entropy: 141.08096313476562, Temp: 2.6989803314208984, KL: 71.32302856445312, Loss: 0.017200268805027008, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13603/20000], Bound: 0.38261643052101135, Entropy: 140.2635955810547, Temp: 2.698981761932373, KL: 73.69868469238281, Loss: 0.015277822501957417, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13604/20000], Bound: 0.3809662461280823, Entropy: 140.15948486328125, Temp: 2.698984146118164, KL: 73.36740112304688, Loss: 0.01500590518116951, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13605/20000], Bound: 0.39914971590042114, Entropy: 141.23663330078125, Temp: 2.6989874839782715, KL: 78.98793029785156, Loss: 0.014471747912466526, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13606/20000], Bound: 0.3750995397567749, Entropy: 141.81915283203125, Temp: 2.6989924907684326, KL: 71.14463806152344, Loss: 0.01599203422665596, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13607/20000], Bound: 0.3716949224472046, Entropy: 142.22201538085938, Temp: 2.6989972591400146, KL: 70.29078674316406, Loss: 0.01576855219900608, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13608/20000], Bound: 0.3857293426990509, Entropy: 139.6797637939453, Temp: 2.6990013122558594, KL: 73.42671203613281, Loss: 0.01745835319161415, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13609/20000], Bound: 0.3648327887058258, Entropy: 142.02598571777344, Temp: 2.6990041732788086, KL: 69.47019958496094, Loss: 0.013676559552550316, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13610/20000], Bound: 0.3907833397388458, Entropy: 142.8282928466797, Temp: 2.6990084648132324, KL: 73.90415954589844, Loss: 0.01931210793554783, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13611/20000], Bound: 0.42576488852500916, Entropy: 139.0928955078125, Temp: 2.699010133743286, KL: 85.62452697753906, Loss: 0.01712809130549431, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13612/20000], Bound: 0.384501576423645, Entropy: 141.73199462890625, Temp: 2.6990127563476562, KL: 74.15599060058594, Loss: 0.015445292927324772, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13613/20000], Bound: 0.4147244095802307, Entropy: 141.0018768310547, Temp: 2.6990160942077637, KL: 81.40127563476562, Loss: 0.018676698207855225, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13614/20000], Bound: 0.4144653379917145, Entropy: 139.2276153564453, Temp: 2.6990184783935547, KL: 83.90376281738281, Loss: 0.01389480009675026, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13615/20000], Bound: 0.39797210693359375, Entropy: 139.99835205078125, Temp: 2.699023962020874, KL: 77.008544921875, Loss: 0.017491238191723824, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13616/20000], Bound: 0.3840865194797516, Entropy: 141.3944549560547, Temp: 2.699028730392456, KL: 73.75544738769531, Loss: 0.015963882207870483, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13617/20000], Bound: 0.4041091501712799, Entropy: 142.26576232910156, Temp: 2.699033498764038, KL: 78.84677124023438, Loss: 0.017474262043833733, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13618/20000], Bound: 0.4136231541633606, Entropy: 140.55484008789062, Temp: 2.699037790298462, KL: 82.59744262695312, Loss: 0.015840839594602585, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13619/20000], Bound: 0.3607485294342041, Entropy: 143.22596740722656, Temp: 2.6990432739257812, KL: 66.88517761230469, Loss: 0.016332359984517097, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13620/20000], Bound: 0.37281906604766846, Entropy: 140.0697479248047, Temp: 2.699047327041626, KL: 71.28590393066406, Loss: 0.014520645141601562, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13621/20000], Bound: 0.41100013256073, Entropy: 140.99290466308594, Temp: 2.699051856994629, KL: 79.91206359863281, Loss: 0.019342757761478424, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13622/20000], Bound: 0.38651031255722046, Entropy: 141.67535400390625, Temp: 2.6990549564361572, KL: 74.29766845703125, Loss: 0.01626715064048767, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13623/20000], Bound: 0.3930457830429077, Entropy: 141.692138671875, Temp: 2.6990578174591064, KL: 77.56024169921875, Loss: 0.01377194095402956, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13624/20000], Bound: 0.38246965408325195, Entropy: 140.66607666015625, Temp: 2.699063301086426, KL: 71.79255676269531, Loss: 0.01873081922531128, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13625/20000], Bound: 0.39553511142730713, Entropy: 141.5666961669922, Temp: 2.699065923690796, KL: 76.54891967773438, Loss: 0.017006192356348038, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13626/20000], Bound: 0.3941761255264282, Entropy: 142.6282196044922, Temp: 2.699068307876587, KL: 74.47064208984375, Loss: 0.02011275291442871, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13627/20000], Bound: 0.3625410795211792, Entropy: 141.14976501464844, Temp: 2.6990678310394287, KL: 67.77450561523438, Loss: 0.015619841404259205, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13628/20000], Bound: 0.3964531123638153, Entropy: 140.44566345214844, Temp: 2.6990673542022705, KL: 75.24221801757812, Loss: 0.01992986910045147, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13629/20000], Bound: 0.3743557631969452, Entropy: 142.149169921875, Temp: 2.699064016342163, KL: 72.12701416015625, Loss: 0.013777665793895721, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13630/20000], Bound: 0.38735389709472656, Entropy: 139.629150390625, Temp: 2.699063301086426, KL: 74.14295959472656, Loss: 0.017010001465678215, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13631/20000], Bound: 0.3968081772327423, Entropy: 140.1912841796875, Temp: 2.6990621089935303, KL: 77.11236572265625, Loss: 0.016660144552588463, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13632/20000], Bound: 0.383505254983902, Entropy: 142.1605682373047, Temp: 2.699061393737793, KL: 71.33137512207031, Loss: 0.020141886547207832, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13633/20000], Bound: 0.38605815172195435, Entropy: 140.838134765625, Temp: 2.6990575790405273, KL: 73.61332702636719, Loss: 0.017290664836764336, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13634/20000], Bound: 0.39587217569351196, Entropy: 139.56378173828125, Temp: 2.6990535259246826, KL: 76.90635681152344, Loss: 0.016528524458408356, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13635/20000], Bound: 0.38799190521240234, Entropy: 142.35208129882812, Temp: 2.699049949645996, KL: 74.32449340820312, Loss: 0.01701894775032997, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13636/20000], Bound: 0.4177708625793457, Entropy: 141.25942993164062, Temp: 2.6990466117858887, KL: 82.99298095703125, Loss: 0.01744929328560829, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13637/20000], Bound: 0.3769669532775879, Entropy: 142.10916137695312, Temp: 2.6990442276000977, KL: 70.71548461914062, Loss: 0.01778143085539341, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13638/20000], Bound: 0.3888954520225525, Entropy: 141.93829345703125, Temp: 2.699040651321411, KL: 75.26463317871094, Loss: 0.015766892582178116, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13639/20000], Bound: 0.37745487689971924, Entropy: 140.56333923339844, Temp: 2.69903826713562, KL: 71.77365112304688, Loss: 0.01608128473162651, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13640/20000], Bound: 0.38299205899238586, Entropy: 141.35862731933594, Temp: 2.699036121368408, KL: 72.72430419921875, Loss: 0.01728527992963791, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13641/20000], Bound: 0.3908698260784149, Entropy: 141.88035583496094, Temp: 2.699033260345459, KL: 76.69404602050781, Loss: 0.014191007241606712, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13642/20000], Bound: 0.39670172333717346, Entropy: 141.97085571289062, Temp: 2.699032783508301, KL: 77.13186645507812, Loss: 0.016565335914492607, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13643/20000], Bound: 0.4020639657974243, Entropy: 141.6796417236328, Temp: 2.69903302192688, KL: 77.03480529785156, Loss: 0.019698377698659897, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13644/20000], Bound: 0.3853769302368164, Entropy: 142.86550903320312, Temp: 2.699031114578247, KL: 73.24951171875, Loss: 0.017596716061234474, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13645/20000], Bound: 0.3945353329181671, Entropy: 141.2562255859375, Temp: 2.699028491973877, KL: 74.78648376464844, Loss: 0.019723666831851006, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13646/20000], Bound: 0.4048590064048767, Entropy: 141.03782653808594, Temp: 2.699023962020874, KL: 77.93540954589844, Loss: 0.01957857795059681, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13647/20000], Bound: 0.40108203887939453, Entropy: 140.78269958496094, Temp: 2.6990177631378174, KL: 78.24092102050781, Loss: 0.016921289265155792, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13648/20000], Bound: 0.40780049562454224, Entropy: 140.2833709716797, Temp: 2.6990127563476562, KL: 78.57339477539062, Loss: 0.02003350481390953, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13649/20000], Bound: 0.3824433386325836, Entropy: 140.646484375, Temp: 2.6990063190460205, KL: 70.20663452148438, Loss: 0.021654224023222923, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13650/20000], Bound: 0.3899160921573639, Entropy: 139.9633331298828, Temp: 2.69899582862854, KL: 76.05191040039062, Loss: 0.014861890114843845, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13651/20000], Bound: 0.3879643678665161, Entropy: 142.13037109375, Temp: 2.6989879608154297, KL: 73.81900024414062, Loss: 0.017939936369657516, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13652/20000], Bound: 0.39720648527145386, Entropy: 142.90635681152344, Temp: 2.6989798545837402, KL: 77.243408203125, Loss: 0.016635160893201828, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13653/20000], Bound: 0.396510511636734, Entropy: 140.00845336914062, Temp: 2.6989729404449463, KL: 77.29403686523438, Loss: 0.016159430146217346, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13654/20000], Bound: 0.4110639691352844, Entropy: 142.11663818359375, Temp: 2.698967456817627, KL: 81.11817932128906, Loss: 0.017143353819847107, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13655/20000], Bound: 0.3978946805000305, Entropy: 142.0876007080078, Temp: 2.698962926864624, KL: 76.11610412597656, Loss: 0.01910140924155712, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13656/20000], Bound: 0.3770383298397064, Entropy: 142.20306396484375, Temp: 2.6989574432373047, KL: 71.46620178222656, Loss: 0.016428038477897644, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13657/20000], Bound: 0.33723825216293335, Entropy: 142.99185180664062, Temp: 2.6989519596099854, KL: 60.18922424316406, Loss: 0.016689347103238106, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13658/20000], Bound: 0.38494157791137695, Entropy: 141.6907196044922, Temp: 2.6989450454711914, KL: 73.59208679199219, Loss: 0.016726534813642502, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13659/20000], Bound: 0.3928009271621704, Entropy: 140.8824920654297, Temp: 2.6989381313323975, KL: 75.63313293457031, Loss: 0.017207268625497818, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13660/20000], Bound: 0.4054409861564636, Entropy: 140.63363647460938, Temp: 2.698931932449341, KL: 80.37503051757812, Loss: 0.015381453558802605, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13661/20000], Bound: 0.38896986842155457, Entropy: 141.0853271484375, Temp: 2.698928117752075, KL: 74.80194091796875, Loss: 0.016663378104567528, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13662/20000], Bound: 0.38048654794692993, Entropy: 140.5048065185547, Temp: 2.6989247798919678, KL: 74.0491943359375, Loss: 0.013485216535627842, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13663/20000], Bound: 0.40734121203422546, Entropy: 138.6422882080078, Temp: 2.6989240646362305, KL: 80.0069580078125, Loss: 0.017120854929089546, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13664/20000], Bound: 0.4237842559814453, Entropy: 141.27845764160156, Temp: 2.6989240646362305, KL: 84.76739501953125, Loss: 0.017581427469849586, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13665/20000], Bound: 0.3863515257835388, Entropy: 140.92225646972656, Temp: 2.6989247798919678, KL: 74.5899658203125, Loss: 0.01563871279358864, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13666/20000], Bound: 0.3840814530849457, Entropy: 141.3083038330078, Temp: 2.6989259719848633, KL: 74.09916687011719, Loss: 0.015323449857532978, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13667/20000], Bound: 0.3878675699234009, Entropy: 140.453125, Temp: 2.698927879333496, KL: 74.08073425292969, Loss: 0.017402121797204018, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13668/20000], Bound: 0.41323813796043396, Entropy: 140.50970458984375, Temp: 2.6989290714263916, KL: 83.18869018554688, Loss: 0.014527739025652409, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13669/20000], Bound: 0.3849456012248993, Entropy: 140.89990234375, Temp: 2.6989331245422363, KL: 74.58146667480469, Loss: 0.014895688742399216, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13670/20000], Bound: 0.4097895920276642, Entropy: 139.87229919433594, Temp: 2.6989378929138184, KL: 79.99081420898438, Loss: 0.018517998978495598, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13671/20000], Bound: 0.37642186880111694, Entropy: 142.2633056640625, Temp: 2.698941707611084, KL: 72.01370239257812, Loss: 0.01508516538888216, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13672/20000], Bound: 0.3989981412887573, Entropy: 141.11380004882812, Temp: 2.698945999145508, KL: 77.52006530761719, Loss: 0.01710723526775837, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13673/20000], Bound: 0.3547259271144867, Entropy: 143.18460083007812, Temp: 2.6989498138427734, KL: 63.34770202636719, Loss: 0.01976148411631584, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13674/20000], Bound: 0.38255128264427185, Entropy: 141.90512084960938, Temp: 2.698949098587036, KL: 73.571044921875, Loss: 0.015478987246751785, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13675/20000], Bound: 0.3955584466457367, Entropy: 141.94252014160156, Temp: 2.698949098587036, KL: 76.21205139160156, Loss: 0.01764192059636116, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13676/20000], Bound: 0.3877243101596832, Entropy: 143.0493927001953, Temp: 2.698948621749878, KL: 74.45341491699219, Loss: 0.016634292900562286, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13677/20000], Bound: 0.39719563722610474, Entropy: 138.91262817382812, Temp: 2.6989481449127197, KL: 77.08258056640625, Loss: 0.0169268436729908, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13678/20000], Bound: 0.3983811140060425, Entropy: 140.85951232910156, Temp: 2.6989481449127197, KL: 76.83184814453125, Loss: 0.018042713403701782, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13679/20000], Bound: 0.40820884704589844, Entropy: 141.24142456054688, Temp: 2.6989471912384033, KL: 79.90849304199219, Loss: 0.017787383869290352, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13680/20000], Bound: 0.39179933071136475, Entropy: 141.31167602539062, Temp: 2.698946237564087, KL: 77.10902404785156, Loss: 0.013927227817475796, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13681/20000], Bound: 0.3860030472278595, Entropy: 143.00796508789062, Temp: 2.6989479064941406, KL: 75.09425354003906, Loss: 0.01451644767075777, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13682/20000], Bound: 0.3942815363407135, Entropy: 140.8671875, Temp: 2.69895076751709, KL: 76.5218505859375, Loss: 0.016369402408599854, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13683/20000], Bound: 0.3860168755054474, Entropy: 139.2216796875, Temp: 2.6989543437957764, KL: 75.30581665039062, Loss: 0.014132029376924038, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13684/20000], Bound: 0.37917953729629517, Entropy: 141.99818420410156, Temp: 2.6989593505859375, KL: 71.42814636230469, Loss: 0.01764172501862049, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13685/20000], Bound: 0.38811755180358887, Entropy: 141.1671142578125, Temp: 2.698962688446045, KL: 74.98893737792969, Loss: 0.015855303034186363, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13686/20000], Bound: 0.4142857491970062, Entropy: 140.26100158691406, Temp: 2.6989662647247314, KL: 81.12420654296875, Loss: 0.018942316994071007, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13687/20000], Bound: 0.3943876326084137, Entropy: 140.39907836914062, Temp: 2.6989686489105225, KL: 76.28349304199219, Loss: 0.01686910167336464, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13688/20000], Bound: 0.39360958337783813, Entropy: 142.28038024902344, Temp: 2.6989707946777344, KL: 73.1275634765625, Loss: 0.022290578112006187, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13689/20000], Bound: 0.386229544878006, Entropy: 141.83290100097656, Temp: 2.6989684104919434, KL: 74.13432312011719, Loss: 0.016417307779192924, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13690/20000], Bound: 0.3919364809989929, Entropy: 140.12925720214844, Temp: 2.6989662647247314, KL: 75.39753723144531, Loss: 0.0171728003770113, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13691/20000], Bound: 0.3887864649295807, Entropy: 141.24966430664062, Temp: 2.6989641189575195, KL: 72.30534362792969, Loss: 0.02118934877216816, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13692/20000], Bound: 0.37821194529533386, Entropy: 140.23741149902344, Temp: 2.698958158493042, KL: 71.77671813964844, Loss: 0.016478924080729485, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13693/20000], Bound: 0.38571467995643616, Entropy: 141.76806640625, Temp: 2.6989526748657227, KL: 75.27989196777344, Loss: 0.014016850851476192, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13694/20000], Bound: 0.3836163878440857, Entropy: 141.18582153320312, Temp: 2.6989498138427734, KL: 72.28511047363281, Loss: 0.01843401789665222, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13695/20000], Bound: 0.370275616645813, Entropy: 139.86131286621094, Temp: 2.6989452838897705, KL: 69.46015930175781, Loss: 0.016556866466999054, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13696/20000], Bound: 0.3757755160331726, Entropy: 140.97767639160156, Temp: 2.6989407539367676, KL: 71.56059265136719, Loss: 0.015580503270030022, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13697/20000], Bound: 0.3962356746196747, Entropy: 141.7001190185547, Temp: 2.698936939239502, KL: 77.124267578125, Loss: 0.016322901472449303, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13698/20000], Bound: 0.38850006461143494, Entropy: 138.94570922851562, Temp: 2.6989340782165527, KL: 75.16970825195312, Loss: 0.01572738215327263, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13699/20000], Bound: 0.38055992126464844, Entropy: 139.9049072265625, Temp: 2.69893217086792, KL: 71.48114013671875, Loss: 0.018282143399119377, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13700/20000], Bound: 0.41557809710502625, Entropy: 141.92758178710938, Temp: 2.6989288330078125, KL: 84.28823852539062, Loss: 0.013808932155370712, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13701/20000], Bound: 0.3992730379104614, Entropy: 140.0763702392578, Temp: 2.6989293098449707, KL: 78.56698608398438, Loss: 0.015318862162530422, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13702/20000], Bound: 0.37281349301338196, Entropy: 141.35072326660156, Temp: 2.6989314556121826, KL: 69.82481384277344, Loss: 0.017223438248038292, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13703/20000], Bound: 0.36319562792778015, Entropy: 143.21726989746094, Temp: 2.698931932449341, KL: 67.68562316894531, Loss: 0.016125384718179703, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13704/20000], Bound: 0.39231759309768677, Entropy: 139.60226440429688, Temp: 2.698931932449341, KL: 76.886962890625, Loss: 0.014620902948081493, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13705/20000], Bound: 0.39203542470932007, Entropy: 141.2057647705078, Temp: 2.6989338397979736, KL: 74.87161254882812, Loss: 0.018200721591711044, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13706/20000], Bound: 0.371148020029068, Entropy: 140.55731201171875, Temp: 2.698934316635132, KL: 69.23841857910156, Loss: 0.01742844097316265, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13707/20000], Bound: 0.4055866599082947, Entropy: 138.34356689453125, Temp: 2.6989331245422363, KL: 78.79248046875, Loss: 0.01839423179626465, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13708/20000], Bound: 0.36593157052993774, Entropy: 141.2020263671875, Temp: 2.6989314556121826, KL: 68.79092407226562, Loss: 0.015510394237935543, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13709/20000], Bound: 0.3936045467853546, Entropy: 142.23641967773438, Temp: 2.698930025100708, KL: 74.65350341796875, Loss: 0.019460612908005714, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13710/20000], Bound: 0.38029107451438904, Entropy: 141.05252075195312, Temp: 2.6989264488220215, KL: 74.3509521484375, Loss: 0.012821533717215061, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13711/20000], Bound: 0.39984822273254395, Entropy: 140.727294921875, Temp: 2.6989262104034424, KL: 79.41192626953125, Loss: 0.014070366509258747, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13712/20000], Bound: 0.37555646896362305, Entropy: 141.07896423339844, Temp: 2.6989285945892334, KL: 72.31399536132812, Loss: 0.014068113639950752, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13713/20000], Bound: 0.4012996256351471, Entropy: 141.5789794921875, Temp: 2.698932409286499, KL: 79.70333862304688, Loss: 0.01433135848492384, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13714/20000], Bound: 0.3954209089279175, Entropy: 141.7340545654297, Temp: 2.6989383697509766, KL: 77.03363037109375, Loss: 0.016044460237026215, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13715/20000], Bound: 0.37981700897216797, Entropy: 140.93264770507812, Temp: 2.6989448070526123, KL: 71.91154479980469, Loss: 0.017087066546082497, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13716/20000], Bound: 0.39395779371261597, Entropy: 139.86270141601562, Temp: 2.6989493370056152, KL: 76.71153259277344, Loss: 0.015841035172343254, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13717/20000], Bound: 0.3834230899810791, Entropy: 141.07557678222656, Temp: 2.6989545822143555, KL: 74.05522155761719, Loss: 0.015050816349685192, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13718/20000], Bound: 0.4077083468437195, Entropy: 139.80731201171875, Temp: 2.698960304260254, KL: 81.14471435546875, Loss: 0.015218136832118034, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13719/20000], Bound: 0.37780264019966125, Entropy: 141.7257843017578, Temp: 2.698967695236206, KL: 71.562255859375, Loss: 0.016657834872603416, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13720/20000], Bound: 0.3803209960460663, Entropy: 140.3351593017578, Temp: 2.6989736557006836, KL: 74.14205932617188, Loss: 0.01322497334331274, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13721/20000], Bound: 0.39307883381843567, Entropy: 140.8112030029297, Temp: 2.698981523513794, KL: 75.42034912109375, Loss: 0.0177534781396389, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13722/20000], Bound: 0.3806188404560089, Entropy: 142.69114685058594, Temp: 2.6989879608154297, KL: 73.21128845214844, Loss: 0.015108983963727951, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13723/20000], Bound: 0.363798588514328, Entropy: 140.40097045898438, Temp: 2.6989946365356445, KL: 69.32525634765625, Loss: 0.013403638266026974, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13724/20000], Bound: 0.36725714802742004, Entropy: 141.74839782714844, Temp: 2.699002504348755, KL: 67.80984497070312, Loss: 0.018024567514657974, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13725/20000], Bound: 0.38560381531715393, Entropy: 142.97430419921875, Temp: 2.699007511138916, KL: 74.48641967773438, Loss: 0.015427511185407639, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13726/20000], Bound: 0.3734160363674164, Entropy: 142.18307495117188, Temp: 2.6990127563476562, KL: 71.16816711425781, Loss: 0.015054838731884956, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13727/20000], Bound: 0.35808083415031433, Entropy: 141.54164123535156, Temp: 2.6990180015563965, KL: 64.78634643554688, Loss: 0.018833506852388382, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13728/20000], Bound: 0.42162925004959106, Entropy: 141.40731811523438, Temp: 2.69901967048645, KL: 84.35911560058594, Loss: 0.017109345644712448, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13729/20000], Bound: 0.39772024750709534, Entropy: 142.3081817626953, Temp: 2.6990222930908203, KL: 76.84092712402344, Loss: 0.01766335591673851, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13730/20000], Bound: 0.3709506392478943, Entropy: 141.5499725341797, Temp: 2.699023962020874, KL: 69.07171630859375, Loss: 0.01763363741338253, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13731/20000], Bound: 0.3722119629383087, Entropy: 141.66661071777344, Temp: 2.699023962020874, KL: 70.07261657714844, Loss: 0.016446534544229507, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13732/20000], Bound: 0.39504751563072205, Entropy: 141.61183166503906, Temp: 2.699023723602295, KL: 77.58781433105469, Loss: 0.014814300462603569, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13733/20000], Bound: 0.3500162661075592, Entropy: 141.41900634765625, Temp: 2.6990249156951904, KL: 64.37347412109375, Loss: 0.01543729193508625, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13734/20000], Bound: 0.40044814348220825, Entropy: 141.7261199951172, Temp: 2.6990256309509277, KL: 78.229736328125, Loss: 0.01659228652715683, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13735/20000], Bound: 0.38472986221313477, Entropy: 140.99270629882812, Temp: 2.699026584625244, KL: 74.252685546875, Loss: 0.015389296226203442, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13736/20000], Bound: 0.41480007767677307, Entropy: 138.88555908203125, Temp: 2.699028491973877, KL: 81.254638671875, Loss: 0.01899113692343235, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13737/20000], Bound: 0.39507994055747986, Entropy: 140.64759826660156, Temp: 2.6990294456481934, KL: 77.26657104492188, Loss: 0.015427191741764545, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13738/20000], Bound: 0.38698893785476685, Entropy: 140.124267578125, Temp: 2.6990315914154053, KL: 74.865234375, Loss: 0.015474259853363037, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13739/20000], Bound: 0.3835253119468689, Entropy: 140.41648864746094, Temp: 2.6990344524383545, KL: 73.22389221191406, Loss: 0.016646558418869972, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13740/20000], Bound: 0.3890165686607361, Entropy: 140.09898376464844, Temp: 2.6990365982055664, KL: 75.45823669433594, Loss: 0.015473880805075169, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13741/20000], Bound: 0.39209073781967163, Entropy: 142.39329528808594, Temp: 2.699039936065674, KL: 74.57394409179688, Loss: 0.018783237785100937, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13742/20000], Bound: 0.3699089586734772, Entropy: 140.99594116210938, Temp: 2.6990408897399902, KL: 67.84483337402344, Loss: 0.01935652643442154, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13743/20000], Bound: 0.4052181541919708, Entropy: 140.73455810546875, Temp: 2.6990387439727783, KL: 80.54887390136719, Loss: 0.01493674237281084, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13744/20000], Bound: 0.38448366522789, Entropy: 140.14088439941406, Temp: 2.6990389823913574, KL: 73.60108947753906, Loss: 0.016463862732052803, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13745/20000], Bound: 0.3866035044193268, Entropy: 141.84005737304688, Temp: 2.6990392208099365, KL: 75.25920104980469, Loss: 0.01453615352511406, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13746/20000], Bound: 0.3821539282798767, Entropy: 143.35940551757812, Temp: 2.6990411281585693, KL: 73.27456665039062, Loss: 0.01581561379134655, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13747/20000], Bound: 0.38603049516677856, Entropy: 141.36090087890625, Temp: 2.6990435123443604, KL: 73.39276123046875, Loss: 0.017684204503893852, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13748/20000], Bound: 0.38584619760513306, Entropy: 141.33961486816406, Temp: 2.6990442276000977, KL: 74.68064880371094, Loss: 0.015198895707726479, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13749/20000], Bound: 0.38053128123283386, Entropy: 139.81765747070312, Temp: 2.6990463733673096, KL: 73.18208312988281, Loss: 0.015116676688194275, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13750/20000], Bound: 0.3914240598678589, Entropy: 142.27084350585938, Temp: 2.6990489959716797, KL: 74.70755004882812, Loss: 0.018172701820731163, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13751/20000], Bound: 0.36695489287376404, Entropy: 140.66856384277344, Temp: 2.6990504264831543, KL: 68.22276306152344, Loss: 0.017101138830184937, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13752/20000], Bound: 0.39188694953918457, Entropy: 139.38784790039062, Temp: 2.699049949645996, KL: 75.21723937988281, Loss: 0.017480574548244476, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13753/20000], Bound: 0.35797515511512756, Entropy: 140.87840270996094, Temp: 2.699049472808838, KL: 67.07925415039062, Loss: 0.014531233347952366, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13754/20000], Bound: 0.36728039383888245, Entropy: 138.0555877685547, Temp: 2.699049234390259, KL: 69.11199951171875, Loss: 0.015624904073774815, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13755/20000], Bound: 0.38431549072265625, Entropy: 143.29640197753906, Temp: 2.6990489959716797, KL: 73.5509033203125, Loss: 0.016466328874230385, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13756/20000], Bound: 0.39760178327560425, Entropy: 140.20399475097656, Temp: 2.6990487575531006, KL: 78.418701171875, Loss: 0.014675657264888287, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13757/20000], Bound: 0.36399227380752563, Entropy: 141.46585083007812, Temp: 2.6990504264831543, KL: 66.56539916992188, Loss: 0.018618104979395866, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13758/20000], Bound: 0.40220263600349426, Entropy: 138.8225555419922, Temp: 2.699049472808838, KL: 79.60484313964844, Loss: 0.015014130622148514, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13759/20000], Bound: 0.3798418641090393, Entropy: 140.54782104492188, Temp: 2.6990504264831543, KL: 72.27870178222656, Loss: 0.01642109826207161, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13760/20000], Bound: 0.3742012679576874, Entropy: 141.79928588867188, Temp: 2.6990509033203125, KL: 69.821044921875, Loss: 0.01796731911599636, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13761/20000], Bound: 0.3916894197463989, Entropy: 142.10862731933594, Temp: 2.699049711227417, KL: 75.61685180664062, Loss: 0.01663271151483059, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13762/20000], Bound: 0.3948120176792145, Entropy: 142.15623474121094, Temp: 2.6990489959716797, KL: 76.32151794433594, Loss: 0.017031513154506683, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13763/20000], Bound: 0.40581652522087097, Entropy: 140.9930877685547, Temp: 2.6990480422973633, KL: 79.62727355957031, Loss: 0.01697668246924877, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13764/20000], Bound: 0.39242732524871826, Entropy: 142.0704803466797, Temp: 2.699047803878784, KL: 75.91127014160156, Loss: 0.016489295288920403, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13765/20000], Bound: 0.38419950008392334, Entropy: 140.47048950195312, Temp: 2.699047803878784, KL: 74.81529235839844, Loss: 0.014061533845961094, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13766/20000], Bound: 0.3910887837409973, Entropy: 138.54054260253906, Temp: 2.699049711227417, KL: 76.76481628417969, Loss: 0.014179179444909096, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13767/20000], Bound: 0.37203699350357056, Entropy: 140.37928771972656, Temp: 2.6990537643432617, KL: 70.43260192871094, Loss: 0.01568729244172573, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13768/20000], Bound: 0.3605363070964813, Entropy: 143.02328491210938, Temp: 2.6990575790405273, KL: 67.65223693847656, Loss: 0.014800962060689926, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13769/20000], Bound: 0.40758663415908813, Entropy: 142.1482391357422, Temp: 2.699061393737793, KL: 79.6854248046875, Loss: 0.017854714766144753, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13770/20000], Bound: 0.3722459077835083, Entropy: 140.97824096679688, Temp: 2.6990644931793213, KL: 69.57667541503906, Loss: 0.017383545637130737, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13771/20000], Bound: 0.3809613883495331, Entropy: 139.44126892089844, Temp: 2.699066162109375, KL: 72.740478515625, Loss: 0.01616540551185608, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13772/20000], Bound: 0.3834942877292633, Entropy: 143.14950561523438, Temp: 2.6990673542022705, KL: 73.92701721191406, Loss: 0.015327628701925278, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13773/20000], Bound: 0.3884730041027069, Entropy: 142.32276916503906, Temp: 2.6990697383880615, KL: 74.72288513183594, Loss: 0.016541747376322746, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13774/20000], Bound: 0.38330912590026855, Entropy: 140.3173065185547, Temp: 2.6990716457366943, KL: 74.75991821289062, Loss: 0.01368508581072092, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13775/20000], Bound: 0.39512911438941956, Entropy: 141.388671875, Temp: 2.699075698852539, KL: 77.89476013183594, Loss: 0.0142908226698637, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13776/20000], Bound: 0.3724045753479004, Entropy: 141.2850799560547, Temp: 2.6990816593170166, KL: 71.34620666503906, Loss: 0.014189649373292923, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13777/20000], Bound: 0.3666982352733612, Entropy: 141.26634216308594, Temp: 2.6990883350372314, KL: 68.80630493164062, Loss: 0.015885597094893456, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13778/20000], Bound: 0.3876233398914337, Entropy: 141.68870544433594, Temp: 2.69909405708313, KL: 75.19796752929688, Loss: 0.015201713889837265, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13779/20000], Bound: 0.38565585017204285, Entropy: 141.59222412109375, Temp: 2.6991004943847656, KL: 73.33963012695312, Loss: 0.017580848187208176, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13780/20000], Bound: 0.375376433134079, Entropy: 140.1249237060547, Temp: 2.6991055011749268, KL: 71.54615783691406, Loss: 0.015396400354802608, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13781/20000], Bound: 0.3918336033821106, Entropy: 141.43048095703125, Temp: 2.699110269546509, KL: 75.92707824707031, Loss: 0.01613711006939411, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13782/20000], Bound: 0.38590937852859497, Entropy: 139.494140625, Temp: 2.69911527633667, KL: 74.19853210449219, Loss: 0.01612677052617073, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13783/20000], Bound: 0.39884814620018005, Entropy: 141.20083618164062, Temp: 2.699119806289673, KL: 77.07708740234375, Loss: 0.017846915870904922, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13784/20000], Bound: 0.3851347863674164, Entropy: 141.39242553710938, Temp: 2.6991236209869385, KL: 72.01339721679688, Loss: 0.019756730645895004, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13785/20000], Bound: 0.35734012722969055, Entropy: 142.6048583984375, Temp: 2.6991240978240967, KL: 67.04927062988281, Loss: 0.014258100651204586, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13786/20000], Bound: 0.37425678968429565, Entropy: 141.62879943847656, Temp: 2.699125289916992, KL: 70.31413269042969, Loss: 0.017083968967199326, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13787/20000], Bound: 0.389888733625412, Entropy: 141.2281951904297, Temp: 2.699125289916992, KL: 73.623291015625, Loss: 0.019347187131643295, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13788/20000], Bound: 0.3516489565372467, Entropy: 141.42369079589844, Temp: 2.6991231441497803, KL: 63.39393615722656, Loss: 0.018091192469000816, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13789/20000], Bound: 0.3827984929084778, Entropy: 142.68948364257812, Temp: 2.699118137359619, KL: 72.48011779785156, Loss: 0.017634259536862373, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13790/20000], Bound: 0.38671591877937317, Entropy: 142.60662841796875, Temp: 2.6991126537323, KL: 74.19879150390625, Loss: 0.01656198315322399, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13791/20000], Bound: 0.38704127073287964, Entropy: 141.41629028320312, Temp: 2.6991076469421387, KL: 75.45944213867188, Loss: 0.014402548782527447, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13792/20000], Bound: 0.3726177513599396, Entropy: 141.6381378173828, Temp: 2.6991050243377686, KL: 69.55398559570312, Loss: 0.017622819170355797, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13793/20000], Bound: 0.4144766330718994, Entropy: 139.7034149169922, Temp: 2.699100971221924, KL: 82.56825256347656, Loss: 0.016376106068491936, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13794/20000], Bound: 0.33600014448165894, Entropy: 142.86932373046875, Temp: 2.699098825454712, KL: 62.09461975097656, Loss: 0.01253686472773552, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13795/20000], Bound: 0.36678028106689453, Entropy: 141.3900909423828, Temp: 2.6990981101989746, KL: 68.96658325195312, Loss: 0.015631871297955513, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13796/20000], Bound: 0.41595691442489624, Entropy: 142.43870544433594, Temp: 2.6990973949432373, KL: 84.35252380371094, Loss: 0.013905667699873447, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13797/20000], Bound: 0.37703537940979004, Entropy: 141.92820739746094, Temp: 2.6991002559661865, KL: 70.57295227050781, Loss: 0.018082408234477043, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13798/20000], Bound: 0.3854953646659851, Entropy: 140.15164184570312, Temp: 2.699101209640503, KL: 75.89726257324219, Loss: 0.012756356969475746, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13799/20000], Bound: 0.41333410143852234, Entropy: 140.35609436035156, Temp: 2.6991052627563477, KL: 81.000244140625, Loss: 0.01863771677017212, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13800/20000], Bound: 0.3857443630695343, Entropy: 141.65879821777344, Temp: 2.699108123779297, KL: 74.43020629882812, Loss: 0.015608449466526508, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13801/20000], Bound: 0.38936492800712585, Entropy: 142.00633239746094, Temp: 2.6991114616394043, KL: 74.54753112792969, Loss: 0.017350638285279274, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13802/20000], Bound: 0.3869292736053467, Entropy: 140.55699157714844, Temp: 2.6991138458251953, KL: 74.9354248046875, Loss: 0.015312728472054005, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13803/20000], Bound: 0.38319626450538635, Entropy: 140.7704620361328, Temp: 2.6991171836853027, KL: 73.43972778320312, Loss: 0.016070475801825523, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13804/20000], Bound: 0.3908267319202423, Entropy: 142.0673370361328, Temp: 2.69912052154541, KL: 73.96266174316406, Loss: 0.019228223711252213, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13805/20000], Bound: 0.38811764121055603, Entropy: 143.10398864746094, Temp: 2.6991214752197266, KL: 75.34156799316406, Loss: 0.015203606337308884, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13806/20000], Bound: 0.4000856578350067, Entropy: 141.9554443359375, Temp: 2.6991236209869385, KL: 78.81271362304688, Loss: 0.01531339343637228, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13807/20000], Bound: 0.408941388130188, Entropy: 139.3317108154297, Temp: 2.699126958847046, KL: 79.89089965820312, Loss: 0.01823076233267784, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13808/20000], Bound: 0.3857886791229248, Entropy: 140.05322265625, Temp: 2.699129343032837, KL: 75.22872924804688, Loss: 0.01415332406759262, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13809/20000], Bound: 0.37646982073783875, Entropy: 140.3061065673828, Temp: 2.69913387298584, KL: 70.94244384765625, Loss: 0.017096856608986855, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13810/20000], Bound: 0.37285998463630676, Entropy: 140.9551544189453, Temp: 2.699136734008789, KL: 71.20782470703125, Loss: 0.01468774862587452, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13811/20000], Bound: 0.38618090748786926, Entropy: 141.5225372314453, Temp: 2.6991405487060547, KL: 72.70323181152344, Loss: 0.01904357224702835, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13812/20000], Bound: 0.3867241442203522, Entropy: 142.51458740234375, Temp: 2.69914174079895, KL: 73.72665405273438, Loss: 0.01744130626320839, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13813/20000], Bound: 0.38924291729927063, Entropy: 141.62533569335938, Temp: 2.6991419792175293, KL: 75.20509338378906, Loss: 0.01606660522520542, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13814/20000], Bound: 0.36491501331329346, Entropy: 142.86265563964844, Temp: 2.6991426944732666, KL: 66.67446899414062, Loss: 0.01889980211853981, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13815/20000], Bound: 0.4049089550971985, Entropy: 140.50595092773438, Temp: 2.6991403102874756, KL: 79.28286743164062, Loss: 0.017111269757151604, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13816/20000], Bound: 0.4147835075855255, Entropy: 139.83384704589844, Temp: 2.699138641357422, KL: 79.66778564453125, Loss: 0.021922418847680092, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13817/20000], Bound: 0.3772011399269104, Entropy: 140.013427734375, Temp: 2.6991336345672607, KL: 70.123046875, Loss: 0.0190044604241848, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13818/20000], Bound: 0.40844032168388367, Entropy: 141.5925750732422, Temp: 2.699126958847046, KL: 80.73779296875, Loss: 0.01638207957148552, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13819/20000], Bound: 0.4034227430820465, Entropy: 138.65223693847656, Temp: 2.6991217136383057, KL: 79.36366271972656, Loss: 0.01613706164062023, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13820/20000], Bound: 0.40408021211624146, Entropy: 140.0548553466797, Temp: 2.699118137359619, KL: 80.4244384765625, Loss: 0.014536457136273384, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13821/20000], Bound: 0.39436691999435425, Entropy: 138.0075225830078, Temp: 2.699117422103882, KL: 76.29966735839844, Loss: 0.01682921312749386, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13822/20000], Bound: 0.3649021089076996, Entropy: 143.58323669433594, Temp: 2.6991169452667236, KL: 67.80728149414062, Loss: 0.016794348135590553, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13823/20000], Bound: 0.39724692702293396, Entropy: 141.08738708496094, Temp: 2.699115514755249, KL: 77.20281982421875, Loss: 0.016733840107917786, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13824/20000], Bound: 0.3883468508720398, Entropy: 141.38075256347656, Temp: 2.6991143226623535, KL: 74.98014831542969, Loss: 0.01599719561636448, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13825/20000], Bound: 0.398311048746109, Entropy: 141.37474060058594, Temp: 2.6991138458251953, KL: 78.59126281738281, Loss: 0.01474645547568798, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13826/20000], Bound: 0.39357149600982666, Entropy: 139.91848754882812, Temp: 2.699115514755249, KL: 75.9908447265625, Loss: 0.016966721042990685, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13827/20000], Bound: 0.37312376499176025, Entropy: 140.73199462890625, Temp: 2.6991167068481445, KL: 72.92140197753906, Loss: 0.011653007008135319, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13828/20000], Bound: 0.38412126898765564, Entropy: 138.9681854248047, Temp: 2.6991217136383057, KL: 74.48194885253906, Loss: 0.014637620188295841, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13829/20000], Bound: 0.3876568078994751, Entropy: 142.8730010986328, Temp: 2.699127435684204, KL: 74.07781982421875, Loss: 0.01729513518512249, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13830/20000], Bound: 0.37912312150001526, Entropy: 141.0948486328125, Temp: 2.699132204055786, KL: 72.73338317871094, Loss: 0.015195080079138279, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13831/20000], Bound: 0.3785153925418854, Entropy: 141.74085998535156, Temp: 2.6991372108459473, KL: 73.33921813964844, Loss: 0.013748090714216232, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13832/20000], Bound: 0.3955119252204895, Entropy: 142.79612731933594, Temp: 2.699143648147583, KL: 77.59130859375, Loss: 0.015063230879604816, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13833/20000], Bound: 0.37821587920188904, Entropy: 141.2523193359375, Temp: 2.6991512775421143, KL: 72.90650939941406, Loss: 0.014389817602932453, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13834/20000], Bound: 0.41985461115837097, Entropy: 139.8841094970703, Temp: 2.6991593837738037, KL: 84.7447509765625, Loss: 0.015387135557830334, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13835/20000], Bound: 0.3735647201538086, Entropy: 141.16619873046875, Temp: 2.699169397354126, KL: 71.01356506347656, Loss: 0.015421445481479168, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13836/20000], Bound: 0.39687493443489075, Entropy: 141.2846221923828, Temp: 2.699178695678711, KL: 77.36369323730469, Loss: 0.016232309862971306, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13837/20000], Bound: 0.39165064692497253, Entropy: 141.4953155517578, Temp: 2.699187755584717, KL: 74.71174621582031, Loss: 0.018289504572749138, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13838/20000], Bound: 0.3901807963848114, Entropy: 140.35382080078125, Temp: 2.6991946697235107, KL: 74.791015625, Loss: 0.017343319952487946, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13839/20000], Bound: 0.3764682412147522, Entropy: 141.17349243164062, Temp: 2.69920015335083, KL: 71.7528076171875, Loss: 0.015595466829836369, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13840/20000], Bound: 0.39143022894859314, Entropy: 139.73062133789062, Temp: 2.6992056369781494, KL: 75.29312133789062, Loss: 0.017092756927013397, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13841/20000], Bound: 0.3758121132850647, Entropy: 140.45082092285156, Temp: 2.6992104053497314, KL: 70.87580871582031, Loss: 0.016870815306901932, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13842/20000], Bound: 0.3874371647834778, Entropy: 141.98861694335938, Temp: 2.699213743209839, KL: 74.41239929199219, Loss: 0.016557279974222183, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13843/20000], Bound: 0.41123175621032715, Entropy: 141.61927795410156, Temp: 2.699216842651367, KL: 81.58122253417969, Loss: 0.01638220250606537, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13844/20000], Bound: 0.42465028166770935, Entropy: 138.11807250976562, Temp: 2.699220895767212, KL: 85.26127624511719, Loss: 0.017164988443255424, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13845/20000], Bound: 0.3975798487663269, Entropy: 141.60585021972656, Temp: 2.699225664138794, KL: 78.40385437011719, Loss: 0.014692949131131172, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13846/20000], Bound: 0.3688070774078369, Entropy: 143.20811462402344, Temp: 2.699232339859009, KL: 69.98147583007812, Loss: 0.01481911726295948, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13847/20000], Bound: 0.395695298910141, Entropy: 142.95140075683594, Temp: 2.6992385387420654, KL: 77.57357788085938, Loss: 0.015197431668639183, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13848/20000], Bound: 0.34254658222198486, Entropy: 141.21009826660156, Temp: 2.6992459297180176, KL: 62.863861083984375, Loss: 0.014422529377043247, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13849/20000], Bound: 0.38132569193840027, Entropy: 140.92892456054688, Temp: 2.6992523670196533, KL: 72.63951110839844, Loss: 0.016549406573176384, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13850/20000], Bound: 0.39126715064048767, Entropy: 142.11810302734375, Temp: 2.6992580890655518, KL: 76.61021423339844, Loss: 0.014564684592187405, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13851/20000], Bound: 0.37555840611457825, Entropy: 141.02064514160156, Temp: 2.6992650032043457, KL: 71.33290100097656, Loss: 0.015889598056674004, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13852/20000], Bound: 0.4164770543575287, Entropy: 140.626220703125, Temp: 2.6992712020874023, KL: 83.99908447265625, Loss: 0.014856209047138691, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13853/20000], Bound: 0.38774603605270386, Entropy: 140.47299194335938, Temp: 2.699279546737671, KL: 76.63829040527344, Loss: 0.012601873837411404, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13854/20000], Bound: 0.381356418132782, Entropy: 140.46142578125, Temp: 2.6992905139923096, KL: 72.73269653320312, Loss: 0.01639357954263687, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13855/20000], Bound: 0.3483954966068268, Entropy: 140.36952209472656, Temp: 2.6993002891540527, KL: 64.32376098632812, Loss: 0.014700775034725666, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13856/20000], Bound: 0.40604400634765625, Entropy: 138.58787536621094, Temp: 2.6993088722229004, KL: 79.48274230957031, Loss: 0.017373502254486084, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13857/20000], Bound: 0.374153196811676, Entropy: 142.7721405029297, Temp: 2.69931697845459, KL: 69.91300964355469, Loss: 0.01777351275086403, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13858/20000], Bound: 0.3936042785644531, Entropy: 141.77511596679688, Temp: 2.699322462081909, KL: 77.66903686523438, Loss: 0.013878011144697666, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13859/20000], Bound: 0.38694724440574646, Entropy: 138.82386779785156, Temp: 2.6993300914764404, KL: 75.49284362792969, Loss: 0.014291990548372269, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13860/20000], Bound: 0.3555046319961548, Entropy: 139.55084228515625, Temp: 2.699338674545288, KL: 67.55720520019531, Loss: 0.012368876487016678, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13861/20000], Bound: 0.36711129546165466, Entropy: 143.03628540039062, Temp: 2.6993486881256104, KL: 70.11257934570312, Loss: 0.013685082085430622, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13862/20000], Bound: 0.39483052492141724, Entropy: 140.00885009765625, Temp: 2.69935941696167, KL: 77.45254516601562, Loss: 0.014949521981179714, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13863/20000], Bound: 0.3846137523651123, Entropy: 139.4556121826172, Temp: 2.699370861053467, KL: 72.69500732421875, Loss: 0.018215229734778404, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13864/20000], Bound: 0.3806841969490051, Entropy: 141.97427368164062, Temp: 2.6993794441223145, KL: 72.26527404785156, Loss: 0.01689983159303665, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13865/20000], Bound: 0.41136664152145386, Entropy: 141.0873565673828, Temp: 2.6993865966796875, KL: 82.279296875, Loss: 0.01516661699861288, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13866/20000], Bound: 0.3766285181045532, Entropy: 143.8764190673828, Temp: 2.699395179748535, KL: 72.28013610839844, Loss: 0.014705714769661427, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13867/20000], Bound: 0.36717697978019714, Entropy: 141.5240020751953, Temp: 2.699404239654541, KL: 68.33157348632812, Loss: 0.01701897755265236, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13868/20000], Bound: 0.3467766344547272, Entropy: 142.93553161621094, Temp: 2.699410915374756, KL: 63.362640380859375, Loss: 0.015654070302844048, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13869/20000], Bound: 0.4021170437335968, Entropy: 142.22616577148438, Temp: 2.699416160583496, KL: 78.81515502929688, Loss: 0.016433391720056534, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13870/20000], Bound: 0.401976615190506, Entropy: 143.69627380371094, Temp: 2.6994214057922363, KL: 78.7257080078125, Loss: 0.01652146503329277, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13871/20000], Bound: 0.3927590250968933, Entropy: 142.2631378173828, Temp: 2.6994271278381348, KL: 76.39543151855469, Loss: 0.015776898711919785, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13872/20000], Bound: 0.36798229813575745, Entropy: 139.72625732421875, Temp: 2.6994330883026123, KL: 68.51615905761719, Loss: 0.01710076816380024, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13873/20000], Bound: 0.38378414511680603, Entropy: 142.32472229003906, Temp: 2.699437141418457, KL: 72.94174194335938, Loss: 0.01731199026107788, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13874/20000], Bound: 0.39352133870124817, Entropy: 142.20172119140625, Temp: 2.6994400024414062, KL: 76.83061218261719, Loss: 0.015386893413960934, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13875/20000], Bound: 0.3719403147697449, Entropy: 141.43148803710938, Temp: 2.699443817138672, KL: 68.29251098632812, Loss: 0.019603358581662178, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13876/20000], Bound: 0.38584408164024353, Entropy: 141.606689453125, Temp: 2.699443817138672, KL: 74.81697082519531, Loss: 0.014949017204344273, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13877/20000], Bound: 0.38063618540763855, Entropy: 140.79017639160156, Temp: 2.6994452476501465, KL: 72.77032470703125, Loss: 0.015939204022288322, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13878/20000], Bound: 0.3699389696121216, Entropy: 142.17242431640625, Temp: 2.6994469165802, KL: 69.00448608398438, Loss: 0.01722724549472332, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13879/20000], Bound: 0.37139207124710083, Entropy: 141.72079467773438, Temp: 2.6994469165802, KL: 69.40591430664062, Loss: 0.017251158133149147, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13880/20000], Bound: 0.39640122652053833, Entropy: 137.7540283203125, Temp: 2.6994454860687256, KL: 78.02473449707031, Loss: 0.014750685542821884, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13881/20000], Bound: 0.4244512915611267, Entropy: 140.30496215820312, Temp: 2.699446439743042, KL: 85.69972229003906, Loss: 0.016241593286395073, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13882/20000], Bound: 0.38015273213386536, Entropy: 143.18634033203125, Temp: 2.699449062347412, KL: 72.59661865234375, Loss: 0.01600205898284912, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13883/20000], Bound: 0.39445388317108154, Entropy: 141.21392822265625, Temp: 2.6994516849517822, KL: 75.08319091796875, Loss: 0.019133074209094048, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13884/20000], Bound: 0.3913472890853882, Entropy: 139.91160583496094, Temp: 2.6994521617889404, KL: 75.44136047363281, Loss: 0.016775252297520638, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13885/20000], Bound: 0.38132771849632263, Entropy: 141.91217041015625, Temp: 2.6994526386260986, KL: 72.66220092773438, Loss: 0.01651020534336567, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13886/20000], Bound: 0.38969719409942627, Entropy: 141.76649475097656, Temp: 2.6994528770446777, KL: 73.96803283691406, Loss: 0.018607351928949356, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13887/20000], Bound: 0.4035666286945343, Entropy: 139.96812438964844, Temp: 2.699451208114624, KL: 79.71441650390625, Loss: 0.015570461750030518, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13888/20000], Bound: 0.40389132499694824, Entropy: 139.9660186767578, Temp: 2.699451446533203, KL: 78.58465576171875, Loss: 0.017842978239059448, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13889/20000], Bound: 0.393746942281723, Entropy: 140.3502655029297, Temp: 2.699451446533203, KL: 78.57879638671875, Loss: 0.012272145599126816, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13890/20000], Bound: 0.396870881319046, Entropy: 139.51368713378906, Temp: 2.6994552612304688, KL: 77.37446594238281, Loss: 0.016212809830904007, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13891/20000], Bound: 0.3827389180660248, Entropy: 143.09884643554688, Temp: 2.6994595527648926, KL: 72.59719848632812, Loss: 0.0173882395029068, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13892/20000], Bound: 0.42570820450782776, Entropy: 140.82867431640625, Temp: 2.699462413787842, KL: 84.48446655273438, Loss: 0.019212329760193825, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13893/20000], Bound: 0.353352814912796, Entropy: 140.81248474121094, Temp: 2.6994643211364746, KL: 65.90104675292969, Loss: 0.014326768927276134, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13894/20000], Bound: 0.37425798177719116, Entropy: 142.93551635742188, Temp: 2.6994667053222656, KL: 68.68460083007812, Loss: 0.020105566829442978, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13895/20000], Bound: 0.3646693527698517, Entropy: 142.37066650390625, Temp: 2.699465036392212, KL: 68.87396240234375, Loss: 0.014699358493089676, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13896/20000], Bound: 0.36739152669906616, Entropy: 139.0287628173828, Temp: 2.6994643211364746, KL: 70.05439758300781, Loss: 0.013941153883934021, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13897/20000], Bound: 0.3691438138484955, Entropy: 142.339599609375, Temp: 2.699465036392212, KL: 68.18389892578125, Loss: 0.01832800917327404, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13898/20000], Bound: 0.39984557032585144, Entropy: 140.83937072753906, Temp: 2.699463367462158, KL: 78.13308715820312, Loss: 0.016443366184830666, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13899/20000], Bound: 0.3990743160247803, Entropy: 139.04055786132812, Temp: 2.699462413787842, KL: 77.73342895507812, Loss: 0.016758834943175316, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13900/20000], Bound: 0.37437185645103455, Entropy: 140.2030792236328, Temp: 2.6994619369506836, KL: 70.1424560546875, Loss: 0.017465714365243912, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13901/20000], Bound: 0.3616268038749695, Entropy: 142.6665496826172, Temp: 2.69946026802063, KL: 67.01008605957031, Loss: 0.01656179316341877, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13902/20000], Bound: 0.381616473197937, Entropy: 141.95388793945312, Temp: 2.6994574069976807, KL: 73.45614624023438, Loss: 0.015194590203464031, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13903/20000], Bound: 0.39740946888923645, Entropy: 139.90298461914062, Temp: 2.699455976486206, KL: 77.19546508789062, Loss: 0.016839949414134026, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13904/20000], Bound: 0.3663463294506073, Entropy: 140.22373962402344, Temp: 2.6994547843933105, KL: 68.70046997070312, Loss: 0.015899822115898132, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13905/20000], Bound: 0.3956405520439148, Entropy: 142.71633911132812, Temp: 2.699453353881836, KL: 77.18084716796875, Loss: 0.015897046774625778, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13906/20000], Bound: 0.38618141412734985, Entropy: 140.89154052734375, Temp: 2.699453353881836, KL: 75.20220947265625, Loss: 0.014417693018913269, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13907/20000], Bound: 0.38189128041267395, Entropy: 143.49575805664062, Temp: 2.6994547843933105, KL: 73.86343383789062, Loss: 0.014587600715458393, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13908/20000], Bound: 0.3906162977218628, Entropy: 143.13401794433594, Temp: 2.6994576454162598, KL: 75.59934997558594, Loss: 0.016085097566246986, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13909/20000], Bound: 0.39211514592170715, Entropy: 140.58680725097656, Temp: 2.699460983276367, KL: 75.44096374511719, Loss: 0.017194153741002083, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13910/20000], Bound: 0.3622645437717438, Entropy: 139.9783172607422, Temp: 2.699463367462158, KL: 67.37080383300781, Loss: 0.01622636802494526, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13911/20000], Bound: 0.40855658054351807, Entropy: 140.18673706054688, Temp: 2.6994645595550537, KL: 80.79501342773438, Loss: 0.01634453423321247, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13912/20000], Bound: 0.38986700773239136, Entropy: 141.3759765625, Temp: 2.6994669437408447, KL: 74.38668823242188, Loss: 0.017924193292856216, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13913/20000], Bound: 0.37778082489967346, Entropy: 143.3592987060547, Temp: 2.6994681358337402, KL: 72.18878173828125, Loss: 0.015489920973777771, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13914/20000], Bound: 0.3654812276363373, Entropy: 142.5758819580078, Temp: 2.699469804763794, KL: 68.30511474609375, Loss: 0.016178419813513756, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13915/20000], Bound: 0.3755860924720764, Entropy: 142.75209045410156, Temp: 2.6994705200195312, KL: 71.6258544921875, Loss: 0.015363468788564205, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13916/20000], Bound: 0.3853587508201599, Entropy: 139.46832275390625, Temp: 2.6994714736938477, KL: 73.19769287109375, Loss: 0.017686616629362106, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13917/20000], Bound: 0.37451276183128357, Entropy: 140.44515991210938, Temp: 2.6994714736938477, KL: 70.63200378417969, Loss: 0.01663387194275856, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13918/20000], Bound: 0.37057778239250183, Entropy: 141.5065155029297, Temp: 2.6994709968566895, KL: 68.88133239746094, Loss: 0.01779271848499775, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13919/20000], Bound: 0.3600800633430481, Entropy: 141.25975036621094, Temp: 2.6994683742523193, KL: 66.28279113769531, Loss: 0.01710333116352558, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13920/20000], Bound: 0.3765471875667572, Entropy: 141.20375061035156, Temp: 2.6994645595550537, KL: 72.20721435546875, Loss: 0.014798121526837349, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13921/20000], Bound: 0.3875925540924072, Entropy: 139.81008911132812, Temp: 2.6994621753692627, KL: 74.66777038574219, Loss: 0.016170572489500046, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13922/20000], Bound: 0.396630197763443, Entropy: 142.41600036621094, Temp: 2.699460506439209, KL: 77.43778991699219, Loss: 0.015963532030582428, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13923/20000], Bound: 0.36145901679992676, Entropy: 142.01080322265625, Temp: 2.6994597911834717, KL: 64.26019287109375, Loss: 0.021567726507782936, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13924/20000], Bound: 0.3806842863559723, Entropy: 141.7238311767578, Temp: 2.699453592300415, KL: 72.70167541503906, Loss: 0.016092173755168915, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13925/20000], Bound: 0.3916188180446625, Entropy: 140.75645446777344, Temp: 2.699448347091675, KL: 77.38916015625, Loss: 0.013315235264599323, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13926/20000], Bound: 0.3746511936187744, Entropy: 141.90411376953125, Temp: 2.699446678161621, KL: 71.22389221191406, Loss: 0.015610849484801292, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13927/20000], Bound: 0.4013613164424896, Entropy: 140.3671417236328, Temp: 2.6994452476501465, KL: 78.92234802246094, Loss: 0.015817496925592422, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13928/20000], Bound: 0.41767922043800354, Entropy: 140.2438507080078, Temp: 2.6994452476501465, KL: 83.354736328125, Loss: 0.016731571406126022, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13929/20000], Bound: 0.37987977266311646, Entropy: 140.26785278320312, Temp: 2.699446439743042, KL: 72.60263061523438, Loss: 0.015844764187932014, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13930/20000], Bound: 0.35713011026382446, Entropy: 141.6381072998047, Temp: 2.6994476318359375, KL: 66.03274536132812, Loss: 0.01603471115231514, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13931/20000], Bound: 0.36647355556488037, Entropy: 142.67660522460938, Temp: 2.6994481086730957, KL: 66.56613159179688, Loss: 0.01991983689367771, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13932/20000], Bound: 0.3861231207847595, Entropy: 140.9775848388672, Temp: 2.699444532394409, KL: 74.09193420410156, Loss: 0.016442595049738884, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13933/20000], Bound: 0.39932480454444885, Entropy: 140.43272399902344, Temp: 2.699441432952881, KL: 79.13002014160156, Loss: 0.014309749007225037, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13934/20000], Bound: 0.36172929406166077, Entropy: 142.01722717285156, Temp: 2.6994411945343018, KL: 66.076171875, Loss: 0.01834491640329361, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13935/20000], Bound: 0.3970634937286377, Entropy: 141.08102416992188, Temp: 2.6994383335113525, KL: 76.791015625, Loss: 0.017399026080965996, Learning Rate: 1.67563759224985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13936/20000], Bound: 0.3983253836631775, Entropy: 141.12103271484375, Temp: 2.6994354724884033, KL: 77.88978576660156, Loss: 0.016056975349783897, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13937/20000], Bound: 0.38061121106147766, Entropy: 141.19554138183594, Temp: 2.6994338035583496, KL: 69.98973083496094, Loss: 0.021076025441288948, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13938/20000], Bound: 0.3799853026866913, Entropy: 141.66127014160156, Temp: 2.699428081512451, KL: 71.8035888671875, Loss: 0.01738111488521099, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13939/20000], Bound: 0.37766513228416443, Entropy: 141.80052185058594, Temp: 2.6994221210479736, KL: 72.11978149414062, Loss: 0.015555605292320251, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13940/20000], Bound: 0.38182464241981506, Entropy: 141.8143310546875, Temp: 2.6994171142578125, KL: 72.29849243164062, Loss: 0.017450151965022087, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13941/20000], Bound: 0.37465667724609375, Entropy: 141.710205078125, Temp: 2.699411392211914, KL: 70.42045593261719, Loss: 0.017101654782891273, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13942/20000], Bound: 0.40774860978126526, Entropy: 139.53765869140625, Temp: 2.6994054317474365, KL: 82.1668701171875, Loss: 0.013352079316973686, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13943/20000], Bound: 0.3907901644706726, Entropy: 138.8566131591797, Temp: 2.699403762817383, KL: 76.13372802734375, Loss: 0.015189292840659618, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13944/20000], Bound: 0.3837307393550873, Entropy: 141.48898315429688, Temp: 2.6994035243988037, KL: 74.04464721679688, Loss: 0.015240075998008251, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13945/20000], Bound: 0.3954038619995117, Entropy: 142.69454956054688, Temp: 2.699404239654541, KL: 75.89138793945312, Loss: 0.018155356869101524, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13946/20000], Bound: 0.39452439546585083, Entropy: 140.6071014404297, Temp: 2.699403762817383, KL: 75.958984375, Loss: 0.01754903234541416, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13947/20000], Bound: 0.36875003576278687, Entropy: 142.2126007080078, Temp: 2.6994028091430664, KL: 69.23146057128906, Loss: 0.016179732978343964, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13948/20000], Bound: 0.39722052216529846, Entropy: 141.5155029296875, Temp: 2.699401617050171, KL: 78.64112854003906, Loss: 0.014057952910661697, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13949/20000], Bound: 0.3540041148662567, Entropy: 141.34925842285156, Temp: 2.6994032859802246, KL: 65.11979675292969, Loss: 0.01610921323299408, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13950/20000], Bound: 0.3947916328907013, Entropy: 142.07525634765625, Temp: 2.6994035243988037, KL: 77.0091552734375, Loss: 0.01574995182454586, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13951/20000], Bound: 0.37272441387176514, Entropy: 142.17588806152344, Temp: 2.699404716491699, KL: 69.56159973144531, Loss: 0.01766752079129219, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13952/20000], Bound: 0.35389864444732666, Entropy: 142.7522430419922, Temp: 2.69940447807312, KL: 63.671112060546875, Loss: 0.01873813569545746, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13953/20000], Bound: 0.3939459025859833, Entropy: 142.90750122070312, Temp: 2.6994006633758545, KL: 75.36686706542969, Loss: 0.018329579383134842, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13954/20000], Bound: 0.3640446066856384, Entropy: 140.9793701171875, Temp: 2.6993961334228516, KL: 68.36796569824219, Loss: 0.015308992937207222, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13955/20000], Bound: 0.40064308047294617, Entropy: 143.50323486328125, Temp: 2.699392080307007, KL: 75.48629760742188, Loss: 0.021784957498311996, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13956/20000], Bound: 0.40679579973220825, Entropy: 140.43016052246094, Temp: 2.6993846893310547, KL: 81.77163696289062, Loss: 0.013552974909543991, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13957/20000], Bound: 0.3501768708229065, Entropy: 139.23126220703125, Temp: 2.6993815898895264, KL: 63.36396789550781, Loss: 0.01739218272268772, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13958/20000], Bound: 0.38963451981544495, Entropy: 140.1081085205078, Temp: 2.699376106262207, KL: 73.73204040527344, Loss: 0.01900976151227951, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13959/20000], Bound: 0.37078019976615906, Entropy: 141.726806640625, Temp: 2.699369430541992, KL: 69.81019592285156, Loss: 0.01617833971977234, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13960/20000], Bound: 0.4079272150993347, Entropy: 139.3322296142578, Temp: 2.6993629932403564, KL: 80.03558349609375, Loss: 0.017398962751030922, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13961/20000], Bound: 0.3571144938468933, Entropy: 140.65060424804688, Temp: 2.699357509613037, KL: 66.7513427734375, Loss: 0.014694895595312119, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13962/20000], Bound: 0.41961395740509033, Entropy: 140.61236572265625, Temp: 2.699352741241455, KL: 83.6134033203125, Loss: 0.017348283901810646, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13963/20000], Bound: 0.3772965371608734, Entropy: 141.52220153808594, Temp: 2.6993491649627686, KL: 71.52699279785156, Loss: 0.016456427052617073, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13964/20000], Bound: 0.37363386154174805, Entropy: 140.88775634765625, Temp: 2.699345588684082, KL: 72.02102661132812, Loss: 0.01359349675476551, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13965/20000], Bound: 0.3749528229236603, Entropy: 141.17596435546875, Temp: 2.6993446350097656, KL: 72.2264404296875, Loss: 0.013913216069340706, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13966/20000], Bound: 0.37944960594177246, Entropy: 141.0283966064453, Temp: 2.699345111846924, KL: 72.24699401855469, Loss: 0.01627248525619507, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13967/20000], Bound: 0.38021233677864075, Entropy: 141.98147583007812, Temp: 2.699345588684082, KL: 73.07644653320312, Loss: 0.015144278295338154, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13968/20000], Bound: 0.36724770069122314, Entropy: 140.1830291748047, Temp: 2.6993470191955566, KL: 67.77137756347656, Loss: 0.018093356862664223, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13969/20000], Bound: 0.3776201903820038, Entropy: 141.00128173828125, Temp: 2.6993460655212402, KL: 69.80671691894531, Loss: 0.019815456122159958, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13970/20000], Bound: 0.4042843282222748, Entropy: 142.3921661376953, Temp: 2.6993420124053955, KL: 78.98439025878906, Loss: 0.0173194520175457, Learning Rate: 1.67563759224985e-05\n",
      "Epoch [13971/20000], Bound: 0.4045328199863434, Entropy: 141.7848358154297, Temp: 2.6993396282196045, KL: 80.83956909179688, Loss: 0.014020992442965508, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13972/20000], Bound: 0.37529343366622925, Entropy: 143.2631378173828, Temp: 2.6993393898010254, KL: 71.26901245117188, Loss: 0.015867644920945168, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13973/20000], Bound: 0.38912761211395264, Entropy: 140.67538452148438, Temp: 2.6993391513824463, KL: 74.35079956054688, Loss: 0.017588285729289055, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13974/20000], Bound: 0.43071892857551575, Entropy: 141.4923553466797, Temp: 2.699338436126709, KL: 88.88963317871094, Loss: 0.013934297487139702, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13975/20000], Bound: 0.3871137499809265, Entropy: 141.545166015625, Temp: 2.6993408203125, KL: 76.01502990722656, Loss: 0.013414895161986351, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13976/20000], Bound: 0.37423428893089294, Entropy: 141.68251037597656, Temp: 2.6993446350097656, KL: 69.97352600097656, Loss: 0.017704710364341736, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13977/20000], Bound: 0.3918147087097168, Entropy: 139.05471801757812, Temp: 2.6993470191955566, KL: 74.92807006835938, Loss: 0.017979472875595093, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13978/20000], Bound: 0.3875051736831665, Entropy: 142.3068084716797, Temp: 2.6993486881256104, KL: 73.65809631347656, Loss: 0.01799246482551098, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13979/20000], Bound: 0.38957202434539795, Entropy: 142.2429656982422, Temp: 2.6993489265441895, KL: 74.83241271972656, Loss: 0.016937432810664177, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13980/20000], Bound: 0.3694353997707367, Entropy: 141.94215393066406, Temp: 2.6993491649627686, KL: 69.71859741210938, Loss: 0.01563814841210842, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13981/20000], Bound: 0.40870630741119385, Entropy: 141.60910034179688, Temp: 2.6993494033813477, KL: 79.23210144042969, Loss: 0.019321909174323082, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13982/20000], Bound: 0.39623183012008667, Entropy: 142.11866760253906, Temp: 2.6993486881256104, KL: 76.56510925292969, Loss: 0.017360512167215347, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13983/20000], Bound: 0.39816534519195557, Entropy: 140.6131134033203, Temp: 2.699347972869873, KL: 75.28350830078125, Loss: 0.020795723423361778, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13984/20000], Bound: 0.4007321000099182, Entropy: 141.98907470703125, Temp: 2.6993448734283447, KL: 78.60882568359375, Loss: 0.016049841418862343, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13985/20000], Bound: 0.3772489130496979, Entropy: 141.23797607421875, Temp: 2.699343204498291, KL: 70.460205078125, Loss: 0.018406962975859642, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13986/20000], Bound: 0.35494041442871094, Entropy: 142.45826721191406, Temp: 2.6993401050567627, KL: 65.50971984863281, Loss: 0.01586983911693096, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13987/20000], Bound: 0.38533487915992737, Entropy: 141.25144958496094, Temp: 2.6993367671966553, KL: 74.66143798828125, Loss: 0.014961306937038898, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13988/20000], Bound: 0.4087758958339691, Entropy: 141.37857055664062, Temp: 2.6993348598480225, KL: 80.74964904785156, Loss: 0.016549687832593918, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13989/20000], Bound: 0.3843725323677063, Entropy: 141.85618591308594, Temp: 2.699333667755127, KL: 75.10464477539062, Loss: 0.013621553778648376, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13990/20000], Bound: 0.39335575699806213, Entropy: 141.76266479492188, Temp: 2.6993343830108643, KL: 76.24615478515625, Loss: 0.016378050670027733, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13991/20000], Bound: 0.3713659346103668, Entropy: 140.67665100097656, Temp: 2.6993350982666016, KL: 68.6109619140625, Loss: 0.01870896853506565, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13992/20000], Bound: 0.38582634925842285, Entropy: 141.69602966308594, Temp: 2.699334144592285, KL: 74.41935729980469, Loss: 0.015674889087677002, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13993/20000], Bound: 0.40931978821754456, Entropy: 141.53733825683594, Temp: 2.699333667755127, KL: 79.94819641113281, Loss: 0.018338069319725037, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13994/20000], Bound: 0.3976866900920868, Entropy: 140.60678100585938, Temp: 2.6993329524993896, KL: 77.79086303710938, Loss: 0.015888188034296036, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13995/20000], Bound: 0.37306272983551025, Entropy: 139.8993682861328, Temp: 2.6993329524993896, KL: 70.92172241210938, Loss: 0.01532682403922081, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13996/20000], Bound: 0.3796338737010956, Entropy: 142.25965881347656, Temp: 2.699333429336548, KL: 71.96943664550781, Loss: 0.016885096207261086, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13997/20000], Bound: 0.380262553691864, Entropy: 141.39601135253906, Temp: 2.699333429336548, KL: 71.26422119140625, Loss: 0.01852787844836712, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13998/20000], Bound: 0.42117276787757874, Entropy: 140.1310577392578, Temp: 2.699331760406494, KL: 85.48846435546875, Loss: 0.014760980382561684, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [13999/20000], Bound: 0.3944666087627411, Entropy: 139.09864807128906, Temp: 2.6993324756622314, KL: 77.1507568359375, Loss: 0.015309237875044346, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14000/20000], Bound: 0.40012380480766296, Entropy: 138.91067504882812, Temp: 2.699334144592285, KL: 78.09298706054688, Loss: 0.016669750213623047, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14001/20000], Bound: 0.40002092719078064, Entropy: 141.98892211914062, Temp: 2.699335813522339, KL: 78.28718566894531, Loss: 0.016253342851996422, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14002/20000], Bound: 0.3609204888343811, Entropy: 141.7974853515625, Temp: 2.699337959289551, KL: 68.19981384277344, Loss: 0.01398902852088213, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14003/20000], Bound: 0.4087313711643219, Entropy: 140.6753387451172, Temp: 2.6993408203125, KL: 81.77906799316406, Loss: 0.014618094079196453, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14004/20000], Bound: 0.3984525799751282, Entropy: 139.64601135253906, Temp: 2.6993448734283447, KL: 77.82025146484375, Loss: 0.016254844143986702, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14005/20000], Bound: 0.39115068316459656, Entropy: 141.5159149169922, Temp: 2.6993494033813477, KL: 75.18260192871094, Loss: 0.0171466376632452, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14006/20000], Bound: 0.3742162585258484, Entropy: 140.67062377929688, Temp: 2.6993532180786133, KL: 71.01530456542969, Loss: 0.015765491873025894, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14007/20000], Bound: 0.36119478940963745, Entropy: 142.54693603515625, Temp: 2.6993565559387207, KL: 67.38201904296875, Loss: 0.01564689539372921, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14008/20000], Bound: 0.36735206842422485, Entropy: 141.32437133789062, Temp: 2.69935941696167, KL: 68.43878173828125, Loss: 0.016912071034312248, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14009/20000], Bound: 0.3799348771572113, Entropy: 142.67236328125, Temp: 2.6993613243103027, KL: 72.99375915527344, Loss: 0.015149044804275036, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14010/20000], Bound: 0.3822314441204071, Entropy: 140.45932006835938, Temp: 2.6993634700775146, KL: 73.009033203125, Loss: 0.016351953148841858, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14011/20000], Bound: 0.3823453187942505, Entropy: 140.93333435058594, Temp: 2.6993653774261475, KL: 71.59272766113281, Loss: 0.0190365519374609, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14012/20000], Bound: 0.3753252625465393, Entropy: 141.17669677734375, Temp: 2.6993656158447266, KL: 70.97236633300781, Loss: 0.016434282064437866, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14013/20000], Bound: 0.37519368529319763, Entropy: 139.44827270507812, Temp: 2.6993653774261475, KL: 71.36213684082031, Loss: 0.01564236544072628, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14014/20000], Bound: 0.40518316626548767, Entropy: 142.41079711914062, Temp: 2.6993653774261475, KL: 78.0135498046875, Loss: 0.019616931676864624, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14015/20000], Bound: 0.3827623128890991, Entropy: 142.0678253173828, Temp: 2.699364185333252, KL: 74.07217407226562, Loss: 0.014667908661067486, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14016/20000], Bound: 0.3960372507572174, Entropy: 141.59384155273438, Temp: 2.699364185333252, KL: 78.42759704589844, Loss: 0.013804148882627487, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14017/20000], Bound: 0.4117388427257538, Entropy: 141.89859008789062, Temp: 2.6993660926818848, KL: 83.24737548828125, Loss: 0.013581953942775726, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14018/20000], Bound: 0.3886162340641022, Entropy: 139.88162231445312, Temp: 2.6993701457977295, KL: 73.69021606445312, Loss: 0.01853484846651554, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14019/20000], Bound: 0.37631866335868835, Entropy: 139.97447204589844, Temp: 2.6993727684020996, KL: 71.22352600097656, Loss: 0.016497675329446793, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14020/20000], Bound: 0.39459896087646484, Entropy: 140.5161895751953, Temp: 2.6993746757507324, KL: 76.29341125488281, Loss: 0.016970084980130196, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14021/20000], Bound: 0.3902129530906677, Entropy: 141.51376342773438, Temp: 2.6993765830993652, KL: 75.64321899414062, Loss: 0.015783844515681267, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14022/20000], Bound: 0.37522444128990173, Entropy: 140.97445678710938, Temp: 2.699378728866577, KL: 71.11154174804688, Loss: 0.01612299121916294, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14023/20000], Bound: 0.40292054414749146, Entropy: 143.2328643798828, Temp: 2.69938063621521, KL: 79.78276062011719, Loss: 0.015085289254784584, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14024/20000], Bound: 0.3827818036079407, Entropy: 142.2637176513672, Temp: 2.699383497238159, KL: 74.36186218261719, Loss: 0.014141993597149849, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14025/20000], Bound: 0.41103965044021606, Entropy: 140.55630493164062, Temp: 2.699387550354004, KL: 81.4898681640625, Loss: 0.01644555665552616, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14026/20000], Bound: 0.38150474429130554, Entropy: 141.77772521972656, Temp: 2.699392080307007, KL: 73.45387268066406, Loss: 0.015138208866119385, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14027/20000], Bound: 0.3748459815979004, Entropy: 141.5638885498047, Temp: 2.699396848678589, KL: 70.22702026367188, Loss: 0.017560375854372978, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14028/20000], Bound: 0.39438942074775696, Entropy: 140.3386688232422, Temp: 2.699399948120117, KL: 76.80372619628906, Loss: 0.01591048762202263, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14029/20000], Bound: 0.35104116797447205, Entropy: 141.8211669921875, Temp: 2.6994035243988037, KL: 65.54086303710938, Loss: 0.013803922571241856, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14030/20000], Bound: 0.38299986720085144, Entropy: 141.74053955078125, Temp: 2.6994071006774902, KL: 73.41703796386719, Loss: 0.016009492799639702, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14031/20000], Bound: 0.3999180793762207, Entropy: 140.26437377929688, Temp: 2.6994106769561768, KL: 78.81929016113281, Loss: 0.015211770310997963, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14032/20000], Bound: 0.38096362352371216, Entropy: 139.82281494140625, Temp: 2.6994152069091797, KL: 73.05387878417969, Loss: 0.01558915339410305, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14033/20000], Bound: 0.3781667649745941, Entropy: 142.24485778808594, Temp: 2.6994194984436035, KL: 73.50141906738281, Loss: 0.013264134526252747, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14034/20000], Bound: 0.3996334969997406, Entropy: 140.4370880126953, Temp: 2.699424982070923, KL: 77.40760803222656, Loss: 0.017669906839728355, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14035/20000], Bound: 0.40713992714881897, Entropy: 140.4916534423828, Temp: 2.699429750442505, KL: 80.63218688964844, Loss: 0.015855707228183746, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14036/20000], Bound: 0.39539265632629395, Entropy: 141.95730590820312, Temp: 2.699434995651245, KL: 77.28733825683594, Loss: 0.015563850291073322, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14037/20000], Bound: 0.41817760467529297, Entropy: 139.74420166015625, Temp: 2.6994407176971436, KL: 84.32559204101562, Loss: 0.015215483494102955, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14038/20000], Bound: 0.3848426640033722, Entropy: 141.46424865722656, Temp: 2.6994473934173584, KL: 71.42958068847656, Loss: 0.020683061331510544, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14039/20000], Bound: 0.36250072717666626, Entropy: 141.8167724609375, Temp: 2.699450731277466, KL: 66.53849792480469, Loss: 0.01789119653403759, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14040/20000], Bound: 0.3932761549949646, Entropy: 142.36668395996094, Temp: 2.6994526386260986, KL: 73.68873596191406, Loss: 0.02107265591621399, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14041/20000], Bound: 0.3857465982437134, Entropy: 139.1383819580078, Temp: 2.6994516849517822, KL: 74.40571594238281, Loss: 0.015658212825655937, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14042/20000], Bound: 0.37685465812683105, Entropy: 141.5064697265625, Temp: 2.699451208114624, KL: 72.78775024414062, Loss: 0.013886499218642712, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14043/20000], Bound: 0.3748916983604431, Entropy: 137.95155334472656, Temp: 2.6994521617889404, KL: 71.02156066894531, Loss: 0.01611345075070858, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14044/20000], Bound: 0.3759077191352844, Entropy: 142.60321044921875, Temp: 2.6994528770446777, KL: 71.94482421875, Loss: 0.014943605288863182, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14045/20000], Bound: 0.38211047649383545, Entropy: 142.0316619873047, Temp: 2.6994540691375732, KL: 73.23480224609375, Loss: 0.0158696286380291, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14046/20000], Bound: 0.36153167486190796, Entropy: 139.26962280273438, Temp: 2.699455499649048, KL: 67.4154052734375, Loss: 0.015761416405439377, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14047/20000], Bound: 0.3715640902519226, Entropy: 140.1752166748047, Temp: 2.6994564533233643, KL: 68.93193054199219, Loss: 0.018220080062747, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14048/20000], Bound: 0.3867237865924835, Entropy: 140.52357482910156, Temp: 2.699455738067627, KL: 74.324462890625, Loss: 0.01633652113378048, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14049/20000], Bound: 0.37151360511779785, Entropy: 141.9737548828125, Temp: 2.6994552612304688, KL: 69.22163391113281, Loss: 0.01765679009258747, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14050/20000], Bound: 0.376473605632782, Entropy: 140.13890075683594, Temp: 2.699453592300415, KL: 72.70848083496094, Loss: 0.013830368407070637, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14051/20000], Bound: 0.37617653608322144, Entropy: 142.502685546875, Temp: 2.699453592300415, KL: 71.77503967285156, Loss: 0.01540115661919117, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14052/20000], Bound: 0.35825449228286743, Entropy: 142.18760681152344, Temp: 2.699453830718994, KL: 66.76063537597656, Loss: 0.01526959054172039, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14053/20000], Bound: 0.4072674810886383, Entropy: 139.7392120361328, Temp: 2.6994540691375732, KL: 81.74070739746094, Loss: 0.01387379877269268, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14054/20000], Bound: 0.4088074266910553, Entropy: 138.8906707763672, Temp: 2.6994564533233643, KL: 81.71379089355469, Loss: 0.014782752841711044, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14055/20000], Bound: 0.36994338035583496, Entropy: 140.1615753173828, Temp: 2.69946026802063, KL: 70.80439758300781, Loss: 0.013895852491259575, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14056/20000], Bound: 0.3855358064174652, Entropy: 141.2520294189453, Temp: 2.6994645595550537, KL: 73.97206115722656, Loss: 0.016347799450159073, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14057/20000], Bound: 0.3808787167072296, Entropy: 141.43064880371094, Temp: 2.6994686126708984, KL: 73.80125427246094, Loss: 0.014159832149744034, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14058/20000], Bound: 0.38084807991981506, Entropy: 141.0583038330078, Temp: 2.6994736194610596, KL: 72.93528747558594, Loss: 0.015747440978884697, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14059/20000], Bound: 0.36469903588294983, Entropy: 142.692138671875, Temp: 2.6994781494140625, KL: 66.66668701171875, Loss: 0.01880333200097084, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14060/20000], Bound: 0.41206881403923035, Entropy: 142.1399383544922, Temp: 2.6994805335998535, KL: 79.92555236816406, Loss: 0.019921177998185158, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14061/20000], Bound: 0.3714182376861572, Entropy: 139.896240234375, Temp: 2.699481248855591, KL: 68.90087890625, Loss: 0.018200673162937164, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14062/20000], Bound: 0.3752356171607971, Entropy: 141.5480499267578, Temp: 2.6994805335998535, KL: 71.52017211914062, Loss: 0.015372967347502708, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14063/20000], Bound: 0.37340566515922546, Entropy: 140.75025939941406, Temp: 2.6994805335998535, KL: 71.27621459960938, Loss: 0.01485330518335104, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14064/20000], Bound: 0.37503328919410706, Entropy: 141.63040161132812, Temp: 2.6994805335998535, KL: 70.44577026367188, Loss: 0.017255427315831184, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14065/20000], Bound: 0.373622328042984, Entropy: 141.30178833007812, Temp: 2.6994800567626953, KL: 70.57070922851562, Loss: 0.01627490483224392, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14066/20000], Bound: 0.4019779860973358, Entropy: 141.10276794433594, Temp: 2.699479579925537, KL: 80.47976684570312, Loss: 0.013273917138576508, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14067/20000], Bound: 0.3893534541130066, Entropy: 140.82679748535156, Temp: 2.699481248855591, KL: 76.90486145019531, Loss: 0.01298138964921236, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14068/20000], Bound: 0.3949819803237915, Entropy: 141.58734130859375, Temp: 2.6994850635528564, KL: 77.00886535644531, Loss: 0.01585540734231472, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14069/20000], Bound: 0.384318083524704, Entropy: 140.564208984375, Temp: 2.699489116668701, KL: 73.25863647460938, Loss: 0.017012905329465866, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14070/20000], Bound: 0.3661879003047943, Entropy: 140.74395751953125, Temp: 2.6994924545288086, KL: 68.98031616210938, Loss: 0.01529865711927414, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14071/20000], Bound: 0.3877357840538025, Entropy: 141.3273468017578, Temp: 2.699495553970337, KL: 74.48655700683594, Loss: 0.016584064811468124, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14072/20000], Bound: 0.37881553173065186, Entropy: 139.47885131835938, Temp: 2.699498176574707, KL: 71.61088562011719, Loss: 0.017113041132688522, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14073/20000], Bound: 0.37532946467399597, Entropy: 141.15811157226562, Temp: 2.69950008392334, KL: 70.48136901855469, Loss: 0.01734708994626999, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14074/20000], Bound: 0.3807539641857147, Entropy: 143.7659912109375, Temp: 2.699500799179077, KL: 72.05857849121094, Loss: 0.017321070656180382, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14075/20000], Bound: 0.40270188450813293, Entropy: 141.37890625, Temp: 2.6995010375976562, KL: 78.08456420898438, Loss: 0.018110908567905426, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14076/20000], Bound: 0.3659806251525879, Entropy: 142.46221923828125, Temp: 2.699500799179077, KL: 69.05718994140625, Loss: 0.015047561377286911, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14077/20000], Bound: 0.4137803316116333, Entropy: 141.12631225585938, Temp: 2.699500799179077, KL: 81.74481201171875, Loss: 0.017513571307063103, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14078/20000], Bound: 0.36877891421318054, Entropy: 142.25119018554688, Temp: 2.6995010375976562, KL: 68.66635131835938, Loss: 0.017242439091205597, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14079/20000], Bound: 0.3985629081726074, Entropy: 140.62554931640625, Temp: 2.699500322341919, KL: 78.20703125, Loss: 0.015600617043673992, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14080/20000], Bound: 0.39045196771621704, Entropy: 141.80361938476562, Temp: 2.699500560760498, KL: 75.23829650878906, Loss: 0.016664912924170494, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14081/20000], Bound: 0.3900552988052368, Entropy: 140.48403930664062, Temp: 2.699500799179077, KL: 75.14860534667969, Loss: 0.01661553606390953, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14082/20000], Bound: 0.37990036606788635, Entropy: 140.2159423828125, Temp: 2.6995010375976562, KL: 72.31268310546875, Loss: 0.016393322497606277, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14083/20000], Bound: 0.3954842984676361, Entropy: 139.01475524902344, Temp: 2.6995012760162354, KL: 76.8878173828125, Loss: 0.016354678198695183, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14084/20000], Bound: 0.39394059777259827, Entropy: 143.00132751464844, Temp: 2.6995017528533936, KL: 74.66464233398438, Loss: 0.019628256559371948, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14085/20000], Bound: 0.37761080265045166, Entropy: 142.40126037597656, Temp: 2.699500560760498, KL: 70.87054443359375, Loss: 0.017841124907135963, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14086/20000], Bound: 0.4089372754096985, Entropy: 140.67324829101562, Temp: 2.6994986534118652, KL: 81.5093994140625, Loss: 0.015234259888529778, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14087/20000], Bound: 0.3780985474586487, Entropy: 142.33421325683594, Temp: 2.699498176574707, KL: 72.54219055175781, Loss: 0.015005156397819519, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14088/20000], Bound: 0.39366358518600464, Entropy: 140.6223602294922, Temp: 2.699498414993286, KL: 77.23130798339844, Loss: 0.014722928404808044, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14089/20000], Bound: 0.3886078894138336, Entropy: 141.82420349121094, Temp: 2.69950008392334, KL: 75.00875854492188, Loss: 0.016089264303445816, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14090/20000], Bound: 0.421188622713089, Entropy: 139.89268493652344, Temp: 2.6995017528533936, KL: 84.89724731445312, Loss: 0.015867061913013458, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14091/20000], Bound: 0.4053541421890259, Entropy: 139.32200622558594, Temp: 2.6995046138763428, KL: 79.05519104003906, Loss: 0.01778385415673256, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14092/20000], Bound: 0.38672855496406555, Entropy: 139.88742065429688, Temp: 2.699506998062134, KL: 75.96199035644531, Loss: 0.013306577689945698, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14093/20000], Bound: 0.37598422169685364, Entropy: 140.6805877685547, Temp: 2.6995112895965576, KL: 71.85044860839844, Loss: 0.015159637667238712, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14094/20000], Bound: 0.3610250949859619, Entropy: 142.62254333496094, Temp: 2.6995155811309814, KL: 67.29197692871094, Loss: 0.015726489946246147, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14095/20000], Bound: 0.3846549093723297, Entropy: 140.93548583984375, Temp: 2.699519157409668, KL: 71.71929931640625, Loss: 0.02004581317305565, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14096/20000], Bound: 0.4153241515159607, Entropy: 140.80355834960938, Temp: 2.6995201110839844, KL: 82.11186218261719, Loss: 0.017703669145703316, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14097/20000], Bound: 0.3588271141052246, Entropy: 143.04063415527344, Temp: 2.69952130317688, KL: 67.07560729980469, Loss: 0.014983998611569405, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14098/20000], Bound: 0.376282274723053, Entropy: 142.1987762451172, Temp: 2.6995224952697754, KL: 69.86518859863281, Loss: 0.018995441496372223, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14099/20000], Bound: 0.4021122455596924, Entropy: 141.4688720703125, Temp: 2.699521541595459, KL: 78.7615966796875, Loss: 0.016530970111489296, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14100/20000], Bound: 0.37420326471328735, Entropy: 140.81932067871094, Temp: 2.69952130317688, KL: 70.04383850097656, Loss: 0.017559349536895752, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14101/20000], Bound: 0.3585849106311798, Entropy: 142.67510986328125, Temp: 2.6995201110839844, KL: 64.46853637695312, Loss: 0.019686967134475708, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14102/20000], Bound: 0.36597970128059387, Entropy: 140.53749084472656, Temp: 2.6995162963867188, KL: 67.91023254394531, Loss: 0.017171571031212807, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14103/20000], Bound: 0.3809969425201416, Entropy: 139.29115295410156, Temp: 2.699512004852295, KL: 74.83566284179688, Loss: 0.012307698838412762, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14104/20000], Bound: 0.3996739983558655, Entropy: 141.8332977294922, Temp: 2.699510097503662, KL: 77.6220703125, Loss: 0.017295772209763527, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14105/20000], Bound: 0.3745444118976593, Entropy: 140.46580505371094, Temp: 2.6995086669921875, KL: 71.24102783203125, Loss: 0.015522927977144718, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14106/20000], Bound: 0.4006510376930237, Entropy: 141.89620971679688, Temp: 2.699507474899292, KL: 76.84652709960938, Loss: 0.01927088387310505, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14107/20000], Bound: 0.395799458026886, Entropy: 138.88681030273438, Temp: 2.69950532913208, KL: 75.94174194335938, Loss: 0.01827961578965187, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14108/20000], Bound: 0.38830792903900146, Entropy: 141.60733032226562, Temp: 2.69950270652771, KL: 74.92694091796875, Loss: 0.016078269109129906, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14109/20000], Bound: 0.3856113851070404, Entropy: 141.7418670654297, Temp: 2.699500322341919, KL: 74.46876525878906, Loss: 0.015468882396817207, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14110/20000], Bound: 0.3934052586555481, Entropy: 142.30445861816406, Temp: 2.6994991302490234, KL: 77.26412963867188, Loss: 0.014521160162985325, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14111/20000], Bound: 0.37526872754096985, Entropy: 140.7171630859375, Temp: 2.6994993686676025, KL: 71.26924133300781, Loss: 0.01585547626018524, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14112/20000], Bound: 0.414679616689682, Entropy: 142.35865783691406, Temp: 2.6994996070861816, KL: 80.66514587402344, Loss: 0.020019706338644028, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14113/20000], Bound: 0.3862663507461548, Entropy: 141.48062133789062, Temp: 2.6994986534118652, KL: 73.58097839355469, Loss: 0.017466850578784943, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14114/20000], Bound: 0.3990832567214966, Entropy: 139.92620849609375, Temp: 2.6994972229003906, KL: 77.88856506347656, Loss: 0.01647680066525936, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14115/20000], Bound: 0.38068437576293945, Entropy: 141.33230590820312, Temp: 2.6994965076446533, KL: 74.03585815429688, Loss: 0.013621454127132893, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14116/20000], Bound: 0.3830192983150482, Entropy: 141.22265625, Temp: 2.6994972229003906, KL: 73.98849487304688, Loss: 0.014962278306484222, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14117/20000], Bound: 0.3968755006790161, Entropy: 140.21896362304688, Temp: 2.6994986534118652, KL: 76.84347534179688, Loss: 0.017199255526065826, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14118/20000], Bound: 0.4082680940628052, Entropy: 141.86361694335938, Temp: 2.69950008392334, KL: 80.43960571289062, Loss: 0.016842186450958252, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14119/20000], Bound: 0.37432798743247986, Entropy: 140.52157592773438, Temp: 2.6995015144348145, KL: 70.99372863769531, Loss: 0.015866000205278397, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14120/20000], Bound: 0.3725540339946747, Entropy: 142.5987091064453, Temp: 2.699503183364868, KL: 71.06404113769531, Loss: 0.014795233495533466, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14121/20000], Bound: 0.407884806394577, Entropy: 140.32286071777344, Temp: 2.699505090713501, KL: 78.61051940917969, Loss: 0.02001623809337616, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14122/20000], Bound: 0.3923233449459076, Entropy: 139.7039031982422, Temp: 2.699505567550659, KL: 77.0726318359375, Loss: 0.014285828918218613, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14123/20000], Bound: 0.3704532980918884, Entropy: 139.8930206298828, Temp: 2.699507236480713, KL: 68.88920593261719, Loss: 0.017712650820612907, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14124/20000], Bound: 0.3773743808269501, Entropy: 142.8067626953125, Temp: 2.699507713317871, KL: 70.82559204101562, Loss: 0.017798349261283875, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14125/20000], Bound: 0.3962194323539734, Entropy: 138.76718139648438, Temp: 2.699506998062134, KL: 76.4534912109375, Loss: 0.017561903223395348, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14126/20000], Bound: 0.3807843029499054, Entropy: 141.05458068847656, Temp: 2.6995060443878174, KL: 73.345458984375, Loss: 0.01495383121073246, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14127/20000], Bound: 0.3856981098651886, Entropy: 142.0450439453125, Temp: 2.6995060443878174, KL: 74.77066040039062, Loss: 0.01495655532926321, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14128/20000], Bound: 0.3796658217906952, Entropy: 140.9342498779297, Temp: 2.699506998062134, KL: 70.59748840332031, Loss: 0.019444730132818222, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14129/20000], Bound: 0.37976035475730896, Entropy: 141.5370330810547, Temp: 2.6995060443878174, KL: 72.73013305664062, Loss: 0.015545225702226162, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14130/20000], Bound: 0.3697747588157654, Entropy: 141.798583984375, Temp: 2.69950532913208, KL: 67.8038330078125, Loss: 0.019364897161722183, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14131/20000], Bound: 0.3718317449092865, Entropy: 140.25119018554688, Temp: 2.69950270652771, KL: 68.2943115234375, Loss: 0.01954299584031105, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14132/20000], Bound: 0.39685437083244324, Entropy: 141.7965087890625, Temp: 2.699497938156128, KL: 77.556884765625, Loss: 0.015866296365857124, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14133/20000], Bound: 0.3771704435348511, Entropy: 141.05393981933594, Temp: 2.6994943618774414, KL: 71.38589477539062, Loss: 0.016651757061481476, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14134/20000], Bound: 0.3973037600517273, Entropy: 141.55902099609375, Temp: 2.699490785598755, KL: 76.90805053710938, Loss: 0.017314603552222252, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14135/20000], Bound: 0.38403165340423584, Entropy: 141.99929809570312, Temp: 2.6994874477386475, KL: 74.31494140625, Loss: 0.014902195893228054, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14136/20000], Bound: 0.3992882966995239, Entropy: 141.1375732421875, Temp: 2.6994855403900146, KL: 78.00643920898438, Loss: 0.01637120731174946, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14137/20000], Bound: 0.37312906980514526, Entropy: 141.8941650390625, Temp: 2.69948410987854, KL: 71.53109741210938, Loss: 0.014234590344130993, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14138/20000], Bound: 0.42009297013282776, Entropy: 140.6999969482422, Temp: 2.699483871459961, KL: 84.16702270507812, Loss: 0.01659633405506611, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14139/20000], Bound: 0.3721829354763031, Entropy: 141.2373046875, Temp: 2.6994845867156982, KL: 69.90119934082031, Loss: 0.01675240881741047, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14140/20000], Bound: 0.40021365880966187, Entropy: 138.94667053222656, Temp: 2.6994845867156982, KL: 79.030517578125, Loss: 0.01498423982411623, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14141/20000], Bound: 0.3754153549671173, Entropy: 142.46507263183594, Temp: 2.699486017227173, KL: 68.7354736328125, Loss: 0.020626358687877655, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14142/20000], Bound: 0.3898707330226898, Entropy: 139.06387329101562, Temp: 2.699484348297119, KL: 75.60556030273438, Loss: 0.015668759122490883, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14143/20000], Bound: 0.39060038328170776, Entropy: 142.5358428955078, Temp: 2.699483871459961, KL: 74.70489501953125, Loss: 0.017733396962285042, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14144/20000], Bound: 0.38592657446861267, Entropy: 140.9942169189453, Temp: 2.6994824409484863, KL: 74.83206176757812, Loss: 0.01496597658842802, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14145/20000], Bound: 0.38861218094825745, Entropy: 139.88302612304688, Temp: 2.6994822025299072, KL: 74.79220581054688, Loss: 0.016492504626512527, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14146/20000], Bound: 0.38275760412216187, Entropy: 140.36114501953125, Temp: 2.699481964111328, KL: 73.39108276367188, Loss: 0.01592801697552204, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14147/20000], Bound: 0.4027281701564789, Entropy: 140.15213012695312, Temp: 2.6994822025299072, KL: 77.15341186523438, Loss: 0.0198499895632267, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14148/20000], Bound: 0.37865695357322693, Entropy: 140.28717041015625, Temp: 2.6994807720184326, KL: 72.15850830078125, Loss: 0.01601387932896614, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14149/20000], Bound: 0.38758450746536255, Entropy: 141.4644317626953, Temp: 2.699479818344116, KL: 74.95423889160156, Loss: 0.015635808929800987, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14150/20000], Bound: 0.387527734041214, Entropy: 143.9280242919922, Temp: 2.699479341506958, KL: 75.33299255371094, Loss: 0.014903533272445202, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14151/20000], Bound: 0.38481107354164124, Entropy: 140.081787109375, Temp: 2.6994800567626953, KL: 73.05760192871094, Loss: 0.017650840803980827, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14152/20000], Bound: 0.40376710891723633, Entropy: 140.62548828125, Temp: 2.699479818344116, KL: 78.27490234375, Loss: 0.01834813319146633, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14153/20000], Bound: 0.38423797488212585, Entropy: 141.61070251464844, Temp: 2.699479103088379, KL: 74.1551513671875, Loss: 0.015309157781302929, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14154/20000], Bound: 0.3885151445865631, Entropy: 140.5319061279297, Temp: 2.699479103088379, KL: 75.77491760253906, Loss: 0.014619695954024792, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14155/20000], Bound: 0.3815714716911316, Entropy: 140.40170288085938, Temp: 2.6994800567626953, KL: 72.9210205078125, Loss: 0.016161752864718437, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14156/20000], Bound: 0.39463770389556885, Entropy: 140.49813842773438, Temp: 2.699481248855591, KL: 75.66838073730469, Loss: 0.01814991980791092, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14157/20000], Bound: 0.37168043851852417, Entropy: 140.4462432861328, Temp: 2.69948148727417, KL: 69.88359069824219, Loss: 0.01651911623775959, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14158/20000], Bound: 0.3866681158542633, Entropy: 140.18142700195312, Temp: 2.699481248855591, KL: 74.33226013183594, Loss: 0.016292257234454155, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14159/20000], Bound: 0.38965359330177307, Entropy: 141.48828125, Temp: 2.699481248855591, KL: 76.14956665039062, Loss: 0.014543243683874607, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14160/20000], Bound: 0.3635171353816986, Entropy: 141.4069061279297, Temp: 2.6994826793670654, KL: 67.34547424316406, Loss: 0.01692771352827549, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14161/20000], Bound: 0.3695582449436188, Entropy: 141.1139373779297, Temp: 2.6994829177856445, KL: 71.251220703125, Loss: 0.012865292839705944, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14162/20000], Bound: 0.4118705093860626, Entropy: 138.82351684570312, Temp: 2.6994845867156982, KL: 80.95085144042969, Loss: 0.017910843715071678, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14163/20000], Bound: 0.35371801257133484, Entropy: 142.30966186523438, Temp: 2.699486255645752, KL: 64.59437561035156, Loss: 0.016935445368289948, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14164/20000], Bound: 0.40218329429626465, Entropy: 140.34312438964844, Temp: 2.699486494064331, KL: 79.32502746582031, Loss: 0.015526357106864452, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14165/20000], Bound: 0.37580350041389465, Entropy: 142.07598876953125, Temp: 2.6994876861572266, KL: 70.26072692871094, Loss: 0.018007738515734673, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14166/20000], Bound: 0.40837612748146057, Entropy: 140.80825805664062, Temp: 2.6994876861572266, KL: 80.77255249023438, Loss: 0.01628565602004528, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14167/20000], Bound: 0.40671607851982117, Entropy: 140.08013916015625, Temp: 2.699488401412964, KL: 78.4617919921875, Loss: 0.019640300422906876, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14168/20000], Bound: 0.40331974625587463, Entropy: 140.7888946533203, Temp: 2.6994879245758057, KL: 78.44815063476562, Loss: 0.017779437825083733, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14169/20000], Bound: 0.3831579387187958, Entropy: 142.26876831054688, Temp: 2.6994872093200684, KL: 73.87521362304688, Loss: 0.015246524475514889, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14170/20000], Bound: 0.39739280939102173, Entropy: 141.0904541015625, Temp: 2.6994872093200684, KL: 76.98141479492188, Loss: 0.017227567732334137, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14171/20000], Bound: 0.4076294004917145, Entropy: 142.9814910888672, Temp: 2.6994872093200684, KL: 79.81182861328125, Loss: 0.017648544162511826, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14172/20000], Bound: 0.3843551278114319, Entropy: 140.73141479492188, Temp: 2.6994872093200684, KL: 73.62928771972656, Loss: 0.016346320509910583, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14173/20000], Bound: 0.400087833404541, Entropy: 142.10169982910156, Temp: 2.6994872093200684, KL: 79.586181640625, Loss: 0.013885668478906155, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14174/20000], Bound: 0.38790950179100037, Entropy: 141.27955627441406, Temp: 2.6994893550872803, KL: 74.77130126953125, Loss: 0.01615063287317753, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14175/20000], Bound: 0.3665221333503723, Entropy: 139.7669219970703, Temp: 2.699491262435913, KL: 68.73886108398438, Loss: 0.01592128537595272, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14176/20000], Bound: 0.3890035152435303, Entropy: 140.85044860839844, Temp: 2.699492931365967, KL: 74.92098999023438, Loss: 0.01646619848906994, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14177/20000], Bound: 0.3609582781791687, Entropy: 141.11825561523438, Temp: 2.6994943618774414, KL: 66.97421264648438, Loss: 0.01628005877137184, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14178/20000], Bound: 0.390495628118515, Entropy: 141.4039764404297, Temp: 2.6994950771331787, KL: 76.63710021972656, Loss: 0.014097742736339569, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14179/20000], Bound: 0.3815889358520508, Entropy: 140.6571044921875, Temp: 2.6994972229003906, KL: 70.83348083496094, Loss: 0.020037824288010597, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14180/20000], Bound: 0.36618366837501526, Entropy: 141.454833984375, Temp: 2.6994972229003906, KL: 67.68934631347656, Loss: 0.017687605693936348, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14181/20000], Bound: 0.39615216851234436, Entropy: 142.6630096435547, Temp: 2.699495553970337, KL: 76.90022277832031, Loss: 0.01669752225279808, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14182/20000], Bound: 0.3953603506088257, Entropy: 140.15884399414062, Temp: 2.6994943618774414, KL: 77.44337463378906, Loss: 0.015257754363119602, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14183/20000], Bound: 0.39009302854537964, Entropy: 141.11557006835938, Temp: 2.6994943618774414, KL: 72.85617065429688, Loss: 0.020882004871964455, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14184/20000], Bound: 0.38237839937210083, Entropy: 140.23928833007812, Temp: 2.6994919776916504, KL: 73.07206726074219, Loss: 0.0163152776658535, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14185/20000], Bound: 0.3887113332748413, Entropy: 140.00096130371094, Temp: 2.6994898319244385, KL: 74.81399536132812, Loss: 0.016505954787135124, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14186/20000], Bound: 0.37881970405578613, Entropy: 142.5363311767578, Temp: 2.6994876861572266, KL: 71.58189392089844, Loss: 0.01716887764632702, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14187/20000], Bound: 0.39635616540908813, Entropy: 141.4518280029297, Temp: 2.6994855403900146, KL: 76.33956909179688, Loss: 0.01784765161573887, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14188/20000], Bound: 0.38419386744499207, Entropy: 142.39892578125, Temp: 2.6994831562042236, KL: 74.32916259765625, Loss: 0.014963150024414062, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14189/20000], Bound: 0.3984690010547638, Entropy: 138.41012573242188, Temp: 2.69948148727417, KL: 77.26686096191406, Loss: 0.017290178686380386, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14190/20000], Bound: 0.36744624376296997, Entropy: 142.04550170898438, Temp: 2.6994800567626953, KL: 67.94566345214844, Loss: 0.017875870689749718, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14191/20000], Bound: 0.36191922426223755, Entropy: 142.24343872070312, Temp: 2.699477434158325, KL: 67.26332092285156, Loss: 0.016245385631918907, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14192/20000], Bound: 0.37929895520210266, Entropy: 142.04080200195312, Temp: 2.699474573135376, KL: 72.60116577148438, Loss: 0.015537049621343613, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14193/20000], Bound: 0.40751704573631287, Entropy: 142.30384826660156, Temp: 2.699472427368164, KL: 80.32926940917969, Loss: 0.016627363860607147, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14194/20000], Bound: 0.39071860909461975, Entropy: 141.95782470703125, Temp: 2.6994709968566895, KL: 76.64982604980469, Loss: 0.014195144176483154, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14195/20000], Bound: 0.40313076972961426, Entropy: 141.33877563476562, Temp: 2.6994709968566895, KL: 79.83450317382812, Loss: 0.015106795355677605, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14196/20000], Bound: 0.390821635723114, Entropy: 140.02557373046875, Temp: 2.699472665786743, KL: 75.24824523925781, Loss: 0.016847163438796997, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14197/20000], Bound: 0.39329057931900024, Entropy: 144.41668701171875, Temp: 2.6994738578796387, KL: 75.43400573730469, Loss: 0.017848080024123192, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14198/20000], Bound: 0.3823603391647339, Entropy: 145.0189971923828, Temp: 2.699474573135376, KL: 72.80467224121094, Loss: 0.0168006531894207, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14199/20000], Bound: 0.3749934732913971, Entropy: 141.43260192871094, Temp: 2.699474811553955, KL: 70.86778259277344, Loss: 0.016452524811029434, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14200/20000], Bound: 0.37645041942596436, Entropy: 140.76722717285156, Temp: 2.699474811553955, KL: 72.63381958007812, Loss: 0.013956503942608833, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14201/20000], Bound: 0.39490965008735657, Entropy: 141.23715209960938, Temp: 2.6994760036468506, KL: 75.10896301269531, Loss: 0.019334744662046432, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14202/20000], Bound: 0.38621383905410767, Entropy: 139.87852478027344, Temp: 2.6994755268096924, KL: 75.46455383300781, Loss: 0.01394948735833168, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14203/20000], Bound: 0.4110167622566223, Entropy: 141.12220764160156, Temp: 2.699476718902588, KL: 81.55728149414062, Loss: 0.01630881242454052, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14204/20000], Bound: 0.3971909284591675, Entropy: 139.23912048339844, Temp: 2.6994786262512207, KL: 77.87496948242188, Loss: 0.015461617149412632, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14205/20000], Bound: 0.3621302545070648, Entropy: 140.84304809570312, Temp: 2.699481248855591, KL: 67.02742004394531, Loss: 0.01679244078695774, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14206/20000], Bound: 0.38695719838142395, Entropy: 141.9317169189453, Temp: 2.6994829177856445, KL: 75.96713256835938, Loss: 0.013420374132692814, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14207/20000], Bound: 0.38915011286735535, Entropy: 141.481201171875, Temp: 2.699486017227173, KL: 73.30361938476562, Loss: 0.019541360437870026, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14208/20000], Bound: 0.3555392026901245, Entropy: 142.58949279785156, Temp: 2.6994869709014893, KL: 65.75143432617188, Loss: 0.015732645988464355, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14209/20000], Bound: 0.40324658155441284, Entropy: 141.157958984375, Temp: 2.6994876861572266, KL: 80.10623168945312, Loss: 0.014667832292616367, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14210/20000], Bound: 0.3801080286502838, Entropy: 142.28866577148438, Temp: 2.6994900703430176, KL: 73.1292724609375, Loss: 0.014991874806582928, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14211/20000], Bound: 0.40024444460868835, Entropy: 139.16319274902344, Temp: 2.6994926929473877, KL: 79.27632141113281, Loss: 0.014545990154147148, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14212/20000], Bound: 0.3757690191268921, Entropy: 141.525634765625, Temp: 2.6994965076446533, KL: 72.12155151367188, Loss: 0.01454285066574812, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14213/20000], Bound: 0.39734068512916565, Entropy: 140.80477905273438, Temp: 2.6995010375976562, KL: 78.19544982910156, Loss: 0.014950470998883247, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14214/20000], Bound: 0.40534457564353943, Entropy: 140.10098266601562, Temp: 2.6995060443878174, KL: 78.37672424316406, Loss: 0.019035207107663155, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14215/20000], Bound: 0.372702419757843, Entropy: 140.0316619873047, Temp: 2.699509859085083, KL: 71.50094604492188, Loss: 0.014064627699553967, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14216/20000], Bound: 0.39531517028808594, Entropy: 141.0196075439453, Temp: 2.699514389038086, KL: 78.01640319824219, Loss: 0.01417186576873064, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14217/20000], Bound: 0.3778735399246216, Entropy: 140.84381103515625, Temp: 2.6995201110839844, KL: 73.26454162597656, Loss: 0.013547331094741821, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14218/20000], Bound: 0.38394594192504883, Entropy: 141.72764587402344, Temp: 2.69952654838562, KL: 74.49559020996094, Loss: 0.01452182698994875, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14219/20000], Bound: 0.39350566267967224, Entropy: 142.55516052246094, Temp: 2.699533224105835, KL: 75.99465942382812, Loss: 0.01692754216492176, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14220/20000], Bound: 0.374337375164032, Entropy: 141.11337280273438, Temp: 2.6995396614074707, KL: 71.77236938476562, Loss: 0.014429144561290741, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14221/20000], Bound: 0.38894936442375183, Entropy: 140.9523162841797, Temp: 2.6995460987091064, KL: 74.39759826660156, Loss: 0.017406728118658066, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14222/20000], Bound: 0.37741169333457947, Entropy: 141.4586181640625, Temp: 2.6995513439178467, KL: 73.10464477539062, Loss: 0.013597412034869194, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14223/20000], Bound: 0.37161821126937866, Entropy: 139.9298858642578, Temp: 2.6995575428009033, KL: 69.61735534667969, Loss: 0.016979936510324478, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14224/20000], Bound: 0.3905387222766876, Entropy: 139.4598846435547, Temp: 2.6995625495910645, KL: 75.41841125488281, Loss: 0.016379013657569885, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14225/20000], Bound: 0.35105839371681213, Entropy: 142.0980987548828, Temp: 2.6995670795440674, KL: 64.01472473144531, Loss: 0.016640661284327507, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14226/20000], Bound: 0.37201666831970215, Entropy: 140.40916442871094, Temp: 2.6995701789855957, KL: 70.55096435546875, Loss: 0.015461668372154236, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14227/20000], Bound: 0.3801753520965576, Entropy: 141.1824951171875, Temp: 2.699573040008545, KL: 73.92465209960938, Loss: 0.013555523939430714, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14228/20000], Bound: 0.39812442660331726, Entropy: 139.51231384277344, Temp: 2.6995773315429688, KL: 78.4776611328125, Loss: 0.014859098941087723, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14229/20000], Bound: 0.36657512187957764, Entropy: 141.27088928222656, Temp: 2.69958233833313, KL: 69.91432189941406, Loss: 0.013772696256637573, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14230/20000], Bound: 0.3826541006565094, Entropy: 142.33926391601562, Temp: 2.6995880603790283, KL: 73.28286743164062, Loss: 0.016073761507868767, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14231/20000], Bound: 0.372096449136734, Entropy: 141.113525390625, Temp: 2.6995930671691895, KL: 70.2662353515625, Loss: 0.01603139378130436, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14232/20000], Bound: 0.3896353840827942, Entropy: 141.001220703125, Temp: 2.6995975971221924, KL: 74.84986877441406, Loss: 0.016941707581281662, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14233/20000], Bound: 0.3777221739292145, Entropy: 141.2848358154297, Temp: 2.699601650238037, KL: 73.3330078125, Loss: 0.013340549543499947, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14234/20000], Bound: 0.3909894526004791, Entropy: 139.40834045410156, Temp: 2.6996066570281982, KL: 75.33973693847656, Loss: 0.01677021011710167, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14235/20000], Bound: 0.37823715806007385, Entropy: 141.3427276611328, Temp: 2.6996114253997803, KL: 71.95863342285156, Loss: 0.016160989180207253, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14236/20000], Bound: 0.3942049443721771, Entropy: 142.0229949951172, Temp: 2.699615478515625, KL: 76.97209167480469, Loss: 0.015499924309551716, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14237/20000], Bound: 0.3794823884963989, Entropy: 140.67340087890625, Temp: 2.699620008468628, KL: 70.94180297851562, Loss: 0.018709758296608925, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14238/20000], Bound: 0.4192480742931366, Entropy: 139.8568878173828, Temp: 2.699622631072998, KL: 82.50294494628906, Loss: 0.019200198352336884, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14239/20000], Bound: 0.36996719241142273, Entropy: 142.8478240966797, Temp: 2.699624538421631, KL: 70.06153869628906, Loss: 0.01528571080416441, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14240/20000], Bound: 0.3949299156665802, Entropy: 139.6040496826172, Temp: 2.6996262073516846, KL: 76.62532043457031, Loss: 0.016538670286536217, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14241/20000], Bound: 0.36981499195098877, Entropy: 142.17318725585938, Temp: 2.6996283531188965, KL: 69.11831665039062, Loss: 0.016952408477663994, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14242/20000], Bound: 0.41101518273353577, Entropy: 140.01217651367188, Temp: 2.699629306793213, KL: 79.88267517089844, Loss: 0.019411055371165276, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14243/20000], Bound: 0.388672411441803, Entropy: 140.48117065429688, Temp: 2.699629306793213, KL: 74.65737915039062, Loss: 0.016776185482740402, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14244/20000], Bound: 0.4002743065357208, Entropy: 140.7261505126953, Temp: 2.6996288299560547, KL: 79.411865234375, Loss: 0.01431285310536623, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14245/20000], Bound: 0.3612959682941437, Entropy: 140.609130859375, Temp: 2.6996304988861084, KL: 65.69490051269531, Loss: 0.01882646605372429, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14246/20000], Bound: 0.37393561005592346, Entropy: 141.67337036132812, Temp: 2.699629783630371, KL: 71.00250244140625, Loss: 0.015642598271369934, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14247/20000], Bound: 0.39561280608177185, Entropy: 141.23324584960938, Temp: 2.699629306793213, KL: 76.48104858398438, Loss: 0.01717963255941868, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14248/20000], Bound: 0.3665880262851715, Entropy: 141.74766540527344, Temp: 2.6996288299560547, KL: 68.96522521972656, Loss: 0.015537755563855171, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14249/20000], Bound: 0.39107567071914673, Entropy: 141.44517517089844, Temp: 2.6996285915374756, KL: 75.4730224609375, Loss: 0.016570432111620903, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14250/20000], Bound: 0.39026498794555664, Entropy: 141.3038787841797, Temp: 2.6996281147003174, KL: 73.42791748046875, Loss: 0.019917484372854233, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14251/20000], Bound: 0.37926343083381653, Entropy: 140.16444396972656, Temp: 2.6996259689331055, KL: 71.47113037109375, Loss: 0.017612311989068985, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14252/20000], Bound: 0.40964072942733765, Entropy: 140.94480895996094, Temp: 2.699622869491577, KL: 80.26084899902344, Loss: 0.017941221594810486, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14253/20000], Bound: 0.3940357267856598, Entropy: 140.45504760742188, Temp: 2.699620246887207, KL: 77.47982788085938, Loss: 0.014467154629528522, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14254/20000], Bound: 0.3712084889411926, Entropy: 141.39964294433594, Temp: 2.6996192932128906, KL: 70.64347839355469, Loss: 0.014863318763673306, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14255/20000], Bound: 0.3802277743816376, Entropy: 141.79051208496094, Temp: 2.6996188163757324, KL: 72.16018676757812, Loss: 0.016852011904120445, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14256/20000], Bound: 0.38257619738578796, Entropy: 139.21360778808594, Temp: 2.699618339538574, KL: 72.62728881835938, Loss: 0.017246369272470474, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14257/20000], Bound: 0.3621006906032562, Entropy: 142.24267578125, Temp: 2.6996171474456787, KL: 67.36027526855469, Loss: 0.01616152748465538, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14258/20000], Bound: 0.3836005926132202, Entropy: 140.04652404785156, Temp: 2.699615478515625, KL: 71.49772644042969, Loss: 0.01988920383155346, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14259/20000], Bound: 0.3886418342590332, Entropy: 140.4001007080078, Temp: 2.6996121406555176, KL: 75.85884094238281, Loss: 0.014534222893416882, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14260/20000], Bound: 0.38557127118110657, Entropy: 142.8993377685547, Temp: 2.6996099948883057, KL: 73.64448547363281, Loss: 0.016974937170743942, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14261/20000], Bound: 0.3934704065322876, Entropy: 139.9183807373047, Temp: 2.699608087539673, KL: 77.39447021484375, Loss: 0.014316396787762642, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14262/20000], Bound: 0.3858761489391327, Entropy: 137.8429412841797, Temp: 2.6996078491210938, KL: 75.27191162109375, Loss: 0.014125242829322815, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14263/20000], Bound: 0.37788620591163635, Entropy: 138.95892333984375, Temp: 2.6996090412139893, KL: 73.24905395507812, Loss: 0.013583638705313206, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14264/20000], Bound: 0.3638324439525604, Entropy: 143.85511779785156, Temp: 2.6996114253997803, KL: 67.94143676757812, Loss: 0.015989767387509346, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14265/20000], Bound: 0.38347166776657104, Entropy: 141.45584106445312, Temp: 2.699613094329834, KL: 74.05966186523438, Loss: 0.015074798837304115, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14266/20000], Bound: 0.40132445096969604, Entropy: 140.832763671875, Temp: 2.699615478515625, KL: 79.70123291015625, Loss: 0.014356250874698162, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14267/20000], Bound: 0.3856539726257324, Entropy: 140.73255920410156, Temp: 2.6996192932128906, KL: 74.591796875, Loss: 0.015265099704265594, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14268/20000], Bound: 0.37194162607192993, Entropy: 140.08474731445312, Temp: 2.6996235847473145, KL: 70.1971435546875, Loss: 0.016077715903520584, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14269/20000], Bound: 0.3586080074310303, Entropy: 141.67344665527344, Temp: 2.699627161026001, KL: 68.26408386230469, Loss: 0.012669851072132587, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14270/20000], Bound: 0.40118643641471863, Entropy: 140.6692352294922, Temp: 2.699631929397583, KL: 79.64131164550781, Loss: 0.014391216449439526, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14271/20000], Bound: 0.4031839668750763, Entropy: 139.8908233642578, Temp: 2.6996378898620605, KL: 76.97193908691406, Loss: 0.020439738407731056, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14272/20000], Bound: 0.3706955909729004, Entropy: 140.975830078125, Temp: 2.699641466140747, KL: 70.1031494140625, Loss: 0.015593298710882664, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14273/20000], Bound: 0.39904534816741943, Entropy: 142.16519165039062, Temp: 2.6996448040008545, KL: 77.88211059570312, Loss: 0.016469283029437065, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14274/20000], Bound: 0.38573166728019714, Entropy: 139.85137939453125, Temp: 2.699648141860962, KL: 74.02326965332031, Loss: 0.016360312700271606, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14275/20000], Bound: 0.378248006105423, Entropy: 141.91622924804688, Temp: 2.6996512413024902, KL: 73.16094970703125, Loss: 0.013940330594778061, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14276/20000], Bound: 0.40971624851226807, Entropy: 139.1829833984375, Temp: 2.699655294418335, KL: 80.96908569335938, Loss: 0.016672084107995033, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14277/20000], Bound: 0.38702428340911865, Entropy: 141.53646850585938, Temp: 2.6996593475341797, KL: 74.30966186523438, Loss: 0.016528239473700523, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14278/20000], Bound: 0.39545774459838867, Entropy: 140.46206665039062, Temp: 2.6996631622314453, KL: 75.63838195800781, Loss: 0.01865577884018421, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14279/20000], Bound: 0.3791416585445404, Entropy: 142.5829620361328, Temp: 2.6996655464172363, KL: 70.97592163085938, Loss: 0.018464714288711548, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14280/20000], Bound: 0.3987971246242523, Entropy: 143.6330108642578, Temp: 2.6996665000915527, KL: 77.80674743652344, Loss: 0.016472479328513145, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14281/20000], Bound: 0.36692917346954346, Entropy: 140.98695373535156, Temp: 2.6996676921844482, KL: 68.950439453125, Loss: 0.015744594857096672, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14282/20000], Bound: 0.36961930990219116, Entropy: 141.9613494873047, Temp: 2.6996686458587646, KL: 68.76458740234375, Loss: 0.017504623159766197, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14283/20000], Bound: 0.38482850790023804, Entropy: 142.21180725097656, Temp: 2.6996684074401855, KL: 74.47357177734375, Loss: 0.015039369463920593, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14284/20000], Bound: 0.38167181611061096, Entropy: 139.74777221679688, Temp: 2.6996688842773438, KL: 72.24470520019531, Loss: 0.01746983453631401, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14285/20000], Bound: 0.3969138264656067, Entropy: 142.40155029296875, Temp: 2.6996688842773438, KL: 78.11700439453125, Loss: 0.014863195829093456, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14286/20000], Bound: 0.3737664222717285, Entropy: 140.6769561767578, Temp: 2.69966983795166, KL: 70.12330627441406, Loss: 0.017181528732180595, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14287/20000], Bound: 0.3730970621109009, Entropy: 142.30125427246094, Temp: 2.6996700763702393, KL: 69.42338562011719, Loss: 0.018122944980859756, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14288/20000], Bound: 0.3685430884361267, Entropy: 143.334228515625, Temp: 2.6996688842773438, KL: 70.74650573730469, Loss: 0.013266892172396183, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14289/20000], Bound: 0.36952081322669983, Entropy: 143.23695373535156, Temp: 2.699669361114502, KL: 70.20460510253906, Loss: 0.01478568185120821, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14290/20000], Bound: 0.38880154490470886, Entropy: 140.64613342285156, Temp: 2.6996703147888184, KL: 74.23934936523438, Loss: 0.01762077584862709, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14291/20000], Bound: 0.40272343158721924, Entropy: 140.6514434814453, Temp: 2.6996705532073975, KL: 80.374755859375, Loss: 0.01388279628008604, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14292/20000], Bound: 0.4083900451660156, Entropy: 141.05125427246094, Temp: 2.6996726989746094, KL: 80.53514099121094, Loss: 0.016735047101974487, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14293/20000], Bound: 0.35609108209609985, Entropy: 140.849609375, Temp: 2.6996750831604004, KL: 66.34571838378906, Loss: 0.014918820932507515, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14294/20000], Bound: 0.40546685457229614, Entropy: 139.94764709472656, Temp: 2.6996774673461914, KL: 78.22122192382812, Loss: 0.01939273253083229, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14295/20000], Bound: 0.41086840629577637, Entropy: 137.3424530029297, Temp: 2.699678421020508, KL: 81.559814453125, Loss: 0.01622314751148224, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14296/20000], Bound: 0.3919851779937744, Entropy: 139.34886169433594, Temp: 2.6996800899505615, KL: 76.76637268066406, Loss: 0.014670586213469505, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14297/20000], Bound: 0.37520208954811096, Entropy: 139.84921264648438, Temp: 2.6996829509735107, KL: 72.20663452148438, Loss: 0.014085480011999607, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14298/20000], Bound: 0.3697330057621002, Entropy: 137.15921020507812, Temp: 2.6996867656707764, KL: 68.56480407714844, Loss: 0.017934752628207207, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14299/20000], Bound: 0.3936072289943695, Entropy: 141.1593780517578, Temp: 2.699688673019409, KL: 75.18038940429688, Loss: 0.018492547795176506, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14300/20000], Bound: 0.38361120223999023, Entropy: 140.2439727783203, Temp: 2.6996896266937256, KL: 73.76246643066406, Loss: 0.015701012685894966, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14301/20000], Bound: 0.3866638243198395, Entropy: 141.48419189453125, Temp: 2.699690818786621, KL: 74.61592102050781, Loss: 0.01576642505824566, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14302/20000], Bound: 0.3825686573982239, Entropy: 139.00062561035156, Temp: 2.6996922492980957, KL: 71.72401428222656, Loss: 0.018915889784693718, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14303/20000], Bound: 0.39830607175827026, Entropy: 141.95298767089844, Temp: 2.6996920108795166, KL: 78.44325256347656, Loss: 0.01502382941544056, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14304/20000], Bound: 0.36564159393310547, Entropy: 142.74461364746094, Temp: 2.699693202972412, KL: 67.66484069824219, Loss: 0.01745007187128067, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14305/20000], Bound: 0.4012216329574585, Entropy: 141.16946411132812, Temp: 2.699692964553833, KL: 77.47080993652344, Loss: 0.018431218340992928, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14306/20000], Bound: 0.372920423746109, Entropy: 141.41551208496094, Temp: 2.6996920108795166, KL: 71.34442138671875, Loss: 0.014471625909209251, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14307/20000], Bound: 0.3911726772785187, Entropy: 139.55645751953125, Temp: 2.6996920108795166, KL: 75.21162414550781, Loss: 0.01710793375968933, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14308/20000], Bound: 0.36144620180130005, Entropy: 141.08192443847656, Temp: 2.6996920108795166, KL: 67.84556579589844, Loss: 0.014922015368938446, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14309/20000], Bound: 0.3802458941936493, Entropy: 139.17979431152344, Temp: 2.6996920108795166, KL: 73.42137145996094, Loss: 0.014526546001434326, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14310/20000], Bound: 0.3666752874851227, Entropy: 141.61485290527344, Temp: 2.699692964553833, KL: 69.22154235839844, Loss: 0.015109348110854626, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14311/20000], Bound: 0.37645870447158813, Entropy: 141.39178466796875, Temp: 2.6996941566467285, KL: 70.62950134277344, Loss: 0.017675083130598068, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14312/20000], Bound: 0.40036237239837646, Entropy: 141.5169677734375, Temp: 2.6996941566467285, KL: 79.18038940429688, Loss: 0.014790822751820087, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14313/20000], Bound: 0.3954187035560608, Entropy: 139.49465942382812, Temp: 2.699695587158203, KL: 77.15673828125, Loss: 0.015822554007172585, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14314/20000], Bound: 0.3956873118877411, Entropy: 140.25912475585938, Temp: 2.699697732925415, KL: 77.99861145019531, Loss: 0.014410468749701977, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14315/20000], Bound: 0.38824373483657837, Entropy: 139.9751434326172, Temp: 2.6997010707855225, KL: 75.96102905273438, Loss: 0.014130126684904099, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14316/20000], Bound: 0.3978971540927887, Entropy: 141.29566955566406, Temp: 2.6997056007385254, KL: 77.41786193847656, Loss: 0.01669829897582531, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14317/20000], Bound: 0.39985278248786926, Entropy: 139.84866333007812, Temp: 2.69970965385437, KL: 78.00410461425781, Loss: 0.016688615083694458, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14318/20000], Bound: 0.3810029625892639, Entropy: 142.33399963378906, Temp: 2.699713945388794, KL: 72.87744140625, Loss: 0.015939680859446526, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14319/20000], Bound: 0.38434016704559326, Entropy: 141.79400634765625, Temp: 2.6997177600860596, KL: 74.45259094238281, Loss: 0.01481553167104721, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14320/20000], Bound: 0.3957083523273468, Entropy: 141.6501007080078, Temp: 2.6997222900390625, KL: 77.51748657226562, Loss: 0.015313315205276012, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14321/20000], Bound: 0.37794414162635803, Entropy: 139.83226013183594, Temp: 2.6997275352478027, KL: 71.9078369140625, Loss: 0.016099659726023674, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14322/20000], Bound: 0.38577646017074585, Entropy: 142.68930053710938, Temp: 2.6997318267822266, KL: 74.1331787109375, Loss: 0.016181616112589836, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14323/20000], Bound: 0.3665699064731598, Entropy: 140.20401000976562, Temp: 2.6997361183166504, KL: 69.33853149414062, Loss: 0.014837721362709999, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14324/20000], Bound: 0.367399662733078, Entropy: 141.988525390625, Temp: 2.699740409851074, KL: 67.70050048828125, Loss: 0.01830734871327877, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14325/20000], Bound: 0.4192483723163605, Entropy: 140.57675170898438, Temp: 2.699742555618286, KL: 84.03089904785156, Loss: 0.016371745616197586, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14326/20000], Bound: 0.41843321919441223, Entropy: 141.46408081054688, Temp: 2.6997456550598145, KL: 82.91719055175781, Loss: 0.017972180619835854, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14327/20000], Bound: 0.3887193500995636, Entropy: 140.5634002685547, Temp: 2.6997485160827637, KL: 73.33065795898438, Loss: 0.019259806722402573, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14328/20000], Bound: 0.3739018440246582, Entropy: 141.741943359375, Temp: 2.699749708175659, KL: 70.45172119140625, Loss: 0.016645783558487892, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14329/20000], Bound: 0.3762568533420563, Entropy: 141.0663604736328, Temp: 2.6997501850128174, KL: 72.94828796386719, Loss: 0.013273634947836399, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14330/20000], Bound: 0.3927677273750305, Entropy: 141.88259887695312, Temp: 2.69975209236145, KL: 74.414794921875, Loss: 0.019452925771474838, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14331/20000], Bound: 0.4160469174385071, Entropy: 141.6565399169922, Temp: 2.6997523307800293, KL: 83.06111145019531, Loss: 0.016355933621525764, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14332/20000], Bound: 0.42234474420547485, Entropy: 141.0736541748047, Temp: 2.699753761291504, KL: 84.53684997558594, Loss: 0.017195917665958405, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14333/20000], Bound: 0.40218210220336914, Entropy: 140.820068359375, Temp: 2.6997554302215576, KL: 77.48861694335938, Loss: 0.01892947219312191, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14334/20000], Bound: 0.40005481243133545, Entropy: 143.2154998779297, Temp: 2.699756145477295, KL: 79.05174255371094, Loss: 0.01486015785485506, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14335/20000], Bound: 0.3878004550933838, Entropy: 140.79725646972656, Temp: 2.6997580528259277, KL: 73.69911193847656, Loss: 0.01807978004217148, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14336/20000], Bound: 0.375832200050354, Entropy: 144.0841064453125, Temp: 2.6997592449188232, KL: 72.35792541503906, Loss: 0.01414106972515583, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14337/20000], Bound: 0.3913758397102356, Entropy: 140.21710205078125, Temp: 2.699761152267456, KL: 75.42024230957031, Loss: 0.016832713037729263, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14338/20000], Bound: 0.3770869970321655, Entropy: 141.1468963623047, Temp: 2.699763059616089, KL: 69.69915771484375, Loss: 0.019733356311917305, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14339/20000], Bound: 0.3854652941226959, Entropy: 140.99932861328125, Temp: 2.6997623443603516, KL: 74.42094421386719, Loss: 0.015481055714190006, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14340/20000], Bound: 0.38480329513549805, Entropy: 140.1415557861328, Temp: 2.6997623443603516, KL: 74.15518188476562, Loss: 0.015616293996572495, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14341/20000], Bound: 0.35175344347953796, Entropy: 141.82179260253906, Temp: 2.6997628211975098, KL: 66.033203125, Loss: 0.013261008076369762, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14342/20000], Bound: 0.41731229424476624, Entropy: 141.77342224121094, Temp: 2.6997642517089844, KL: 83.15203857421875, Loss: 0.01690278947353363, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14343/20000], Bound: 0.3981657326221466, Entropy: 141.18777465820312, Temp: 2.699766159057617, KL: 76.69346618652344, Loss: 0.018188077956438065, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14344/20000], Bound: 0.37118440866470337, Entropy: 140.6393585205078, Temp: 2.6997673511505127, KL: 70.76853942871094, Loss: 0.0146202826872468, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14345/20000], Bound: 0.4029029309749603, Entropy: 141.88441467285156, Temp: 2.6997690200805664, KL: 79.32612609863281, Loss: 0.015925291925668716, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14346/20000], Bound: 0.3760489821434021, Entropy: 142.3153076171875, Temp: 2.6997714042663574, KL: 71.55726623535156, Loss: 0.015739381313323975, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14347/20000], Bound: 0.3753664791584015, Entropy: 139.02987670898438, Temp: 2.6997737884521484, KL: 71.05323791503906, Loss: 0.016309810802340508, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14348/20000], Bound: 0.3431854844093323, Entropy: 140.1778106689453, Temp: 2.699775457382202, KL: 62.5556640625, Loss: 0.015321667306125164, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14349/20000], Bound: 0.4006044864654541, Entropy: 143.43606567382812, Temp: 2.6997766494750977, KL: 78.2105712890625, Loss: 0.016721336171030998, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14350/20000], Bound: 0.38518160581588745, Entropy: 141.23715209960938, Temp: 2.699777841567993, KL: 73.57061767578125, Loss: 0.016903020441532135, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14351/20000], Bound: 0.3680950999259949, Entropy: 142.54159545898438, Temp: 2.6997787952423096, KL: 68.4571533203125, Loss: 0.017272042110562325, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14352/20000], Bound: 0.40408098697662354, Entropy: 139.65509033203125, Temp: 2.6997785568237305, KL: 78.60443115234375, Loss: 0.01791464351117611, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14353/20000], Bound: 0.37677401304244995, Entropy: 141.64239501953125, Temp: 2.6997780799865723, KL: 72.572021484375, Loss: 0.014246153645217419, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14354/20000], Bound: 0.3603443503379822, Entropy: 141.35134887695312, Temp: 2.6997787952423096, KL: 66.79899597167969, Loss: 0.016287069767713547, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14355/20000], Bound: 0.3903921842575073, Entropy: 139.52166748046875, Temp: 2.6997785568237305, KL: 73.43251037597656, Loss: 0.019979292526841164, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14356/20000], Bound: 0.3928001821041107, Entropy: 138.83396911621094, Temp: 2.6997766494750977, KL: 75.97798156738281, Loss: 0.01657581701874733, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14357/20000], Bound: 0.37539154291152954, Entropy: 141.52589416503906, Temp: 2.699775218963623, KL: 71.24734497070312, Loss: 0.015963654965162277, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14358/20000], Bound: 0.3590736985206604, Entropy: 141.63543701171875, Temp: 2.6997735500335693, KL: 66.69425964355469, Loss: 0.015820294618606567, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14359/20000], Bound: 0.373158723115921, Entropy: 140.55165100097656, Temp: 2.6997716426849365, KL: 70.34869384765625, Loss: 0.016442719846963882, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14360/20000], Bound: 0.38669946789741516, Entropy: 142.25152587890625, Temp: 2.6997697353363037, KL: 76.16120910644531, Loss: 0.01292452309280634, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14361/20000], Bound: 0.3890116512775421, Entropy: 141.7109375, Temp: 2.699770212173462, KL: 74.27659606933594, Loss: 0.017666563391685486, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14362/20000], Bound: 0.35398390889167786, Entropy: 142.5385284423828, Temp: 2.699769973754883, KL: 66.64772033691406, Loss: 0.013271689414978027, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14363/20000], Bound: 0.39547809958457947, Entropy: 139.98426818847656, Temp: 2.69977068901062, KL: 77.84811401367188, Loss: 0.014575379900634289, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14364/20000], Bound: 0.369485080242157, Entropy: 140.0774688720703, Temp: 2.699772596359253, KL: 70.18299865722656, Loss: 0.014807749539613724, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14365/20000], Bound: 0.3679165244102478, Entropy: 143.0435333251953, Temp: 2.699775218963623, KL: 70.00947570800781, Loss: 0.014303101226687431, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14366/20000], Bound: 0.3804721236228943, Entropy: 139.75222778320312, Temp: 2.699777841567993, KL: 73.17312622070312, Loss: 0.015108227729797363, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14367/20000], Bound: 0.4110904037952423, Entropy: 140.7643585205078, Temp: 2.6997814178466797, KL: 82.60478210449219, Loss: 0.014413328841328621, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14368/20000], Bound: 0.3920223116874695, Entropy: 141.513916015625, Temp: 2.6997861862182617, KL: 75.46614074707031, Loss: 0.017099890857934952, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14369/20000], Bound: 0.35796231031417847, Entropy: 140.5795440673828, Temp: 2.6997904777526855, KL: 64.85711669921875, Loss: 0.01864592544734478, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14370/20000], Bound: 0.36690306663513184, Entropy: 141.05287170410156, Temp: 2.69979190826416, KL: 69.49708557128906, Loss: 0.014719505794346333, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14371/20000], Bound: 0.39313817024230957, Entropy: 141.87425231933594, Temp: 2.699794054031372, KL: 73.69210815429688, Loss: 0.02099374681711197, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14372/20000], Bound: 0.3912280201911926, Entropy: 140.57237243652344, Temp: 2.6997933387756348, KL: 75.99295043945312, Loss: 0.015691915526986122, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14373/20000], Bound: 0.37454596161842346, Entropy: 142.1942138671875, Temp: 2.6997933387756348, KL: 71.09039306640625, Loss: 0.015805209055542946, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14374/20000], Bound: 0.3909773528575897, Entropy: 140.18968200683594, Temp: 2.6997933387756348, KL: 75.71003723144531, Loss: 0.01607952080667019, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14375/20000], Bound: 0.3876219391822815, Entropy: 140.1141815185547, Temp: 2.699793815612793, KL: 72.97825622558594, Loss: 0.019318481907248497, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14376/20000], Bound: 0.4076771140098572, Entropy: 139.421875, Temp: 2.6997926235198975, KL: 80.27584838867188, Loss: 0.01681881584227085, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14377/20000], Bound: 0.40582510828971863, Entropy: 139.56298828125, Temp: 2.69979190826416, KL: 79.04168701171875, Loss: 0.01807338185608387, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14378/20000], Bound: 0.3940451741218567, Entropy: 141.13140869140625, Temp: 2.6997909545898438, KL: 76.96717834472656, Loss: 0.015423458069562912, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14379/20000], Bound: 0.37596648931503296, Entropy: 142.41258239746094, Temp: 2.699791193008423, KL: 72.66600036621094, Loss: 0.013642266392707825, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14380/20000], Bound: 0.3811609745025635, Entropy: 142.37026977539062, Temp: 2.6997928619384766, KL: 72.57965087890625, Loss: 0.016576601192355156, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14381/20000], Bound: 0.39028409123420715, Entropy: 139.68853759765625, Temp: 2.699794054031372, KL: 76.44102478027344, Loss: 0.014348926022648811, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14382/20000], Bound: 0.39465898275375366, Entropy: 141.55047607421875, Temp: 2.699796438217163, KL: 75.09974670410156, Loss: 0.019217440858483315, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14383/20000], Bound: 0.3667902648448944, Entropy: 143.0301513671875, Temp: 2.6997973918914795, KL: 67.66253662109375, Loss: 0.01805786043405533, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14384/20000], Bound: 0.39101970195770264, Entropy: 139.82583618164062, Temp: 2.699796676635742, KL: 76.90592956542969, Loss: 0.013887781649827957, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14385/20000], Bound: 0.38584595918655396, Entropy: 144.45823669433594, Temp: 2.6997976303100586, KL: 73.3983154296875, Loss: 0.017580680549144745, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14386/20000], Bound: 0.3901073932647705, Entropy: 141.56398010253906, Temp: 2.699798107147217, KL: 75.08634948730469, Loss: 0.01676183007657528, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14387/20000], Bound: 0.4095894694328308, Entropy: 139.07958984375, Temp: 2.699798107147217, KL: 81.54421997070312, Loss: 0.015537535771727562, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14388/20000], Bound: 0.41988492012023926, Entropy: 141.58436584472656, Temp: 2.6997997760772705, KL: 82.89756774902344, Loss: 0.018832648172974586, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14389/20000], Bound: 0.3822518587112427, Entropy: 141.65176391601562, Temp: 2.699800729751587, KL: 72.39871215820312, Loss: 0.017497070133686066, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14390/20000], Bound: 0.4132949411869049, Entropy: 141.81375122070312, Temp: 2.699800968170166, KL: 83.48612976074219, Loss: 0.014018788933753967, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14391/20000], Bound: 0.3699934184551239, Entropy: 141.48773193359375, Temp: 2.699803590774536, KL: 69.0811767578125, Loss: 0.017116686329245567, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14392/20000], Bound: 0.4019368290901184, Entropy: 141.81875610351562, Temp: 2.6998047828674316, KL: 80.28448486328125, Loss: 0.013616442680358887, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14393/20000], Bound: 0.37585732340812683, Entropy: 141.31991577148438, Temp: 2.699808359146118, KL: 71.47999572753906, Loss: 0.015780797228217125, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14394/20000], Bound: 0.3784957826137543, Entropy: 141.85704040527344, Temp: 2.6998112201690674, KL: 70.73295593261719, Loss: 0.01857072114944458, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14395/20000], Bound: 0.3741280734539032, Entropy: 139.5938720703125, Temp: 2.699812650680542, KL: 71.7742919921875, Loss: 0.014316946268081665, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14396/20000], Bound: 0.4128950834274292, Entropy: 142.06137084960938, Temp: 2.699814796447754, KL: 82.62257385253906, Loss: 0.015393480658531189, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14397/20000], Bound: 0.4155133366584778, Entropy: 140.2196502685547, Temp: 2.6998181343078613, KL: 82.77090454101562, Loss: 0.01659296452999115, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14398/20000], Bound: 0.3833281099796295, Entropy: 140.96499633789062, Temp: 2.699822187423706, KL: 72.4796142578125, Loss: 0.01792575791478157, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14399/20000], Bound: 0.3783831000328064, Entropy: 141.88304138183594, Temp: 2.699824810028076, KL: 72.89408874511719, Loss: 0.014508295804262161, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14400/20000], Bound: 0.3691079318523407, Entropy: 141.22848510742188, Temp: 2.6998281478881836, KL: 70.80998229980469, Loss: 0.013448312878608704, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14401/20000], Bound: 0.38280192017555237, Entropy: 139.83642578125, Temp: 2.6998324394226074, KL: 74.24055480957031, Loss: 0.014381766319274902, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14402/20000], Bound: 0.3875812590122223, Entropy: 141.0664520263672, Temp: 2.6998374462127686, KL: 74.35336303710938, Loss: 0.01675013266503811, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14403/20000], Bound: 0.3862662613391876, Entropy: 142.07431030273438, Temp: 2.6998417377471924, KL: 73.27169799804688, Loss: 0.018042463809251785, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14404/20000], Bound: 0.3841927945613861, Entropy: 139.85105895996094, Temp: 2.6998448371887207, KL: 75.085693359375, Loss: 0.013564899563789368, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14405/20000], Bound: 0.3868557810783386, Entropy: 141.732666015625, Temp: 2.6998493671417236, KL: 73.21913146972656, Loss: 0.018458442762494087, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14406/20000], Bound: 0.3907346725463867, Entropy: 139.23123168945312, Temp: 2.699852228164673, KL: 76.56907653808594, Loss: 0.01435722317546606, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14407/20000], Bound: 0.38777631521224976, Entropy: 143.06678771972656, Temp: 2.6998562812805176, KL: 72.52113342285156, Loss: 0.020249085500836372, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14408/20000], Bound: 0.38753485679626465, Entropy: 141.88909912109375, Temp: 2.699857473373413, KL: 75.36532592773438, Loss: 0.014851114712655544, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14409/20000], Bound: 0.428192675113678, Entropy: 141.5785675048828, Temp: 2.699859857559204, KL: 84.35881042480469, Loss: 0.020875688642263412, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14410/20000], Bound: 0.3839406371116638, Entropy: 139.87413024902344, Temp: 2.6998605728149414, KL: 72.89375305175781, Loss: 0.017488626763224602, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14411/20000], Bound: 0.38474035263061523, Entropy: 141.6446990966797, Temp: 2.6998610496520996, KL: 74.49684143066406, Loss: 0.014950541779398918, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14412/20000], Bound: 0.40026426315307617, Entropy: 141.21482849121094, Temp: 2.699861764907837, KL: 77.43309020996094, Loss: 0.017974374815821648, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14413/20000], Bound: 0.374472051858902, Entropy: 141.2017059326172, Temp: 2.699862241744995, KL: 71.6790771484375, Loss: 0.01467632781714201, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14414/20000], Bound: 0.3871948719024658, Entropy: 141.83139038085938, Temp: 2.6998634338378906, KL: 74.56936645507812, Loss: 0.016141340136528015, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14415/20000], Bound: 0.41742321848869324, Entropy: 143.05783081054688, Temp: 2.699864625930786, KL: 81.47306823730469, Loss: 0.020075976848602295, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14416/20000], Bound: 0.3710385262966156, Entropy: 140.43154907226562, Temp: 2.699864625930786, KL: 69.89442443847656, Loss: 0.01616283692419529, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14417/20000], Bound: 0.39759573340415955, Entropy: 141.49618530273438, Temp: 2.699864387512207, KL: 77.09495544433594, Loss: 0.017132237553596497, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14418/20000], Bound: 0.39172807335853577, Entropy: 141.87936401367188, Temp: 2.699864387512207, KL: 75.83760070800781, Loss: 0.01625244691967964, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14419/20000], Bound: 0.42073115706443787, Entropy: 142.04627990722656, Temp: 2.699864387512207, KL: 83.72758483886719, Loss: 0.017777128145098686, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14420/20000], Bound: 0.3929939270019531, Entropy: 138.98471069335938, Temp: 2.6998648643493652, KL: 74.45718383789062, Loss: 0.01949871890246868, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14421/20000], Bound: 0.4042259752750397, Entropy: 140.48158264160156, Temp: 2.699863910675049, KL: 79.69696044921875, Loss: 0.015972565859556198, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14422/20000], Bound: 0.4047015309333801, Entropy: 140.2122802734375, Temp: 2.6998636722564697, KL: 79.32841491699219, Loss: 0.016918914392590523, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14423/20000], Bound: 0.3615301549434662, Entropy: 141.58770751953125, Temp: 2.699863910675049, KL: 67.2034912109375, Loss: 0.01615624502301216, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14424/20000], Bound: 0.3775026500225067, Entropy: 140.6256561279297, Temp: 2.6998634338378906, KL: 71.3101806640625, Loss: 0.016972145065665245, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14425/20000], Bound: 0.38086289167404175, Entropy: 141.7769012451172, Temp: 2.699862480163574, KL: 72.89454650878906, Loss: 0.01583428494632244, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14426/20000], Bound: 0.3676721751689911, Entropy: 142.27178955078125, Temp: 2.699861764907837, KL: 70.32218933105469, Loss: 0.013596247881650925, Learning Rate: 1.172946314574895e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14427/20000], Bound: 0.390934556722641, Entropy: 141.09828186035156, Temp: 2.699862480163574, KL: 76.54737854003906, Loss: 0.014506216160953045, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14428/20000], Bound: 0.37661829590797424, Entropy: 140.99359130859375, Temp: 2.699864387512207, KL: 72.48765563964844, Loss: 0.014320251531898975, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14429/20000], Bound: 0.3888150155544281, Entropy: 141.2227020263672, Temp: 2.699867010116577, KL: 74.0260009765625, Loss: 0.01802491396665573, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14430/20000], Bound: 0.34548741579055786, Entropy: 142.34837341308594, Temp: 2.699868679046631, KL: 63.940093994140625, Loss: 0.013930024579167366, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14431/20000], Bound: 0.4052632451057434, Entropy: 141.5253143310547, Temp: 2.6998705863952637, KL: 81.518310546875, Loss: 0.013175314292311668, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14432/20000], Bound: 0.3726549744606018, Entropy: 141.63499450683594, Temp: 2.6998746395111084, KL: 70.10569763183594, Loss: 0.01662670262157917, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14433/20000], Bound: 0.3778505325317383, Entropy: 140.36947631835938, Temp: 2.6998775005340576, KL: 70.048095703125, Loss: 0.019495148211717606, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14434/20000], Bound: 0.3797294497489929, Entropy: 140.6856231689453, Temp: 2.699878454208374, KL: 72.44456481933594, Loss: 0.016060879454016685, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14435/20000], Bound: 0.37554866075515747, Entropy: 141.7672882080078, Temp: 2.6998794078826904, KL: 72.68418884277344, Loss: 0.013387162238359451, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14436/20000], Bound: 0.37481263279914856, Entropy: 140.61927795410156, Temp: 2.6998815536499023, KL: 71.86314392089844, Loss: 0.014516495168209076, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14437/20000], Bound: 0.3635450303554535, Entropy: 141.74755859375, Temp: 2.6998844146728516, KL: 67.83316040039062, Loss: 0.01604209654033184, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14438/20000], Bound: 0.3806455433368683, Entropy: 140.66110229492188, Temp: 2.6998867988586426, KL: 73.56520080566406, Loss: 0.014476033858954906, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14439/20000], Bound: 0.3837306797504425, Entropy: 140.10609436035156, Temp: 2.699889659881592, KL: 73.88615417480469, Loss: 0.01553802564740181, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14440/20000], Bound: 0.39909449219703674, Entropy: 140.1290283203125, Temp: 2.69989275932312, KL: 76.27397155761719, Loss: 0.019476907327771187, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14441/20000], Bound: 0.3984301686286926, Entropy: 141.12637329101562, Temp: 2.6998941898345947, KL: 77.74674987792969, Loss: 0.01638399437069893, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14442/20000], Bound: 0.41956108808517456, Entropy: 142.33702087402344, Temp: 2.6998960971832275, KL: 84.94570922851562, Loss: 0.014856795780360699, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14443/20000], Bound: 0.3959522247314453, Entropy: 142.30247497558594, Temp: 2.699899673461914, KL: 77.34390258789062, Loss: 0.015770113095641136, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14444/20000], Bound: 0.379937082529068, Entropy: 141.2068328857422, Temp: 2.699903726577759, KL: 73.77043151855469, Loss: 0.013716790825128555, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14445/20000], Bound: 0.3540360629558563, Entropy: 141.5557403564453, Temp: 2.69990873336792, KL: 65.72314453125, Loss: 0.015011996030807495, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14446/20000], Bound: 0.4125211536884308, Entropy: 140.60968017578125, Temp: 2.699913263320923, KL: 82.49461364746094, Loss: 0.015421454794704914, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14447/20000], Bound: 0.4263530373573303, Entropy: 140.3483123779297, Temp: 2.699918508529663, KL: 86.15922546386719, Loss: 0.016485368832945824, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14448/20000], Bound: 0.37376725673675537, Entropy: 141.69720458984375, Temp: 2.6999247074127197, KL: 69.1260986328125, Loss: 0.019030731171369553, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14449/20000], Bound: 0.3934163451194763, Entropy: 140.11859130859375, Temp: 2.6999282836914062, KL: 76.792724609375, Loss: 0.01540452428162098, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14450/20000], Bound: 0.38868317008018494, Entropy: 140.58065795898438, Temp: 2.69993257522583, KL: 75.00660705566406, Loss: 0.0161379836499691, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14451/20000], Bound: 0.3734024167060852, Entropy: 141.10787963867188, Temp: 2.699936628341675, KL: 70.8135986328125, Loss: 0.015712285414338112, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14452/20000], Bound: 0.3737766444683075, Entropy: 141.48764038085938, Temp: 2.6999404430389404, KL: 70.08485412597656, Loss: 0.01726030372083187, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14453/20000], Bound: 0.40144839882850647, Entropy: 141.61178588867188, Temp: 2.6999428272247314, KL: 78.34400939941406, Loss: 0.01694164052605629, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14454/20000], Bound: 0.37731102108955383, Entropy: 140.0659942626953, Temp: 2.6999452114105225, KL: 71.69378662109375, Loss: 0.016160253435373306, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14455/20000], Bound: 0.3956473767757416, Entropy: 140.62515258789062, Temp: 2.6999473571777344, KL: 77.61126708984375, Loss: 0.015108468011021614, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14456/20000], Bound: 0.38660138845443726, Entropy: 142.42906188964844, Temp: 2.6999504566192627, KL: 75.02531433105469, Loss: 0.01497691124677658, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14457/20000], Bound: 0.39376258850097656, Entropy: 140.2889404296875, Temp: 2.699954032897949, KL: 75.83914184570312, Loss: 0.01735970750451088, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14458/20000], Bound: 0.3502318263053894, Entropy: 140.72842407226562, Temp: 2.6999571323394775, KL: 64.6146240234375, Loss: 0.015108034014701843, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14459/20000], Bound: 0.3830312192440033, Entropy: 140.82901000976562, Temp: 2.6999597549438477, KL: 73.94842529296875, Loss: 0.015047205612063408, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14460/20000], Bound: 0.38338154554367065, Entropy: 142.31527709960938, Temp: 2.699962615966797, KL: 73.23316955566406, Loss: 0.016560131683945656, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14461/20000], Bound: 0.3857601284980774, Entropy: 141.52639770507812, Temp: 2.699965000152588, KL: 73.30607604980469, Loss: 0.017706643790006638, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14462/20000], Bound: 0.358444482088089, Entropy: 142.50852966308594, Temp: 2.6999664306640625, KL: 67.25408935546875, Loss: 0.01445832196623087, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14463/20000], Bound: 0.3928574323654175, Entropy: 138.7347869873047, Temp: 2.6999683380126953, KL: 75.746337890625, Loss: 0.01703779399394989, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14464/20000], Bound: 0.3876461088657379, Entropy: 142.5191192626953, Temp: 2.69996976852417, KL: 70.85224914550781, Loss: 0.02327004447579384, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14465/20000], Bound: 0.38025328516960144, Entropy: 140.36398315429688, Temp: 2.6999671459198, KL: 72.91604614257812, Loss: 0.015468890778720379, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14466/20000], Bound: 0.3406115770339966, Entropy: 142.49549865722656, Temp: 2.699965476989746, KL: 61.89543151855469, Loss: 0.015240022912621498, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14467/20000], Bound: 0.3970445990562439, Entropy: 140.8282928466797, Temp: 2.699963092803955, KL: 77.21435546875, Loss: 0.016609519720077515, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14468/20000], Bound: 0.3815756142139435, Entropy: 140.79771423339844, Temp: 2.6999611854553223, KL: 74.19419860839844, Loss: 0.013810436241328716, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14469/20000], Bound: 0.4034058153629303, Entropy: 139.44131469726562, Temp: 2.699960947036743, KL: 79.46098327636719, Loss: 0.015955965965986252, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14470/20000], Bound: 0.37426456809043884, Entropy: 140.49549865722656, Temp: 2.6999616622924805, KL: 71.94859313964844, Loss: 0.014067959040403366, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14471/20000], Bound: 0.3759779930114746, Entropy: 141.85398864746094, Temp: 2.699963331222534, KL: 70.70655822753906, Loss: 0.017278624698519707, Learning Rate: 1.172946314574895e-05\n",
      "Epoch [14472/20000], Bound: 0.3655031621456146, Entropy: 141.560791015625, Temp: 2.6999638080596924, KL: 68.14997863769531, Loss: 0.016481053084135056, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14473/20000], Bound: 0.33706673979759216, Entropy: 142.6029510498047, Temp: 2.6999638080596924, KL: 61.2913818359375, Loss: 0.014568001963198185, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14474/20000], Bound: 0.36913278698921204, Entropy: 140.91224670410156, Temp: 2.699963331222534, KL: 69.38536071777344, Loss: 0.016100827604532242, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14475/20000], Bound: 0.37721118330955505, Entropy: 142.1356658935547, Temp: 2.699963092803955, KL: 70.645751953125, Loss: 0.018047994002699852, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14476/20000], Bound: 0.402273029088974, Entropy: 139.1667022705078, Temp: 2.6999619007110596, KL: 79.7393798828125, Loss: 0.014813508838415146, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14477/20000], Bound: 0.3680076599121094, Entropy: 142.7279052734375, Temp: 2.6999619007110596, KL: 70.07537841796875, Loss: 0.014230651780962944, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14478/20000], Bound: 0.3788336515426636, Entropy: 141.61585998535156, Temp: 2.6999623775482178, KL: 72.29341125488281, Loss: 0.015862589702010155, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14479/20000], Bound: 0.3725864589214325, Entropy: 141.69371032714844, Temp: 2.699963092803955, KL: 71.941650390625, Loss: 0.013191201724112034, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14480/20000], Bound: 0.3727901875972748, Entropy: 140.20530700683594, Temp: 2.6999645233154297, KL: 70.90847778320312, Loss: 0.015212414786219597, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14481/20000], Bound: 0.4332914650440216, Entropy: 142.2952117919922, Temp: 2.6999664306640625, KL: 87.9815673828125, Loss: 0.017112599685788155, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14482/20000], Bound: 0.3936930000782013, Entropy: 142.3633575439453, Temp: 2.6999685764312744, KL: 76.06640625, Loss: 0.016900965943932533, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14483/20000], Bound: 0.3904000222682953, Entropy: 139.93109130859375, Temp: 2.6999704837799072, KL: 74.04031372070312, Loss: 0.018859490752220154, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14484/20000], Bound: 0.41269493103027344, Entropy: 142.21678161621094, Temp: 2.6999714374542236, KL: 81.38409423828125, Loss: 0.0175761915743351, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14485/20000], Bound: 0.41585129499435425, Entropy: 140.10476684570312, Temp: 2.699972629547119, KL: 80.43368530273438, Loss: 0.021113522350788116, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14486/20000], Bound: 0.37167149782180786, Entropy: 140.93145751953125, Temp: 2.69997239112854, KL: 69.99650573730469, Loss: 0.016309237107634544, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14487/20000], Bound: 0.3698640763759613, Entropy: 141.00791931152344, Temp: 2.699971914291382, KL: 70.33404541015625, Loss: 0.01472957618534565, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14488/20000], Bound: 0.38452503085136414, Entropy: 141.04978942871094, Temp: 2.699971914291382, KL: 74.57550048828125, Loss: 0.014689902774989605, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14489/20000], Bound: 0.38492435216903687, Entropy: 141.74563598632812, Temp: 2.699972629547119, KL: 73.09800720214844, Loss: 0.017641214653849602, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14490/20000], Bound: 0.3692493438720703, Entropy: 142.61656188964844, Temp: 2.699972629547119, KL: 68.9598388671875, Loss: 0.016950316727161407, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14491/20000], Bound: 0.36361944675445557, Entropy: 142.10606384277344, Temp: 2.699972152709961, KL: 67.96893310546875, Loss: 0.015830276533961296, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14492/20000], Bound: 0.37959522008895874, Entropy: 141.9127960205078, Temp: 2.6999714374542236, KL: 71.87738037109375, Loss: 0.017040200531482697, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14493/20000], Bound: 0.3875230848789215, Entropy: 141.24400329589844, Temp: 2.6999704837799072, KL: 72.28291320800781, Loss: 0.020554043352603912, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14494/20000], Bound: 0.39542657136917114, Entropy: 139.3417205810547, Temp: 2.699968099594116, KL: 79.26206970214844, Loss: 0.011930717155337334, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14495/20000], Bound: 0.42007845640182495, Entropy: 140.5088653564453, Temp: 2.699967861175537, KL: 83.7822265625, Loss: 0.017306020483374596, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14496/20000], Bound: 0.39155876636505127, Entropy: 142.25523376464844, Temp: 2.699968099594116, KL: 76.51768493652344, Loss: 0.014901801943778992, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14497/20000], Bound: 0.4303799569606781, Entropy: 140.73751831054688, Temp: 2.6999692916870117, KL: 87.44595336914062, Loss: 0.016420170664787292, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14498/20000], Bound: 0.3684999644756317, Entropy: 141.83934020996094, Temp: 2.6999711990356445, KL: 69.12442016601562, Loss: 0.016250871121883392, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14499/20000], Bound: 0.37750402092933655, Entropy: 141.1269073486328, Temp: 2.699972629547119, KL: 71.24720764160156, Loss: 0.01709039695560932, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14500/20000], Bound: 0.3797439634799957, Entropy: 139.6494903564453, Temp: 2.6999735832214355, KL: 72.47163391113281, Loss: 0.01601935550570488, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14501/20000], Bound: 0.39371606707572937, Entropy: 141.53475952148438, Temp: 2.699974298477173, KL: 74.75834655761719, Loss: 0.01933598704636097, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14502/20000], Bound: 0.37361738085746765, Entropy: 141.89793395996094, Temp: 2.6999740600585938, KL: 71.0047607421875, Loss: 0.01547257136553526, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14503/20000], Bound: 0.3886561095714569, Entropy: 141.22254943847656, Temp: 2.6999740600585938, KL: 75.74630737304688, Loss: 0.014753873459994793, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14504/20000], Bound: 0.39407479763031006, Entropy: 140.76683044433594, Temp: 2.699974775314331, KL: 76.03082275390625, Loss: 0.017175452783703804, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14505/20000], Bound: 0.3881145119667053, Entropy: 142.14051818847656, Temp: 2.6999752521514893, KL: 73.34881591796875, Loss: 0.018900355324149132, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14506/20000], Bound: 0.39522191882133484, Entropy: 139.5789794921875, Temp: 2.699974775314331, KL: 77.35955810546875, Loss: 0.015342003665864468, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14507/20000], Bound: 0.3531760275363922, Entropy: 141.34007263183594, Temp: 2.69997501373291, KL: 66.20414733886719, Loss: 0.013678371906280518, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14508/20000], Bound: 0.36086705327033997, Entropy: 141.74363708496094, Temp: 2.6999757289886475, KL: 65.17427062988281, Loss: 0.019569428637623787, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14509/20000], Bound: 0.3798578083515167, Entropy: 139.73193359375, Temp: 2.699974536895752, KL: 71.48361206054688, Loss: 0.017909927293658257, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14510/20000], Bound: 0.38697049021720886, Entropy: 140.92929077148438, Temp: 2.6999728679656982, KL: 73.79521179199219, Loss: 0.017454639077186584, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14511/20000], Bound: 0.3938211500644684, Entropy: 138.90261840820312, Temp: 2.6999707221984863, KL: 76.98686218261719, Loss: 0.015266411006450653, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14512/20000], Bound: 0.3714495301246643, Entropy: 141.86654663085938, Temp: 2.699969530105591, KL: 70.78030395507812, Loss: 0.014740373939275742, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14513/20000], Bound: 0.38496100902557373, Entropy: 141.21229553222656, Temp: 2.6999690532684326, KL: 74.695068359375, Loss: 0.014703411608934402, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14514/20000], Bound: 0.3798271119594574, Entropy: 141.83558654785156, Temp: 2.6999692916870117, KL: 73.59881591796875, Loss: 0.013976375572383404, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14515/20000], Bound: 0.3752468526363373, Entropy: 142.52911376953125, Temp: 2.699970245361328, KL: 72.34709167480469, Loss: 0.013851827941834927, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14516/20000], Bound: 0.3829396963119507, Entropy: 141.24989318847656, Temp: 2.699972152709961, KL: 75.26005554199219, Loss: 0.01256913598626852, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14517/20000], Bound: 0.3702600300312042, Entropy: 142.8201904296875, Temp: 2.6999752521514893, KL: 68.85453796386719, Loss: 0.017678380012512207, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14518/20000], Bound: 0.3708863854408264, Entropy: 143.39425659179688, Temp: 2.699977397918701, KL: 69.38694763183594, Loss: 0.017023131251335144, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14519/20000], Bound: 0.3863545358181, Entropy: 140.3458709716797, Temp: 2.6999785900115967, KL: 74.59138488769531, Loss: 0.01564743183553219, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14520/20000], Bound: 0.36391210556030273, Entropy: 141.92893981933594, Temp: 2.6999800205230713, KL: 66.7364501953125, Loss: 0.01826574094593525, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14521/20000], Bound: 0.3717186450958252, Entropy: 141.5109100341797, Temp: 2.6999802589416504, KL: 70.35679626464844, Loss: 0.01566706784069538, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14522/20000], Bound: 0.3536249101161957, Entropy: 142.1664276123047, Temp: 2.6999804973602295, KL: 65.98814392089844, Loss: 0.014309773221611977, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14523/20000], Bound: 0.3840944170951843, Entropy: 139.6117706298828, Temp: 2.6999809741973877, KL: 73.19076538085938, Loss: 0.01702243462204933, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14524/20000], Bound: 0.37158751487731934, Entropy: 141.73928833007812, Temp: 2.6999809741973877, KL: 70.46875, Loss: 0.015390383079648018, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14525/20000], Bound: 0.410731703042984, Entropy: 139.60916137695312, Temp: 2.699981212615967, KL: 81.57749938964844, Loss: 0.016117019578814507, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14526/20000], Bound: 0.40578269958496094, Entropy: 140.07110595703125, Temp: 2.699982166290283, KL: 81.51463317871094, Loss: 0.01347205601632595, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14527/20000], Bound: 0.3843736946582794, Entropy: 142.23475646972656, Temp: 2.699984550476074, KL: 75.00352478027344, Loss: 0.013815839774906635, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14528/20000], Bound: 0.3889583349227905, Entropy: 141.94639587402344, Temp: 2.6999878883361816, KL: 74.25469970703125, Loss: 0.017680121585726738, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14529/20000], Bound: 0.41398465633392334, Entropy: 141.16493225097656, Temp: 2.6999902725219727, KL: 81.47048950195312, Loss: 0.0181416105479002, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14530/20000], Bound: 0.3605761229991913, Entropy: 141.91929626464844, Temp: 2.6999926567077637, KL: 66.72036743164062, Loss: 0.016554905101656914, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14531/20000], Bound: 0.3919541537761688, Entropy: 140.59619140625, Temp: 2.699993848800659, KL: 75.77078247070312, Loss: 0.016500499099493027, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14532/20000], Bound: 0.3442143201828003, Entropy: 142.17466735839844, Temp: 2.699995279312134, KL: 62.0224609375, Loss: 0.016833756119012833, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14533/20000], Bound: 0.3859069347381592, Entropy: 140.5522003173828, Temp: 2.699995517730713, KL: 72.33506774902344, Loss: 0.019584279507398605, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14534/20000], Bound: 0.3824286460876465, Entropy: 140.35096740722656, Temp: 2.6999943256378174, KL: 71.68426513671875, Loss: 0.01891663856804371, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14535/20000], Bound: 0.3997567892074585, Entropy: 142.9503173828125, Temp: 2.6999924182891846, KL: 77.09580993652344, Loss: 0.01832052879035473, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14536/20000], Bound: 0.37540900707244873, Entropy: 139.49737548828125, Temp: 2.6999900341033936, KL: 71.91845703125, Loss: 0.014731922186911106, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14537/20000], Bound: 0.36002323031425476, Entropy: 140.94325256347656, Temp: 2.699988603591919, KL: 68.27197265625, Loss: 0.013393823057413101, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14538/20000], Bound: 0.3856953978538513, Entropy: 141.4044952392578, Temp: 2.6999878883361816, KL: 75.18569946289062, Loss: 0.014191069640219212, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14539/20000], Bound: 0.42609134316444397, Entropy: 140.19021606445312, Temp: 2.69998836517334, KL: 85.76141357421875, Loss: 0.0170727651566267, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14540/20000], Bound: 0.4381188452243805, Entropy: 138.714599609375, Temp: 2.6999893188476562, KL: 89.88340759277344, Loss: 0.016400642693042755, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14541/20000], Bound: 0.37229061126708984, Entropy: 140.21336364746094, Temp: 2.699991226196289, KL: 69.97834777832031, Loss: 0.016670577228069305, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14542/20000], Bound: 0.39448779821395874, Entropy: 139.98509216308594, Temp: 2.6999926567077637, KL: 76.58023071289062, Loss: 0.016383854672312737, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14543/20000], Bound: 0.3878183960914612, Entropy: 141.71978759765625, Temp: 2.6999940872192383, KL: 74.42037963867188, Loss: 0.016755804419517517, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14544/20000], Bound: 0.4231094717979431, Entropy: 139.2240447998047, Temp: 2.699995517730713, KL: 85.62718200683594, Loss: 0.015615623444318771, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14545/20000], Bound: 0.39709025621414185, Entropy: 141.47149658203125, Temp: 2.699997663497925, KL: 77.08013916015625, Loss: 0.01688345894217491, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14546/20000], Bound: 0.4099982976913452, Entropy: 139.3635711669922, Temp: 2.6999998092651367, KL: 81.45838928222656, Loss: 0.015927281230688095, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14547/20000], Bound: 0.37958234548568726, Entropy: 141.720703125, Temp: 2.700002670288086, KL: 72.85679626464844, Loss: 0.015219883993268013, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14548/20000], Bound: 0.36933785676956177, Entropy: 140.04281616210938, Temp: 2.700005531311035, KL: 68.890380859375, Loss: 0.017125872895121574, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14549/20000], Bound: 0.3984290361404419, Entropy: 140.8450927734375, Temp: 2.700007438659668, KL: 79.16700744628906, Loss: 0.013754336163401604, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14550/20000], Bound: 0.3699788451194763, Entropy: 142.18736267089844, Temp: 2.7000105381011963, KL: 68.00363159179688, Loss: 0.019106028601527214, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14551/20000], Bound: 0.3788870573043823, Entropy: 141.21775817871094, Temp: 2.700011968612671, KL: 72.60499572753906, Loss: 0.015314572490751743, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14552/20000], Bound: 0.4083459675312042, Entropy: 140.16786193847656, Temp: 2.7000133991241455, KL: 80.21043395996094, Loss: 0.01731523685157299, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14553/20000], Bound: 0.3894079923629761, Entropy: 142.53538513183594, Temp: 2.700015068054199, KL: 76.11660766601562, Loss: 0.014476277865469456, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14554/20000], Bound: 0.36786243319511414, Entropy: 142.06005859375, Temp: 2.700017213821411, KL: 68.39244079589844, Loss: 0.017271248623728752, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14555/20000], Bound: 0.3841201364994049, Entropy: 142.1267547607422, Temp: 2.7000186443328857, KL: 73.4315185546875, Loss: 0.016590755432844162, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14556/20000], Bound: 0.4206010103225708, Entropy: 140.22454833984375, Temp: 2.700019598007202, KL: 84.56552124023438, Loss: 0.01615305431187153, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14557/20000], Bound: 0.40554121136665344, Entropy: 139.47314453125, Temp: 2.700021743774414, KL: 79.77952575683594, Loss: 0.01655140146613121, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14558/20000], Bound: 0.37905722856521606, Entropy: 142.2815399169922, Temp: 2.700023651123047, KL: 73.13510131835938, Loss: 0.014423909597098827, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14559/20000], Bound: 0.37626707553863525, Entropy: 139.72044372558594, Temp: 2.700026273727417, KL: 72.27908325195312, Loss: 0.014520932920277119, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14560/20000], Bound: 0.39405879378318787, Entropy: 141.95736694335938, Temp: 2.7000293731689453, KL: 75.81126403808594, Loss: 0.017573809251189232, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14561/20000], Bound: 0.3818962275981903, Entropy: 139.578125, Temp: 2.7000317573547363, KL: 74.20339965820312, Loss: 0.013966122642159462, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14562/20000], Bound: 0.395920991897583, Entropy: 141.92041015625, Temp: 2.7000348567962646, KL: 77.57177734375, Loss: 0.015332330949604511, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14563/20000], Bound: 0.3579528331756592, Entropy: 141.79837036132812, Temp: 2.700038433074951, KL: 65.90220642089844, Loss: 0.01670728623867035, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14564/20000], Bound: 0.3762912154197693, Entropy: 140.48345947265625, Temp: 2.700040817260742, KL: 72.26348876953125, Loss: 0.01456279493868351, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14565/20000], Bound: 0.3845137059688568, Entropy: 141.9123992919922, Temp: 2.7000434398651123, KL: 71.51826477050781, Loss: 0.020345954224467278, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14566/20000], Bound: 0.3719661235809326, Entropy: 140.86636352539062, Temp: 2.7000443935394287, KL: 70.4376220703125, Loss: 0.01564881019294262, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14567/20000], Bound: 0.3998158872127533, Entropy: 141.12794494628906, Temp: 2.700045108795166, KL: 76.80671691894531, Loss: 0.018888888880610466, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14568/20000], Bound: 0.3920937180519104, Entropy: 140.80140686035156, Temp: 2.700045108795166, KL: 76.17837524414062, Loss: 0.01582220382988453, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14569/20000], Bound: 0.41150206327438354, Entropy: 140.63833618164062, Temp: 2.700045585632324, KL: 82.00433349609375, Loss: 0.015758972615003586, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14570/20000], Bound: 0.3443394601345062, Entropy: 142.1382598876953, Temp: 2.700047016143799, KL: 63.37367248535156, Loss: 0.01439558994024992, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14571/20000], Bound: 0.39856499433517456, Entropy: 141.8254852294922, Temp: 2.7000482082366943, KL: 78.85520935058594, Loss: 0.01440693810582161, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14572/20000], Bound: 0.3505869209766388, Entropy: 142.60787963867188, Temp: 2.7000503540039062, KL: 64.96908569335938, Loss: 0.014634580351412296, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14573/20000], Bound: 0.40509071946144104, Entropy: 140.49020385742188, Temp: 2.700052499771118, KL: 80.21357727050781, Loss: 0.015497693791985512, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14574/20000], Bound: 0.3601700961589813, Entropy: 142.75180053710938, Temp: 2.700054883956909, KL: 66.26028442382812, Loss: 0.017196040600538254, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14575/20000], Bound: 0.3900023102760315, Entropy: 143.09536743164062, Temp: 2.700056552886963, KL: 75.32345581054688, Loss: 0.016268042847514153, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14576/20000], Bound: 0.36611130833625793, Entropy: 141.46815490722656, Temp: 2.7000579833984375, KL: 70.23191833496094, Loss: 0.012945355847477913, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14577/20000], Bound: 0.3853866755962372, Entropy: 142.31680297851562, Temp: 2.7000603675842285, KL: 73.53903198242188, Loss: 0.01707451418042183, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14578/20000], Bound: 0.39029762148857117, Entropy: 141.80307006835938, Temp: 2.7000622749328613, KL: 74.64657592773438, Loss: 0.01768193021416664, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14579/20000], Bound: 0.4012995958328247, Entropy: 140.2484588623047, Temp: 2.700063467025757, KL: 78.32687377929688, Loss: 0.01689235493540764, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14580/20000], Bound: 0.37863972783088684, Entropy: 141.615966796875, Temp: 2.7000648975372314, KL: 70.32647705078125, Loss: 0.0194022785872221, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14581/20000], Bound: 0.38950270414352417, Entropy: 141.27479553222656, Temp: 2.7000646591186523, KL: 75.12301635742188, Loss: 0.016368083655834198, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14582/20000], Bound: 0.41380172967910767, Entropy: 142.39266967773438, Temp: 2.7000646591186523, KL: 82.30267333984375, Loss: 0.01649838499724865, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14583/20000], Bound: 0.40771210193634033, Entropy: 140.71131896972656, Temp: 2.7000653743743896, KL: 78.08050537109375, Loss: 0.02090642601251602, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14584/20000], Bound: 0.39412036538124084, Entropy: 139.88955688476562, Temp: 2.7000644207000732, KL: 76.1370849609375, Loss: 0.017004383727908134, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14585/20000], Bound: 0.38150015473365784, Entropy: 138.60464477539062, Temp: 2.700063705444336, KL: 73.74136352539062, Loss: 0.014609494246542454, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14586/20000], Bound: 0.3720211386680603, Entropy: 142.00804138183594, Temp: 2.700063705444336, KL: 70.25918579101562, Loss: 0.016008498147130013, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14587/20000], Bound: 0.3782150447368622, Entropy: 140.0839385986328, Temp: 2.700063705444336, KL: 70.74128723144531, Loss: 0.01840738020837307, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14588/20000], Bound: 0.3908817768096924, Entropy: 141.23678588867188, Temp: 2.7000627517700195, KL: 74.49642944335938, Loss: 0.01827744022011757, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14589/20000], Bound: 0.382415235042572, Entropy: 140.9479217529297, Temp: 2.700061082839966, KL: 73.51707458496094, Loss: 0.01551598496735096, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14590/20000], Bound: 0.38654467463493347, Entropy: 141.55430603027344, Temp: 2.7000598907470703, KL: 75.22171020507812, Loss: 0.014583648182451725, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14591/20000], Bound: 0.3916456997394562, Entropy: 139.97879028320312, Temp: 2.700059652328491, KL: 75.16590881347656, Loss: 0.017453288659453392, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14592/20000], Bound: 0.4199143350124359, Entropy: 139.98631286621094, Temp: 2.700059175491333, KL: 84.07325744628906, Loss: 0.016674824059009552, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14593/20000], Bound: 0.38756251335144043, Entropy: 141.63905334472656, Temp: 2.700059413909912, KL: 72.94831848144531, Loss: 0.019343862310051918, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14594/20000], Bound: 0.3784874379634857, Entropy: 142.39700317382812, Temp: 2.700058698654175, KL: 70.32623291015625, Loss: 0.019321372732520103, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14595/20000], Bound: 0.39360833168029785, Entropy: 141.2638702392578, Temp: 2.700056552886963, KL: 78.04998779296875, Loss: 0.013182339258491993, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14596/20000], Bound: 0.3816681504249573, Entropy: 142.31040954589844, Temp: 2.7000558376312256, KL: 73.22032165527344, Loss: 0.01566445454955101, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14597/20000], Bound: 0.39604535698890686, Entropy: 140.993896484375, Temp: 2.7000558376312256, KL: 76.50114440917969, Loss: 0.017383301630616188, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14598/20000], Bound: 0.39954081177711487, Entropy: 141.1577911376953, Temp: 2.7000558376312256, KL: 79.5341796875, Loss: 0.013686727732419968, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14599/20000], Bound: 0.3708763122558594, Entropy: 141.63446044921875, Temp: 2.700056791305542, KL: 68.94656372070312, Loss: 0.017833974212408066, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14600/20000], Bound: 0.3943445384502411, Entropy: 142.37142944335938, Temp: 2.700057029724121, KL: 76.3988037109375, Loss: 0.016642123460769653, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14601/20000], Bound: 0.39616137742996216, Entropy: 139.91709899902344, Temp: 2.7000572681427, KL: 78.3477783203125, Loss: 0.014027226716279984, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14602/20000], Bound: 0.4082109034061432, Entropy: 141.5936279296875, Temp: 2.700058937072754, KL: 78.34507751464844, Loss: 0.020694591104984283, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14603/20000], Bound: 0.398895800113678, Entropy: 139.4967498779297, Temp: 2.700058937072754, KL: 78.36802673339844, Loss: 0.015491193160414696, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14604/20000], Bound: 0.38673585653305054, Entropy: 140.29344177246094, Temp: 2.7000598907470703, KL: 75.76805114746094, Loss: 0.013675225898623466, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14605/20000], Bound: 0.36438122391700745, Entropy: 143.8675994873047, Temp: 2.700061798095703, KL: 66.71649169921875, Loss: 0.018548710271716118, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14606/20000], Bound: 0.38941535353660583, Entropy: 139.94044494628906, Temp: 2.7000622749328613, KL: 75.15841674804688, Loss: 0.016255106776952744, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14607/20000], Bound: 0.3980262279510498, Entropy: 140.84231567382812, Temp: 2.7000627517700195, KL: 77.8385009765625, Loss: 0.015993675217032433, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14608/20000], Bound: 0.41066932678222656, Entropy: 140.82069396972656, Temp: 2.700063467025757, KL: 81.31266784667969, Loss: 0.01657336950302124, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14609/20000], Bound: 0.3803565502166748, Entropy: 141.1757049560547, Temp: 2.7000648975372314, KL: 72.43830871582031, Loss: 0.01640971191227436, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14610/20000], Bound: 0.3774140775203705, Entropy: 138.64825439453125, Temp: 2.700066089630127, KL: 72.99935913085938, Loss: 0.013798519968986511, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14611/20000], Bound: 0.389218807220459, Entropy: 140.77627563476562, Temp: 2.7000679969787598, KL: 73.60905456542969, Loss: 0.019017644226551056, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14612/20000], Bound: 0.3743681311607361, Entropy: 141.33792114257812, Temp: 2.700068712234497, KL: 68.94378662109375, Loss: 0.019688241183757782, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14613/20000], Bound: 0.3765731751918793, Entropy: 140.82969665527344, Temp: 2.7000677585601807, KL: 71.69537353515625, Loss: 0.015765218064188957, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14614/20000], Bound: 0.40494269132614136, Entropy: 140.64915466308594, Temp: 2.7000670433044434, KL: 79.18501281738281, Loss: 0.017320360988378525, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14615/20000], Bound: 0.36862632632255554, Entropy: 142.16476440429688, Temp: 2.700066566467285, KL: 70.1053466796875, Loss: 0.014501649886369705, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14616/20000], Bound: 0.3755532503128052, Entropy: 140.5715789794922, Temp: 2.700066566467285, KL: 69.89244079589844, Loss: 0.01856115832924843, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14617/20000], Bound: 0.39951640367507935, Entropy: 140.83749389648438, Temp: 2.7000653743743896, KL: 77.78837585449219, Loss: 0.016906287521123886, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14618/20000], Bound: 0.38850200176239014, Entropy: 141.38169860839844, Temp: 2.7000646591186523, KL: 73.78282165527344, Loss: 0.018307244405150414, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14619/20000], Bound: 0.3961692452430725, Entropy: 140.39015197753906, Temp: 2.7000629901885986, KL: 76.49226379394531, Loss: 0.017467670142650604, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14620/20000], Bound: 0.38040828704833984, Entropy: 140.84104919433594, Temp: 2.700061559677124, KL: 72.84828186035156, Loss: 0.01567816734313965, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14621/20000], Bound: 0.3866102695465088, Entropy: 140.8505401611328, Temp: 2.7000603675842285, KL: 72.36607360839844, Loss: 0.019907202571630478, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14622/20000], Bound: 0.3851592540740967, Entropy: 140.79861450195312, Temp: 2.7000579833984375, KL: 73.109375, Loss: 0.0177475418895483, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14623/20000], Bound: 0.3659511208534241, Entropy: 140.77537536621094, Temp: 2.7000553607940674, KL: 68.94282531738281, Loss: 0.015248448587954044, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14624/20000], Bound: 0.35921746492385864, Entropy: 141.68496704101562, Temp: 2.7000529766082764, KL: 66.85389709472656, Loss: 0.01560147013515234, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14625/20000], Bound: 0.37742936611175537, Entropy: 142.2073211669922, Temp: 2.7000508308410645, KL: 72.75601196289062, Loss: 0.014257203787565231, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14626/20000], Bound: 0.39336761832237244, Entropy: 141.80038452148438, Temp: 2.70004940032959, KL: 76.42739868164062, Loss: 0.016055643558502197, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14627/20000], Bound: 0.37897104024887085, Entropy: 143.21798706054688, Temp: 2.7000486850738525, KL: 72.78364562988281, Loss: 0.015028929337859154, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14628/20000], Bound: 0.36183467507362366, Entropy: 142.25350952148438, Temp: 2.7000482082366943, KL: 67.03388977050781, Loss: 0.016630489379167557, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14629/20000], Bound: 0.3796907067298889, Entropy: 140.03477478027344, Temp: 2.700047254562378, KL: 69.78109741210938, Loss: 0.020973868668079376, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14630/20000], Bound: 0.37742820382118225, Entropy: 141.11766052246094, Temp: 2.700044631958008, KL: 71.26460266113281, Loss: 0.01701834425330162, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14631/20000], Bound: 0.3544553518295288, Entropy: 141.10215759277344, Temp: 2.7000417709350586, KL: 65.41522216796875, Loss: 0.01579955965280533, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14632/20000], Bound: 0.3636800944805145, Entropy: 143.20591735839844, Temp: 2.7000389099121094, KL: 68.05751037597656, Loss: 0.015698468312621117, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14633/20000], Bound: 0.3852823078632355, Entropy: 141.82211303710938, Temp: 2.70003604888916, KL: 73.24559020996094, Loss: 0.017561452463269234, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14634/20000], Bound: 0.3773920238018036, Entropy: 141.34523010253906, Temp: 2.700032949447632, KL: 72.9832763671875, Loss: 0.013816281221807003, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14635/20000], Bound: 0.3843907117843628, Entropy: 140.88197326660156, Temp: 2.700031280517578, KL: 75.05522155761719, Loss: 0.013729735277593136, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14636/20000], Bound: 0.3935350477695465, Entropy: 140.7660675048828, Temp: 2.70003080368042, KL: 77.19720458984375, Loss: 0.014721271581947803, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14637/20000], Bound: 0.3851229250431061, Entropy: 140.8986358642578, Temp: 2.700031280517578, KL: 73.38261413574219, Loss: 0.017221719026565552, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14638/20000], Bound: 0.40156227350234985, Entropy: 141.81417846679688, Temp: 2.700031280517578, KL: 78.79829406738281, Loss: 0.016164135187864304, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14639/20000], Bound: 0.3783303499221802, Entropy: 140.42430114746094, Temp: 2.7000317573547363, KL: 72.29765319824219, Loss: 0.01558653824031353, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14640/20000], Bound: 0.3801577687263489, Entropy: 141.37123107910156, Temp: 2.7000322341918945, KL: 73.29792785644531, Loss: 0.01471114344894886, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14641/20000], Bound: 0.3900493085384369, Entropy: 139.58714294433594, Temp: 2.70003342628479, KL: 74.99031066894531, Loss: 0.016910258680582047, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14642/20000], Bound: 0.38568609952926636, Entropy: 141.245849609375, Temp: 2.7000346183776855, KL: 75.08277893066406, Loss: 0.01437710877507925, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14643/20000], Bound: 0.3834891617298126, Entropy: 140.19711303710938, Temp: 2.7000362873077393, KL: 71.22628784179688, Loss: 0.020335059612989426, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14644/20000], Bound: 0.38586893677711487, Entropy: 140.7299041748047, Temp: 2.7000362873077393, KL: 74.16452026367188, Loss: 0.016176283359527588, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14645/20000], Bound: 0.4037167727947235, Entropy: 142.35092163085938, Temp: 2.7000362873077393, KL: 77.5804443359375, Loss: 0.019611459225416183, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14646/20000], Bound: 0.3831619620323181, Entropy: 142.5415802001953, Temp: 2.700035572052002, KL: 74.71267700195312, Loss: 0.013702914118766785, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14647/20000], Bound: 0.3940316438674927, Entropy: 141.5450897216797, Temp: 2.70003604888916, KL: 75.63861083984375, Loss: 0.01787874475121498, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14648/20000], Bound: 0.36407235264778137, Entropy: 140.81838989257812, Temp: 2.70003604888916, KL: 68.93276977539062, Loss: 0.014282776042819023, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14649/20000], Bound: 0.3967399299144745, Entropy: 140.3386993408203, Temp: 2.7000362873077393, KL: 76.91094970703125, Loss: 0.017004968598484993, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14650/20000], Bound: 0.40583598613739014, Entropy: 141.50686645507812, Temp: 2.7000367641448975, KL: 79.58164978027344, Loss: 0.01708185113966465, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14651/20000], Bound: 0.398003488779068, Entropy: 142.01931762695312, Temp: 2.7000372409820557, KL: 78.65872192382812, Loss: 0.014462034218013287, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14652/20000], Bound: 0.3779619038105011, Entropy: 140.6106414794922, Temp: 2.7000389099121094, KL: 71.68595886230469, Loss: 0.016522681340575218, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14653/20000], Bound: 0.38274726271629333, Entropy: 141.70913696289062, Temp: 2.700039863586426, KL: 74.07780456542969, Loss: 0.01465575210750103, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14654/20000], Bound: 0.4102250337600708, Entropy: 141.70191955566406, Temp: 2.7000417709350586, KL: 81.3809814453125, Loss: 0.016197916120290756, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14655/20000], Bound: 0.36481183767318726, Entropy: 141.63233947753906, Temp: 2.7000439167022705, KL: 67.23677062988281, Loss: 0.017810583114624023, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14656/20000], Bound: 0.38787779211997986, Entropy: 140.32728576660156, Temp: 2.700045108795166, KL: 76.37312316894531, Loss: 0.013172286562621593, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14657/20000], Bound: 0.4153258502483368, Entropy: 137.92784118652344, Temp: 2.700047254562378, KL: 82.31822204589844, Loss: 0.01732795126736164, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14658/20000], Bound: 0.39464500546455383, Entropy: 140.9837646484375, Temp: 2.700049638748169, KL: 75.85328674316406, Loss: 0.017816511914134026, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14659/20000], Bound: 0.37906426191329956, Entropy: 141.27369689941406, Temp: 2.7000515460968018, KL: 72.9595947265625, Loss: 0.014752969145774841, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14660/20000], Bound: 0.3996289372444153, Entropy: 141.6975555419922, Temp: 2.7000536918640137, KL: 77.96101379394531, Loss: 0.01664845459163189, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14661/20000], Bound: 0.3623542785644531, Entropy: 140.0808868408203, Temp: 2.7000558376312256, KL: 68.46995544433594, Loss: 0.014242291450500488, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14662/20000], Bound: 0.38373425602912903, Entropy: 140.29112243652344, Temp: 2.7000584602355957, KL: 72.60981750488281, Loss: 0.017905021086335182, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14663/20000], Bound: 0.3907485604286194, Entropy: 141.78546142578125, Temp: 2.7000598907470703, KL: 75.19874572753906, Loss: 0.016904441639780998, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14664/20000], Bound: 0.37847286462783813, Entropy: 140.57421875, Temp: 2.700061321258545, KL: 71.619140625, Loss: 0.016919348388910294, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14665/20000], Bound: 0.39377889037132263, Entropy: 140.54481506347656, Temp: 2.7000622749328613, KL: 74.91511535644531, Loss: 0.01908073201775551, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14666/20000], Bound: 0.36725136637687683, Entropy: 142.09161376953125, Temp: 2.7000620365142822, KL: 66.33096313476562, Loss: 0.020767798647284508, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14667/20000], Bound: 0.3956632614135742, Entropy: 142.06008911132812, Temp: 2.700059652328491, KL: 76.29768371582031, Loss: 0.01755080372095108, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14668/20000], Bound: 0.40650102496147156, Entropy: 141.13011169433594, Temp: 2.7000575065612793, KL: 81.44160461425781, Loss: 0.014007661491632462, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14669/20000], Bound: 0.3900940716266632, Entropy: 139.82424926757812, Temp: 2.700056791305542, KL: 74.15695190429688, Loss: 0.018478000536561012, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14670/20000], Bound: 0.3972030282020569, Entropy: 141.5709228515625, Temp: 2.7000553607940674, KL: 77.87480163574219, Loss: 0.015474336221814156, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14671/20000], Bound: 0.3976615071296692, Entropy: 141.1008758544922, Temp: 2.7000551223754883, KL: 77.56683349609375, Loss: 0.01629636436700821, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14672/20000], Bound: 0.39376553893089294, Entropy: 144.53334045410156, Temp: 2.7000551223754883, KL: 77.74221801757812, Loss: 0.013838091865181923, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14673/20000], Bound: 0.36906155943870544, Entropy: 142.0550537109375, Temp: 2.7000560760498047, KL: 69.08842468261719, Loss: 0.016613919287919998, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14674/20000], Bound: 0.34618163108825684, Entropy: 141.8499755859375, Temp: 2.700056791305542, KL: 62.55155944824219, Loss: 0.01685682311654091, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14675/20000], Bound: 0.3997814655303955, Entropy: 141.6105194091797, Temp: 2.700056552886963, KL: 78.33709716796875, Loss: 0.016036050394177437, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14676/20000], Bound: 0.3568494915962219, Entropy: 139.7724609375, Temp: 2.700056552886963, KL: 66.83413696289062, Loss: 0.014409835450351238, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14677/20000], Bound: 0.36503076553344727, Entropy: 141.10958862304688, Temp: 2.700056552886963, KL: 67.70747375488281, Loss: 0.01705366000533104, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14678/20000], Bound: 0.38609567284584045, Entropy: 141.8043670654297, Temp: 2.7000560760498047, KL: 74.52992248535156, Loss: 0.015622187405824661, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14679/20000], Bound: 0.3846513032913208, Entropy: 141.0134735107422, Temp: 2.7000560760498047, KL: 73.41761779785156, Loss: 0.01690293289721012, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14680/20000], Bound: 0.390313059091568, Entropy: 140.49185180664062, Temp: 2.7000558376312256, KL: 75.58180236816406, Loss: 0.0159583929926157, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14681/20000], Bound: 0.3921312987804413, Entropy: 142.80076599121094, Temp: 2.7000560760498047, KL: 76.3541259765625, Loss: 0.015517325140535831, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14682/20000], Bound: 0.37192240357398987, Entropy: 140.2499237060547, Temp: 2.700056791305542, KL: 69.75128173828125, Loss: 0.016896778717637062, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14683/20000], Bound: 0.3853280544281006, Entropy: 141.3285369873047, Temp: 2.700057029724121, KL: 73.83021545410156, Loss: 0.016503674909472466, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14684/20000], Bound: 0.37357667088508606, Entropy: 141.13528442382812, Temp: 2.700057029724121, KL: 72.05264282226562, Loss: 0.01351121161133051, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14685/20000], Bound: 0.39409926533699036, Entropy: 141.15077209472656, Temp: 2.7000582218170166, KL: 76.82890319824219, Loss: 0.015711676329374313, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14686/20000], Bound: 0.38925421237945557, Entropy: 141.70445251464844, Temp: 2.700059413909912, KL: 73.97718811035156, Loss: 0.01835509017109871, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14687/20000], Bound: 0.3876100480556488, Entropy: 141.4794921875, Temp: 2.7000601291656494, KL: 74.20755004882812, Loss: 0.017037708312273026, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14688/20000], Bound: 0.36901599168777466, Entropy: 140.97760009765625, Temp: 2.7000606060028076, KL: 70.68782043457031, Loss: 0.013628174550831318, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14689/20000], Bound: 0.33985310792922974, Entropy: 141.6055450439453, Temp: 2.700061798095703, KL: 62.25605773925781, Loss: 0.01418902724981308, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14690/20000], Bound: 0.3954674005508423, Entropy: 139.39491271972656, Temp: 2.7000627517700195, KL: 77.62370300292969, Loss: 0.014988088048994541, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14691/20000], Bound: 0.37802404165267944, Entropy: 141.33343505859375, Temp: 2.7000644207000732, KL: 70.92355346679688, Loss: 0.017967890948057175, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14692/20000], Bound: 0.39722493290901184, Entropy: 141.416259765625, Temp: 2.7000651359558105, KL: 77.81381225585938, Loss: 0.015599388629198074, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14693/20000], Bound: 0.36906346678733826, Entropy: 141.85916137695312, Temp: 2.700066328048706, KL: 69.47084045410156, Loss: 0.01590684801340103, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14694/20000], Bound: 0.3731861114501953, Entropy: 140.34677124023438, Temp: 2.7000675201416016, KL: 69.90629577636719, Loss: 0.01727885566651821, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14695/20000], Bound: 0.38512179255485535, Entropy: 140.80967712402344, Temp: 2.7000679969787598, KL: 74.78807067871094, Loss: 0.014618798159062862, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14696/20000], Bound: 0.3835271894931793, Entropy: 142.5536651611328, Temp: 2.700068950653076, KL: 72.01016235351562, Loss: 0.018904175609350204, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14697/20000], Bound: 0.39002132415771484, Entropy: 141.59022521972656, Temp: 2.700068950653076, KL: 74.22366333007812, Loss: 0.018315065652132034, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14698/20000], Bound: 0.3789595067501068, Entropy: 140.92105102539062, Temp: 2.700068473815918, KL: 70.62464904785156, Loss: 0.019021010026335716, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14699/20000], Bound: 0.403654545545578, Entropy: 141.21995544433594, Temp: 2.700066566467285, KL: 78.0601806640625, Loss: 0.01868887059390545, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14700/20000], Bound: 0.39406919479370117, Entropy: 140.70668029785156, Temp: 2.7000644207000732, KL: 76.42929077148438, Loss: 0.016435299068689346, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14701/20000], Bound: 0.3461061418056488, Entropy: 142.76841735839844, Temp: 2.7000627517700195, KL: 64.08050537109375, Loss: 0.01398703083395958, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14702/20000], Bound: 0.3575250804424286, Entropy: 143.39356994628906, Temp: 2.700061321258545, KL: 67.47294616699219, Loss: 0.013576926663517952, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14703/20000], Bound: 0.39363035559654236, Entropy: 144.55706787109375, Temp: 2.7000606060028076, KL: 76.31280517578125, Loss: 0.016411367803812027, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14704/20000], Bound: 0.39465010166168213, Entropy: 140.21800231933594, Temp: 2.7000601291656494, KL: 76.61091613769531, Loss: 0.016416428610682487, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14705/20000], Bound: 0.38829606771469116, Entropy: 141.3801727294922, Temp: 2.7000598907470703, KL: 74.37690734863281, Loss: 0.017095502465963364, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14706/20000], Bound: 0.3677513301372528, Entropy: 140.97731018066406, Temp: 2.700059652328491, KL: 68.92218017578125, Loss: 0.016232170164585114, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14707/20000], Bound: 0.38208308815956116, Entropy: 142.41587829589844, Temp: 2.700058937072754, KL: 73.87850952148438, Loss: 0.014668291434645653, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14708/20000], Bound: 0.3807784616947174, Entropy: 139.8856201171875, Temp: 2.700059175491333, KL: 73.71160888671875, Loss: 0.014277727343142033, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14709/20000], Bound: 0.41261184215545654, Entropy: 142.0278778076172, Temp: 2.7000601291656494, KL: 81.9896240234375, Loss: 0.016409149393439293, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14710/20000], Bound: 0.37666457891464233, Entropy: 141.885986328125, Temp: 2.700061559677124, KL: 72.63148498535156, Loss: 0.014080346561968327, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14711/20000], Bound: 0.39634981751441956, Entropy: 139.5994110107422, Temp: 2.700063467025757, KL: 76.78030395507812, Loss: 0.017033230513334274, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14712/20000], Bound: 0.39305010437965393, Entropy: 141.82215881347656, Temp: 2.7000653743743896, KL: 75.37101745605469, Loss: 0.017838753759860992, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14713/20000], Bound: 0.4067898690700531, Entropy: 140.8582305908203, Temp: 2.700066566467285, KL: 80.86167907714844, Loss: 0.015242453664541245, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14714/20000], Bound: 0.39548155665397644, Entropy: 139.537353515625, Temp: 2.700068712234497, KL: 75.51446533203125, Loss: 0.01890174299478531, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14715/20000], Bound: 0.36394041776657104, Entropy: 142.2583770751953, Temp: 2.7000699043273926, KL: 67.71588134765625, Loss: 0.016467489302158356, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14716/20000], Bound: 0.3873308300971985, Entropy: 142.13319396972656, Temp: 2.700070381164551, KL: 73.44194030761719, Loss: 0.018304510042071342, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14717/20000], Bound: 0.4003430902957916, Entropy: 140.65020751953125, Temp: 2.7000701427459717, KL: 76.26351928710938, Loss: 0.02018558233976364, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14718/20000], Bound: 0.4052976071834564, Entropy: 141.77244567871094, Temp: 2.700068712234497, KL: 78.41159057617188, Loss: 0.018949711695313454, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14719/20000], Bound: 0.3651513159275055, Entropy: 142.1650390625, Temp: 2.7000670433044434, KL: 68.86788940429688, Loss: 0.014968030154705048, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14720/20000], Bound: 0.38424110412597656, Entropy: 138.4577178955078, Temp: 2.7000653743743896, KL: 73.77450561523438, Loss: 0.016021156683564186, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14721/20000], Bound: 0.387292742729187, Entropy: 140.98797607421875, Temp: 2.7000644207000732, KL: 74.63143920898438, Loss: 0.016081135720014572, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14722/20000], Bound: 0.35901010036468506, Entropy: 141.53692626953125, Temp: 2.700063467025757, KL: 66.27873229980469, Loss: 0.016558943316340446, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14723/20000], Bound: 0.3737815320491791, Entropy: 143.0599822998047, Temp: 2.7000622749328613, KL: 71.80078125, Loss: 0.01408629585057497, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14724/20000], Bound: 0.3900633156299591, Entropy: 142.138427734375, Temp: 2.700061798095703, KL: 75.60543823242188, Loss: 0.015779025852680206, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14725/20000], Bound: 0.3845440149307251, Entropy: 138.48365783691406, Temp: 2.700061798095703, KL: 74.28260803222656, Loss: 0.015243344940245152, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14726/20000], Bound: 0.3793962299823761, Entropy: 139.51808166503906, Temp: 2.7000622749328613, KL: 72.79327392578125, Loss: 0.015238518826663494, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14727/20000], Bound: 0.37108469009399414, Entropy: 141.04335021972656, Temp: 2.7000627517700195, KL: 70.52113342285156, Loss: 0.015028276480734348, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14728/20000], Bound: 0.3812869191169739, Entropy: 141.73690795898438, Temp: 2.700063705444336, KL: 73.08277893066406, Loss: 0.01571475714445114, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14729/20000], Bound: 0.3772301971912384, Entropy: 141.8088836669922, Temp: 2.7000648975372314, KL: 72.51123046875, Loss: 0.014604438096284866, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14730/20000], Bound: 0.36739757657051086, Entropy: 142.88169860839844, Temp: 2.700066328048706, KL: 70.21052551269531, Loss: 0.013660460710525513, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14731/20000], Bound: 0.37981754541397095, Entropy: 140.78497314453125, Temp: 2.700068712234497, KL: 72.82275390625, Loss: 0.015409312210977077, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14732/20000], Bound: 0.37118375301361084, Entropy: 141.9818878173828, Temp: 2.700070858001709, KL: 69.77197265625, Loss: 0.016468029469251633, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14733/20000], Bound: 0.3921842575073242, Entropy: 140.58106994628906, Temp: 2.7000725269317627, KL: 76.82144165039062, Loss: 0.014680957421660423, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14734/20000], Bound: 0.3865901827812195, Entropy: 140.24618530273438, Temp: 2.700075149536133, KL: 74.78855895996094, Loss: 0.015410508960485458, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14735/20000], Bound: 0.37189674377441406, Entropy: 140.990966796875, Temp: 2.700077772140503, KL: 71.71881103515625, Loss: 0.01323988288640976, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14736/20000], Bound: 0.36953508853912354, Entropy: 142.22152709960938, Temp: 2.7000811100006104, KL: 70.54721069335938, Loss: 0.014162298291921616, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14737/20000], Bound: 0.42098239064216614, Entropy: 141.00775146484375, Temp: 2.700084924697876, KL: 86.19149780273438, Loss: 0.013359691016376019, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14738/20000], Bound: 0.3761277198791504, Entropy: 142.0013427734375, Temp: 2.700090169906616, KL: 71.63937377929688, Loss: 0.01563195511698723, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14739/20000], Bound: 0.3767954409122467, Entropy: 138.74673461914062, Temp: 2.7000951766967773, KL: 72.1632080078125, Loss: 0.015017499215900898, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14740/20000], Bound: 0.35481250286102295, Entropy: 142.50453186035156, Temp: 2.7000999450683594, KL: 65.93568420410156, Loss: 0.015020550228655338, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14741/20000], Bound: 0.3815965950489044, Entropy: 140.3307342529297, Temp: 2.700104236602783, KL: 74.2188720703125, Loss: 0.013777418993413448, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14742/20000], Bound: 0.3603484034538269, Entropy: 141.984619140625, Temp: 2.7001090049743652, KL: 65.73493957519531, Loss: 0.01826206035912037, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14743/20000], Bound: 0.3669392764568329, Entropy: 141.62315368652344, Temp: 2.7001121044158936, KL: 69.5537109375, Loss: 0.014636354520916939, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14744/20000], Bound: 0.40222758054733276, Entropy: 141.5802459716797, Temp: 2.700115203857422, KL: 79.619873046875, Loss: 0.01501124631613493, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14745/20000], Bound: 0.38141879439353943, Entropy: 140.21807861328125, Temp: 2.7001190185546875, KL: 73.14891052246094, Loss: 0.015663495287299156, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14746/20000], Bound: 0.3827160894870758, Entropy: 139.1705780029297, Temp: 2.700122594833374, KL: 74.3302001953125, Loss: 0.01417238637804985, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14747/20000], Bound: 0.3792886435985565, Entropy: 141.46083068847656, Temp: 2.700126886367798, KL: 69.88114929199219, Loss: 0.020574146881699562, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14748/20000], Bound: 0.36822211742401123, Entropy: 141.181396484375, Temp: 2.7001287937164307, KL: 68.28079223632812, Loss: 0.017668049782514572, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14749/20000], Bound: 0.37080714106559753, Entropy: 142.2758026123047, Temp: 2.700129747390747, KL: 69.58821105957031, Loss: 0.01660984754562378, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14750/20000], Bound: 0.397102952003479, Entropy: 142.83778381347656, Temp: 2.7001302242279053, KL: 76.961181640625, Loss: 0.01711195707321167, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14751/20000], Bound: 0.39079856872558594, Entropy: 141.28793334960938, Temp: 2.7001307010650635, KL: 74.66644287109375, Loss: 0.017917977645993233, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14752/20000], Bound: 0.38313037157058716, Entropy: 140.84902954101562, Temp: 2.7001304626464844, KL: 74.144287109375, Loss: 0.014739420264959335, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14753/20000], Bound: 0.3855644166469574, Entropy: 140.86599731445312, Temp: 2.7001309394836426, KL: 73.52072143554688, Loss: 0.017204931005835533, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14754/20000], Bound: 0.3853522539138794, Entropy: 140.18385314941406, Temp: 2.7001311779022217, KL: 74.65469360351562, Loss: 0.014990645460784435, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14755/20000], Bound: 0.39268189668655396, Entropy: 141.21444702148438, Temp: 2.700131893157959, KL: 76.44676208496094, Loss: 0.01564653031527996, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14756/20000], Bound: 0.3776872754096985, Entropy: 140.61585998535156, Temp: 2.7001328468322754, KL: 72.12486267089844, Loss: 0.01556420512497425, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14757/20000], Bound: 0.39363282918930054, Entropy: 140.15049743652344, Temp: 2.70013427734375, KL: 76.386474609375, Loss: 0.01627695932984352, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14758/20000], Bound: 0.3865325450897217, Entropy: 141.1706085205078, Temp: 2.7001354694366455, KL: 73.25199890136719, Loss: 0.018225273117423058, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14759/20000], Bound: 0.37854042649269104, Entropy: 142.8878173828125, Temp: 2.7001359462738037, KL: 71.474365234375, Loss: 0.017224164679646492, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14760/20000], Bound: 0.39355921745300293, Entropy: 140.18663024902344, Temp: 2.7001359462738037, KL: 74.40380859375, Loss: 0.019908243790268898, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14761/20000], Bound: 0.3857616186141968, Entropy: 141.53501892089844, Temp: 2.700134754180908, KL: 74.41075134277344, Loss: 0.015663255006074905, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14762/20000], Bound: 0.3862970173358917, Entropy: 140.56866455078125, Temp: 2.70013427734375, KL: 72.96453857421875, Loss: 0.01863033138215542, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14763/20000], Bound: 0.39697960019111633, Entropy: 140.78114318847656, Temp: 2.7001326084136963, KL: 77.41502380371094, Loss: 0.016203906387090683, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14764/20000], Bound: 0.35247331857681274, Entropy: 142.96981811523438, Temp: 2.70013165473938, KL: 65.69830322265625, Loss: 0.014254476875066757, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14765/20000], Bound: 0.38720646500587463, Entropy: 139.53944396972656, Temp: 2.7001309394836426, KL: 74.38688659667969, Loss: 0.01648794673383236, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14766/20000], Bound: 0.41706281900405884, Entropy: 141.34288024902344, Temp: 2.7001302242279053, KL: 84.05415344238281, Loss: 0.01509514544159174, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14767/20000], Bound: 0.3695310056209564, Entropy: 141.11773681640625, Temp: 2.7001309394836426, KL: 69.39390563964844, Loss: 0.016296224668622017, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14768/20000], Bound: 0.35644564032554626, Entropy: 144.33258056640625, Temp: 2.7001311779022217, KL: 65.68659973144531, Loss: 0.016326308250427246, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14769/20000], Bound: 0.39022791385650635, Entropy: 140.54058837890625, Temp: 2.7001309394836426, KL: 75.17475891113281, Loss: 0.01666656881570816, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14770/20000], Bound: 0.4032226502895355, Entropy: 138.9202880859375, Temp: 2.7001307010650635, KL: 77.04180908203125, Loss: 0.020335998386144638, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14771/20000], Bound: 0.39120903611183167, Entropy: 140.73887634277344, Temp: 2.700129270553589, KL: 75.12358093261719, Loss: 0.01729465089738369, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14772/20000], Bound: 0.3653445243835449, Entropy: 141.63677978515625, Temp: 2.7001278400421143, KL: 68.59040832519531, Loss: 0.015583588741719723, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14773/20000], Bound: 0.3762296736240387, Entropy: 142.3545684814453, Temp: 2.7001264095306396, KL: 70.99183654785156, Loss: 0.01688561774790287, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14774/20000], Bound: 0.39119240641593933, Entropy: 139.57041931152344, Temp: 2.700124979019165, KL: 75.45587158203125, Loss: 0.016670268028974533, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14775/20000], Bound: 0.3513818681240082, Entropy: 141.14193725585938, Temp: 2.7001235485076904, KL: 65.79904174804688, Loss: 0.013506561517715454, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14776/20000], Bound: 0.3772999942302704, Entropy: 142.2938690185547, Temp: 2.700122833251953, KL: 70.5926513671875, Loss: 0.018194930627942085, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14777/20000], Bound: 0.3688860833644867, Entropy: 141.57730102539062, Temp: 2.7001214027404785, KL: 69.07270812988281, Loss: 0.016551101580262184, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14778/20000], Bound: 0.37434569001197815, Entropy: 140.2061309814453, Temp: 2.7001194953918457, KL: 71.33084106445312, Loss: 0.015256386250257492, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14779/20000], Bound: 0.36933404207229614, Entropy: 140.0850067138672, Temp: 2.70011830329895, KL: 68.34722900390625, Loss: 0.018130546435713768, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14780/20000], Bound: 0.3865213096141815, Entropy: 141.3556365966797, Temp: 2.700115919113159, KL: 74.40750122070312, Loss: 0.01607929728925228, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14781/20000], Bound: 0.395281583070755, Entropy: 141.10964965820312, Temp: 2.7001142501831055, KL: 75.47727966308594, Loss: 0.0188615620136261, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14782/20000], Bound: 0.38321802020072937, Entropy: 141.2528076171875, Temp: 2.7001116275787354, KL: 74.22640991210938, Loss: 0.014634243212640285, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14783/20000], Bound: 0.3853726387023926, Entropy: 140.74098205566406, Temp: 2.7001101970672607, KL: 73.50459289550781, Loss: 0.017131172120571136, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14784/20000], Bound: 0.3869699537754059, Entropy: 141.06126403808594, Temp: 2.700108766555786, KL: 75.59251403808594, Loss: 0.014127333648502827, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14785/20000], Bound: 0.3929455578327179, Entropy: 142.08094787597656, Temp: 2.700108289718628, KL: 75.83416748046875, Loss: 0.01692444458603859, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14786/20000], Bound: 0.38236358761787415, Entropy: 140.4445037841797, Temp: 2.7001078128814697, KL: 73.75181579589844, Loss: 0.015053970739245415, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14787/20000], Bound: 0.397399365901947, Entropy: 140.92770385742188, Temp: 2.7001078128814697, KL: 77.47834777832031, Loss: 0.016316762194037437, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14788/20000], Bound: 0.37568962574005127, Entropy: 140.9368133544922, Temp: 2.700108289718628, KL: 70.42453002929688, Loss: 0.017648648470640182, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14789/20000], Bound: 0.4127240777015686, Entropy: 138.87356567382812, Temp: 2.700108051300049, KL: 81.65309143066406, Loss: 0.017095861956477165, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14790/20000], Bound: 0.3876386880874634, Entropy: 142.2301483154297, Temp: 2.700108051300049, KL: 73.62413024902344, Loss: 0.018134020268917084, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14791/20000], Bound: 0.390357106924057, Entropy: 138.15550231933594, Temp: 2.7001073360443115, KL: 75.98406982421875, Loss: 0.01523790042847395, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14792/20000], Bound: 0.3643103539943695, Entropy: 143.8240203857422, Temp: 2.7001073360443115, KL: 68.83595275878906, Loss: 0.01458717416971922, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14793/20000], Bound: 0.3937762677669525, Entropy: 140.30825805664062, Temp: 2.7001078128814697, KL: 77.81967163085938, Loss: 0.013701064512133598, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14794/20000], Bound: 0.3869173526763916, Entropy: 143.19189453125, Temp: 2.7001092433929443, KL: 73.98988342285156, Loss: 0.0170665942132473, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14795/20000], Bound: 0.39985230565071106, Entropy: 139.44195556640625, Temp: 2.700110673904419, KL: 78.030517578125, Loss: 0.01664331927895546, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14796/20000], Bound: 0.39831671118736267, Entropy: 142.18707275390625, Temp: 2.7001121044158936, KL: 75.7777099609375, Loss: 0.019969921559095383, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14797/20000], Bound: 0.4149535000324249, Entropy: 140.77108764648438, Temp: 2.7001121044158936, KL: 83.85702514648438, Loss: 0.014269196428358555, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14798/20000], Bound: 0.4169216454029083, Entropy: 141.64146423339844, Temp: 2.700113534927368, KL: 82.85157775878906, Loss: 0.017242027446627617, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14799/20000], Bound: 0.4001242518424988, Entropy: 141.17356872558594, Temp: 2.700115203857422, KL: 77.55708312988281, Loss: 0.017669936642050743, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14800/20000], Bound: 0.4009528160095215, Entropy: 143.39454650878906, Temp: 2.7001168727874756, KL: 79.33027648925781, Loss: 0.014843389391899109, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14801/20000], Bound: 0.37818169593811035, Entropy: 140.24029541015625, Temp: 2.7001190185546875, KL: 72.15388488769531, Loss: 0.015774192288517952, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14802/20000], Bound: 0.3841419219970703, Entropy: 140.13226318359375, Temp: 2.7001214027404785, KL: 74.20941162109375, Loss: 0.015162917785346508, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14803/20000], Bound: 0.3738052546977997, Entropy: 139.90652465820312, Temp: 2.7001237869262695, KL: 71.21681213378906, Loss: 0.015180809423327446, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14804/20000], Bound: 0.3545054793357849, Entropy: 142.470458984375, Temp: 2.7001261711120605, KL: 65.517822265625, Loss: 0.015636049211025238, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14805/20000], Bound: 0.3741467297077179, Entropy: 140.68382263183594, Temp: 2.7001283168792725, KL: 71.0189208984375, Loss: 0.01572844758629799, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14806/20000], Bound: 0.3598426878452301, Entropy: 139.29672241210938, Temp: 2.700129985809326, KL: 66.18914794921875, Loss: 0.017158018425107002, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14807/20000], Bound: 0.3763940930366516, Entropy: 141.634521484375, Temp: 2.7001309394836426, KL: 72.24899291992188, Loss: 0.014645208604633808, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14808/20000], Bound: 0.3886167109012604, Entropy: 140.73974609375, Temp: 2.700132369995117, KL: 74.14540100097656, Loss: 0.01769855245947838, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14809/20000], Bound: 0.3642705976963043, Entropy: 142.94789123535156, Temp: 2.7001328468322754, KL: 67.13552856445312, Loss: 0.01771535724401474, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14810/20000], Bound: 0.40021586418151855, Entropy: 139.40521240234375, Temp: 2.7001326084136963, KL: 78.60200500488281, Loss: 0.015785664319992065, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14811/20000], Bound: 0.4272550642490387, Entropy: 141.0347900390625, Temp: 2.7001326084136963, KL: 85.88005065917969, Loss: 0.01752249337732792, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14812/20000], Bound: 0.35855230689048767, Entropy: 140.97169494628906, Temp: 2.7001333236694336, KL: 67.72511291503906, Loss: 0.013643387705087662, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14813/20000], Bound: 0.3825122117996216, Entropy: 140.75047302246094, Temp: 2.700134515762329, KL: 72.73565673828125, Loss: 0.017015712335705757, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14814/20000], Bound: 0.37476998567581177, Entropy: 140.13099670410156, Temp: 2.7001354694366455, KL: 70.79350280761719, Loss: 0.016476809978485107, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14815/20000], Bound: 0.39866629242897034, Entropy: 141.46961975097656, Temp: 2.7001359462738037, KL: 78.69096374511719, Loss: 0.014767746441066265, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14816/20000], Bound: 0.39240017533302307, Entropy: 142.0325164794922, Temp: 2.7001373767852783, KL: 77.66450500488281, Loss: 0.013238100335001945, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14817/20000], Bound: 0.41450539231300354, Entropy: 138.71731567382812, Temp: 2.7001399993896484, KL: 82.012939453125, Loss: 0.01743179000914097, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14818/20000], Bound: 0.3820631504058838, Entropy: 142.42434692382812, Temp: 2.7001426219940186, KL: 72.4146728515625, Loss: 0.017369039356708527, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14819/20000], Bound: 0.4017764925956726, Entropy: 140.53875732421875, Temp: 2.7001447677612305, KL: 77.21202087402344, Loss: 0.01922099106013775, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14820/20000], Bound: 0.3890712261199951, Entropy: 139.26512145996094, Temp: 2.700145721435547, KL: 76.39468383789062, Loss: 0.013779949396848679, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14821/20000], Bound: 0.38294774293899536, Entropy: 143.9074249267578, Temp: 2.700147867202759, KL: 73.78959655761719, Loss: 0.015298168174922466, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14822/20000], Bound: 0.40971773862838745, Entropy: 140.75559997558594, Temp: 2.7001500129699707, KL: 79.32730102539062, Loss: 0.019718211144208908, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14823/20000], Bound: 0.3784567415714264, Entropy: 140.89830017089844, Temp: 2.7001514434814453, KL: 72.02482604980469, Loss: 0.01616029255092144, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14824/20000], Bound: 0.3876483738422394, Entropy: 140.13002014160156, Temp: 2.7001523971557617, KL: 75.4619140625, Loss: 0.014736490324139595, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14825/20000], Bound: 0.36658114194869995, Entropy: 140.0684051513672, Temp: 2.7001543045043945, KL: 69.44380187988281, Loss: 0.014652206562459469, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14826/20000], Bound: 0.3974081873893738, Entropy: 141.01669311523438, Temp: 2.7001562118530273, KL: 77.97293090820312, Loss: 0.015406209044158459, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14827/20000], Bound: 0.3908465504646301, Entropy: 142.20555114746094, Temp: 2.7001585960388184, KL: 75.57460021972656, Loss: 0.01626262627542019, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14828/20000], Bound: 0.4129593074321747, Entropy: 142.85520935058594, Temp: 2.7001612186431885, KL: 81.76261901855469, Loss: 0.01702573336660862, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14829/20000], Bound: 0.41713395714759827, Entropy: 139.19810485839844, Temp: 2.7001636028289795, KL: 85.28768920898438, Loss: 0.012851562350988388, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14830/20000], Bound: 0.38125497102737427, Entropy: 141.59837341308594, Temp: 2.7001678943634033, KL: 71.8145751953125, Loss: 0.018046941608190536, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14831/20000], Bound: 0.3765285015106201, Entropy: 142.70108032226562, Temp: 2.7001712322235107, KL: 71.79496765136719, Loss: 0.015557901933789253, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14832/20000], Bound: 0.3698347806930542, Entropy: 140.63563537597656, Temp: 2.700174331665039, KL: 70.01025390625, Loss: 0.015315458178520203, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14833/20000], Bound: 0.38998857140541077, Entropy: 141.102294921875, Temp: 2.7001771926879883, KL: 75.62060546875, Loss: 0.01571144349873066, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14834/20000], Bound: 0.406522661447525, Entropy: 143.2462158203125, Temp: 2.7001802921295166, KL: 79.97236633300781, Loss: 0.01674170047044754, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14835/20000], Bound: 0.4010670781135559, Entropy: 141.30548095703125, Temp: 2.700183391571045, KL: 78.14646911621094, Loss: 0.017099203541874886, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14836/20000], Bound: 0.386096328496933, Entropy: 140.15603637695312, Temp: 2.700186252593994, KL: 73.796630859375, Loss: 0.016981614753603935, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14837/20000], Bound: 0.3859730064868927, Entropy: 141.71426391601562, Temp: 2.7001888751983643, KL: 73.17039489746094, Loss: 0.018074648454785347, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14838/20000], Bound: 0.3605722486972809, Entropy: 140.37449645996094, Temp: 2.700190305709839, KL: 66.66053771972656, Loss: 0.016665136441588402, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14839/20000], Bound: 0.38867801427841187, Entropy: 142.21246337890625, Temp: 2.700190782546997, KL: 73.44384765625, Loss: 0.019031386822462082, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14840/20000], Bound: 0.3698153793811798, Entropy: 141.9210968017578, Temp: 2.700190544128418, KL: 69.30169677734375, Loss: 0.016617434099316597, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14841/20000], Bound: 0.38329756259918213, Entropy: 141.6640625, Temp: 2.7001898288726807, KL: 73.91668701171875, Loss: 0.01525128073990345, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14842/20000], Bound: 0.3615832030773163, Entropy: 140.6234588623047, Temp: 2.7001895904541016, KL: 68.37203979492188, Loss: 0.014022557064890862, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14843/20000], Bound: 0.3941590189933777, Entropy: 140.8340606689453, Temp: 2.7001898288726807, KL: 74.29010009765625, Loss: 0.020446747541427612, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14844/20000], Bound: 0.3789718747138977, Entropy: 138.54302978515625, Temp: 2.7001888751983643, KL: 71.81106567382812, Loss: 0.016831617802381516, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14845/20000], Bound: 0.3842233419418335, Entropy: 142.4511260986328, Temp: 2.7001874446868896, KL: 73.98603820800781, Loss: 0.015621009282767773, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14846/20000], Bound: 0.3742273151874542, Entropy: 141.5881805419922, Temp: 2.7001864910125732, KL: 71.98822021484375, Loss: 0.013976853340864182, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14847/20000], Bound: 0.3941041827201843, Entropy: 139.80770874023438, Temp: 2.7001864910125732, KL: 76.24342346191406, Loss: 0.016799772158265114, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14848/20000], Bound: 0.38835597038269043, Entropy: 142.7208709716797, Temp: 2.7001864910125732, KL: 75.007080078125, Loss: 0.015962187200784683, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14849/20000], Bound: 0.37106263637542725, Entropy: 140.64010620117188, Temp: 2.7001867294311523, KL: 68.11564636230469, Loss: 0.01947200484573841, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14850/20000], Bound: 0.375662624835968, Entropy: 140.0527801513672, Temp: 2.7001852989196777, KL: 71.30557250976562, Loss: 0.016003431752324104, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14851/20000], Bound: 0.37899214029312134, Entropy: 142.09375, Temp: 2.7001843452453613, KL: 72.25759887695312, Loss: 0.016015538945794106, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14852/20000], Bound: 0.39682871103286743, Entropy: 142.01473999023438, Temp: 2.700183153152466, KL: 76.02920532226562, Loss: 0.018687769770622253, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14853/20000], Bound: 0.3920615017414093, Entropy: 138.70741271972656, Temp: 2.700181484222412, KL: 74.77740478515625, Loss: 0.018400181084871292, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14854/20000], Bound: 0.39994633197784424, Entropy: 140.990234375, Temp: 2.7001795768737793, KL: 78.90933227539062, Loss: 0.015068480744957924, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14855/20000], Bound: 0.4094296991825104, Entropy: 141.18728637695312, Temp: 2.700178623199463, KL: 80.70050048828125, Loss: 0.0170146431773901, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14856/20000], Bound: 0.3727324604988098, Entropy: 140.4613494873047, Temp: 2.7001779079437256, KL: 71.8299560546875, Loss: 0.013477339409291744, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14857/20000], Bound: 0.3773512542247772, Entropy: 141.39707946777344, Temp: 2.700178623199463, KL: 69.68389892578125, Loss: 0.01990545727312565, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14858/20000], Bound: 0.3926257789134979, Entropy: 141.20077514648438, Temp: 2.7001774311065674, KL: 75.74830627441406, Loss: 0.016909746453166008, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14859/20000], Bound: 0.40159502625465393, Entropy: 141.10012817382812, Temp: 2.700176239013672, KL: 77.56138610839844, Loss: 0.018474096432328224, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14860/20000], Bound: 0.4004566967487335, Entropy: 140.4462127685547, Temp: 2.7001748085021973, KL: 78.52153015136719, Loss: 0.01606791652739048, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14861/20000], Bound: 0.40602946281433105, Entropy: 140.12234497070312, Temp: 2.700173854827881, KL: 78.75674438476562, Loss: 0.01871827244758606, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14862/20000], Bound: 0.38197144865989685, Entropy: 140.69142150878906, Temp: 2.7001724243164062, KL: 71.49520874023438, Loss: 0.019022664055228233, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14863/20000], Bound: 0.36852937936782837, Entropy: 142.64608764648438, Temp: 2.7001702785491943, KL: 69.12080383300781, Loss: 0.016274604946374893, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14864/20000], Bound: 0.394509881734848, Entropy: 143.00341796875, Temp: 2.7001678943634033, KL: 76.39204406738281, Loss: 0.016746072098612785, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14865/20000], Bound: 0.40810057520866394, Entropy: 138.95030212402344, Temp: 2.7001659870147705, KL: 81.10494995117188, Loss: 0.01552345510572195, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14866/20000], Bound: 0.37156006693840027, Entropy: 141.04693603515625, Temp: 2.700165271759033, KL: 69.11148071289062, Loss: 0.017890749499201775, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14867/20000], Bound: 0.37687772512435913, Entropy: 142.627685546875, Temp: 2.7001633644104004, KL: 69.75202941894531, Loss: 0.019526828080415726, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14868/20000], Bound: 0.376809686422348, Entropy: 141.0105743408203, Temp: 2.700160503387451, KL: 70.36181640625, Loss: 0.018361376598477364, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14869/20000], Bound: 0.3742949962615967, Entropy: 139.8809814453125, Temp: 2.7001569271087646, KL: 69.37715148925781, Loss: 0.018847554922103882, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14870/20000], Bound: 0.39685866236686707, Entropy: 141.35238647460938, Temp: 2.7001523971557617, KL: 75.83807373046875, Loss: 0.019057853147387505, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14871/20000], Bound: 0.4035552144050598, Entropy: 140.22991943359375, Temp: 2.7001476287841797, KL: 77.33384704589844, Loss: 0.0199795700609684, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14872/20000], Bound: 0.39409470558166504, Entropy: 141.06982421875, Temp: 2.7001421451568604, KL: 77.45829772949219, Loss: 0.014544530771672726, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14873/20000], Bound: 0.3823416829109192, Entropy: 142.1743621826172, Temp: 2.7001383304595947, KL: 73.20137023925781, Loss: 0.016061753034591675, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14874/20000], Bound: 0.3933677673339844, Entropy: 141.50216674804688, Temp: 2.700134754180908, KL: 76.205078125, Loss: 0.016468195244669914, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14875/20000], Bound: 0.39726513624191284, Entropy: 139.64266967773438, Temp: 2.700131893157959, KL: 77.79563903808594, Loss: 0.01565576158463955, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14876/20000], Bound: 0.394901841878891, Entropy: 138.41262817382812, Temp: 2.700129985809326, KL: 77.74603271484375, Loss: 0.014452755451202393, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14877/20000], Bound: 0.36290785670280457, Entropy: 141.54693603515625, Temp: 2.7001290321350098, KL: 69.31201171875, Loss: 0.012972593307495117, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14878/20000], Bound: 0.4078894853591919, Entropy: 139.79066467285156, Temp: 2.700129270553589, KL: 80.38340759277344, Loss: 0.0167414378374815, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14879/20000], Bound: 0.4009442627429962, Entropy: 141.90267944335938, Temp: 2.700129985809326, KL: 78.81776428222656, Loss: 0.01578783243894577, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14880/20000], Bound: 0.3923202455043793, Entropy: 141.9509735107422, Temp: 2.7001311779022217, KL: 75.32850646972656, Loss: 0.01752016693353653, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14881/20000], Bound: 0.3799896836280823, Entropy: 140.0771942138672, Temp: 2.700131893157959, KL: 72.29118347167969, Loss: 0.01648634672164917, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14882/20000], Bound: 0.3665873110294342, Entropy: 140.29983520507812, Temp: 2.700132369995117, KL: 68.95585632324219, Loss: 0.015558801591396332, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14883/20000], Bound: 0.39385107159614563, Entropy: 141.70481872558594, Temp: 2.7001326084136963, KL: 76.67172241210938, Loss: 0.0158679336309433, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14884/20000], Bound: 0.3766689598560333, Entropy: 142.54368591308594, Temp: 2.7001335620880127, KL: 69.93995666503906, Loss: 0.019067393615841866, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14885/20000], Bound: 0.39015620946884155, Entropy: 142.4935760498047, Temp: 2.7001330852508545, KL: 77.00071716308594, Loss: 0.013246417045593262, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14886/20000], Bound: 0.3781561255455017, Entropy: 141.65162658691406, Temp: 2.700134038925171, KL: 70.28797912597656, Loss: 0.019215840846300125, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14887/20000], Bound: 0.3916040062904358, Entropy: 142.90899658203125, Temp: 2.7001335620880127, KL: 74.66767883300781, Loss: 0.018353823572397232, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14888/20000], Bound: 0.4033963680267334, Entropy: 139.07037353515625, Temp: 2.7001326084136963, KL: 80.90122985839844, Loss: 0.013285462744534016, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14889/20000], Bound: 0.38630396127700806, Entropy: 140.71202087402344, Temp: 2.7001330852508545, KL: 71.87266540527344, Loss: 0.020655956119298935, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14890/20000], Bound: 0.4098624587059021, Entropy: 141.85943603515625, Temp: 2.700131893157959, KL: 81.72032165527344, Loss: 0.015367638319730759, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14891/20000], Bound: 0.40475472807884216, Entropy: 139.3878936767578, Temp: 2.700131893157959, KL: 80.43936157226562, Loss: 0.014893886633217335, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14892/20000], Bound: 0.3872843384742737, Entropy: 141.0712432861328, Temp: 2.7001330852508545, KL: 74.92042541503906, Loss: 0.015542101114988327, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14893/20000], Bound: 0.4157083034515381, Entropy: 140.20652770996094, Temp: 2.700134515762329, KL: 84.426025390625, Loss: 0.01364147663116455, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14894/20000], Bound: 0.3816899061203003, Entropy: 140.80255126953125, Temp: 2.7001376152038574, KL: 74.184814453125, Loss: 0.013890860602259636, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14895/20000], Bound: 0.37143847346305847, Entropy: 141.02565002441406, Temp: 2.700141429901123, KL: 68.86087036132812, Loss: 0.018290361389517784, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14896/20000], Bound: 0.3723771572113037, Entropy: 140.2439422607422, Temp: 2.700143575668335, KL: 69.8994140625, Loss: 0.016863765195012093, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14897/20000], Bound: 0.42971405386924744, Entropy: 139.8672332763672, Temp: 2.7001452445983887, KL: 85.89068603515625, Loss: 0.018918048590421677, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14898/20000], Bound: 0.3571964204311371, Entropy: 140.5734100341797, Temp: 2.700146436691284, KL: 66.05850219726562, Loss: 0.01602652668952942, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14899/20000], Bound: 0.35796284675598145, Entropy: 141.4561767578125, Temp: 2.7001473903656006, KL: 65.38433837890625, Loss: 0.01767219603061676, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14900/20000], Bound: 0.37635570764541626, Entropy: 141.90333557128906, Temp: 2.7001471519470215, KL: 71.37348937988281, Loss: 0.016246164217591286, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14901/20000], Bound: 0.36641329526901245, Entropy: 141.3827667236328, Temp: 2.7001466751098633, KL: 68.71623229980469, Loss: 0.01591130532324314, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14902/20000], Bound: 0.4209188222885132, Entropy: 140.47198486328125, Temp: 2.700146198272705, KL: 84.41539001464844, Loss: 0.016613172367215157, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14903/20000], Bound: 0.3756261169910431, Entropy: 142.44602966308594, Temp: 2.700146436691284, KL: 71.40145874023438, Loss: 0.015806134790182114, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14904/20000], Bound: 0.3886108994483948, Entropy: 139.67576599121094, Temp: 2.7001466751098633, KL: 74.81768798828125, Loss: 0.01645061932504177, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14905/20000], Bound: 0.3526687026023865, Entropy: 140.5924835205078, Temp: 2.7001469135284424, KL: 63.36372375488281, Loss: 0.01867823861539364, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14906/20000], Bound: 0.39985746145248413, Entropy: 142.56837463378906, Temp: 2.7001454830169678, KL: 79.20498657226562, Loss: 0.014471709728240967, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14907/20000], Bound: 0.3998419940471649, Entropy: 140.66688537597656, Temp: 2.7001454830169678, KL: 78.76678466796875, Loss: 0.015274633653461933, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14908/20000], Bound: 0.39140498638153076, Entropy: 142.53009033203125, Temp: 2.700146198272705, KL: 74.06932067871094, Loss: 0.019353637471795082, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14909/20000], Bound: 0.40513667464256287, Entropy: 141.33616638183594, Temp: 2.700145721435547, KL: 78.66372680664062, Loss: 0.018394144251942635, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14910/20000], Bound: 0.359198659658432, Entropy: 141.8418426513672, Temp: 2.7001452445983887, KL: 65.31037902832031, Loss: 0.018450642004609108, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14911/20000], Bound: 0.3929402232170105, Entropy: 142.42002868652344, Temp: 2.7001430988311768, KL: 75.65652465820312, Loss: 0.01725083403289318, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14912/20000], Bound: 0.3692748248577118, Entropy: 141.3739013671875, Temp: 2.700141191482544, KL: 70.25239562988281, Loss: 0.014571578241884708, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14913/20000], Bound: 0.37912946939468384, Entropy: 140.7792510986328, Temp: 2.7001397609710693, KL: 74.40284729003906, Loss: 0.012116038240492344, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14914/20000], Bound: 0.4075845181941986, Entropy: 141.44923400878906, Temp: 2.7001402378082275, KL: 79.4058837890625, Loss: 0.018381668254733086, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14915/20000], Bound: 0.3839627206325531, Entropy: 142.4691925048828, Temp: 2.7001402378082275, KL: 72.98005676269531, Loss: 0.01734308712184429, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14916/20000], Bound: 0.410446435213089, Entropy: 140.55551147460938, Temp: 2.7001399993896484, KL: 81.1295166015625, Loss: 0.016788499429821968, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14917/20000], Bound: 0.4047521948814392, Entropy: 140.42767333984375, Temp: 2.7001399993896484, KL: 80.77638244628906, Loss: 0.014268490485846996, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14918/20000], Bound: 0.3957199156284332, Entropy: 140.1346435546875, Temp: 2.700141429901123, KL: 77.09921264648438, Loss: 0.016098331660032272, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14919/20000], Bound: 0.39618951082229614, Entropy: 141.91937255859375, Temp: 2.7001428604125977, KL: 77.43394470214844, Loss: 0.015735769644379616, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14920/20000], Bound: 0.3782925009727478, Entropy: 142.82603454589844, Temp: 2.7001447677612305, KL: 72.76947021484375, Loss: 0.01469366904348135, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14921/20000], Bound: 0.3561910092830658, Entropy: 141.57566833496094, Temp: 2.7001473903656006, KL: 65.91806030273438, Loss: 0.01576608419418335, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14922/20000], Bound: 0.3779199719429016, Entropy: 141.03375244140625, Temp: 2.7001490592956543, KL: 72.40409851074219, Loss: 0.015171446837484837, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14923/20000], Bound: 0.38180142641067505, Entropy: 140.73956298828125, Temp: 2.700151205062866, KL: 72.90658569335938, Loss: 0.01631774753332138, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14924/20000], Bound: 0.3779633939266205, Entropy: 143.7640838623047, Temp: 2.70015287399292, KL: 72.05915832519531, Loss: 0.01583339460194111, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14925/20000], Bound: 0.3688512146472931, Entropy: 140.78005981445312, Temp: 2.7001547813415527, KL: 70.38916015625, Loss: 0.014095285907387733, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14926/20000], Bound: 0.3534564971923828, Entropy: 143.810302734375, Temp: 2.7001569271087646, KL: 65.20051574707031, Loss: 0.01568281091749668, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14927/20000], Bound: 0.36507901549339294, Entropy: 141.23348999023438, Temp: 2.7001583576202393, KL: 69.40156555175781, Loss: 0.013942672871053219, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14928/20000], Bound: 0.3807344436645508, Entropy: 140.35414123535156, Temp: 2.700160503387451, KL: 72.43582153320312, Loss: 0.016617540270090103, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14929/20000], Bound: 0.3685435950756073, Entropy: 143.18829345703125, Temp: 2.700161933898926, KL: 69.22959899902344, Loss: 0.016080550849437714, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14930/20000], Bound: 0.38808101415634155, Entropy: 141.72579956054688, Temp: 2.7001633644104004, KL: 74.58714294433594, Loss: 0.016590667888522148, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14931/20000], Bound: 0.3578922152519226, Entropy: 141.87054443359375, Temp: 2.700164556503296, KL: 67.43080139160156, Loss: 0.01384616270661354, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14932/20000], Bound: 0.3979583978652954, Entropy: 142.539306640625, Temp: 2.7001659870147705, KL: 78.44633483886719, Loss: 0.014831881038844585, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14933/20000], Bound: 0.3576084077358246, Entropy: 140.8617706298828, Temp: 2.7001683712005615, KL: 65.58712768554688, Loss: 0.01711305044591427, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14934/20000], Bound: 0.3682343661785126, Entropy: 141.38009643554688, Temp: 2.700169324874878, KL: 69.301025390625, Loss: 0.01578565500676632, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14935/20000], Bound: 0.37090587615966797, Entropy: 142.72299194335938, Temp: 2.7001702785491943, KL: 70.70114135742188, Loss: 0.014601428061723709, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14936/20000], Bound: 0.3841771185398102, Entropy: 139.41026306152344, Temp: 2.70017147064209, KL: 74.85992431640625, Loss: 0.013977747410535812, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14937/20000], Bound: 0.38613733649253845, Entropy: 141.00901794433594, Temp: 2.700173854827881, KL: 73.44746398925781, Loss: 0.017650146037340164, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14938/20000], Bound: 0.3774107098579407, Entropy: 140.15188598632812, Temp: 2.7001752853393555, KL: 70.98721313476562, Loss: 0.017523733898997307, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14939/20000], Bound: 0.38588947057724, Entropy: 141.5807342529297, Temp: 2.7001760005950928, KL: 74.21754455566406, Loss: 0.0160903912037611, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14940/20000], Bound: 0.40720754861831665, Entropy: 142.51475524902344, Temp: 2.700176954269409, KL: 79.29238891601562, Loss: 0.01838213950395584, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14941/20000], Bound: 0.42160549759864807, Entropy: 141.662109375, Temp: 2.7001774311065674, KL: 85.35667419433594, Loss: 0.015261335298418999, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14942/20000], Bound: 0.3929201066493988, Entropy: 141.55355834960938, Temp: 2.700178861618042, KL: 77.0135498046875, Loss: 0.014727327041327953, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14943/20000], Bound: 0.3670656383037567, Entropy: 141.67739868164062, Temp: 2.700181245803833, KL: 70.03131103515625, Loss: 0.013818926177918911, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14944/20000], Bound: 0.3998754024505615, Entropy: 142.07742309570312, Temp: 2.7001843452453613, KL: 77.27388000488281, Loss: 0.018057866021990776, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14945/20000], Bound: 0.3724331259727478, Entropy: 141.6632537841797, Temp: 2.7001864910125732, KL: 71.00382995605469, Loss: 0.014848663471639156, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14946/20000], Bound: 0.3855349123477936, Entropy: 141.30908203125, Temp: 2.7001888751983643, KL: 73.35527038574219, Loss: 0.01749592274427414, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14947/20000], Bound: 0.37512874603271484, Entropy: 141.16546630859375, Temp: 2.700190782546997, KL: 69.709716796875, Loss: 0.01867477409541607, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14948/20000], Bound: 0.39086800813674927, Entropy: 140.10508728027344, Temp: 2.700191020965576, KL: 76.15254211425781, Loss: 0.015204392373561859, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14949/20000], Bound: 0.3738104999065399, Entropy: 141.02951049804688, Temp: 2.7001919746398926, KL: 68.48760986328125, Loss: 0.020237905904650688, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14950/20000], Bound: 0.36047568917274475, Entropy: 140.73159790039062, Temp: 2.700191020965576, KL: 68.37800598144531, Loss: 0.01343462709337473, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14951/20000], Bound: 0.3924085795879364, Entropy: 141.20277404785156, Temp: 2.700191020965576, KL: 73.9281005859375, Loss: 0.020161990076303482, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14952/20000], Bound: 0.390186607837677, Entropy: 140.40586853027344, Temp: 2.7001898288726807, KL: 75.44366455078125, Loss: 0.016146766021847725, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14953/20000], Bound: 0.3690509796142578, Entropy: 142.29144287109375, Temp: 2.7001888751983643, KL: 69.9659423828125, Loss: 0.014984477311372757, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14954/20000], Bound: 0.40054672956466675, Entropy: 141.77688598632812, Temp: 2.700188159942627, KL: 79.39215087890625, Loss: 0.014505510218441486, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14955/20000], Bound: 0.41188743710517883, Entropy: 141.28758239746094, Temp: 2.700188398361206, KL: 81.90475463867188, Loss: 0.016161011531949043, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14956/20000], Bound: 0.3695988357067108, Entropy: 140.93826293945312, Temp: 2.7001895904541016, KL: 69.50300598144531, Loss: 0.016130466014146805, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14957/20000], Bound: 0.3738042116165161, Entropy: 141.62376403808594, Temp: 2.700190544128418, KL: 67.77490234375, Loss: 0.02155427448451519, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14958/20000], Bound: 0.40307214856147766, Entropy: 142.23126220703125, Temp: 2.7001888751983643, KL: 78.96798706054688, Loss: 0.016686460003256798, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14959/20000], Bound: 0.38161444664001465, Entropy: 140.4437713623047, Temp: 2.700187921524048, KL: 70.91462707519531, Loss: 0.01990627869963646, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14960/20000], Bound: 0.43364113569259644, Entropy: 140.86737060546875, Temp: 2.7001852989196777, KL: 87.18856811523438, Loss: 0.018786383792757988, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14961/20000], Bound: 0.41911235451698303, Entropy: 140.24856567382812, Temp: 2.700183153152466, KL: 85.04307556152344, Loss: 0.014425218105316162, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14962/20000], Bound: 0.4009004533290863, Entropy: 140.82737731933594, Temp: 2.7001826763153076, KL: 78.79031372070312, Loss: 0.01581505313515663, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14963/20000], Bound: 0.3998241126537323, Entropy: 139.93235778808594, Temp: 2.7001829147338867, KL: 79.59487915039062, Loss: 0.013731749728322029, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14964/20000], Bound: 0.3742789328098297, Entropy: 141.3031768798828, Temp: 2.7001845836639404, KL: 71.53640747070312, Loss: 0.014840858057141304, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14965/20000], Bound: 0.38834816217422485, Entropy: 142.41256713867188, Temp: 2.700186252593994, KL: 74.92379760742188, Loss: 0.016112178564071655, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14966/20000], Bound: 0.42540788650512695, Entropy: 141.06942749023438, Temp: 2.700188159942627, KL: 86.15249633789062, Loss: 0.015959173440933228, Learning Rate: 8.210624202024264e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14967/20000], Bound: 0.39013296365737915, Entropy: 142.732177734375, Temp: 2.700190782546997, KL: 74.37779235839844, Loss: 0.01809133216738701, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14968/20000], Bound: 0.3585227131843567, Entropy: 140.29844665527344, Temp: 2.70019268989563, KL: 66.01805114746094, Loss: 0.016789544373750687, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14969/20000], Bound: 0.4052698612213135, Entropy: 142.40576171875, Temp: 2.7001938819885254, KL: 78.87043762207031, Loss: 0.018085777759552002, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14970/20000], Bound: 0.3759976625442505, Entropy: 141.6357421875, Temp: 2.7001945972442627, KL: 72.77381896972656, Loss: 0.013462971895933151, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14971/20000], Bound: 0.39482468366622925, Entropy: 140.3651123046875, Temp: 2.7001960277557373, KL: 76.21214294433594, Loss: 0.017251592129468918, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14972/20000], Bound: 0.3874223232269287, Entropy: 140.547607421875, Temp: 2.700197458267212, KL: 74.46888732910156, Loss: 0.01645345240831375, Learning Rate: 8.210624202024264e-06\n",
      "Epoch [14973/20000], Bound: 0.3828696012496948, Entropy: 141.69252014160156, Temp: 2.7001984119415283, KL: 73.29603576660156, Loss: 0.016170566901564598, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14974/20000], Bound: 0.38269200921058655, Entropy: 141.63241577148438, Temp: 2.7001991271972656, KL: 73.13101196289062, Loss: 0.016380751505494118, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14975/20000], Bound: 0.39154425263404846, Entropy: 142.31019592285156, Temp: 2.700199842453003, KL: 75.06440734863281, Loss: 0.01758723333477974, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14976/20000], Bound: 0.38116225600242615, Entropy: 141.8115997314453, Temp: 2.700200319290161, KL: 71.67257690429688, Loss: 0.018260423094034195, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14977/20000], Bound: 0.3570363223552704, Entropy: 141.62818908691406, Temp: 2.700200080871582, KL: 64.90048217773438, Loss: 0.018088309094309807, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14978/20000], Bound: 0.4025247097015381, Entropy: 141.0795440673828, Temp: 2.7001988887786865, KL: 78.88157653808594, Loss: 0.016543598845601082, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14979/20000], Bound: 0.36200886964797974, Entropy: 141.03103637695312, Temp: 2.7001984119415283, KL: 68.33918762207031, Loss: 0.01430539134889841, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14980/20000], Bound: 0.36927738785743713, Entropy: 139.5854034423828, Temp: 2.70019793510437, KL: 69.80245971679688, Loss: 0.015406591817736626, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14981/20000], Bound: 0.38332483172416687, Entropy: 140.65609741210938, Temp: 2.700197696685791, KL: 72.24252319335938, Loss: 0.018366115167737007, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14982/20000], Bound: 0.38503995537757874, Entropy: 140.2346954345703, Temp: 2.7001967430114746, KL: 73.46702575683594, Loss: 0.01702209934592247, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14983/20000], Bound: 0.38922104239463806, Entropy: 141.9103240966797, Temp: 2.700195789337158, KL: 75.55462646484375, Loss: 0.015417239628732204, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14984/20000], Bound: 0.39387279748916626, Entropy: 141.39486694335938, Temp: 2.7001953125, KL: 77.16520690917969, Loss: 0.014966575428843498, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14985/20000], Bound: 0.40554264187812805, Entropy: 139.5804901123047, Temp: 2.700195550918579, KL: 80.18052673339844, Loss: 0.015811441466212273, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14986/20000], Bound: 0.3620295524597168, Entropy: 142.47959899902344, Temp: 2.7001962661743164, KL: 67.45367431640625, Loss: 0.015955906361341476, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14987/20000], Bound: 0.3505125641822815, Entropy: 143.06736755371094, Temp: 2.7001967430114746, KL: 65.23564147949219, Loss: 0.01410391554236412, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14988/20000], Bound: 0.36994507908821106, Entropy: 142.68675231933594, Temp: 2.700197219848633, KL: 70.10444641113281, Loss: 0.01519943680614233, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14989/20000], Bound: 0.42224764823913574, Entropy: 139.40597534179688, Temp: 2.700197696685791, KL: 84.75511169433594, Loss: 0.016741294413805008, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14990/20000], Bound: 0.3793734312057495, Entropy: 142.1063690185547, Temp: 2.7001984119415283, KL: 71.94486999511719, Loss: 0.016798533499240875, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14991/20000], Bound: 0.37700337171554565, Entropy: 142.06259155273438, Temp: 2.7001991271972656, KL: 71.64306640625, Loss: 0.016092341393232346, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14992/20000], Bound: 0.3731956481933594, Entropy: 142.95347595214844, Temp: 2.700199842453003, KL: 71.24420166015625, Loss: 0.014807536266744137, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14993/20000], Bound: 0.41439539194107056, Entropy: 140.17843627929688, Temp: 2.7002007961273193, KL: 82.88461303710938, Loss: 0.01575639843940735, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14994/20000], Bound: 0.39571911096572876, Entropy: 140.0590057373047, Temp: 2.700201988220215, KL: 76.59194946289062, Loss: 0.017037760466337204, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14995/20000], Bound: 0.40229684114456177, Entropy: 141.0167694091797, Temp: 2.7002034187316895, KL: 78.40518188476562, Loss: 0.01729973591864109, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14996/20000], Bound: 0.3982223570346832, Entropy: 140.19097900390625, Temp: 2.700204372406006, KL: 76.81678771972656, Loss: 0.01799475960433483, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14997/20000], Bound: 0.38668885827064514, Entropy: 141.9647674560547, Temp: 2.700205087661743, KL: 75.04013061523438, Loss: 0.014999200589954853, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14998/20000], Bound: 0.4057679772377014, Entropy: 139.19009399414062, Temp: 2.7002062797546387, KL: 81.97782897949219, Loss: 0.012608668766915798, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [14999/20000], Bound: 0.4093029201030731, Entropy: 140.3748016357422, Temp: 2.7002086639404297, KL: 80.92057800292969, Loss: 0.016536571085453033, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15000/20000], Bound: 0.3434242904186249, Entropy: 143.1392364501953, Temp: 2.7002112865448, KL: 61.99407958984375, Loss: 0.01648593880236149, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15001/20000], Bound: 0.3617950677871704, Entropy: 142.8095703125, Temp: 2.7002127170562744, KL: 67.21493530273438, Loss: 0.016275804489850998, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15002/20000], Bound: 0.3715503513813019, Entropy: 141.58111572265625, Temp: 2.70021390914917, KL: 70.58876037597656, Loss: 0.015150489285588264, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15003/20000], Bound: 0.38555434346199036, Entropy: 140.11184692382812, Temp: 2.7002148628234863, KL: 72.58169555664062, Loss: 0.018939044326543808, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15004/20000], Bound: 0.3849655091762543, Entropy: 142.12493896484375, Temp: 2.7002153396606445, KL: 74.73222351074219, Loss: 0.014639332890510559, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15005/20000], Bound: 0.39935532212257385, Entropy: 139.301025390625, Temp: 2.700216293334961, KL: 77.57273864746094, Loss: 0.01721831224858761, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15006/20000], Bound: 0.401828795671463, Entropy: 140.3397674560547, Temp: 2.7002170085906982, KL: 78.66796875, Loss: 0.016554534435272217, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15007/20000], Bound: 0.39023345708847046, Entropy: 140.75941467285156, Temp: 2.7002179622650146, KL: 75.19534301757812, Loss: 0.016632255166769028, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15008/20000], Bound: 0.3756921589374542, Entropy: 140.38345336914062, Temp: 2.700218677520752, KL: 69.13423156738281, Loss: 0.020040107890963554, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15009/20000], Bound: 0.3925842344760895, Entropy: 139.94895935058594, Temp: 2.7002182006835938, KL: 77.51959228515625, Loss: 0.01360755693167448, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15010/20000], Bound: 0.36110275983810425, Entropy: 142.25746154785156, Temp: 2.700218677520752, KL: 67.27143859863281, Loss: 0.015810400247573853, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15011/20000], Bound: 0.3851524293422699, Entropy: 139.8865509033203, Temp: 2.700218915939331, KL: 73.88189697265625, Loss: 0.01631469838321209, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15012/20000], Bound: 0.37789374589920044, Entropy: 143.00320434570312, Temp: 2.70021915435791, KL: 72.88813781738281, Loss: 0.014261760748922825, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15013/20000], Bound: 0.37519025802612305, Entropy: 141.77882385253906, Temp: 2.7002201080322266, KL: 71.00224304199219, Loss: 0.016314296051859856, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15014/20000], Bound: 0.34093156456947327, Entropy: 141.70655822753906, Temp: 2.700220823287964, KL: 61.473907470703125, Loss: 0.01618434302508831, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15015/20000], Bound: 0.3862624168395996, Entropy: 142.44085693359375, Temp: 2.700220823287964, KL: 74.68934631347656, Loss: 0.01541852205991745, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15016/20000], Bound: 0.38770073652267456, Entropy: 140.6786651611328, Temp: 2.700220823287964, KL: 74.86683654785156, Loss: 0.0158674493432045, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15017/20000], Bound: 0.379432737827301, Entropy: 141.62564086914062, Temp: 2.700221538543701, KL: 72.06196594238281, Loss: 0.016613595187664032, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15018/20000], Bound: 0.38692259788513184, Entropy: 140.41009521484375, Temp: 2.700221538543701, KL: 74.66195678710938, Loss: 0.015825927257537842, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15019/20000], Bound: 0.40285253524780273, Entropy: 142.68760681152344, Temp: 2.7002220153808594, KL: 79.75242614746094, Loss: 0.015112676657736301, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15020/20000], Bound: 0.38543224334716797, Entropy: 140.49905395507812, Temp: 2.700223207473755, KL: 75.47822570800781, Loss: 0.013509695418179035, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15021/20000], Bound: 0.3786851465702057, Entropy: 141.34271240234375, Temp: 2.7002248764038086, KL: 71.451416015625, Loss: 0.01734466478228569, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15022/20000], Bound: 0.4243673086166382, Entropy: 140.89584350585938, Temp: 2.700226306915283, KL: 85.54293823242188, Loss: 0.016492856666445732, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15023/20000], Bound: 0.36845219135284424, Entropy: 141.79063415527344, Temp: 2.700227975845337, KL: 68.98060607910156, Loss: 0.016494033858180046, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15024/20000], Bound: 0.4125971496105194, Entropy: 141.4247589111328, Temp: 2.7002294063568115, KL: 82.52008056640625, Loss: 0.01542041264474392, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15025/20000], Bound: 0.3676259219646454, Entropy: 142.7267608642578, Temp: 2.7002310752868652, KL: 68.68032836914062, Loss: 0.016615407541394234, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15026/20000], Bound: 0.37448850274086, Entropy: 141.69187927246094, Temp: 2.70023250579834, KL: 71.22756958007812, Loss: 0.015524414367973804, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15027/20000], Bound: 0.40316715836524963, Entropy: 143.14749145507812, Temp: 2.7002339363098145, KL: 78.51567077636719, Loss: 0.017577039077878, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15028/20000], Bound: 0.3881435692310333, Entropy: 140.4909210205078, Temp: 2.700234889984131, KL: 75.47685241699219, Loss: 0.014977714978158474, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15029/20000], Bound: 0.35107818245887756, Entropy: 141.89971923828125, Temp: 2.7002365589141846, KL: 63.92718505859375, Loss: 0.016817495226860046, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15030/20000], Bound: 0.3864644467830658, Entropy: 141.2186737060547, Temp: 2.700237274169922, KL: 75.30497741699219, Loss: 0.014387852512300014, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15031/20000], Bound: 0.3623420000076294, Entropy: 141.348876953125, Temp: 2.7002384662628174, KL: 67.24537658691406, Loss: 0.01650490052998066, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15032/20000], Bound: 0.3975188732147217, Entropy: 139.65257263183594, Temp: 2.7002391815185547, KL: 76.91204833984375, Loss: 0.017432251945137978, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15033/20000], Bound: 0.4071958661079407, Entropy: 140.61412048339844, Temp: 2.700239896774292, KL: 81.97865295410156, Loss: 0.013402110897004604, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15034/20000], Bound: 0.3865237534046173, Entropy: 140.71234130859375, Temp: 2.7002415657043457, KL: 75.24259948730469, Loss: 0.014535433612763882, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15035/20000], Bound: 0.3532432019710541, Entropy: 141.61436462402344, Temp: 2.7002437114715576, KL: 65.97764587402344, Loss: 0.014134532772004604, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15036/20000], Bound: 0.3520699441432953, Entropy: 142.78065490722656, Temp: 2.7002456188201904, KL: 64.89227294921875, Loss: 0.015540356747806072, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15037/20000], Bound: 0.37743809819221497, Entropy: 141.24928283691406, Temp: 2.700247287750244, KL: 72.03379821777344, Loss: 0.015600963495671749, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15038/20000], Bound: 0.39806729555130005, Entropy: 139.9150390625, Temp: 2.7002487182617188, KL: 76.41526794433594, Loss: 0.018653443083167076, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15039/20000], Bound: 0.36892369389533997, Entropy: 143.6527099609375, Temp: 2.7002499103546143, KL: 68.81117248535156, Loss: 0.017056209966540337, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15040/20000], Bound: 0.3886955976486206, Entropy: 140.2658233642578, Temp: 2.7002503871917725, KL: 75.209228515625, Loss: 0.015772460028529167, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15041/20000], Bound: 0.3700428307056427, Entropy: 140.29615783691406, Temp: 2.7002508640289307, KL: 69.06504821777344, Loss: 0.017176060006022453, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15042/20000], Bound: 0.3698384165763855, Entropy: 142.0583038330078, Temp: 2.7002511024475098, KL: 69.29887390136719, Loss: 0.01663530431687832, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15043/20000], Bound: 0.4004514813423157, Entropy: 143.17913818359375, Temp: 2.7002511024475098, KL: 78.09165954589844, Loss: 0.016861779615283012, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15044/20000], Bound: 0.3957643210887909, Entropy: 140.87161254882812, Temp: 2.7002511024475098, KL: 78.03439331054688, Loss: 0.01439205277711153, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15045/20000], Bound: 0.3880288004875183, Entropy: 138.95584106445312, Temp: 2.700251817703247, KL: 75.23762512207031, Loss: 0.01535867154598236, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15046/20000], Bound: 0.40208932757377625, Entropy: 140.67745971679688, Temp: 2.7002530097961426, KL: 78.24581909179688, Loss: 0.017480604350566864, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15047/20000], Bound: 0.37134069204330444, Entropy: 140.64218139648438, Temp: 2.700253963470459, KL: 70.43023681640625, Loss: 0.015333537943661213, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15048/20000], Bound: 0.3831974267959595, Entropy: 141.05484008789062, Temp: 2.7002549171447754, KL: 74.15357971191406, Loss: 0.014759394340217113, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15049/20000], Bound: 0.39763250946998596, Entropy: 140.89402770996094, Temp: 2.700256109237671, KL: 77.23822021484375, Loss: 0.01689082570374012, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15050/20000], Bound: 0.3775932788848877, Entropy: 141.3009033203125, Temp: 2.7002573013305664, KL: 72.07682800292969, Loss: 0.01560413371771574, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15051/20000], Bound: 0.3938767611980438, Entropy: 139.59666442871094, Temp: 2.700258255004883, KL: 77.38827514648438, Loss: 0.014556323178112507, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15052/20000], Bound: 0.35541996359825134, Entropy: 143.11892700195312, Temp: 2.7002601623535156, KL: 66.56475830078125, Loss: 0.014170738868415356, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15053/20000], Bound: 0.382280170917511, Entropy: 140.5808868408203, Temp: 2.7002618312835693, KL: 72.16636657714844, Loss: 0.01794632524251938, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15054/20000], Bound: 0.35567599534988403, Entropy: 141.99159240722656, Temp: 2.700263023376465, KL: 66.10163879394531, Loss: 0.015160655602812767, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15055/20000], Bound: 0.37926816940307617, Entropy: 139.37680053710938, Temp: 2.7002639770507812, KL: 71.21653747558594, Loss: 0.018091470003128052, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15056/20000], Bound: 0.3751675486564636, Entropy: 140.87400817871094, Temp: 2.7002642154693604, KL: 72.08644104003906, Loss: 0.014295014552772045, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15057/20000], Bound: 0.3786715567111969, Entropy: 143.6333770751953, Temp: 2.7002646923065186, KL: 72.79853820800781, Loss: 0.014843272976577282, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15058/20000], Bound: 0.38727304339408875, Entropy: 141.15487670898438, Temp: 2.700265884399414, KL: 75.65274047851562, Loss: 0.014181267470121384, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15059/20000], Bound: 0.42271798849105835, Entropy: 141.61341857910156, Temp: 2.7002675533294678, KL: 84.80868530273438, Loss: 0.016910985112190247, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15060/20000], Bound: 0.3725205659866333, Entropy: 139.8966522216797, Temp: 2.7002692222595215, KL: 70.24734497070312, Loss: 0.01629643514752388, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15061/20000], Bound: 0.37032607197761536, Entropy: 139.7560272216797, Temp: 2.700270891189575, KL: 68.29862976074219, Loss: 0.018744830042123795, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15062/20000], Bound: 0.3705770671367645, Entropy: 140.12088012695312, Temp: 2.7002711296081543, KL: 68.97398376464844, Loss: 0.01762678287923336, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15063/20000], Bound: 0.4045822322368622, Entropy: 141.93785095214844, Temp: 2.7002711296081543, KL: 78.36073303222656, Loss: 0.01864858902990818, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15064/20000], Bound: 0.40225380659103394, Entropy: 139.44471740722656, Temp: 2.700270891189575, KL: 79.08140563964844, Loss: 0.01602446474134922, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15065/20000], Bound: 0.37861743569374084, Entropy: 141.32479858398438, Temp: 2.700270891189575, KL: 70.3905029296875, Loss: 0.019273336976766586, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15066/20000], Bound: 0.3618910014629364, Entropy: 142.39637756347656, Temp: 2.700269937515259, KL: 69.04287719726562, Loss: 0.012941526249051094, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15067/20000], Bound: 0.3722057044506073, Entropy: 141.9533233642578, Temp: 2.7002696990966797, KL: 72.58738708496094, Loss: 0.011796836741268635, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15068/20000], Bound: 0.387370228767395, Entropy: 140.420166015625, Temp: 2.700270652770996, KL: 74.14176940917969, Loss: 0.01703164353966713, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15069/20000], Bound: 0.4030831456184387, Entropy: 141.21762084960938, Temp: 2.7002711296081543, KL: 79.88151550292969, Loss: 0.015001749619841576, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15070/20000], Bound: 0.3884187936782837, Entropy: 141.58091735839844, Temp: 2.700272560119629, KL: 75.36970520019531, Loss: 0.015325581654906273, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15071/20000], Bound: 0.3951627016067505, Entropy: 140.29794311523438, Temp: 2.7002742290496826, KL: 77.41340637207031, Loss: 0.01521284505724907, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15072/20000], Bound: 0.39791589975357056, Entropy: 141.06472778320312, Temp: 2.7002761363983154, KL: 78.72120666503906, Loss: 0.014300699345767498, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15073/20000], Bound: 0.35334599018096924, Entropy: 141.7135467529297, Temp: 2.7002785205841064, KL: 65.13584899902344, Loss: 0.015746500343084335, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15074/20000], Bound: 0.3931775391101837, Entropy: 139.89303588867188, Temp: 2.7002804279327393, KL: 76.62994384765625, Loss: 0.015579096972942352, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15075/20000], Bound: 0.3655146360397339, Entropy: 141.1002197265625, Temp: 2.7002828121185303, KL: 67.1375732421875, Loss: 0.018364164978265762, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15076/20000], Bound: 0.35006824135780334, Entropy: 139.81134033203125, Temp: 2.700284004211426, KL: 65.17904663085938, Loss: 0.013981377705931664, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15077/20000], Bound: 0.35954549908638, Entropy: 142.18724060058594, Temp: 2.700284957885742, KL: 67.3050537109375, Loss: 0.014938334934413433, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15078/20000], Bound: 0.37287741899490356, Entropy: 139.2618865966797, Temp: 2.7002861499786377, KL: 71.32357788085938, Loss: 0.014492754824459553, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15079/20000], Bound: 0.40395593643188477, Entropy: 140.39097595214844, Temp: 2.7002875804901123, KL: 77.91818237304688, Loss: 0.01912088878452778, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15080/20000], Bound: 0.40547844767570496, Entropy: 141.35818481445312, Temp: 2.7002882957458496, KL: 79.8377685546875, Loss: 0.016411418095231056, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15081/20000], Bound: 0.3781982660293579, Entropy: 140.689697265625, Temp: 2.700289487838745, KL: 72.76400756835938, Loss: 0.014654787257313728, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15082/20000], Bound: 0.3875899910926819, Entropy: 140.5081787109375, Temp: 2.7002906799316406, KL: 74.78204345703125, Loss: 0.01596517488360405, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15083/20000], Bound: 0.37650611996650696, Entropy: 140.80255126953125, Temp: 2.7002921104431152, KL: 72.13978576660156, Loss: 0.014908569864928722, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15084/20000], Bound: 0.40057480335235596, Entropy: 141.44558715820312, Temp: 2.70029354095459, KL: 78.60989379882812, Loss: 0.015970589593052864, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15085/20000], Bound: 0.3697755038738251, Entropy: 140.81277465820312, Temp: 2.7002954483032227, KL: 68.899169921875, Loss: 0.017342571169137955, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15086/20000], Bound: 0.37990376353263855, Entropy: 141.43728637695312, Temp: 2.700296640396118, KL: 72.39567565917969, Loss: 0.016248270869255066, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15087/20000], Bound: 0.424312025308609, Entropy: 141.93411254882812, Temp: 2.7002975940704346, KL: 86.17192077636719, Loss: 0.015297381207346916, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15088/20000], Bound: 0.3454861342906952, Entropy: 142.3067169189453, Temp: 2.7002992630004883, KL: 63.66511535644531, Loss: 0.014441785402595997, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15089/20000], Bound: 0.3809700310230255, Entropy: 143.26528930664062, Temp: 2.700300931930542, KL: 72.03668212890625, Loss: 0.017484016716480255, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15090/20000], Bound: 0.3940054476261139, Entropy: 140.43675231933594, Temp: 2.7003018856048584, KL: 75.75399780273438, Loss: 0.01765318028628826, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15091/20000], Bound: 0.38781866431236267, Entropy: 141.47007751464844, Temp: 2.7003026008605957, KL: 75.19010925292969, Loss: 0.015333415940403938, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15092/20000], Bound: 0.3922894299030304, Entropy: 142.15621948242188, Temp: 2.700303554534912, KL: 77.02452087402344, Loss: 0.014364499598741531, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15093/20000], Bound: 0.37399351596832275, Entropy: 141.68116760253906, Temp: 2.700305223464966, KL: 70.19412231445312, Loss: 0.01717587560415268, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15094/20000], Bound: 0.39100149273872375, Entropy: 141.74087524414062, Temp: 2.7003064155578613, KL: 73.60842895507812, Loss: 0.01998888887465, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15095/20000], Bound: 0.3721354603767395, Entropy: 142.94944763183594, Temp: 2.7003064155578613, KL: 70.07095336914062, Loss: 0.01641952432692051, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15096/20000], Bound: 0.3790181279182434, Entropy: 139.2239227294922, Temp: 2.7003061771392822, KL: 73.29122924804688, Loss: 0.014116570353507996, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15097/20000], Bound: 0.39247560501098633, Entropy: 139.00059509277344, Temp: 2.7003066539764404, KL: 75.48649597167969, Loss: 0.017313871532678604, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15098/20000], Bound: 0.35530248284339905, Entropy: 141.48414611816406, Temp: 2.7003068923950195, KL: 65.4791259765625, Loss: 0.016120603308081627, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15099/20000], Bound: 0.35862356424331665, Entropy: 143.4425811767578, Temp: 2.7003066539764404, KL: 66.43357849121094, Loss: 0.016073288396000862, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15100/20000], Bound: 0.3816103935241699, Entropy: 142.62551879882812, Temp: 2.7003061771392822, KL: 73.90191650390625, Loss: 0.014373636804521084, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15101/20000], Bound: 0.36634448170661926, Entropy: 141.0848388671875, Temp: 2.7003064155578613, KL: 68.62808227539062, Loss: 0.016039704903960228, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15102/20000], Bound: 0.3743250370025635, Entropy: 139.38279724121094, Temp: 2.7003064155578613, KL: 71.63694763183594, Loss: 0.014680224470794201, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15103/20000], Bound: 0.3898395895957947, Entropy: 141.67994689941406, Temp: 2.7003068923950195, KL: 74.26992797851562, Loss: 0.018132735043764114, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15104/20000], Bound: 0.36191415786743164, Entropy: 141.5293426513672, Temp: 2.7003068923950195, KL: 68.59880065917969, Loss: 0.013776194304227829, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15105/20000], Bound: 0.37667933106422424, Entropy: 141.70877075195312, Temp: 2.7003071308135986, KL: 72.84175109863281, Loss: 0.013701120391488075, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15106/20000], Bound: 0.39311447739601135, Entropy: 139.16702270507812, Temp: 2.700308084487915, KL: 77.802490234375, Loss: 0.013373811729252338, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15107/20000], Bound: 0.3960934579372406, Entropy: 139.30203247070312, Temp: 2.700309991836548, KL: 76.47198486328125, Loss: 0.017465967684984207, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15108/20000], Bound: 0.3780346214771271, Entropy: 141.6981964111328, Temp: 2.7003114223480225, KL: 71.27757263183594, Loss: 0.017319951206445694, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15109/20000], Bound: 0.37823280692100525, Entropy: 141.59727478027344, Temp: 2.700312614440918, KL: 70.56756591796875, Loss: 0.018740402534604073, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15110/20000], Bound: 0.39013007283210754, Entropy: 143.07713317871094, Temp: 2.700312852859497, KL: 76.62599182128906, Loss: 0.013927929103374481, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15111/20000], Bound: 0.35784441232681274, Entropy: 141.1007843017578, Temp: 2.7003138065338135, KL: 64.62615966796875, Loss: 0.019015789031982422, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15112/20000], Bound: 0.3791447877883911, Entropy: 140.923095703125, Temp: 2.7003135681152344, KL: 73.40463256835938, Loss: 0.013974335044622421, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15113/20000], Bound: 0.34824347496032715, Entropy: 141.7731170654297, Temp: 2.7003138065338135, KL: 63.74578857421875, Loss: 0.015700679272413254, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15114/20000], Bound: 0.385951966047287, Entropy: 141.88742065429688, Temp: 2.7003138065338135, KL: 72.18649291992188, Loss: 0.01988617144525051, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15115/20000], Bound: 0.3961199223995209, Entropy: 142.52235412597656, Temp: 2.700313091278076, KL: 76.64682006835938, Loss: 0.017156783491373062, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15116/20000], Bound: 0.38741156458854675, Entropy: 142.04864501953125, Temp: 2.7003121376037598, KL: 75.37013244628906, Loss: 0.01477991696447134, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15117/20000], Bound: 0.3810408115386963, Entropy: 141.7804718017578, Temp: 2.7003118991851807, KL: 72.38395690917969, Loss: 0.016878992319107056, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15118/20000], Bound: 0.37689462304115295, Entropy: 142.22634887695312, Temp: 2.7003114223480225, KL: 71.48007202148438, Loss: 0.016337160021066666, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15119/20000], Bound: 0.3866986334323883, Entropy: 142.151611328125, Temp: 2.7003111839294434, KL: 76.36592102050781, Loss: 0.012550601735711098, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15120/20000], Bound: 0.39934292435646057, Entropy: 141.0128631591797, Temp: 2.7003116607666016, KL: 76.87019348144531, Loss: 0.01851324550807476, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15121/20000], Bound: 0.4076589345932007, Entropy: 140.40757751464844, Temp: 2.7003118991851807, KL: 79.04638671875, Loss: 0.019090430811047554, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15122/20000], Bound: 0.3986537754535675, Entropy: 141.20741271972656, Temp: 2.7003116607666016, KL: 76.93986511230469, Loss: 0.01800503395497799, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15123/20000], Bound: 0.38624298572540283, Entropy: 142.5283966064453, Temp: 2.7003111839294434, KL: 73.3310546875, Loss: 0.01792392134666443, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15124/20000], Bound: 0.3865480422973633, Entropy: 143.08799743652344, Temp: 2.700310468673706, KL: 73.78913879394531, Loss: 0.017240475863218307, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15125/20000], Bound: 0.39543089270591736, Entropy: 142.29183959960938, Temp: 2.7003095149993896, KL: 74.84446716308594, Loss: 0.020116738975048065, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15126/20000], Bound: 0.3766716718673706, Entropy: 140.82366943359375, Temp: 2.700307607650757, KL: 72.485107421875, Loss: 0.014357379637658596, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15127/20000], Bound: 0.3719601333141327, Entropy: 141.65415954589844, Temp: 2.7003066539764404, KL: 70.96516418457031, Loss: 0.014671003445982933, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15128/20000], Bound: 0.3685421943664551, Entropy: 142.07876586914062, Temp: 2.700305938720703, KL: 68.7056884765625, Loss: 0.017051100730895996, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15129/20000], Bound: 0.3622632622718811, Entropy: 140.63356018066406, Temp: 2.7003047466278076, KL: 67.86541748046875, Loss: 0.01531625259667635, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15130/20000], Bound: 0.3920285999774933, Entropy: 140.53407287597656, Temp: 2.7003040313720703, KL: 74.52326965332031, Loss: 0.018853850662708282, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15131/20000], Bound: 0.4016854763031006, Entropy: 141.979736328125, Temp: 2.7003026008605957, KL: 77.01556396484375, Loss: 0.01953589729964733, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15132/20000], Bound: 0.38808152079582214, Entropy: 141.83184814453125, Temp: 2.700300693511963, KL: 75.23629760742188, Loss: 0.015390177257359028, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15133/20000], Bound: 0.3795647919178009, Entropy: 140.53298950195312, Temp: 2.7002992630004883, KL: 72.80024719238281, Loss: 0.015317837707698345, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15134/20000], Bound: 0.3706238567829132, Entropy: 142.3866424560547, Temp: 2.700298309326172, KL: 69.51914978027344, Loss: 0.01664223149418831, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15135/20000], Bound: 0.3903529942035675, Entropy: 140.66778564453125, Temp: 2.7002973556518555, KL: 75.37916564941406, Loss: 0.01635756529867649, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15136/20000], Bound: 0.3852274715900421, Entropy: 141.17213439941406, Temp: 2.700296401977539, KL: 73.145751953125, Loss: 0.017718935385346413, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15137/20000], Bound: 0.3633042275905609, Entropy: 141.78585815429688, Temp: 2.7002952098846436, KL: 68.82072448730469, Loss: 0.014090844430029392, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15138/20000], Bound: 0.38198158144950867, Entropy: 142.93392944335938, Temp: 2.7002944946289062, KL: 72.70655822753906, Loss: 0.016786040738224983, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15139/20000], Bound: 0.43019652366638184, Entropy: 139.1672821044922, Temp: 2.70029354095459, KL: 86.53266906738281, Loss: 0.018009230494499207, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15140/20000], Bound: 0.366512268781662, Entropy: 142.8878631591797, Temp: 2.7002933025360107, KL: 69.380859375, Loss: 0.01473375502973795, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15141/20000], Bound: 0.3914698660373688, Entropy: 140.42787170410156, Temp: 2.7002930641174316, KL: 77.13241577148438, Loss: 0.013718347065150738, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15142/20000], Bound: 0.35158050060272217, Entropy: 143.3747100830078, Temp: 2.70029354095459, KL: 65.64012145996094, Loss: 0.013904244638979435, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15143/20000], Bound: 0.3756493926048279, Entropy: 141.9676971435547, Temp: 2.7002944946289062, KL: 71.34524536132812, Loss: 0.015923893079161644, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15144/20000], Bound: 0.3900551199913025, Entropy: 141.2443389892578, Temp: 2.7002952098846436, KL: 76.03167724609375, Loss: 0.014987518079578876, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15145/20000], Bound: 0.40120214223861694, Entropy: 140.25625610351562, Temp: 2.700296401977539, KL: 78.32693481445312, Loss: 0.016840722411870956, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15146/20000], Bound: 0.40195974707603455, Entropy: 138.7172088623047, Temp: 2.7002975940704346, KL: 79.32327270507812, Loss: 0.015414362773299217, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15147/20000], Bound: 0.4114808738231659, Entropy: 141.77867126464844, Temp: 2.700299024581909, KL: 81.79058837890625, Loss: 0.0161456149071455, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15148/20000], Bound: 0.4068196415901184, Entropy: 143.50247192382812, Temp: 2.700300931930542, KL: 78.69961547851562, Loss: 0.019264886155724525, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15149/20000], Bound: 0.38091179728507996, Entropy: 140.45164489746094, Temp: 2.7003023624420166, KL: 72.32356262207031, Loss: 0.01692158728837967, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15150/20000], Bound: 0.38085299730300903, Entropy: 142.382568359375, Temp: 2.700303316116333, KL: 72.00393676757812, Loss: 0.01748194359242916, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15151/20000], Bound: 0.39293062686920166, Entropy: 140.18438720703125, Temp: 2.700303554534912, KL: 75.25006103515625, Loss: 0.017999667674303055, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15152/20000], Bound: 0.3755542039871216, Entropy: 141.9606475830078, Temp: 2.700303554534912, KL: 71.93429565429688, Loss: 0.014782615937292576, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15153/20000], Bound: 0.36663708090782166, Entropy: 139.21185302734375, Temp: 2.7003040313720703, KL: 68.69691467285156, Loss: 0.01606578566133976, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15154/20000], Bound: 0.39861488342285156, Entropy: 140.87924194335938, Temp: 2.7003042697906494, KL: 78.35675048828125, Loss: 0.015360011719167233, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15155/20000], Bound: 0.3950803875923157, Entropy: 141.08233642578125, Temp: 2.7003049850463867, KL: 76.48094177246094, Loss: 0.016894711181521416, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15156/20000], Bound: 0.4048853814601898, Entropy: 141.29183959960938, Temp: 2.700305700302124, KL: 79.36140441894531, Loss: 0.016964266076683998, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15157/20000], Bound: 0.3920947313308716, Entropy: 140.74850463867188, Temp: 2.7003064155578613, KL: 76.92256164550781, Loss: 0.014447290450334549, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15158/20000], Bound: 0.40220966935157776, Entropy: 142.24281311035156, Temp: 2.700307607650757, KL: 78.39933776855469, Loss: 0.01726338639855385, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15159/20000], Bound: 0.4061993956565857, Entropy: 138.99842834472656, Temp: 2.7003090381622314, KL: 78.14779663085938, Loss: 0.019941601902246475, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15160/20000], Bound: 0.3539677560329437, Entropy: 140.64707946777344, Temp: 2.7003095149993896, KL: 66.2593994140625, Loss: 0.013986830599606037, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15161/20000], Bound: 0.41583460569381714, Entropy: 140.5108642578125, Temp: 2.700310230255127, KL: 83.84292602539062, Loss: 0.014794488437473774, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15162/20000], Bound: 0.3688409626483917, Entropy: 141.20411682128906, Temp: 2.7003116607666016, KL: 68.25093078613281, Loss: 0.018050456419587135, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15163/20000], Bound: 0.34767845273017883, Entropy: 140.6076202392578, Temp: 2.700312376022339, KL: 62.017974853515625, Loss: 0.018610941246151924, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15164/20000], Bound: 0.37648797035217285, Entropy: 141.82183837890625, Temp: 2.7003116607666016, KL: 70.929443359375, Loss: 0.017140153795480728, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15165/20000], Bound: 0.3897114098072052, Entropy: 142.2772216796875, Temp: 2.7003109455108643, KL: 75.63198852539062, Loss: 0.015541168861091137, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15166/20000], Bound: 0.38030001521110535, Entropy: 140.91790771484375, Temp: 2.700310468673706, KL: 72.6519775390625, Loss: 0.015985900536179543, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15167/20000], Bound: 0.3866337239742279, Entropy: 141.66029357910156, Temp: 2.700309991836548, KL: 73.77247619628906, Loss: 0.017317641526460648, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15168/20000], Bound: 0.38011831045150757, Entropy: 141.89796447753906, Temp: 2.7003092765808105, KL: 72.69900512695312, Loss: 0.015801560133695602, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15169/20000], Bound: 0.37796956300735474, Entropy: 140.74085998535156, Temp: 2.7003087997436523, KL: 71.10749816894531, Loss: 0.01760016195476055, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15170/20000], Bound: 0.3534276783466339, Entropy: 141.54428100585938, Temp: 2.700307846069336, KL: 65.90119934082031, Loss: 0.014371654950082302, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15171/20000], Bound: 0.38985565304756165, Entropy: 141.59161376953125, Temp: 2.7003071308135986, KL: 76.69821166992188, Loss: 0.013645173981785774, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15172/20000], Bound: 0.4012570381164551, Entropy: 138.9757843017578, Temp: 2.700307607650757, KL: 76.61207580566406, Loss: 0.020046420395374298, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15173/20000], Bound: 0.38037413358688354, Entropy: 141.7052764892578, Temp: 2.7003071308135986, KL: 72.474853515625, Loss: 0.016353564336895943, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15174/20000], Bound: 0.3682223856449127, Entropy: 138.78009033203125, Temp: 2.7003068923950195, KL: 68.57453918457031, Loss: 0.01712561957538128, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15175/20000], Bound: 0.396791011095047, Entropy: 143.78704833984375, Temp: 2.700305938720703, KL: 78.334228515625, Loss: 0.014400073327124119, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15176/20000], Bound: 0.3796410858631134, Entropy: 143.6591033935547, Temp: 2.700305938720703, KL: 71.24945068359375, Loss: 0.018230240792036057, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15177/20000], Bound: 0.3719824552536011, Entropy: 142.5211944580078, Temp: 2.700305223464966, KL: 71.09223937988281, Loss: 0.01444752886891365, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15178/20000], Bound: 0.3924623727798462, Entropy: 141.1754913330078, Temp: 2.7003049850463867, KL: 76.2750244140625, Loss: 0.015846535563468933, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15179/20000], Bound: 0.41213712096214294, Entropy: 140.8878631591797, Temp: 2.700305223464966, KL: 79.85546875, Loss: 0.0200969111174345, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15180/20000], Bound: 0.4190051257610321, Entropy: 139.4484405517578, Temp: 2.7003047466278076, KL: 82.49546813964844, Loss: 0.01908310502767563, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15181/20000], Bound: 0.380111426115036, Entropy: 141.1375732421875, Temp: 2.7003040313720703, KL: 73.18203735351562, Loss: 0.014903421513736248, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15182/20000], Bound: 0.38163474202156067, Entropy: 139.9757843017578, Temp: 2.7003040313720703, KL: 74.23440551757812, Loss: 0.01377104502171278, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15183/20000], Bound: 0.4370231032371521, Entropy: 138.8195037841797, Temp: 2.7003042697906494, KL: 90.08489990234375, Loss: 0.015391833148896694, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15184/20000], Bound: 0.3736605644226074, Entropy: 143.45852661132812, Temp: 2.700305938720703, KL: 71.87136840820312, Loss: 0.013893667608499527, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15185/20000], Bound: 0.39428815245628357, Entropy: 139.9329071044922, Temp: 2.700307846069336, KL: 76.15951538085938, Loss: 0.01705676130950451, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15186/20000], Bound: 0.365201860666275, Entropy: 141.15194702148438, Temp: 2.7003097534179688, KL: 68.17070007324219, Loss: 0.016287455335259438, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15187/20000], Bound: 0.3924488425254822, Entropy: 141.32000732421875, Temp: 2.7003111839294434, KL: 75.01971435546875, Loss: 0.018163619562983513, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15188/20000], Bound: 0.3873288929462433, Entropy: 141.7534942626953, Temp: 2.7003116607666016, KL: 73.33157348632812, Loss: 0.018509823828935623, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15189/20000], Bound: 0.385870099067688, Entropy: 143.67787170410156, Temp: 2.7003118991851807, KL: 73.23915100097656, Loss: 0.017892803996801376, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15190/20000], Bound: 0.36621418595314026, Entropy: 141.0972442626953, Temp: 2.7003116607666016, KL: 68.91755676269531, Loss: 0.015435365960001945, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15191/20000], Bound: 0.4034493565559387, Entropy: 139.01678466796875, Temp: 2.7003114223480225, KL: 78.23274230957031, Loss: 0.018257932737469673, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15192/20000], Bound: 0.3703249990940094, Entropy: 139.95187377929688, Temp: 2.7003111839294434, KL: 69.66119384765625, Loss: 0.01622156612575054, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15193/20000], Bound: 0.3876851797103882, Entropy: 141.2032928466797, Temp: 2.700310468673706, KL: 75.04301452636719, Loss: 0.015533585101366043, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15194/20000], Bound: 0.39886921644210815, Entropy: 140.9684600830078, Temp: 2.700310230255127, KL: 78.47721862792969, Loss: 0.015276910737156868, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15195/20000], Bound: 0.39996621012687683, Entropy: 141.267333984375, Temp: 2.700310468673706, KL: 77.69354248046875, Loss: 0.01733197458088398, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15196/20000], Bound: 0.3824231028556824, Entropy: 141.53623962402344, Temp: 2.7003109455108643, KL: 72.88897705078125, Loss: 0.0166854839771986, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15197/20000], Bound: 0.35983872413635254, Entropy: 142.4267120361328, Temp: 2.7003111839294434, KL: 68.68777465820312, Loss: 0.012530701234936714, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15198/20000], Bound: 0.38133618235588074, Entropy: 141.5267333984375, Temp: 2.7003118991851807, KL: 71.82003784179688, Loss: 0.018081510439515114, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15199/20000], Bound: 0.3931375741958618, Entropy: 141.10691833496094, Temp: 2.7003121376037598, KL: 75.15606689453125, Loss: 0.018286632373929024, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15200/20000], Bound: 0.3868628144264221, Entropy: 139.84051513671875, Temp: 2.7003118991851807, KL: 74.19775390625, Loss: 0.016654008999466896, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15201/20000], Bound: 0.38528627157211304, Entropy: 141.3062744140625, Temp: 2.7003116607666016, KL: 75.32395935058594, Loss: 0.01371751632541418, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15202/20000], Bound: 0.3900226056575775, Entropy: 140.26742553710938, Temp: 2.700312376022339, KL: 74.91055297851562, Loss: 0.017045944929122925, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15203/20000], Bound: 0.3653803765773773, Entropy: 143.177001953125, Temp: 2.700312852859497, KL: 68.17173767089844, Loss: 0.01637907512485981, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15204/20000], Bound: 0.4136556088924408, Entropy: 140.4872589111328, Temp: 2.700313091278076, KL: 79.23521423339844, Loss: 0.022098613902926445, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15205/20000], Bound: 0.3734188973903656, Entropy: 140.01426696777344, Temp: 2.7003118991851807, KL: 70.72872924804688, Loss: 0.015881316736340523, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15206/20000], Bound: 0.37968313694000244, Entropy: 142.9177703857422, Temp: 2.7003111839294434, KL: 73.4559326171875, Loss: 0.014167148619890213, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15207/20000], Bound: 0.39732927083969116, Entropy: 143.1432342529297, Temp: 2.7003109455108643, KL: 77.8243408203125, Loss: 0.015639619901776314, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15208/20000], Bound: 0.36149001121520996, Entropy: 139.82199096679688, Temp: 2.7003111839294434, KL: 67.39926147460938, Loss: 0.015776215121150017, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15209/20000], Bound: 0.3895648121833801, Entropy: 141.75450134277344, Temp: 2.7003111839294434, KL: 76.22200012207031, Loss: 0.014369117096066475, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15210/20000], Bound: 0.41011106967926025, Entropy: 138.68919372558594, Temp: 2.7003116607666016, KL: 81.83993530273438, Loss: 0.015287184156477451, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15211/20000], Bound: 0.3886722922325134, Entropy: 140.61386108398438, Temp: 2.700313091278076, KL: 73.87875366210938, Loss: 0.01822398230433464, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15212/20000], Bound: 0.4004400372505188, Entropy: 140.10023498535156, Temp: 2.7003138065338135, KL: 77.10978698730469, Loss: 0.018674110993742943, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15213/20000], Bound: 0.36533549427986145, Entropy: 141.21932983398438, Temp: 2.7003138065338135, KL: 67.78619384765625, Loss: 0.017069460824131966, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15214/20000], Bound: 0.3877933919429779, Entropy: 140.6359405517578, Temp: 2.7003138065338135, KL: 73.890869140625, Loss: 0.017725560814142227, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15215/20000], Bound: 0.3758160471916199, Entropy: 141.76991271972656, Temp: 2.700313091278076, KL: 72.29238891601562, Loss: 0.01425890438258648, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15216/20000], Bound: 0.38549861311912537, Entropy: 140.94558715820312, Temp: 2.700313091278076, KL: 74.32182312011719, Loss: 0.01568770222365856, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15217/20000], Bound: 0.3798004388809204, Entropy: 141.74176025390625, Temp: 2.700313091278076, KL: 72.69926452636719, Loss: 0.01563100703060627, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15218/20000], Bound: 0.3904997706413269, Entropy: 141.7509002685547, Temp: 2.7003135681152344, KL: 76.80319213867188, Loss: 0.01380068063735962, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15219/20000], Bound: 0.40575337409973145, Entropy: 140.99911499023438, Temp: 2.700314521789551, KL: 80.39669799804688, Loss: 0.015529479831457138, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15220/20000], Bound: 0.3952339291572571, Entropy: 140.20677185058594, Temp: 2.7003161907196045, KL: 76.35673522949219, Loss: 0.017208809033036232, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15221/20000], Bound: 0.37439867854118347, Entropy: 141.79718017578125, Temp: 2.7003173828125, KL: 71.01954650878906, Loss: 0.01586262695491314, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15222/20000], Bound: 0.38821643590927124, Entropy: 140.64442443847656, Temp: 2.7003188133239746, KL: 73.27056884765625, Loss: 0.019103199243545532, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15223/20000], Bound: 0.3843512535095215, Entropy: 140.95909118652344, Temp: 2.700319290161133, KL: 74.5948486328125, Loss: 0.014563774690032005, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15224/20000], Bound: 0.3667827248573303, Entropy: 140.18760681152344, Temp: 2.700320243835449, KL: 68.63130187988281, Loss: 0.01626388542354107, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15225/20000], Bound: 0.4066016674041748, Entropy: 141.9069061279297, Temp: 2.7003207206726074, KL: 79.55934143066406, Loss: 0.017551852390170097, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15226/20000], Bound: 0.38865578174591064, Entropy: 140.605224609375, Temp: 2.7003211975097656, KL: 74.52006530761719, Loss: 0.017027627676725388, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15227/20000], Bound: 0.35411712527275085, Entropy: 141.96058654785156, Temp: 2.700321674346924, KL: 65.38520812988281, Loss: 0.015682673081755638, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15228/20000], Bound: 0.40429359674453735, Entropy: 141.66265869140625, Temp: 2.700321674346924, KL: 77.96759033203125, Loss: 0.019216882064938545, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15229/20000], Bound: 0.3830403983592987, Entropy: 143.48089599609375, Temp: 2.7003211975097656, KL: 73.08763122558594, Loss: 0.01664935052394867, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15230/20000], Bound: 0.3655652701854706, Entropy: 144.3507843017578, Temp: 2.7003207206726074, KL: 69.37335205078125, Loss: 0.014251111075282097, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15231/20000], Bound: 0.3822298049926758, Entropy: 141.92466735839844, Temp: 2.7003207206726074, KL: 72.80838012695312, Loss: 0.01673097163438797, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15232/20000], Bound: 0.3927629292011261, Entropy: 141.6585235595703, Temp: 2.7003207206726074, KL: 75.72354125976562, Loss: 0.01703166402876377, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15233/20000], Bound: 0.3950479030609131, Entropy: 140.32769775390625, Temp: 2.7003204822540283, KL: 78.11674499511719, Loss: 0.013848219066858292, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15234/20000], Bound: 0.3804613947868347, Entropy: 140.2734375, Temp: 2.7003209590911865, KL: 73.33729553222656, Loss: 0.014803457073867321, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15235/20000], Bound: 0.40606558322906494, Entropy: 141.1316375732422, Temp: 2.700322151184082, KL: 80.02566528320312, Loss: 0.016390157863497734, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15236/20000], Bound: 0.39531296491622925, Entropy: 140.56507873535156, Temp: 2.7003233432769775, KL: 77.79795837402344, Loss: 0.014583509415388107, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15237/20000], Bound: 0.372394323348999, Entropy: 138.79812622070312, Temp: 2.7003252506256104, KL: 71.59774780273438, Loss: 0.013729619793593884, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15238/20000], Bound: 0.3972603678703308, Entropy: 141.1410369873047, Temp: 2.7003273963928223, KL: 76.9359130859375, Loss: 0.017246954143047333, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15239/20000], Bound: 0.3427181839942932, Entropy: 143.7689971923828, Temp: 2.700329542160034, KL: 61.48588562011719, Loss: 0.017068926244974136, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15240/20000], Bound: 0.3632872700691223, Entropy: 140.9730224609375, Temp: 2.7003304958343506, KL: 67.25343322753906, Loss: 0.01698429137468338, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15241/20000], Bound: 0.37275609374046326, Entropy: 141.83447265625, Temp: 2.7003307342529297, KL: 70.66024780273438, Loss: 0.015657106414437294, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15242/20000], Bound: 0.3891478180885315, Entropy: 142.6213836669922, Temp: 2.700331211090088, KL: 74.11637878417969, Loss: 0.018041955307126045, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15243/20000], Bound: 0.37344732880592346, Entropy: 144.09739685058594, Temp: 2.700331211090088, KL: 68.44758605957031, Loss: 0.020120400935411453, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15244/20000], Bound: 0.3586563467979431, Entropy: 141.6949005126953, Temp: 2.7003300189971924, KL: 68.25662231445312, Loss: 0.012714901007711887, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15245/20000], Bound: 0.3902674913406372, Entropy: 140.18603515625, Temp: 2.700329542160034, KL: 75.43292236328125, Loss: 0.01621187850832939, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15246/20000], Bound: 0.36640745401382446, Entropy: 142.54306030273438, Temp: 2.700329303741455, KL: 70.02294921875, Loss: 0.013490116223692894, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15247/20000], Bound: 0.3890950381755829, Entropy: 140.2399139404297, Temp: 2.700329542160034, KL: 74.83270263671875, Loss: 0.016686927527189255, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15248/20000], Bound: 0.3628874123096466, Entropy: 142.87539672851562, Temp: 2.7003297805786133, KL: 67.63702392578125, Loss: 0.016065170988440514, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15249/20000], Bound: 0.37251970171928406, Entropy: 140.183837890625, Temp: 2.7003297805786133, KL: 71.39070129394531, Loss: 0.014179418794810772, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15250/20000], Bound: 0.386907160282135, Entropy: 143.1808624267578, Temp: 2.7003302574157715, KL: 74.46287536621094, Loss: 0.016187207773327827, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15251/20000], Bound: 0.38533034920692444, Entropy: 142.34954833984375, Temp: 2.7003304958343506, KL: 75.06761169433594, Loss: 0.014216152019798756, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15252/20000], Bound: 0.39540979266166687, Entropy: 142.70668029785156, Temp: 2.700331687927246, KL: 75.73899841308594, Loss: 0.018449008464813232, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15253/20000], Bound: 0.40942472219467163, Entropy: 141.26588439941406, Temp: 2.7003324031829834, KL: 80.60453796386719, Loss: 0.017191119492053986, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15254/20000], Bound: 0.3994417190551758, Entropy: 141.599609375, Temp: 2.7003331184387207, KL: 79.11579895019531, Loss: 0.014409832656383514, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15255/20000], Bound: 0.3815913498401642, Entropy: 142.19947814941406, Temp: 2.700334310531616, KL: 73.60137939453125, Loss: 0.014920149929821491, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15256/20000], Bound: 0.37881943583488464, Entropy: 140.1966552734375, Temp: 2.70033597946167, KL: 71.36201477050781, Loss: 0.01758282072842121, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15257/20000], Bound: 0.3952194154262543, Entropy: 140.267578125, Temp: 2.7003369331359863, KL: 76.96046447753906, Loss: 0.016083164140582085, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15258/20000], Bound: 0.3590947985649109, Entropy: 141.6332244873047, Temp: 2.700338125228882, KL: 66.80450439453125, Loss: 0.01563139818608761, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15259/20000], Bound: 0.3538420796394348, Entropy: 144.056640625, Temp: 2.7003390789031982, KL: 65.24552917480469, Loss: 0.015799567103385925, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15260/20000], Bound: 0.37403756380081177, Entropy: 141.5455780029297, Temp: 2.7003397941589355, KL: 70.44612121582031, Loss: 0.016732923686504364, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15261/20000], Bound: 0.3891296982765198, Entropy: 142.92030334472656, Temp: 2.7003400325775146, KL: 75.09918212890625, Loss: 0.01621236838400364, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15262/20000], Bound: 0.4006810188293457, Entropy: 140.43954467773438, Temp: 2.7003402709960938, KL: 78.2403564453125, Loss: 0.016713855788111687, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15263/20000], Bound: 0.38221052289009094, Entropy: 140.16336059570312, Temp: 2.700340747833252, KL: 72.64773559570312, Loss: 0.017018266022205353, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15264/20000], Bound: 0.3860098719596863, Entropy: 141.56198120117188, Temp: 2.700340986251831, KL: 73.439453125, Loss: 0.017597641795873642, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15265/20000], Bound: 0.37355977296829224, Entropy: 142.3798370361328, Temp: 2.700340747833252, KL: 70.69554138183594, Loss: 0.016017716377973557, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15266/20000], Bound: 0.36736974120140076, Entropy: 141.73358154296875, Temp: 2.700340509414673, KL: 67.857421875, Loss: 0.01800532266497612, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15267/20000], Bound: 0.36752262711524963, Entropy: 140.51873779296875, Temp: 2.7003397941589355, KL: 69.14836120605469, Loss: 0.015695350244641304, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15268/20000], Bound: 0.38274407386779785, Entropy: 143.4445343017578, Temp: 2.700338840484619, KL: 72.4808349609375, Loss: 0.01761382631957531, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15269/20000], Bound: 0.397112101316452, Entropy: 142.0465850830078, Temp: 2.7003376483917236, KL: 76.66708374023438, Loss: 0.01766345277428627, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15270/20000], Bound: 0.3670986592769623, Entropy: 143.63534545898438, Temp: 2.7003366947174072, KL: 69.30641174316406, Loss: 0.015179877169430256, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15271/20000], Bound: 0.3935922086238861, Entropy: 139.86538696289062, Temp: 2.700335741043091, KL: 76.82234191894531, Loss: 0.01544963289052248, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15272/20000], Bound: 0.3810589909553528, Entropy: 142.5870819091797, Temp: 2.7003352642059326, KL: 73.95877075195312, Loss: 0.01397298090159893, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15273/20000], Bound: 0.3868057429790497, Entropy: 140.07118225097656, Temp: 2.7003355026245117, KL: 74.60746765136719, Loss: 0.015864742919802666, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15274/20000], Bound: 0.3785650134086609, Entropy: 141.33450317382812, Temp: 2.70033597946167, KL: 71.25807189941406, Loss: 0.017639419063925743, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15275/20000], Bound: 0.3812088370323181, Entropy: 141.9538116455078, Temp: 2.70033597946167, KL: 73.34574890136719, Loss: 0.015188374556601048, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15276/20000], Bound: 0.40138423442840576, Entropy: 138.56277465820312, Temp: 2.700336217880249, KL: 77.48995971679688, Loss: 0.018491392955183983, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15277/20000], Bound: 0.3572886884212494, Entropy: 140.57199096679688, Temp: 2.70033597946167, KL: 66.48677062988281, Loss: 0.015282747335731983, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15278/20000], Bound: 0.3971729576587677, Entropy: 140.7320556640625, Temp: 2.700335741043091, KL: 77.4005126953125, Loss: 0.01633881777524948, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15279/20000], Bound: 0.40072229504585266, Entropy: 142.83970642089844, Temp: 2.700335741043091, KL: 75.78388977050781, Loss: 0.021285049617290497, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15280/20000], Bound: 0.4021628201007843, Entropy: 142.45945739746094, Temp: 2.7003345489501953, KL: 77.72438049316406, Loss: 0.0184874776750803, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15281/20000], Bound: 0.3925469219684601, Entropy: 140.7487335205078, Temp: 2.7003333568573, KL: 75.84024047851562, Loss: 0.01669798046350479, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15282/20000], Bound: 0.393793523311615, Entropy: 142.96983337402344, Temp: 2.7003321647644043, KL: 75.59156799316406, Loss: 0.017838431522250175, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15283/20000], Bound: 0.4092950224876404, Entropy: 140.80484008789062, Temp: 2.7003307342529297, KL: 81.07672119140625, Loss: 0.016244279220700264, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15284/20000], Bound: 0.37508445978164673, Entropy: 140.7664031982422, Temp: 2.7003302574157715, KL: 71.58323669433594, Loss: 0.01518321130424738, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15285/20000], Bound: 0.37663501501083374, Entropy: 142.3074188232422, Temp: 2.7003297805786133, KL: 71.5421142578125, Loss: 0.016084160655736923, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15286/20000], Bound: 0.39117157459259033, Entropy: 142.04052734375, Temp: 2.700329303741455, KL: 75.48466491699219, Loss: 0.016607454046607018, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15287/20000], Bound: 0.3844975233078003, Entropy: 142.80531311035156, Temp: 2.700328826904297, KL: 74.52403259277344, Loss: 0.01477375440299511, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15288/20000], Bound: 0.36117446422576904, Entropy: 142.38314819335938, Temp: 2.700329065322876, KL: 67.51930236816406, Loss: 0.01538966502994299, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15289/20000], Bound: 0.39682522416114807, Entropy: 141.660888671875, Temp: 2.700329065322876, KL: 77.2550048828125, Loss: 0.016417449340224266, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15290/20000], Bound: 0.40087825059890747, Entropy: 140.81085205078125, Temp: 2.700329303741455, KL: 79.68516540527344, Loss: 0.014147350564599037, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15291/20000], Bound: 0.4085313379764557, Entropy: 139.9381561279297, Temp: 2.7003304958343506, KL: 79.91163635253906, Loss: 0.017975151538848877, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15292/20000], Bound: 0.34994423389434814, Entropy: 143.20838928222656, Temp: 2.700331211090088, KL: 62.8829345703125, Loss: 0.01816970854997635, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15293/20000], Bound: 0.3837069571018219, Entropy: 139.73446655273438, Temp: 2.700330972671509, KL: 74.06959533691406, Loss: 0.015189632773399353, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15294/20000], Bound: 0.3694080412387848, Entropy: 143.940185546875, Temp: 2.700331211090088, KL: 70.361328125, Loss: 0.014441713690757751, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15295/20000], Bound: 0.4191502630710602, Entropy: 141.27786254882812, Temp: 2.700331687927246, KL: 83.62590026855469, Loss: 0.017072545364499092, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15296/20000], Bound: 0.39040452241897583, Entropy: 139.3988037109375, Temp: 2.7003326416015625, KL: 75.9576416015625, Loss: 0.015314776450395584, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15297/20000], Bound: 0.400703489780426, Entropy: 139.92007446289062, Temp: 2.700333595275879, KL: 78.34773254394531, Loss: 0.016527390107512474, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15298/20000], Bound: 0.36369097232818604, Entropy: 142.56332397460938, Temp: 2.7003347873687744, KL: 68.50022888183594, Loss: 0.014886738732457161, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15299/20000], Bound: 0.37687593698501587, Entropy: 141.7408905029297, Temp: 2.70033597946167, KL: 71.8507080078125, Loss: 0.015641162171959877, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15300/20000], Bound: 0.3848881423473358, Entropy: 138.57676696777344, Temp: 2.7003369331359863, KL: 74.16360473632812, Loss: 0.01565169170498848, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15301/20000], Bound: 0.37998461723327637, Entropy: 142.11456298828125, Temp: 2.700338363647461, KL: 73.024658203125, Loss: 0.015127277001738548, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15302/20000], Bound: 0.3563419282436371, Entropy: 144.1225128173828, Temp: 2.7003400325775146, KL: 66.60530090332031, Loss: 0.014573069289326668, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15303/20000], Bound: 0.3657986521720886, Entropy: 143.472900390625, Temp: 2.7003414630889893, KL: 68.93440246582031, Loss: 0.015186438336968422, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15304/20000], Bound: 0.4130280911922455, Entropy: 141.364990234375, Temp: 2.700342893600464, KL: 80.34938049316406, Loss: 0.019683070480823517, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15305/20000], Bound: 0.4159940481185913, Entropy: 141.96923828125, Temp: 2.700343608856201, KL: 83.44685363769531, Loss: 0.015618275851011276, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15306/20000], Bound: 0.40506500005722046, Entropy: 141.32797241210938, Temp: 2.7003448009490967, KL: 79.94358825683594, Loss: 0.015986377373337746, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15307/20000], Bound: 0.3902042806148529, Entropy: 141.9800567626953, Temp: 2.7003464698791504, KL: 73.45803833007812, Loss: 0.019834408536553383, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15308/20000], Bound: 0.36932486295700073, Entropy: 140.4732208251953, Temp: 2.7003471851348877, KL: 69.21232604980469, Loss: 0.016525549814105034, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15309/20000], Bound: 0.438869446516037, Entropy: 140.9309539794922, Temp: 2.700347661972046, KL: 88.07814025878906, Loss: 0.020186474546790123, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15310/20000], Bound: 0.39230260252952576, Entropy: 141.9568634033203, Temp: 2.700347661972046, KL: 76.74786376953125, Loss: 0.01488441787660122, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15311/20000], Bound: 0.38564783334732056, Entropy: 142.52281188964844, Temp: 2.700348138809204, KL: 73.40487670898438, Loss: 0.01746629737317562, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15312/20000], Bound: 0.382307767868042, Entropy: 142.78318786621094, Temp: 2.700348377227783, KL: 73.29466247558594, Loss: 0.015872681513428688, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15313/20000], Bound: 0.398454874753952, Entropy: 141.92864990234375, Temp: 2.7003486156463623, KL: 77.80224609375, Loss: 0.016299238428473473, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15314/20000], Bound: 0.40747350454330444, Entropy: 139.31942749023438, Temp: 2.7003490924835205, KL: 81.80531311035156, Loss: 0.013878954574465752, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15315/20000], Bound: 0.3702940344810486, Entropy: 142.4579315185547, Temp: 2.700350522994995, KL: 68.43855285644531, Loss: 0.018469421193003654, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15316/20000], Bound: 0.39024513959884644, Entropy: 139.3193359375, Temp: 2.7003512382507324, KL: 76.00077819824219, Loss: 0.015148493461310863, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15317/20000], Bound: 0.3962218165397644, Entropy: 141.24362182617188, Temp: 2.700352191925049, KL: 77.42660522460938, Loss: 0.015769112855196, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15318/20000], Bound: 0.37814000248908997, Entropy: 141.60289001464844, Temp: 2.7003536224365234, KL: 71.13145446777344, Loss: 0.01764710433781147, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15319/20000], Bound: 0.3418613374233246, Entropy: 141.83155822753906, Temp: 2.7003540992736816, KL: 61.88751220703125, Loss: 0.015890609472990036, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15320/20000], Bound: 0.4040597379207611, Entropy: 141.29306030273438, Temp: 2.7003540992736816, KL: 81.3123779296875, Loss: 0.012894269078969955, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15321/20000], Bound: 0.3585120737552643, Entropy: 138.5162811279297, Temp: 2.7003555297851562, KL: 67.09364318847656, Loss: 0.014793618582189083, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15322/20000], Bound: 0.3827037513256073, Entropy: 141.7120819091797, Temp: 2.7003567218780518, KL: 73.49826049804688, Loss: 0.015708444640040398, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15323/20000], Bound: 0.3858838379383087, Entropy: 140.61032104492188, Temp: 2.7003581523895264, KL: 73.55648803710938, Loss: 0.017313027754426003, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15324/20000], Bound: 0.36959922313690186, Entropy: 142.08297729492188, Temp: 2.700359344482422, KL: 69.48916625976562, Loss: 0.016157669946551323, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15325/20000], Bound: 0.4039592742919922, Entropy: 140.36929321289062, Temp: 2.70035982131958, KL: 79.01216125488281, Loss: 0.017097745090723038, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15326/20000], Bound: 0.38248422741889954, Entropy: 141.430419921875, Temp: 2.7003605365753174, KL: 73.44488525390625, Loss: 0.015689363703131676, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15327/20000], Bound: 0.3886779248714447, Entropy: 142.0548553466797, Temp: 2.700361490249634, KL: 74.62654113769531, Loss: 0.016842801123857498, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15328/20000], Bound: 0.39746031165122986, Entropy: 140.07205200195312, Temp: 2.700362205505371, KL: 77.625, Loss: 0.01608113758265972, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15329/20000], Bound: 0.39132633805274963, Entropy: 139.5912628173828, Temp: 2.7003631591796875, KL: 75.0399169921875, Loss: 0.01751547120511532, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15330/20000], Bound: 0.3843642771244049, Entropy: 141.21240234375, Temp: 2.7003636360168457, KL: 74.11727905273438, Loss: 0.015455455519258976, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15331/20000], Bound: 0.4042123556137085, Entropy: 141.46214294433594, Temp: 2.700364589691162, KL: 78.96968078613281, Loss: 0.01731673814356327, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15332/20000], Bound: 0.37662169337272644, Entropy: 139.99337768554688, Temp: 2.7003655433654785, KL: 72.59696960449219, Loss: 0.014124231413006783, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15333/20000], Bound: 0.38892778754234314, Entropy: 140.71119689941406, Temp: 2.700366973876953, KL: 75.90406799316406, Loss: 0.014612856321036816, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15334/20000], Bound: 0.3562129735946655, Entropy: 142.19081115722656, Temp: 2.700368642807007, KL: 66.1368408203125, Loss: 0.015373975969851017, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15335/20000], Bound: 0.39296913146972656, Entropy: 140.89224243164062, Temp: 2.7003700733184814, KL: 76.1666259765625, Loss: 0.016324156895279884, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15336/20000], Bound: 0.36164161562919617, Entropy: 143.9959716796875, Temp: 2.700371503829956, KL: 67.6434326171875, Loss: 0.015403577126562595, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15337/20000], Bound: 0.39911675453186035, Entropy: 139.10877990722656, Temp: 2.7003729343414307, KL: 76.67079162597656, Loss: 0.018758531659841537, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15338/20000], Bound: 0.3613165616989136, Entropy: 142.05426025390625, Temp: 2.700373411178589, KL: 67.95582580566406, Loss: 0.014655757695436478, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15339/20000], Bound: 0.366997092962265, Entropy: 142.04095458984375, Temp: 2.700374126434326, KL: 69.14329528808594, Loss: 0.015428855083882809, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15340/20000], Bound: 0.3880910277366638, Entropy: 140.53762817382812, Temp: 2.7003748416900635, KL: 74.42385864257812, Loss: 0.016900351271033287, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15341/20000], Bound: 0.3995726406574249, Entropy: 142.2748565673828, Temp: 2.7003753185272217, KL: 78.18974304199219, Loss: 0.016197066754102707, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15342/20000], Bound: 0.39052194356918335, Entropy: 140.50096130371094, Temp: 2.700376272201538, KL: 76.86595153808594, Loss: 0.013697134330868721, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15343/20000], Bound: 0.37359049916267395, Entropy: 142.3651580810547, Temp: 2.7003777027130127, KL: 71.92225646972656, Loss: 0.013762913644313812, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15344/20000], Bound: 0.38755548000335693, Entropy: 140.45419311523438, Temp: 2.7003798484802246, KL: 76.54811096191406, Loss: 0.012677245773375034, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15345/20000], Bound: 0.36090579628944397, Entropy: 142.2518310546875, Temp: 2.700382709503174, KL: 67.6190185546875, Loss: 0.015065483748912811, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15346/20000], Bound: 0.4078430235385895, Entropy: 140.29779052734375, Temp: 2.700385332107544, KL: 81.47398376464844, Loss: 0.014698836021125317, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15347/20000], Bound: 0.35684117674827576, Entropy: 141.365966796875, Temp: 2.7003886699676514, KL: 67.05438232421875, Loss: 0.014000377617776394, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15348/20000], Bound: 0.3718167543411255, Entropy: 142.2032928466797, Temp: 2.700392007827759, KL: 68.57121276855469, Loss: 0.019028544425964355, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15349/20000], Bound: 0.3439866602420807, Entropy: 143.4915313720703, Temp: 2.7003939151763916, KL: 63.381103515625, Loss: 0.014204878360033035, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15350/20000], Bound: 0.3764405846595764, Entropy: 141.88690185546875, Temp: 2.7003958225250244, KL: 71.97689819335938, Loss: 0.015176153741776943, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15351/20000], Bound: 0.3828819990158081, Entropy: 141.14801025390625, Temp: 2.700397491455078, KL: 73.43389892578125, Loss: 0.01592372916638851, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15352/20000], Bound: 0.390347957611084, Entropy: 140.44906616210938, Temp: 2.700399398803711, KL: 74.91952514648438, Loss: 0.017206795513629913, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15353/20000], Bound: 0.36998894810676575, Entropy: 141.08993530273438, Temp: 2.7004008293151855, KL: 68.05160522460938, Loss: 0.019025249406695366, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15354/20000], Bound: 0.4248400926589966, Entropy: 140.66796875, Temp: 2.7004010677337646, KL: 85.07687377929688, Loss: 0.01762821339070797, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15355/20000], Bound: 0.3927643895149231, Entropy: 140.768310546875, Temp: 2.700401782989502, KL: 77.43609619140625, Loss: 0.0138623071834445, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15356/20000], Bound: 0.4022999405860901, Entropy: 140.6982421875, Temp: 2.7004032135009766, KL: 79.85777282714844, Loss: 0.014613809064030647, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15357/20000], Bound: 0.4004029631614685, Entropy: 141.7266387939453, Temp: 2.7004051208496094, KL: 77.48831176757812, Loss: 0.017953641712665558, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15358/20000], Bound: 0.39291712641716003, Entropy: 142.10797119140625, Temp: 2.700406551361084, KL: 76.83901977539062, Loss: 0.01505113486200571, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15359/20000], Bound: 0.38249513506889343, Entropy: 142.5824432373047, Temp: 2.700408697128296, KL: 73.26657104492188, Loss: 0.01602582260966301, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15360/20000], Bound: 0.37114086747169495, Entropy: 140.60064697265625, Temp: 2.7004103660583496, KL: 69.66508483886719, Loss: 0.016645995900034904, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15361/20000], Bound: 0.40285468101501465, Entropy: 139.30722045898438, Temp: 2.700411796569824, KL: 79.31968688964844, Loss: 0.01591707207262516, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15362/20000], Bound: 0.3955775201320648, Entropy: 140.1221466064453, Temp: 2.700413703918457, KL: 78.44210815429688, Loss: 0.013536565005779266, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15363/20000], Bound: 0.4179554879665375, Entropy: 140.253662109375, Temp: 2.700416088104248, KL: 84.09524536132812, Loss: 0.015527364797890186, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15364/20000], Bound: 0.39112037420272827, Entropy: 142.15234375, Temp: 2.7004191875457764, KL: 75.31929016113281, Loss: 0.016886649653315544, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15365/20000], Bound: 0.3539292514324188, Entropy: 141.40060424804688, Temp: 2.7004218101501465, KL: 65.75, Loss: 0.01491110771894455, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15366/20000], Bound: 0.3975549638271332, Entropy: 139.93148803710938, Temp: 2.7004241943359375, KL: 76.69673156738281, Loss: 0.017852460965514183, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15367/20000], Bound: 0.37461018562316895, Entropy: 142.97227478027344, Temp: 2.7004261016845703, KL: 70.72767639160156, Loss: 0.01651623100042343, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15368/20000], Bound: 0.4059191048145294, Entropy: 139.50418090820312, Temp: 2.700427770614624, KL: 80.11456298828125, Loss: 0.01614520139992237, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15369/20000], Bound: 0.4185311198234558, Entropy: 140.78662109375, Temp: 2.7004294395446777, KL: 82.33329772949219, Loss: 0.019115906208753586, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15370/20000], Bound: 0.38064315915107727, Entropy: 139.68429565429688, Temp: 2.7004308700561523, KL: 73.05372619628906, Loss: 0.015426844358444214, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15371/20000], Bound: 0.3767942786216736, Entropy: 142.6745147705078, Temp: 2.700432538986206, KL: 72.89376831054688, Loss: 0.01366720162332058, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15372/20000], Bound: 0.3525998890399933, Entropy: 142.6544647216797, Temp: 2.700434446334839, KL: 65.32783508300781, Loss: 0.01500790100544691, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15373/20000], Bound: 0.3895273208618164, Entropy: 141.48419189453125, Temp: 2.7004361152648926, KL: 74.94935607910156, Loss: 0.016706380993127823, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15374/20000], Bound: 0.3989916145801544, Entropy: 141.3566131591797, Temp: 2.7004377841949463, KL: 77.18901062011719, Loss: 0.01773073896765709, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15375/20000], Bound: 0.40137946605682373, Entropy: 141.98165893554688, Temp: 2.700439214706421, KL: 78.89817810058594, Loss: 0.0158823411911726, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15376/20000], Bound: 0.38285040855407715, Entropy: 141.4442596435547, Temp: 2.7004406452178955, KL: 73.53675842285156, Loss: 0.01571670174598694, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15377/20000], Bound: 0.3907793462276459, Entropy: 140.84939575195312, Temp: 2.700442314147949, KL: 76.6121826171875, Loss: 0.014307579956948757, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15378/20000], Bound: 0.4189937710762024, Entropy: 139.4823455810547, Temp: 2.700444221496582, KL: 82.62420654296875, Loss: 0.018839692696928978, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15379/20000], Bound: 0.36323535442352295, Entropy: 144.11187744140625, Temp: 2.700446128845215, KL: 66.33937072753906, Loss: 0.018650446087121964, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15380/20000], Bound: 0.3800216317176819, Entropy: 140.97825622558594, Temp: 2.700446605682373, KL: 71.63896179199219, Loss: 0.01771373674273491, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15381/20000], Bound: 0.39525365829467773, Entropy: 142.64662170410156, Temp: 2.700446605682373, KL: 77.29429626464844, Loss: 0.01548486202955246, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15382/20000], Bound: 0.3730446994304657, Entropy: 140.4427947998047, Temp: 2.7004470825195312, KL: 70.93110656738281, Loss: 0.015309472568333149, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15383/20000], Bound: 0.3995930552482605, Entropy: 141.54421997070312, Temp: 2.7004477977752686, KL: 77.33331298828125, Loss: 0.01779472455382347, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15384/20000], Bound: 0.3722222149372101, Entropy: 140.42657470703125, Temp: 2.7004482746124268, KL: 70.85398864746094, Loss: 0.015016731806099415, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15385/20000], Bound: 0.3690713346004486, Entropy: 142.1911163330078, Temp: 2.700448751449585, KL: 67.30198669433594, Loss: 0.019929811358451843, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15386/20000], Bound: 0.41657567024230957, Entropy: 141.0236053466797, Temp: 2.7004477977752686, KL: 83.06002807617188, Loss: 0.01666407100856304, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15387/20000], Bound: 0.3744693100452423, Entropy: 142.026123046875, Temp: 2.7004475593566895, KL: 71.40960693359375, Loss: 0.015178980305790901, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15388/20000], Bound: 0.38948917388916016, Entropy: 141.72817993164062, Temp: 2.7004475593566895, KL: 75.23049926757812, Loss: 0.016165224835276604, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15389/20000], Bound: 0.3632909655570984, Entropy: 140.54286193847656, Temp: 2.7004475593566895, KL: 68.884033203125, Loss: 0.013967971317470074, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15390/20000], Bound: 0.41603296995162964, Entropy: 140.65281677246094, Temp: 2.7004480361938477, KL: 82.65104675292969, Loss: 0.01711483672261238, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15391/20000], Bound: 0.3775172233581543, Entropy: 143.04043579101562, Temp: 2.700448751449585, KL: 68.90042114257812, Loss: 0.021446483209729195, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15392/20000], Bound: 0.4085116684436798, Entropy: 142.205078125, Temp: 2.7004477977752686, KL: 80.48139953613281, Loss: 0.016910327598452568, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15393/20000], Bound: 0.3976072669029236, Entropy: 142.2372283935547, Temp: 2.7004470825195312, KL: 76.170654296875, Loss: 0.018855459988117218, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15394/20000], Bound: 0.3569677472114563, Entropy: 140.56874084472656, Temp: 2.700446128845215, KL: 64.70895385742188, Loss: 0.018409039825201035, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15395/20000], Bound: 0.37080827355384827, Entropy: 141.38925170898438, Temp: 2.700443983078003, KL: 71.34445190429688, Loss: 0.013361151330173016, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15396/20000], Bound: 0.33897164463996887, Entropy: 141.16578674316406, Temp: 2.7004430294036865, KL: 62.69596862792969, Loss: 0.012931655161082745, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15397/20000], Bound: 0.3863970935344696, Entropy: 140.2287139892578, Temp: 2.700442314147949, KL: 72.91645812988281, Loss: 0.01877589151263237, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15398/20000], Bound: 0.39767640829086304, Entropy: 142.6824951171875, Temp: 2.7004408836364746, KL: 77.77716064453125, Loss: 0.01591881923377514, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15399/20000], Bound: 0.4274299144744873, Entropy: 141.7830810546875, Temp: 2.7004401683807373, KL: 86.84805297851562, Loss: 0.015834106132388115, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15400/20000], Bound: 0.39509037137031555, Entropy: 142.47994995117188, Temp: 2.7004404067993164, KL: 75.9091796875, Loss: 0.01796007715165615, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15401/20000], Bound: 0.3704807162284851, Entropy: 141.84295654296875, Temp: 2.7004401683807373, KL: 69.71316528320312, Loss: 0.016208557412028313, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15402/20000], Bound: 0.3758862316608429, Entropy: 141.96827697753906, Temp: 2.700439929962158, KL: 70.40960693359375, Loss: 0.01778344251215458, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15403/20000], Bound: 0.3966529369354248, Entropy: 139.63058471679688, Temp: 2.700439214706421, KL: 78.26377868652344, Loss: 0.014456229284405708, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15404/20000], Bound: 0.3922659158706665, Entropy: 142.27427673339844, Temp: 2.700439214706421, KL: 77.91587829589844, Loss: 0.012702662497758865, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15405/20000], Bound: 0.386333167552948, Entropy: 141.30209350585938, Temp: 2.7004401683807373, KL: 73.51895141601562, Loss: 0.017625855281949043, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15406/20000], Bound: 0.3944012224674225, Entropy: 141.40084838867188, Temp: 2.7004408836364746, KL: 77.21836853027344, Loss: 0.015159263275563717, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15407/20000], Bound: 0.3967863619327545, Entropy: 140.88156127929688, Temp: 2.70044207572937, KL: 78.93284606933594, Loss: 0.013290567323565483, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15408/20000], Bound: 0.3793348968029022, Entropy: 140.2989959716797, Temp: 2.700444221496582, KL: 73.62165832519531, Loss: 0.013675336726009846, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15409/20000], Bound: 0.3851695954799652, Entropy: 139.92662048339844, Temp: 2.7004470825195312, KL: 75.16513061523438, Loss: 0.013950041495263577, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15410/20000], Bound: 0.36622384190559387, Entropy: 142.50076293945312, Temp: 2.7004499435424805, KL: 65.88397216796875, Loss: 0.021058332175016403, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15411/20000], Bound: 0.3688785433769226, Entropy: 142.07833862304688, Temp: 2.700451135635376, KL: 68.16468811035156, Loss: 0.018230978399515152, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15412/20000], Bound: 0.3674313724040985, Entropy: 142.2919158935547, Temp: 2.700451374053955, KL: 69.85205078125, Loss: 0.014345395378768444, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15413/20000], Bound: 0.39251455664634705, Entropy: 143.0241241455078, Temp: 2.7004520893096924, KL: 76.35952758789062, Loss: 0.015719961374998093, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15414/20000], Bound: 0.36951330304145813, Entropy: 141.3275909423828, Temp: 2.7004528045654297, KL: 70.68728637695312, Loss: 0.013894750736653805, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15415/20000], Bound: 0.37863922119140625, Entropy: 139.22352600097656, Temp: 2.7004542350769043, KL: 72.49333190917969, Loss: 0.015392869710922241, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15416/20000], Bound: 0.3839651346206665, Entropy: 140.82679748535156, Temp: 2.700455665588379, KL: 73.67298889160156, Loss: 0.016064025461673737, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15417/20000], Bound: 0.390582799911499, Entropy: 139.868408203125, Temp: 2.7004568576812744, KL: 75.27397155761719, Loss: 0.01667865552008152, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15418/20000], Bound: 0.423951655626297, Entropy: 141.00399780273438, Temp: 2.70045804977417, KL: 86.59068298339844, Loss: 0.01431796234101057, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15419/20000], Bound: 0.36983662843704224, Entropy: 141.5701446533203, Temp: 2.700460195541382, KL: 69.69961547851562, Loss: 0.015894008800387383, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15420/20000], Bound: 0.38963308930397034, Entropy: 141.60926818847656, Temp: 2.7004623413085938, KL: 74.99787902832031, Loss: 0.016674183309078217, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15421/20000], Bound: 0.39165785908699036, Entropy: 141.55218505859375, Temp: 2.7004640102386475, KL: 76.59089660644531, Loss: 0.014825052581727505, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15422/20000], Bound: 0.4053537845611572, Entropy: 141.59661865234375, Temp: 2.7004663944244385, KL: 79.88023376464844, Loss: 0.016265302896499634, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15423/20000], Bound: 0.397938996553421, Entropy: 140.87208557128906, Temp: 2.7004685401916504, KL: 77.99093627929688, Loss: 0.015667501837015152, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15424/20000], Bound: 0.39541080594062805, Entropy: 140.5721893310547, Temp: 2.7004709243774414, KL: 77.18235778808594, Loss: 0.01577836275100708, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15425/20000], Bound: 0.4107953608036041, Entropy: 142.42990112304688, Temp: 2.7004735469818115, KL: 81.22946166992188, Loss: 0.01680227927863598, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15426/20000], Bound: 0.3879034221172333, Entropy: 141.08819580078125, Temp: 2.7004761695861816, KL: 75.09236145019531, Loss: 0.015561903826892376, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15427/20000], Bound: 0.39373502135276794, Entropy: 140.5071258544922, Temp: 2.700479030609131, KL: 72.82260131835938, Loss: 0.022934602573513985, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15428/20000], Bound: 0.3617098331451416, Entropy: 143.1643524169922, Temp: 2.700479745864868, KL: 67.87849426269531, Loss: 0.015004794113337994, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15429/20000], Bound: 0.40319716930389404, Entropy: 140.5282745361328, Temp: 2.7004802227020264, KL: 80.5784912109375, Loss: 0.013776648789644241, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15430/20000], Bound: 0.39315855503082275, Entropy: 139.56521606445312, Temp: 2.70048189163208, KL: 76.104248046875, Loss: 0.016543978825211525, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15431/20000], Bound: 0.38223862648010254, Entropy: 141.07705688476562, Temp: 2.7004833221435547, KL: 73.34971618652344, Loss: 0.015734821557998657, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15432/20000], Bound: 0.4017621874809265, Entropy: 141.5816192626953, Temp: 2.7004849910736084, KL: 78.46978759765625, Loss: 0.016887355595827103, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15433/20000], Bound: 0.38221976161003113, Entropy: 143.39964294433594, Temp: 2.700486421585083, KL: 72.21136474609375, Loss: 0.017832407727837563, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15434/20000], Bound: 0.40277090668678284, Entropy: 139.03536987304688, Temp: 2.7004873752593994, KL: 77.86923217773438, Loss: 0.018557008355855942, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15435/20000], Bound: 0.3965414762496948, Entropy: 140.72996520996094, Temp: 2.7004878520965576, KL: 75.96751403808594, Loss: 0.018647179007530212, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15436/20000], Bound: 0.38196662068367004, Entropy: 141.5690155029297, Temp: 2.7004878520965576, KL: 73.60023498535156, Loss: 0.01512502133846283, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15437/20000], Bound: 0.3606902062892914, Entropy: 140.14138793945312, Temp: 2.7004880905151367, KL: 67.35418701171875, Loss: 0.015444417484104633, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15438/20000], Bound: 0.39068007469177246, Entropy: 142.2059783935547, Temp: 2.700488328933716, KL: 75.82345581054688, Loss: 0.015714433044195175, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15439/20000], Bound: 0.38241642713546753, Entropy: 141.89834594726562, Temp: 2.700488805770874, KL: 71.90386962890625, Loss: 0.018507326021790504, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15440/20000], Bound: 0.3894837498664856, Entropy: 141.31919860839844, Temp: 2.700488567352295, KL: 75.38287353515625, Loss: 0.015880519524216652, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15441/20000], Bound: 0.37084850668907166, Entropy: 140.1278076171875, Temp: 2.700488567352295, KL: 68.15322875976562, Loss: 0.01929137483239174, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15442/20000], Bound: 0.3822984993457794, Entropy: 141.494384765625, Temp: 2.7004873752593994, KL: 72.01150512695312, Loss: 0.018244720995426178, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15443/20000], Bound: 0.36834076046943665, Entropy: 143.00218200683594, Temp: 2.7004857063293457, KL: 69.07307434082031, Loss: 0.016266198828816414, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15444/20000], Bound: 0.407621294260025, Entropy: 140.96728515625, Temp: 2.700484275817871, KL: 79.76350402832031, Loss: 0.01774328015744686, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15445/20000], Bound: 0.39164847135543823, Entropy: 141.94390869140625, Temp: 2.7004830837249756, KL: 74.90225219726562, Loss: 0.017946705222129822, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15446/20000], Bound: 0.41356319189071655, Entropy: 142.65060424804688, Temp: 2.700481414794922, KL: 84.01802062988281, Loss: 0.013192638754844666, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15447/20000], Bound: 0.411997526884079, Entropy: 139.6518096923828, Temp: 2.700481414794922, KL: 80.00677490234375, Loss: 0.01974005252122879, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15448/20000], Bound: 0.3967803418636322, Entropy: 139.84280395507812, Temp: 2.7004806995391846, KL: 76.9923095703125, Loss: 0.01688063144683838, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15449/20000], Bound: 0.39235919713974, Entropy: 140.628173828125, Temp: 2.7004802227020264, KL: 76.74928283691406, Loss: 0.014913929626345634, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15450/20000], Bound: 0.4020937383174896, Entropy: 140.7559356689453, Temp: 2.7004802227020264, KL: 77.90640258789062, Loss: 0.018113648518919945, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15451/20000], Bound: 0.3843449354171753, Entropy: 140.79579162597656, Temp: 2.7004799842834473, KL: 74.04823303222656, Loss: 0.015573953278362751, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15452/20000], Bound: 0.3964422643184662, Entropy: 140.47825622558594, Temp: 2.7004802227020264, KL: 78.28033447265625, Loss: 0.014310497790575027, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15453/20000], Bound: 0.395955353975296, Entropy: 141.33212280273438, Temp: 2.7004811763763428, KL: 77.23515319824219, Loss: 0.01597883738577366, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15454/20000], Bound: 0.40433716773986816, Entropy: 139.8955535888672, Temp: 2.7004823684692383, KL: 78.40411376953125, Loss: 0.01843426376581192, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15455/20000], Bound: 0.39154374599456787, Entropy: 141.3140106201172, Temp: 2.7004830837249756, KL: 76.52931213378906, Loss: 0.014877147041261196, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15456/20000], Bound: 0.38600367307662964, Entropy: 141.61984252929688, Temp: 2.700484037399292, KL: 74.27796936035156, Loss: 0.016042940318584442, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15457/20000], Bound: 0.4107779264450073, Entropy: 141.55018615722656, Temp: 2.7004852294921875, KL: 81.13056945800781, Loss: 0.016975758597254753, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15458/20000], Bound: 0.40605926513671875, Entropy: 143.33309936523438, Temp: 2.700486421585083, KL: 80.09869384765625, Loss: 0.016253117471933365, Learning Rate: 5.7474369414169845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15459/20000], Bound: 0.39623117446899414, Entropy: 142.7734375, Temp: 2.7004880905151367, KL: 77.42741394042969, Loss: 0.015774091705679893, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15460/20000], Bound: 0.36215487122535706, Entropy: 142.17752075195312, Temp: 2.7004897594451904, KL: 67.41743469238281, Loss: 0.01609063521027565, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15461/20000], Bound: 0.3891695439815521, Entropy: 140.43423461914062, Temp: 2.700491189956665, KL: 73.92788696289062, Loss: 0.01840408332645893, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15462/20000], Bound: 0.3940497636795044, Entropy: 141.76649475097656, Temp: 2.7004919052124023, KL: 76.67108154296875, Loss: 0.015981046482920647, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15463/20000], Bound: 0.3815916180610657, Entropy: 143.2245635986328, Temp: 2.7004926204681396, KL: 72.68540954589844, Loss: 0.016617679968476295, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15464/20000], Bound: 0.37726595997810364, Entropy: 141.95391845703125, Temp: 2.700493335723877, KL: 71.89273071289062, Loss: 0.01577255129814148, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15465/20000], Bound: 0.36805540323257446, Entropy: 142.41517639160156, Temp: 2.7004940509796143, KL: 69.59098815917969, Loss: 0.015157224610447884, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15466/20000], Bound: 0.42358359694480896, Entropy: 138.73263549804688, Temp: 2.7004947662353516, KL: 86.15068054199219, Loss: 0.014922799542546272, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15467/20000], Bound: 0.33819815516471863, Entropy: 141.68063354492188, Temp: 2.7004964351654053, KL: 60.41363525390625, Loss: 0.016767308115959167, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15468/20000], Bound: 0.365702360868454, Entropy: 141.9092559814453, Temp: 2.7004971504211426, KL: 69.42713928222656, Loss: 0.014224895276129246, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15469/20000], Bound: 0.39606234431266785, Entropy: 140.2093963623047, Temp: 2.700498104095459, KL: 75.35934448242188, Loss: 0.01951071433722973, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15470/20000], Bound: 0.4032653570175171, Entropy: 143.0401611328125, Temp: 2.700498342514038, KL: 79.47285461425781, Loss: 0.01586167886853218, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15471/20000], Bound: 0.3969971239566803, Entropy: 140.45196533203125, Temp: 2.7004990577697754, KL: 76.09919738769531, Loss: 0.018653325736522675, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15472/20000], Bound: 0.3670322597026825, Entropy: 140.04124450683594, Temp: 2.7004990577697754, KL: 68.60195922851562, Loss: 0.01645062491297722, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15473/20000], Bound: 0.4060628414154053, Entropy: 141.26687622070312, Temp: 2.7004988193511963, KL: 79.37545776367188, Loss: 0.017594320699572563, Learning Rate: 5.7474369414169845e-06\n",
      "Epoch [15474/20000], Bound: 0.35586878657341003, Entropy: 141.1916961669922, Temp: 2.700498580932617, KL: 64.75189208984375, Loss: 0.017761217430233955, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15475/20000], Bound: 0.3880845904350281, Entropy: 141.56883239746094, Temp: 2.70049786567688, KL: 75.92575073242188, Loss: 0.01411717664450407, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15476/20000], Bound: 0.3732394576072693, Entropy: 141.2034454345703, Temp: 2.700497627258301, KL: 69.60653686523438, Loss: 0.017865540459752083, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15477/20000], Bound: 0.3738296329975128, Entropy: 141.71299743652344, Temp: 2.7004971504211426, KL: 70.52949523925781, Loss: 0.016469566151499748, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15478/20000], Bound: 0.3984690010547638, Entropy: 140.94863891601562, Temp: 2.7004964351654053, KL: 78.27635192871094, Loss: 0.015430639497935772, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15479/20000], Bound: 0.38659149408340454, Entropy: 141.5126190185547, Temp: 2.700496196746826, KL: 75.22709655761719, Loss: 0.014603176154196262, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15480/20000], Bound: 0.3619212210178375, Entropy: 141.18310546875, Temp: 2.7004964351654053, KL: 67.0667724609375, Loss: 0.016618050634860992, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15481/20000], Bound: 0.4037071764469147, Entropy: 140.6185302734375, Temp: 2.7004964351654053, KL: 79.57720947265625, Loss: 0.01591317169368267, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15482/20000], Bound: 0.38182467222213745, Entropy: 144.12408447265625, Temp: 2.7004966735839844, KL: 73.13661193847656, Loss: 0.015907326713204384, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15483/20000], Bound: 0.37115478515625, Entropy: 141.13909912109375, Temp: 2.7004969120025635, KL: 70.71882629394531, Loss: 0.014703012071549892, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15484/20000], Bound: 0.40147167444229126, Entropy: 141.7242889404297, Temp: 2.7004973888397217, KL: 77.78800964355469, Loss: 0.017989320680499077, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15485/20000], Bound: 0.4128221273422241, Entropy: 142.80804443359375, Temp: 2.700497627258301, KL: 81.45733642578125, Loss: 0.01751742511987686, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15486/20000], Bound: 0.39362379908561707, Entropy: 139.74856567382812, Temp: 2.70049786567688, KL: 75.31596374511719, Loss: 0.018257519230246544, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15487/20000], Bound: 0.35780730843544006, Entropy: 142.92974853515625, Temp: 2.70049786567688, KL: 66.57374572753906, Loss: 0.015391740016639233, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15488/20000], Bound: 0.36518967151641846, Entropy: 141.00929260253906, Temp: 2.70049786567688, KL: 69.38294982910156, Loss: 0.014038052409887314, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15489/20000], Bound: 0.4057856798171997, Entropy: 139.19676208496094, Temp: 2.700498104095459, KL: 78.9146728515625, Loss: 0.01829337142407894, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15490/20000], Bound: 0.380144327878952, Entropy: 142.4386444091797, Temp: 2.700498104095459, KL: 72.97801208496094, Loss: 0.015300577506422997, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15491/20000], Bound: 0.38071000576019287, Entropy: 142.1571807861328, Temp: 2.700498342514038, KL: 72.91917419433594, Loss: 0.01571236364543438, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15492/20000], Bound: 0.3650246858596802, Entropy: 141.96517944335938, Temp: 2.700498580932617, KL: 69.52095031738281, Loss: 0.01369610894471407, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15493/20000], Bound: 0.36185628175735474, Entropy: 142.20750427246094, Temp: 2.7004992961883545, KL: 67.81175231933594, Loss: 0.015204870142042637, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15494/20000], Bound: 0.38992366194725037, Entropy: 141.5076904296875, Temp: 2.7004997730255127, KL: 76.87980651855469, Loss: 0.013347813859581947, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15495/20000], Bound: 0.3728598952293396, Entropy: 141.71136474609375, Temp: 2.700500965118408, KL: 69.66421508789062, Loss: 0.017557667568325996, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15496/20000], Bound: 0.36802539229393005, Entropy: 141.07237243652344, Temp: 2.7005016803741455, KL: 70.766845703125, Loss: 0.012964410707354546, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15497/20000], Bound: 0.3787769675254822, Entropy: 143.9341583251953, Temp: 2.700502872467041, KL: 70.33859252929688, Loss: 0.01945641078054905, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15498/20000], Bound: 0.3852095901966095, Entropy: 141.3009796142578, Temp: 2.70050311088562, KL: 72.88987731933594, Loss: 0.018184786662459373, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15499/20000], Bound: 0.3760540187358856, Entropy: 143.070068359375, Temp: 2.70050311088562, KL: 71.37998962402344, Loss: 0.016076551750302315, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15500/20000], Bound: 0.3853771388530731, Entropy: 142.76593017578125, Temp: 2.70050311088562, KL: 74.30549621582031, Loss: 0.015654128044843674, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15501/20000], Bound: 0.37407270073890686, Entropy: 140.9158172607422, Temp: 2.700503349304199, KL: 70.96694946289062, Loss: 0.015788599848747253, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15502/20000], Bound: 0.3767245411872864, Entropy: 139.9455108642578, Temp: 2.7005035877227783, KL: 72.072998046875, Loss: 0.015150398947298527, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15503/20000], Bound: 0.36207646131515503, Entropy: 142.144287109375, Temp: 2.7005038261413574, KL: 68.46392822265625, Loss: 0.014112244360148907, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15504/20000], Bound: 0.40179842710494995, Entropy: 138.2926025390625, Temp: 2.7005043029785156, KL: 78.30274963378906, Loss: 0.017216797918081284, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15505/20000], Bound: 0.3830323815345764, Entropy: 142.1173858642578, Temp: 2.700504779815674, KL: 73.56599426269531, Loss: 0.015760965645313263, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15506/20000], Bound: 0.3971584737300873, Entropy: 139.71444702148438, Temp: 2.700505256652832, KL: 76.87129211425781, Loss: 0.017312366515398026, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15507/20000], Bound: 0.3666113317012787, Entropy: 141.538818359375, Temp: 2.700505495071411, KL: 67.68949890136719, Loss: 0.017919078469276428, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15508/20000], Bound: 0.3808329403400421, Entropy: 139.9462127685547, Temp: 2.700505495071411, KL: 73.82046508789062, Loss: 0.01410954724997282, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15509/20000], Bound: 0.3774765431880951, Entropy: 139.82142639160156, Temp: 2.7005057334899902, KL: 72.06808471679688, Loss: 0.015560206957161427, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15510/20000], Bound: 0.3789388835430145, Entropy: 141.0887908935547, Temp: 2.7005062103271484, KL: 72.86663818359375, Loss: 0.014862223528325558, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15511/20000], Bound: 0.4019509255886078, Entropy: 141.65159606933594, Temp: 2.7005069255828857, KL: 78.45184326171875, Loss: 0.017025070264935493, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15512/20000], Bound: 0.3819076120853424, Entropy: 141.70262145996094, Temp: 2.700507640838623, KL: 73.46104431152344, Loss: 0.01535123586654663, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15513/20000], Bound: 0.41412806510925293, Entropy: 139.90908813476562, Temp: 2.7005083560943604, KL: 82.86996459960938, Loss: 0.015636375173926353, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15514/20000], Bound: 0.39561229944229126, Entropy: 142.1531219482422, Temp: 2.7005093097686768, KL: 75.18577575683594, Loss: 0.019585728645324707, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15515/20000], Bound: 0.38757121562957764, Entropy: 141.16172790527344, Temp: 2.700509786605835, KL: 76.01101684570312, Loss: 0.013681580312550068, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15516/20000], Bound: 0.4007008671760559, Entropy: 141.30369567871094, Temp: 2.7005109786987305, KL: 78.68905639648438, Loss: 0.015895705670118332, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15517/20000], Bound: 0.394981324672699, Entropy: 140.93101501464844, Temp: 2.700512170791626, KL: 76.84242248535156, Loss: 0.01617315784096718, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15518/20000], Bound: 0.36241239309310913, Entropy: 143.93438720703125, Temp: 2.7005133628845215, KL: 68.27885437011719, Loss: 0.01463021244853735, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15519/20000], Bound: 0.36474764347076416, Entropy: 141.26895141601562, Temp: 2.700514793395996, KL: 66.13795471191406, Loss: 0.01981479302048683, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15520/20000], Bound: 0.39757615327835083, Entropy: 142.2238006591797, Temp: 2.700515031814575, KL: 76.75794982910156, Loss: 0.017751570791006088, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15521/20000], Bound: 0.396040141582489, Entropy: 140.36814880371094, Temp: 2.700515031814575, KL: 78.174560546875, Loss: 0.014286324381828308, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15522/20000], Bound: 0.3829878866672516, Entropy: 140.12725830078125, Temp: 2.7005157470703125, KL: 74.55085754394531, Loss: 0.01391364261507988, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15523/20000], Bound: 0.3898460566997528, Entropy: 140.69187927246094, Temp: 2.700516700744629, KL: 73.89753723144531, Loss: 0.018827522173523903, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15524/20000], Bound: 0.3803216218948364, Entropy: 141.0598602294922, Temp: 2.700517177581787, KL: 72.46705627441406, Loss: 0.01634165830910206, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15525/20000], Bound: 0.4103851616382599, Entropy: 140.2593536376953, Temp: 2.7005176544189453, KL: 81.17431640625, Loss: 0.016675179824233055, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15526/20000], Bound: 0.36640024185180664, Entropy: 139.10366821289062, Temp: 2.7005183696746826, KL: 69.77398681640625, Loss: 0.013948987238109112, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15527/20000], Bound: 0.39215022325515747, Entropy: 140.30410766601562, Temp: 2.70051908493042, KL: 73.61306762695312, Loss: 0.02060716599225998, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15528/20000], Bound: 0.41024819016456604, Entropy: 140.0999755859375, Temp: 2.70051908493042, KL: 79.85441589355469, Loss: 0.019042372703552246, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15529/20000], Bound: 0.39020460844039917, Entropy: 140.4563446044922, Temp: 2.700518846511841, KL: 75.86410522460938, Loss: 0.015381108969449997, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15530/20000], Bound: 0.3762504756450653, Entropy: 143.45806884765625, Temp: 2.700518846511841, KL: 72.13941955566406, Loss: 0.014775157906115055, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15531/20000], Bound: 0.3836878836154938, Entropy: 136.37576293945312, Temp: 2.70051908493042, KL: 73.71229553222656, Loss: 0.015842657536268234, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15532/20000], Bound: 0.3652251064777374, Entropy: 142.039794921875, Temp: 2.700519561767578, KL: 67.11709594726562, Loss: 0.018252000212669373, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15533/20000], Bound: 0.4139697849750519, Entropy: 142.82232666015625, Temp: 2.700519323348999, KL: 80.25762939453125, Loss: 0.02038416638970375, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15534/20000], Bound: 0.394681841135025, Entropy: 140.0546112060547, Temp: 2.7005186080932617, KL: 76.23956298828125, Loss: 0.017125682905316353, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15535/20000], Bound: 0.3718365728855133, Entropy: 140.17543029785156, Temp: 2.7005181312561035, KL: 69.07875061035156, Loss: 0.018100226297974586, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15536/20000], Bound: 0.3943304121494293, Entropy: 140.71734619140625, Temp: 2.700516939163208, KL: 77.83522033691406, Loss: 0.01397921983152628, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15537/20000], Bound: 0.37074562907218933, Entropy: 140.82052612304688, Temp: 2.70051646232605, KL: 68.39474487304688, Loss: 0.018790103495121002, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15538/20000], Bound: 0.3866727948188782, Entropy: 140.0009002685547, Temp: 2.7005157470703125, KL: 75.32432556152344, Loss: 0.0144672691822052, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15539/20000], Bound: 0.38082441687583923, Entropy: 140.6099090576172, Temp: 2.7005152702331543, KL: 71.89616394042969, Loss: 0.01766793057322502, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15540/20000], Bound: 0.3978661596775055, Entropy: 140.84732055664062, Temp: 2.700514554977417, KL: 78.50396728515625, Loss: 0.014678090810775757, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15541/20000], Bound: 0.3768583834171295, Entropy: 140.61334228515625, Temp: 2.700514316558838, KL: 72.21638488769531, Loss: 0.014956285245716572, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15542/20000], Bound: 0.3791891932487488, Entropy: 143.82681274414062, Temp: 2.700514316558838, KL: 71.59660339355469, Loss: 0.01734752580523491, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15543/20000], Bound: 0.40045011043548584, Entropy: 143.71682739257812, Temp: 2.700514078140259, KL: 78.41714477539062, Loss: 0.016260892152786255, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15544/20000], Bound: 0.3610009551048279, Entropy: 142.67254638671875, Temp: 2.700514078140259, KL: 66.93896484375, Loss: 0.016375210136175156, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15545/20000], Bound: 0.39892080426216125, Entropy: 141.12445068359375, Temp: 2.7005138397216797, KL: 79.27688598632812, Loss: 0.01382680144160986, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15546/20000], Bound: 0.36237046122550964, Entropy: 143.65585327148438, Temp: 2.700514316558838, KL: 68.7703857421875, Loss: 0.013698306865990162, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15547/20000], Bound: 0.38514217734336853, Entropy: 141.23069763183594, Temp: 2.700515031814575, KL: 73.41215515136719, Loss: 0.01718153990805149, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15548/20000], Bound: 0.38515251874923706, Entropy: 140.6522979736328, Temp: 2.7005155086517334, KL: 74.11587524414062, Loss: 0.015884211286902428, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15549/20000], Bound: 0.3513692021369934, Entropy: 141.0572509765625, Temp: 2.7005159854888916, KL: 65.54258728027344, Loss: 0.013978013768792152, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15550/20000], Bound: 0.3818337619304657, Entropy: 141.6798553466797, Temp: 2.700516700744629, KL: 74.2027587890625, Loss: 0.013938415795564651, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15551/20000], Bound: 0.407918244600296, Entropy: 141.01466369628906, Temp: 2.7005178928375244, KL: 80.21354675292969, Loss: 0.017075926065444946, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15552/20000], Bound: 0.3736644685268402, Entropy: 140.5142059326172, Temp: 2.700518846511841, KL: 69.39811706542969, Loss: 0.018476873636245728, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15553/20000], Bound: 0.3726753890514374, Entropy: 141.45619201660156, Temp: 2.700519323348999, KL: 69.84677124023438, Loss: 0.017122099176049232, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15554/20000], Bound: 0.3939511477947235, Entropy: 140.71800231933594, Temp: 2.700519561767578, KL: 76.79794311523438, Loss: 0.015692593529820442, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15555/20000], Bound: 0.374855637550354, Entropy: 142.863525390625, Temp: 2.7005200386047363, KL: 70.48011779785156, Loss: 0.01710572838783264, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15556/20000], Bound: 0.38333994150161743, Entropy: 142.49032592773438, Temp: 2.7005200386047363, KL: 73.75942993164062, Loss: 0.015568283386528492, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15557/20000], Bound: 0.37270882725715637, Entropy: 141.80734252929688, Temp: 2.7005202770233154, KL: 70.83433532714844, Loss: 0.015311374329030514, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15558/20000], Bound: 0.40233954787254333, Entropy: 141.45980834960938, Temp: 2.7005205154418945, KL: 79.76153564453125, Loss: 0.014815136790275574, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15559/20000], Bound: 0.3563472628593445, Entropy: 141.097900390625, Temp: 2.700521230697632, KL: 66.64707946777344, Loss: 0.01449991948902607, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15560/20000], Bound: 0.39525553584098816, Entropy: 141.11224365234375, Temp: 2.7005221843719482, KL: 77.15974426269531, Loss: 0.01573575660586357, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15561/20000], Bound: 0.37556201219558716, Entropy: 143.3137664794922, Temp: 2.7005228996276855, KL: 69.48381042480469, Loss: 0.019325794652104378, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15562/20000], Bound: 0.37660685181617737, Entropy: 141.95022583007812, Temp: 2.7005231380462646, KL: 71.23184204101562, Loss: 0.01664528250694275, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15563/20000], Bound: 0.38564348220825195, Entropy: 141.2731475830078, Temp: 2.7005231380462646, KL: 74.01258850097656, Loss: 0.01634027063846588, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15564/20000], Bound: 0.3586622476577759, Entropy: 141.69752502441406, Temp: 2.7005231380462646, KL: 65.3509521484375, Loss: 0.018099470064044, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15565/20000], Bound: 0.3972972631454468, Entropy: 141.57235717773438, Temp: 2.7005226612091064, KL: 78.4178466796875, Loss: 0.014525230042636395, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15566/20000], Bound: 0.37366798520088196, Entropy: 140.85484313964844, Temp: 2.7005226612091064, KL: 70.58160400390625, Loss: 0.016287529841065407, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15567/20000], Bound: 0.3612307012081146, Entropy: 142.88226318359375, Temp: 2.7005226612091064, KL: 66.30917358398438, Loss: 0.017660999670624733, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15568/20000], Bound: 0.43320518732070923, Entropy: 141.9149932861328, Temp: 2.700521945953369, KL: 88.45585632324219, Loss: 0.01619095355272293, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15569/20000], Bound: 0.35365545749664307, Entropy: 142.8806610107422, Temp: 2.700521945953369, KL: 62.898040771484375, Loss: 0.020051054656505585, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15570/20000], Bound: 0.3968852460384369, Entropy: 139.64657592773438, Temp: 2.7005207538604736, KL: 78.89105224609375, Loss: 0.013423051685094833, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15571/20000], Bound: 0.38688889145851135, Entropy: 141.82794189453125, Temp: 2.7005205154418945, KL: 74.10736083984375, Loss: 0.01683730259537697, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15572/20000], Bound: 0.37751340866088867, Entropy: 141.87481689453125, Temp: 2.7005202770233154, KL: 72.98870849609375, Loss: 0.013875510543584824, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15573/20000], Bound: 0.38044339418411255, Entropy: 138.93833923339844, Temp: 2.7005205154418945, KL: 72.58120727539062, Loss: 0.016195548698306084, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15574/20000], Bound: 0.39962807297706604, Entropy: 139.11412048339844, Temp: 2.7005207538604736, KL: 78.73634338378906, Loss: 0.015216982923448086, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15575/20000], Bound: 0.40671566128730774, Entropy: 140.76817321777344, Temp: 2.700521469116211, KL: 80.33015441894531, Loss: 0.016190143302083015, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15576/20000], Bound: 0.3837054371833801, Entropy: 142.3753204345703, Temp: 2.7005224227905273, KL: 71.11561584472656, Loss: 0.02065986581146717, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15577/20000], Bound: 0.3634360432624817, Entropy: 141.49842834472656, Temp: 2.7005221843719482, KL: 69.14682006835938, Loss: 0.01355784758925438, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15578/20000], Bound: 0.3961127996444702, Entropy: 141.33998107910156, Temp: 2.7005224227905273, KL: 77.73246765136719, Loss: 0.015144748613238335, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15579/20000], Bound: 0.3695753514766693, Entropy: 140.47889709472656, Temp: 2.7005228996276855, KL: 69.78030395507812, Loss: 0.015607325360178947, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15580/20000], Bound: 0.36122027039527893, Entropy: 139.78187561035156, Temp: 2.7005233764648438, KL: 68.71319580078125, Loss: 0.01320455502718687, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15581/20000], Bound: 0.39597851037979126, Entropy: 142.71734619140625, Temp: 2.70052433013916, KL: 76.94731140136719, Loss: 0.016524894163012505, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15582/20000], Bound: 0.3887893557548523, Entropy: 142.01771545410156, Temp: 2.7005252838134766, KL: 73.20027160644531, Loss: 0.019545426592230797, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15583/20000], Bound: 0.3881831765174866, Entropy: 140.76844787597656, Temp: 2.7005255222320557, KL: 74.33543395996094, Loss: 0.01711527816951275, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15584/20000], Bound: 0.3819410502910614, Entropy: 141.32974243164062, Temp: 2.7005255222320557, KL: 73.81465148925781, Loss: 0.014714648015797138, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15585/20000], Bound: 0.38645052909851074, Entropy: 140.32801818847656, Temp: 2.700525999069214, KL: 71.6070556640625, Loss: 0.021229788661003113, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15586/20000], Bound: 0.3681654632091522, Entropy: 142.4013214111328, Temp: 2.7005252838134766, KL: 69.90364074707031, Loss: 0.014636518433690071, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15587/20000], Bound: 0.40381625294685364, Entropy: 142.12277221679688, Temp: 2.7005248069763184, KL: 78.51898193359375, Loss: 0.017933225259184837, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15588/20000], Bound: 0.35419875383377075, Entropy: 142.7882843017578, Temp: 2.70052433013916, KL: 65.21017456054688, Loss: 0.016050346195697784, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15589/20000], Bound: 0.3539878726005554, Entropy: 141.06088256835938, Temp: 2.700523614883423, KL: 65.28811645507812, Loss: 0.015797244384884834, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15590/20000], Bound: 0.387328565120697, Entropy: 140.70187377929688, Temp: 2.7005228996276855, KL: 74.7840576171875, Loss: 0.015822123736143112, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15591/20000], Bound: 0.40807458758354187, Entropy: 141.0814208984375, Temp: 2.7005226612091064, KL: 79.19700622558594, Loss: 0.01904529705643654, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15592/20000], Bound: 0.39088955521583557, Entropy: 140.94244384765625, Temp: 2.700521945953369, KL: 73.43534851074219, Loss: 0.020250186324119568, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15593/20000], Bound: 0.38432249426841736, Entropy: 140.65292358398438, Temp: 2.7005205154418945, KL: 72.65277099609375, Loss: 0.01814592070877552, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15594/20000], Bound: 0.34486547112464905, Entropy: 142.32740783691406, Temp: 2.700518846511841, KL: 63.17706298828125, Loss: 0.015030822716653347, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15595/20000], Bound: 0.37725189328193665, Entropy: 141.5120086669922, Temp: 2.700517416000366, KL: 72.02880859375, Loss: 0.015513296239078045, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15596/20000], Bound: 0.3769380748271942, Entropy: 140.79620361328125, Temp: 2.7005159854888916, KL: 71.13734436035156, Loss: 0.01699661649763584, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15597/20000], Bound: 0.36481237411499023, Entropy: 141.89947509765625, Temp: 2.700514793395996, KL: 68.40200805664062, Loss: 0.01565680466592312, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15598/20000], Bound: 0.4192395806312561, Entropy: 142.370849609375, Temp: 2.7005133628845215, KL: 84.47406005859375, Loss: 0.015554800629615784, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15599/20000], Bound: 0.38936343789100647, Entropy: 139.4432830810547, Temp: 2.7005128860473633, KL: 73.77720642089844, Loss: 0.018788421526551247, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15600/20000], Bound: 0.3770591914653778, Entropy: 140.91012573242188, Temp: 2.700511932373047, KL: 73.35177612304688, Loss: 0.012961078435182571, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15601/20000], Bound: 0.379950612783432, Entropy: 140.00030517578125, Temp: 2.7005116939544678, KL: 73.72911071777344, Loss: 0.013806336559355259, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15602/20000], Bound: 0.3804355561733246, Entropy: 140.82591247558594, Temp: 2.700511932373047, KL: 72.81587219238281, Loss: 0.015756815671920776, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15603/20000], Bound: 0.3917771577835083, Entropy: 144.7641143798828, Temp: 2.700512170791626, KL: 76.1529541015625, Loss: 0.015701310709118843, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15604/20000], Bound: 0.38320910930633545, Entropy: 140.21002197265625, Temp: 2.700512647628784, KL: 72.46903991699219, Loss: 0.017886999994516373, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15605/20000], Bound: 0.36959993839263916, Entropy: 140.23931884765625, Temp: 2.7005128860473633, KL: 70.0020751953125, Loss: 0.015209599398076534, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15606/20000], Bound: 0.3734428882598877, Entropy: 140.42083740234375, Temp: 2.7005128860473633, KL: 70.77468872070312, Loss: 0.015810618177056313, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15607/20000], Bound: 0.4068964123725891, Entropy: 141.2850799560547, Temp: 2.7005131244659424, KL: 78.71409606933594, Loss: 0.019282778725028038, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15608/20000], Bound: 0.3666518032550812, Entropy: 143.32553100585938, Temp: 2.7005128860473633, KL: 70.22996520996094, Loss: 0.013236735947430134, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15609/20000], Bound: 0.37802356481552124, Entropy: 140.01100158691406, Temp: 2.7005131244659424, KL: 69.71453857421875, Loss: 0.020209690555930138, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15610/20000], Bound: 0.36352014541625977, Entropy: 143.14918518066406, Temp: 2.700512647628784, KL: 68.17207336425781, Loss: 0.015406443737447262, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15611/20000], Bound: 0.3626146614551544, Entropy: 143.0602569580078, Temp: 2.700512170791626, KL: 68.23342895507812, Loss: 0.014819920063018799, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15612/20000], Bound: 0.35961678624153137, Entropy: 143.03018188476562, Temp: 2.7005116939544678, KL: 68.03182983398438, Loss: 0.013631572015583515, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15613/20000], Bound: 0.4195285141468048, Entropy: 140.74659729003906, Temp: 2.7005116939544678, KL: 83.14195251464844, Loss: 0.01818510703742504, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15614/20000], Bound: 0.39251965284347534, Entropy: 140.32212829589844, Temp: 2.7005116939544678, KL: 75.20326232910156, Loss: 0.01786412112414837, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15615/20000], Bound: 0.40153738856315613, Entropy: 140.82888793945312, Temp: 2.7005114555358887, KL: 79.07850646972656, Loss: 0.015636354684829712, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15616/20000], Bound: 0.3913332223892212, Entropy: 143.4655303955078, Temp: 2.7005116939544678, KL: 74.20216369628906, Loss: 0.019071616232395172, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15617/20000], Bound: 0.4004022181034088, Entropy: 139.82229614257812, Temp: 2.7005112171173096, KL: 78.67523193359375, Loss: 0.015756625682115555, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15618/20000], Bound: 0.37835752964019775, Entropy: 139.5865478515625, Temp: 2.7005112171173096, KL: 70.311767578125, Loss: 0.019282132387161255, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15619/20000], Bound: 0.36010274291038513, Entropy: 141.33946228027344, Temp: 2.7005105018615723, KL: 66.83064270019531, Loss: 0.01610824279487133, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15620/20000], Bound: 0.3586888015270233, Entropy: 140.2249755859375, Temp: 2.700509548187256, KL: 66.89141845703125, Loss: 0.015260986052453518, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15621/20000], Bound: 0.3896562159061432, Entropy: 141.222412109375, Temp: 2.7005090713500977, KL: 75.96525573730469, Loss: 0.014896024018526077, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15622/20000], Bound: 0.39268943667411804, Entropy: 140.1390380859375, Temp: 2.7005088329315186, KL: 77.41966247558594, Loss: 0.01385296881198883, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15623/20000], Bound: 0.39439982175827026, Entropy: 144.51844787597656, Temp: 2.7005090713500977, KL: 78.16511535644531, Loss: 0.013406270183622837, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15624/20000], Bound: 0.3825615346431732, Entropy: 140.3017578125, Temp: 2.700509786605835, KL: 73.69210815429688, Loss: 0.015274508856236935, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15625/20000], Bound: 0.4060668349266052, Entropy: 140.25985717773438, Temp: 2.7005109786987305, KL: 78.78924560546875, Loss: 0.01868203654885292, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15626/20000], Bound: 0.389413446187973, Entropy: 142.43214416503906, Temp: 2.7005116939544678, KL: 75.55819702148438, Loss: 0.01551800686866045, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15627/20000], Bound: 0.3982509672641754, Entropy: 141.21873474121094, Temp: 2.700512647628784, KL: 79.09895324707031, Loss: 0.01378786563873291, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15628/20000], Bound: 0.3873763978481293, Entropy: 140.816162109375, Temp: 2.700514078140259, KL: 73.83578491210938, Loss: 0.017603665590286255, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15629/20000], Bound: 0.3706611692905426, Entropy: 140.0301971435547, Temp: 2.7005152702331543, KL: 71.37535095214844, Loss: 0.013226879760622978, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15630/20000], Bound: 0.37815195322036743, Entropy: 141.36241149902344, Temp: 2.700516700744629, KL: 73.15484619140625, Loss: 0.013908500783145428, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15631/20000], Bound: 0.39013656973838806, Entropy: 142.23666381835938, Temp: 2.7005186080932617, KL: 74.70237731933594, Loss: 0.017495106905698776, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15632/20000], Bound: 0.37556397914886475, Entropy: 140.67178344726562, Temp: 2.7005200386047363, KL: 72.25340270996094, Loss: 0.01419894676655531, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15633/20000], Bound: 0.3869117200374603, Entropy: 141.8002166748047, Temp: 2.70052170753479, KL: 75.96809387207031, Loss: 0.013404527679085732, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15634/20000], Bound: 0.4083581268787384, Entropy: 140.8241729736328, Temp: 2.700523614883423, KL: 80.03883361816406, Loss: 0.017644815146923065, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15635/20000], Bound: 0.3654657006263733, Entropy: 139.33963012695312, Temp: 2.7005257606506348, KL: 68.749267578125, Loss: 0.015356161631643772, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15636/20000], Bound: 0.3896327316761017, Entropy: 140.59939575195312, Temp: 2.7005274295806885, KL: 75.94500732421875, Loss: 0.014920976012945175, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15637/20000], Bound: 0.38075554370880127, Entropy: 143.57815551757812, Temp: 2.7005293369293213, KL: 71.70793151855469, Loss: 0.017979655414819717, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15638/20000], Bound: 0.36678004264831543, Entropy: 139.992431640625, Temp: 2.700530767440796, KL: 69.524169921875, Loss: 0.014610960148274899, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15639/20000], Bound: 0.39812466502189636, Entropy: 140.76885986328125, Temp: 2.7005321979522705, KL: 78.1617431640625, Loss: 0.015453900210559368, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15640/20000], Bound: 0.38756218552589417, Entropy: 141.98036193847656, Temp: 2.700533628463745, KL: 73.26338195800781, Loss: 0.018764112144708633, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15641/20000], Bound: 0.3733462691307068, Entropy: 143.32113647460938, Temp: 2.7005345821380615, KL: 70.63528442382812, Loss: 0.016017712652683258, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15642/20000], Bound: 0.3697799742221832, Entropy: 141.66944885253906, Temp: 2.700535535812378, KL: 69.15655517578125, Loss: 0.01687020994722843, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15643/20000], Bound: 0.3784710168838501, Entropy: 141.07701110839844, Temp: 2.700535774230957, KL: 72.50572204589844, Loss: 0.015280835330486298, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15644/20000], Bound: 0.36432772874832153, Entropy: 140.29209899902344, Temp: 2.7005364894866943, KL: 68.02458190917969, Loss: 0.016102056950330734, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15645/20000], Bound: 0.39056649804115295, Entropy: 141.1887969970703, Temp: 2.7005369663238525, KL: 75.83938598632812, Loss: 0.015623685903847218, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15646/20000], Bound: 0.3620854318141937, Entropy: 142.95787048339844, Temp: 2.70053768157959, KL: 67.39083862304688, Loss: 0.01610400341451168, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15647/20000], Bound: 0.3901093602180481, Entropy: 140.26654052734375, Temp: 2.700538158416748, KL: 75.4664306640625, Loss: 0.01606586016714573, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15648/20000], Bound: 0.38808977603912354, Entropy: 141.56919860839844, Temp: 2.7005386352539062, KL: 73.80134582519531, Loss: 0.018053686246275902, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15649/20000], Bound: 0.36746299266815186, Entropy: 142.66575622558594, Temp: 2.7005388736724854, KL: 68.6439208984375, Loss: 0.016599571332335472, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15650/20000], Bound: 0.36894023418426514, Entropy: 140.64247131347656, Temp: 2.7005388736724854, KL: 69.80335998535156, Loss: 0.01523012574762106, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15651/20000], Bound: 0.401258647441864, Entropy: 140.68467712402344, Temp: 2.7005388736724854, KL: 79.01023864746094, Loss: 0.015609120018780231, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15652/20000], Bound: 0.37934455275535583, Entropy: 140.6444091796875, Temp: 2.7005388736724854, KL: 71.77694702148438, Loss: 0.01709684729576111, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15653/20000], Bound: 0.3825816810131073, Entropy: 141.5642547607422, Temp: 2.7005388736724854, KL: 73.85249328613281, Loss: 0.014988640323281288, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15654/20000], Bound: 0.3451944589614868, Entropy: 143.378173828125, Temp: 2.7005395889282227, KL: 60.73435974121094, Loss: 0.019721154123544693, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15655/20000], Bound: 0.3897922933101654, Entropy: 142.34507751464844, Temp: 2.7005386352539062, KL: 74.12355041503906, Loss: 0.018380088731646538, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15656/20000], Bound: 0.35879281163215637, Entropy: 143.63148498535156, Temp: 2.7005374431610107, KL: 66.58314514160156, Loss: 0.015885936096310616, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15657/20000], Bound: 0.4136193096637726, Entropy: 141.10809326171875, Temp: 2.7005362510681152, KL: 82.59077453613281, Loss: 0.01586735248565674, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15658/20000], Bound: 0.3886931240558624, Entropy: 141.694580078125, Temp: 2.700535774230957, KL: 74.19374084472656, Loss: 0.017653945833444595, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15659/20000], Bound: 0.38530921936035156, Entropy: 142.45584106445312, Temp: 2.7005350589752197, KL: 73.69778442382812, Loss: 0.01674296148121357, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15660/20000], Bound: 0.37235119938850403, Entropy: 140.60069274902344, Temp: 2.7005343437194824, KL: 70.50570678710938, Loss: 0.015730617567896843, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15661/20000], Bound: 0.38105592131614685, Entropy: 142.43490600585938, Temp: 2.700533628463745, KL: 73.41903686523438, Loss: 0.014972563832998276, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15662/20000], Bound: 0.3845265209674835, Entropy: 141.97482299804688, Temp: 2.700533390045166, KL: 72.5921630859375, Loss: 0.018368160352110863, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15663/20000], Bound: 0.3859016001224518, Entropy: 142.12728881835938, Temp: 2.7005326747894287, KL: 74.516845703125, Loss: 0.015546038746833801, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15664/20000], Bound: 0.354669988155365, Entropy: 142.99720764160156, Temp: 2.7005324363708496, KL: 65.544677734375, Loss: 0.015674220398068428, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15665/20000], Bound: 0.3621719181537628, Entropy: 143.45050048828125, Temp: 2.7005319595336914, KL: 67.42619323730469, Loss: 0.016083618625998497, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15666/20000], Bound: 0.3712521493434906, Entropy: 143.2811279296875, Temp: 2.700531244277954, KL: 69.75047302246094, Loss: 0.016547653824090958, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15667/20000], Bound: 0.3712170124053955, Entropy: 140.6525421142578, Temp: 2.700530529022217, KL: 69.85160827636719, Loss: 0.01634182035923004, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15668/20000], Bound: 0.38693201541900635, Entropy: 142.1979217529297, Temp: 2.7005295753479004, KL: 74.508056640625, Loss: 0.01611882820725441, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15669/20000], Bound: 0.3984028100967407, Entropy: 141.9586639404297, Temp: 2.700529098510742, KL: 78.28398132324219, Loss: 0.015380422584712505, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15670/20000], Bound: 0.38747039437294006, Entropy: 141.42269897460938, Temp: 2.700528621673584, KL: 75.82064819335938, Loss: 0.01397965382784605, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15671/20000], Bound: 0.40461087226867676, Entropy: 141.28929138183594, Temp: 2.700529098510742, KL: 79.96812438964844, Loss: 0.015690797939896584, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15672/20000], Bound: 0.413303017616272, Entropy: 141.31092834472656, Temp: 2.7005295753479004, KL: 79.34465026855469, Loss: 0.021699627861380577, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15673/20000], Bound: 0.3870333135128021, Entropy: 142.90870666503906, Temp: 2.7005293369293213, KL: 74.95899963378906, Loss: 0.015338648110628128, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15674/20000], Bound: 0.3878437280654907, Entropy: 143.2798309326172, Temp: 2.7005293369293213, KL: 72.56077575683594, Loss: 0.020217275246977806, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15675/20000], Bound: 0.359025239944458, Entropy: 141.259765625, Temp: 2.700528621673584, KL: 65.61827087402344, Loss: 0.01779303140938282, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15676/20000], Bound: 0.3817580044269562, Entropy: 141.6712646484375, Temp: 2.7005276679992676, KL: 72.96162414550781, Loss: 0.016195816919207573, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15677/20000], Bound: 0.38459670543670654, Entropy: 141.00196838378906, Temp: 2.700526475906372, KL: 73.79876708984375, Loss: 0.016171880066394806, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15678/20000], Bound: 0.3901994824409485, Entropy: 141.05514526367188, Temp: 2.7005257606506348, KL: 75.11418151855469, Loss: 0.016766896471381187, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15679/20000], Bound: 0.39903661608695984, Entropy: 141.79861450195312, Temp: 2.7005248069763184, KL: 77.03466796875, Loss: 0.018042031675577164, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15680/20000], Bound: 0.3990843594074249, Entropy: 140.33633422851562, Temp: 2.700523853302002, KL: 78.744384765625, Loss: 0.014902801252901554, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15681/20000], Bound: 0.4051998555660248, Entropy: 139.38876342773438, Temp: 2.7005233764648438, KL: 81.41392517089844, Loss: 0.013340775854885578, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15682/20000], Bound: 0.3696209490299225, Entropy: 140.80174255371094, Temp: 2.700523853302002, KL: 70.800048828125, Loss: 0.013743321411311626, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15683/20000], Bound: 0.3704354763031006, Entropy: 141.84768676757812, Temp: 2.7005248069763184, KL: 70.68655395507812, Loss: 0.01438315398991108, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15684/20000], Bound: 0.3725188970565796, Entropy: 140.959716796875, Temp: 2.700525999069214, KL: 71.47193908691406, Loss: 0.01403034571558237, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15685/20000], Bound: 0.37019628286361694, Entropy: 140.39727783203125, Temp: 2.7005271911621094, KL: 70.23536682128906, Loss: 0.015092304907739162, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15686/20000], Bound: 0.3917875587940216, Entropy: 141.0281982421875, Temp: 2.700528621673584, KL: 74.72296142578125, Loss: 0.0183547493070364, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15687/20000], Bound: 0.3816731572151184, Entropy: 142.61656188964844, Temp: 2.7005293369293213, KL: 72.29345703125, Loss: 0.017387457191944122, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15688/20000], Bound: 0.38339969515800476, Entropy: 140.31344604492188, Temp: 2.7005298137664795, KL: 73.23739624023438, Loss: 0.01656702719628811, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15689/20000], Bound: 0.37460586428642273, Entropy: 141.36328125, Temp: 2.7005302906036377, KL: 71.92013549804688, Loss: 0.014306985773146152, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15690/20000], Bound: 0.38189688324928284, Entropy: 143.24288940429688, Temp: 2.700531005859375, KL: 72.69154357910156, Loss: 0.016770433634519577, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15691/20000], Bound: 0.38131973147392273, Entropy: 142.0323028564453, Temp: 2.700531482696533, KL: 72.86351013183594, Loss: 0.016142457723617554, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15692/20000], Bound: 0.4066135883331299, Entropy: 143.13308715820312, Temp: 2.7005319595336914, KL: 79.95515441894531, Loss: 0.01682770624756813, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15693/20000], Bound: 0.3966820538043976, Entropy: 141.56895446777344, Temp: 2.7005324363708496, KL: 75.92556762695312, Loss: 0.018802274018526077, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15694/20000], Bound: 0.41007164120674133, Entropy: 139.50831604003906, Temp: 2.7005326747894287, KL: 79.4583740234375, Loss: 0.019676940515637398, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15695/20000], Bound: 0.3728824853897095, Entropy: 142.68185424804688, Temp: 2.7005324363708496, KL: 69.69491577148438, Loss: 0.017513057217001915, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15696/20000], Bound: 0.3971911370754242, Entropy: 141.7743682861328, Temp: 2.7005321979522705, KL: 77.72705078125, Loss: 0.015746111050248146, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15697/20000], Bound: 0.40131711959838867, Entropy: 141.8762969970703, Temp: 2.7005319595336914, KL: 77.93733215332031, Loss: 0.017627831548452377, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15698/20000], Bound: 0.3871598243713379, Entropy: 140.3524932861328, Temp: 2.7005317211151123, KL: 74.07086181640625, Loss: 0.017051439732313156, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15699/20000], Bound: 0.37602823972702026, Entropy: 142.2554931640625, Temp: 2.700531244277954, KL: 72.39930725097656, Loss: 0.014175781048834324, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15700/20000], Bound: 0.3879353702068329, Entropy: 140.37925720214844, Temp: 2.700531244277954, KL: 72.90058898925781, Loss: 0.019637756049633026, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15701/20000], Bound: 0.3791615962982178, Entropy: 140.96621704101562, Temp: 2.700530529022217, KL: 72.41584777832031, Loss: 0.01581607386469841, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15702/20000], Bound: 0.3978252410888672, Entropy: 140.08135986328125, Temp: 2.7005300521850586, KL: 78.0916748046875, Loss: 0.015419108793139458, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15703/20000], Bound: 0.3814621567726135, Entropy: 140.25726318359375, Temp: 2.7005300521850586, KL: 72.50588989257812, Loss: 0.01688096486032009, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15704/20000], Bound: 0.3540615737438202, Entropy: 142.70677185058594, Temp: 2.7005298137664795, KL: 64.03034973144531, Loss: 0.018164029344916344, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15705/20000], Bound: 0.38656750321388245, Entropy: 141.451416015625, Temp: 2.700529098510742, KL: 75.045654296875, Loss: 0.01492649782449007, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15706/20000], Bound: 0.3763578236103058, Entropy: 141.06427001953125, Temp: 2.700528621673584, KL: 69.3193359375, Loss: 0.020053720101714134, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15707/20000], Bound: 0.372072696685791, Entropy: 140.90408325195312, Temp: 2.7005274295806885, KL: 70.95756530761719, Loss: 0.014746525324881077, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15708/20000], Bound: 0.40844589471817017, Entropy: 140.22750854492188, Temp: 2.700526475906372, KL: 80.81982421875, Loss: 0.016247829422354698, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15709/20000], Bound: 0.39365071058273315, Entropy: 140.8935546875, Temp: 2.700525999069214, KL: 75.70704650878906, Loss: 0.017548399046063423, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15710/20000], Bound: 0.38229694962501526, Entropy: 140.49095153808594, Temp: 2.7005255222320557, KL: 74.85546875, Loss: 0.012978632003068924, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15711/20000], Bound: 0.3774738311767578, Entropy: 138.89657592773438, Temp: 2.7005257606506348, KL: 71.93939208984375, Loss: 0.015797240659594536, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15712/20000], Bound: 0.39819473028182983, Entropy: 143.49053955078125, Temp: 2.700525999069214, KL: 77.20518493652344, Loss: 0.017263423651456833, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15713/20000], Bound: 0.3964357078075409, Entropy: 142.49386596679688, Temp: 2.700525999069214, KL: 77.53526306152344, Loss: 0.01568685658276081, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15714/20000], Bound: 0.377532035112381, Entropy: 141.74046325683594, Temp: 2.700526475906372, KL: 71.37344360351562, Loss: 0.016876136884093285, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15715/20000], Bound: 0.3983299136161804, Entropy: 141.0411834716797, Temp: 2.700526714324951, KL: 78.150146484375, Loss: 0.015588156878948212, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15716/20000], Bound: 0.3899167478084564, Entropy: 144.26791381835938, Temp: 2.7005271911621094, KL: 74.57341003417969, Loss: 0.01761462353169918, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15717/20000], Bound: 0.4104000926017761, Entropy: 140.14508056640625, Temp: 2.7005274295806885, KL: 79.02870178222656, Loss: 0.020656215026974678, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15718/20000], Bound: 0.3640921413898468, Entropy: 140.20481872558594, Temp: 2.7005269527435303, KL: 69.54927062988281, Loss: 0.0131558021530509, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15719/20000], Bound: 0.39451903104782104, Entropy: 140.36996459960938, Temp: 2.7005271911621094, KL: 73.8724365234375, Loss: 0.0214194618165493, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15720/20000], Bound: 0.3738475739955902, Entropy: 142.74847412109375, Temp: 2.700526237487793, KL: 70.15730285644531, Loss: 0.017168376594781876, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15721/20000], Bound: 0.39625540375709534, Entropy: 139.5287628173828, Temp: 2.7005255222320557, KL: 77.25654602050781, Loss: 0.016104072332382202, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15722/20000], Bound: 0.38560572266578674, Entropy: 141.27734375, Temp: 2.7005248069763184, KL: 73.25442504882812, Loss: 0.01772363670170307, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15723/20000], Bound: 0.3760893940925598, Entropy: 140.4152374267578, Temp: 2.700523853302002, KL: 71.97761535644531, Loss: 0.014989014714956284, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15724/20000], Bound: 0.3952735364437103, Entropy: 140.82550048828125, Temp: 2.7005231380462646, KL: 77.1021728515625, Loss: 0.01585218869149685, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15725/20000], Bound: 0.3854161500930786, Entropy: 141.8492889404297, Temp: 2.7005228996276855, KL: 73.19175720214844, Loss: 0.01773740164935589, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15726/20000], Bound: 0.37209752202033997, Entropy: 141.33514404296875, Temp: 2.7005226612091064, KL: 70.27789306640625, Loss: 0.016018066555261612, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15727/20000], Bound: 0.36887606978416443, Entropy: 140.65963745117188, Temp: 2.7005221843719482, KL: 70.59063720703125, Loss: 0.013738549314439297, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15728/20000], Bound: 0.37501028180122375, Entropy: 139.20460510253906, Temp: 2.7005221843719482, KL: 71.68617248535156, Loss: 0.014954881742596626, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15729/20000], Bound: 0.3876867890357971, Entropy: 142.1156463623047, Temp: 2.7005221843719482, KL: 75.19003295898438, Loss: 0.015264270827174187, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15730/20000], Bound: 0.3900917172431946, Entropy: 140.5754852294922, Temp: 2.7005226612091064, KL: 76.16288757324219, Loss: 0.014766654931008816, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15731/20000], Bound: 0.3761001527309418, Entropy: 139.76466369628906, Temp: 2.7005231380462646, KL: 71.92018127441406, Loss: 0.015101084485650063, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15732/20000], Bound: 0.39490219950675964, Entropy: 143.24600219726562, Temp: 2.700524091720581, KL: 76.08039855957031, Loss: 0.017540881410241127, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15733/20000], Bound: 0.39356347918510437, Entropy: 141.67587280273438, Temp: 2.7005248069763184, KL: 75.11273193359375, Loss: 0.018601126968860626, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15734/20000], Bound: 0.37978872656822205, Entropy: 141.94068908691406, Temp: 2.7005250453948975, KL: 72.635009765625, Loss: 0.01574557088315487, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15735/20000], Bound: 0.3655136823654175, Entropy: 142.705322265625, Temp: 2.7005252838134766, KL: 68.2720947265625, Loss: 0.016264814883470535, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15736/20000], Bound: 0.3881404995918274, Entropy: 140.40635681152344, Temp: 2.7005252838134766, KL: 75.39785766601562, Loss: 0.0151250921189785, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15737/20000], Bound: 0.37718382477760315, Entropy: 141.7263946533203, Temp: 2.7005257606506348, KL: 72.29243469238281, Loss: 0.014988977462053299, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15738/20000], Bound: 0.4179344177246094, Entropy: 142.22991943359375, Temp: 2.700525999069214, KL: 82.87129211425781, Loss: 0.017782816663384438, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15739/20000], Bound: 0.3667325973510742, Entropy: 141.41983032226562, Temp: 2.700526714324951, KL: 68.81999206542969, Loss: 0.015889808535575867, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15740/20000], Bound: 0.4267893135547638, Entropy: 140.06439208984375, Temp: 2.7005271911621094, KL: 86.25624084472656, Loss: 0.016563022509217262, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15741/20000], Bound: 0.4078250825405121, Entropy: 141.51779174804688, Temp: 2.700528144836426, KL: 81.02001953125, Loss: 0.015530931763350964, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15742/20000], Bound: 0.39361581206321716, Entropy: 142.72344970703125, Temp: 2.700529098510742, KL: 72.34019470214844, Loss: 0.023763084784150124, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15743/20000], Bound: 0.3996832072734833, Entropy: 141.65525817871094, Temp: 2.700528621673584, KL: 77.78553771972656, Loss: 0.01700781285762787, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15744/20000], Bound: 0.37248581647872925, Entropy: 142.48187255859375, Temp: 2.700528621673584, KL: 70.28001403808594, Loss: 0.016219662502408028, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15745/20000], Bound: 0.3744066059589386, Entropy: 140.51698303222656, Temp: 2.700528144836426, KL: 71.76594543457031, Loss: 0.014486679807305336, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15746/20000], Bound: 0.38621649146080017, Entropy: 142.470703125, Temp: 2.700528144836426, KL: 72.31193542480469, Loss: 0.01979833096265793, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15747/20000], Bound: 0.4001380503177643, Entropy: 143.0937042236328, Temp: 2.7005274295806885, KL: 79.66337585449219, Loss: 0.013781647197902203, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15748/20000], Bound: 0.40779170393943787, Entropy: 142.16246032714844, Temp: 2.7005274295806885, KL: 81.06105041503906, Loss: 0.015436322428286076, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15749/20000], Bound: 0.39217087626457214, Entropy: 142.98207092285156, Temp: 2.7005279064178467, KL: 75.955322265625, Loss: 0.016281813383102417, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15750/20000], Bound: 0.3859447240829468, Entropy: 142.34649658203125, Temp: 2.700528383255005, KL: 74.81648254394531, Loss: 0.015014483593404293, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15751/20000], Bound: 0.3534387946128845, Entropy: 142.0030975341797, Temp: 2.700529098510742, KL: 64.8394775390625, Loss: 0.01634487695991993, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15752/20000], Bound: 0.372264564037323, Entropy: 142.2220916748047, Temp: 2.7005295753479004, KL: 70.26962280273438, Loss: 0.016121841967105865, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15753/20000], Bound: 0.40817058086395264, Entropy: 141.14524841308594, Temp: 2.7005298137664795, KL: 80.56596374511719, Loss: 0.016564298421144485, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15754/20000], Bound: 0.3827619254589081, Entropy: 140.97511291503906, Temp: 2.7005302906036377, KL: 73.74765014648438, Loss: 0.015279504470527172, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15755/20000], Bound: 0.3885822594165802, Entropy: 140.2127685546875, Temp: 2.700531005859375, KL: 72.58990478515625, Loss: 0.020563315600156784, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15756/20000], Bound: 0.38633015751838684, Entropy: 143.066650390625, Temp: 2.700530767440796, KL: 73.80381774902344, Loss: 0.017097540199756622, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15757/20000], Bound: 0.40032079815864563, Entropy: 140.80311584472656, Temp: 2.700530529022217, KL: 77.72576904296875, Loss: 0.017469851300120354, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15758/20000], Bound: 0.37350332736968994, Entropy: 140.4652862548828, Temp: 2.7005302906036377, KL: 71.05796813964844, Loss: 0.015318314544856548, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15759/20000], Bound: 0.37143364548683167, Entropy: 139.9712677001953, Temp: 2.7005300521850586, KL: 68.73208618164062, Loss: 0.018529100343585014, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15760/20000], Bound: 0.4390115737915039, Entropy: 140.11300659179688, Temp: 2.7005293369293213, KL: 87.99687194824219, Loss: 0.0204220712184906, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15761/20000], Bound: 0.3697240650653839, Entropy: 140.42654418945312, Temp: 2.700528621673584, KL: 68.04841613769531, Loss: 0.018892373889684677, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15762/20000], Bound: 0.39810633659362793, Entropy: 140.474365234375, Temp: 2.7005271911621094, KL: 77.01762390136719, Loss: 0.017562124878168106, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15763/20000], Bound: 0.3938739001750946, Entropy: 141.17140197753906, Temp: 2.700525999069214, KL: 76.41020202636719, Loss: 0.01636834815144539, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15764/20000], Bound: 0.40326356887817383, Entropy: 140.3319549560547, Temp: 2.7005248069763184, KL: 79.17289733886719, Loss: 0.016416314989328384, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15765/20000], Bound: 0.40075141191482544, Entropy: 140.31246948242188, Temp: 2.700523853302002, KL: 79.90478515625, Loss: 0.013672802597284317, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15766/20000], Bound: 0.3929588496685028, Entropy: 139.44119262695312, Temp: 2.700523853302002, KL: 75.12159729003906, Loss: 0.018254835158586502, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15767/20000], Bound: 0.3688255548477173, Entropy: 140.53578186035156, Temp: 2.700523614883423, KL: 69.46879577636719, Loss: 0.01578904315829277, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15768/20000], Bound: 0.3684786856174469, Entropy: 143.15660095214844, Temp: 2.7005233764648438, KL: 69.30819702148438, Loss: 0.015903769060969353, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15769/20000], Bound: 0.3554033935070038, Entropy: 141.8358612060547, Temp: 2.7005228996276855, KL: 65.66229248046875, Loss: 0.015835203230381012, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15770/20000], Bound: 0.3798559308052063, Entropy: 140.9638214111328, Temp: 2.7005226612091064, KL: 71.30186462402344, Loss: 0.01824982464313507, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15771/20000], Bound: 0.40713122487068176, Entropy: 141.4776153564453, Temp: 2.70052170753479, KL: 77.36521911621094, Loss: 0.021911079064011574, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15772/20000], Bound: 0.3841983377933502, Entropy: 139.96775817871094, Temp: 2.7005200386047363, KL: 73.63407897949219, Loss: 0.016262207180261612, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15773/20000], Bound: 0.39599767327308655, Entropy: 141.76541137695312, Temp: 2.7005186080932617, KL: 76.46466064453125, Loss: 0.017428969964385033, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15774/20000], Bound: 0.35315507650375366, Entropy: 141.39268493652344, Temp: 2.700517416000366, KL: 66.21371459960938, Loss: 0.013654212467372417, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15775/20000], Bound: 0.3825254440307617, Entropy: 140.47743225097656, Temp: 2.70051646232605, KL: 73.80827331542969, Loss: 0.015040104277431965, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15776/20000], Bound: 0.4035746157169342, Entropy: 143.36412048339844, Temp: 2.7005157470703125, KL: 78.79934692382812, Loss: 0.017280155792832375, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15777/20000], Bound: 0.39970001578330994, Entropy: 141.24234008789062, Temp: 2.700515031814575, KL: 76.67625427246094, Loss: 0.019070807844400406, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15778/20000], Bound: 0.39359477162361145, Entropy: 141.28883361816406, Temp: 2.700514078140259, KL: 76.57560729980469, Loss: 0.015909599140286446, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15779/20000], Bound: 0.3296283781528473, Entropy: 143.6778564453125, Temp: 2.7005133628845215, KL: 59.281646728515625, Loss: 0.01456378772854805, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15780/20000], Bound: 0.375278115272522, Entropy: 139.88609313964844, Temp: 2.7005128860473633, KL: 70.40614318847656, Loss: 0.017467085272073746, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15781/20000], Bound: 0.38594818115234375, Entropy: 141.97021484375, Temp: 2.700511932373047, KL: 72.91712951660156, Loss: 0.018532853573560715, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15782/20000], Bound: 0.3876427710056305, Entropy: 142.44725036621094, Temp: 2.7005105018615723, KL: 75.10586547851562, Loss: 0.015396148897707462, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15783/20000], Bound: 0.3803711533546448, Entropy: 140.00357055664062, Temp: 2.700509548187256, KL: 73.60939025878906, Loss: 0.014253085479140282, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15784/20000], Bound: 0.3726915717124939, Entropy: 140.49574279785156, Temp: 2.7005090713500977, KL: 71.26388549804688, Loss: 0.014506792649626732, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15785/20000], Bound: 0.4020550847053528, Entropy: 141.2960662841797, Temp: 2.7005090713500977, KL: 78.13145446777344, Loss: 0.017675848677754402, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15786/20000], Bound: 0.38357746601104736, Entropy: 140.56883239746094, Temp: 2.7005088329315186, KL: 71.85740661621094, Loss: 0.019217483699321747, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15787/20000], Bound: 0.3785114884376526, Entropy: 142.19024658203125, Temp: 2.7005081176757812, KL: 71.61111450195312, Loss: 0.016958562657237053, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15788/20000], Bound: 0.37797442078590393, Entropy: 142.3223419189453, Temp: 2.700507164001465, KL: 71.36210632324219, Loss: 0.01713290624320507, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15789/20000], Bound: 0.3901357650756836, Entropy: 141.0602264404297, Temp: 2.7005062103271484, KL: 76.28575134277344, Loss: 0.014562949538230896, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15790/20000], Bound: 0.4081577658653259, Entropy: 139.86842346191406, Temp: 2.7005057334899902, KL: 79.906494140625, Loss: 0.017777903005480766, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15791/20000], Bound: 0.399040162563324, Entropy: 141.37933349609375, Temp: 2.700505495071411, KL: 76.97882080078125, Loss: 0.018147271126508713, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15792/20000], Bound: 0.39313751459121704, Entropy: 140.9795379638672, Temp: 2.700504779815674, KL: 75.16505432128906, Loss: 0.018271656706929207, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15793/20000], Bound: 0.38321489095687866, Entropy: 140.5832061767578, Temp: 2.7005038261413574, KL: 74.07293701171875, Loss: 0.01492045633494854, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15794/20000], Bound: 0.4121675491333008, Entropy: 139.52821350097656, Temp: 2.700503349304199, KL: 81.72412109375, Loss: 0.01665601134300232, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15795/20000], Bound: 0.3750612735748291, Entropy: 141.2632293701172, Temp: 2.70050311088562, KL: 69.87260437011719, Loss: 0.018339652568101883, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15796/20000], Bound: 0.3606012165546417, Entropy: 141.76760864257812, Temp: 2.700502634048462, KL: 68.45372009277344, Loss: 0.013362421654164791, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15797/20000], Bound: 0.3751884400844574, Entropy: 143.0926971435547, Temp: 2.700502395629883, KL: 70.61477661132812, Loss: 0.017033087089657784, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15798/20000], Bound: 0.3858208656311035, Entropy: 141.2204132080078, Temp: 2.7005019187927246, KL: 74.63040161132812, Loss: 0.015291946940124035, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15799/20000], Bound: 0.3717464804649353, Entropy: 142.3261260986328, Temp: 2.7005016803741455, KL: 70.11279296875, Loss: 0.016137905418872833, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15800/20000], Bound: 0.4189901053905487, Entropy: 142.6266632080078, Temp: 2.7005014419555664, KL: 83.78573608398438, Loss: 0.016687599942088127, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15801/20000], Bound: 0.36164000630378723, Entropy: 143.64407348632812, Temp: 2.7005016803741455, KL: 68.17857360839844, Loss: 0.014412965625524521, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15802/20000], Bound: 0.38749101758003235, Entropy: 141.06280517578125, Temp: 2.7005019187927246, KL: 74.8311767578125, Loss: 0.015822533518075943, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15803/20000], Bound: 0.3754350244998932, Entropy: 139.93728637695312, Temp: 2.700502395629883, KL: 72.64813232421875, Loss: 0.013399357907474041, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15804/20000], Bound: 0.400251567363739, Entropy: 141.42539978027344, Temp: 2.70050311088562, KL: 78.29425048828125, Loss: 0.01637886092066765, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15805/20000], Bound: 0.39309945702552795, Entropy: 139.68051147460938, Temp: 2.7005040645599365, KL: 75.72538757324219, Loss: 0.017213454470038414, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15806/20000], Bound: 0.3779260218143463, Entropy: 141.05776977539062, Temp: 2.700505018234253, KL: 69.03726196289062, Loss: 0.02141154184937477, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15807/20000], Bound: 0.3621272146701813, Entropy: 142.81602478027344, Temp: 2.7005045413970947, KL: 67.27752685546875, Loss: 0.01633531227707863, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15808/20000], Bound: 0.4155590832233429, Entropy: 141.43026733398438, Temp: 2.7005040645599365, KL: 83.67454528808594, Loss: 0.014953025616705418, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15809/20000], Bound: 0.3710504472255707, Entropy: 142.4362335205078, Temp: 2.7005043029785156, KL: 68.36541748046875, Loss: 0.01900533214211464, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15810/20000], Bound: 0.37485963106155396, Entropy: 139.22006225585938, Temp: 2.7005038261413574, KL: 69.61064147949219, Loss: 0.018717586994171143, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15811/20000], Bound: 0.4136880338191986, Entropy: 140.41819763183594, Temp: 2.700502872467041, KL: 82.80162048339844, Loss: 0.015515257604420185, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15812/20000], Bound: 0.41045162081718445, Entropy: 141.13458251953125, Temp: 2.700502395629883, KL: 82.22378540039062, Loss: 0.014769129455089569, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15813/20000], Bound: 0.3662724196910858, Entropy: 140.34475708007812, Temp: 2.700502634048462, KL: 68.81704711914062, Loss: 0.015653537586331367, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15814/20000], Bound: 0.39191684126853943, Entropy: 141.88693237304688, Temp: 2.700502634048462, KL: 74.74853515625, Loss: 0.01837761700153351, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15815/20000], Bound: 0.37901571393013, Entropy: 138.8329620361328, Temp: 2.700502395629883, KL: 71.07273864746094, Loss: 0.01822466216981411, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15816/20000], Bound: 0.403154194355011, Entropy: 140.847900390625, Temp: 2.7005016803741455, KL: 79.15948486328125, Loss: 0.01638038083910942, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15817/20000], Bound: 0.37006592750549316, Entropy: 140.4737091064453, Temp: 2.7005012035369873, KL: 69.93885803222656, Loss: 0.015572340227663517, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15818/20000], Bound: 0.38579636812210083, Entropy: 140.24365234375, Temp: 2.700500965118408, KL: 74.55267333984375, Loss: 0.01542259193956852, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15819/20000], Bound: 0.38489651679992676, Entropy: 141.95460510253906, Temp: 2.700500965118408, KL: 74.14385986328125, Loss: 0.01569424755871296, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15820/20000], Bound: 0.4135760962963104, Entropy: 141.25048828125, Temp: 2.700500965118408, KL: 81.69036865234375, Loss: 0.01750975288450718, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15821/20000], Bound: 0.38971850275993347, Entropy: 141.09767150878906, Temp: 2.7005012035369873, KL: 74.26860046386719, Loss: 0.018071124330163002, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15822/20000], Bound: 0.38500097393989563, Entropy: 138.75462341308594, Temp: 2.700500965118408, KL: 74.88970947265625, Loss: 0.014369591139256954, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15823/20000], Bound: 0.38684961199760437, Entropy: 140.75975036621094, Temp: 2.7005012035369873, KL: 75.64796447753906, Loss: 0.013963460922241211, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15824/20000], Bound: 0.39084383845329285, Entropy: 141.52886962890625, Temp: 2.7005021572113037, KL: 74.93983459472656, Loss: 0.017439600080251694, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15825/20000], Bound: 0.38575834035873413, Entropy: 142.1173553466797, Temp: 2.700502872467041, KL: 75.09561157226562, Loss: 0.014396864920854568, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15826/20000], Bound: 0.40213504433631897, Entropy: 140.7329864501953, Temp: 2.7005035877227783, KL: 78.81599426269531, Loss: 0.01645256020128727, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15827/20000], Bound: 0.379095196723938, Entropy: 142.5028076171875, Temp: 2.700504779815674, KL: 71.92111206054688, Loss: 0.016696393489837646, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15828/20000], Bound: 0.4013880491256714, Entropy: 141.69558715820312, Temp: 2.700505495071411, KL: 78.58882141113281, Loss: 0.016460485756397247, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15829/20000], Bound: 0.40699735283851624, Entropy: 142.16107177734375, Temp: 2.7005064487457275, KL: 80.00361633300781, Loss: 0.016951389610767365, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15830/20000], Bound: 0.39940303564071655, Entropy: 141.14402770996094, Temp: 2.700507402420044, KL: 77.16177368164062, Loss: 0.01800823211669922, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15831/20000], Bound: 0.40004977583885193, Entropy: 140.07650756835938, Temp: 2.7005081176757812, KL: 78.16177368164062, Loss: 0.01651298813521862, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15832/20000], Bound: 0.3887852132320404, Entropy: 140.43357849121094, Temp: 2.7005088329315186, KL: 75.84413146972656, Loss: 0.014647933654487133, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15833/20000], Bound: 0.4039193093776703, Entropy: 140.99813842773438, Temp: 2.700509786605835, KL: 77.34571838378906, Loss: 0.020162472501397133, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15834/20000], Bound: 0.3865349292755127, Entropy: 140.02243041992188, Temp: 2.700510263442993, KL: 73.92535400390625, Loss: 0.016982920467853546, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15835/20000], Bound: 0.3870714008808136, Entropy: 138.39117431640625, Temp: 2.7005105018615723, KL: 75.47994995117188, Loss: 0.014394531957805157, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15836/20000], Bound: 0.36963436007499695, Entropy: 141.43045043945312, Temp: 2.7005112171173096, KL: 70.04704284667969, Loss: 0.015144502744078636, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15837/20000], Bound: 0.41462910175323486, Entropy: 139.89605712890625, Temp: 2.700511932373047, KL: 82.1824951171875, Loss: 0.01719142682850361, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15838/20000], Bound: 0.3998397886753082, Entropy: 141.5425567626953, Temp: 2.7005128860473633, KL: 77.46463012695312, Loss: 0.017688101157546043, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15839/20000], Bound: 0.38765060901641846, Entropy: 141.69363403320312, Temp: 2.7005133628845215, KL: 73.58038330078125, Loss: 0.018224848434329033, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15840/20000], Bound: 0.38797351717948914, Entropy: 141.67140197753906, Temp: 2.7005136013031006, KL: 75.20703125, Loss: 0.015387889929115772, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15841/20000], Bound: 0.3781658411026001, Entropy: 139.00135803222656, Temp: 2.700514078140259, KL: 71.01747131347656, Loss: 0.01787322573363781, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15842/20000], Bound: 0.38587772846221924, Entropy: 142.20628356933594, Temp: 2.700514078140259, KL: 73.48445129394531, Loss: 0.01744445599615574, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15843/20000], Bound: 0.3803271949291229, Entropy: 141.20623779296875, Temp: 2.7005138397216797, KL: 72.12937927246094, Loss: 0.016969852149486542, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15844/20000], Bound: 0.40402308106422424, Entropy: 142.03610229492188, Temp: 2.7005136013031006, KL: 80.00813293457031, Loss: 0.015290563926100731, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15845/20000], Bound: 0.38467350602149963, Entropy: 140.54551696777344, Temp: 2.7005138397216797, KL: 73.53938293457031, Loss: 0.016693396493792534, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15846/20000], Bound: 0.38287681341171265, Entropy: 141.33946228027344, Temp: 2.7005138397216797, KL: 73.13467407226562, Loss: 0.01647597923874855, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15847/20000], Bound: 0.3928714096546173, Entropy: 140.83596801757812, Temp: 2.7005138397216797, KL: 76.09500122070312, Loss: 0.016404809430241585, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15848/20000], Bound: 0.3701857328414917, Entropy: 141.79556274414062, Temp: 2.700514078140259, KL: 69.91740417480469, Loss: 0.015675341710448265, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15849/20000], Bound: 0.38719457387924194, Entropy: 140.74855041503906, Temp: 2.700514316558838, KL: 73.15501403808594, Loss: 0.018765784800052643, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15850/20000], Bound: 0.3922726809978485, Entropy: 140.4484100341797, Temp: 2.7005138397216797, KL: 75.40763854980469, Loss: 0.017351167276501656, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15851/20000], Bound: 0.37838202714920044, Entropy: 141.18943786621094, Temp: 2.7005133628845215, KL: 72.71876525878906, Loss: 0.014838640578091145, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15852/20000], Bound: 0.38359957933425903, Entropy: 141.86863708496094, Temp: 2.7005131244659424, KL: 73.25886535644531, Loss: 0.01663460023701191, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15853/20000], Bound: 0.39469119906425476, Entropy: 140.30068969726562, Temp: 2.7005128860473633, KL: 74.87455749511719, Loss: 0.019658032804727554, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15854/20000], Bound: 0.37579864263534546, Entropy: 141.59800720214844, Temp: 2.700512409210205, KL: 71.80023193359375, Loss: 0.01516269613057375, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15855/20000], Bound: 0.37741032242774963, Entropy: 142.95497131347656, Temp: 2.700511932373047, KL: 73.28759765625, Loss: 0.01326703280210495, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15856/20000], Bound: 0.3668223023414612, Entropy: 142.38417053222656, Temp: 2.700512170791626, KL: 69.82215881347656, Loss: 0.014081296510994434, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15857/20000], Bound: 0.39473479986190796, Entropy: 140.809814453125, Temp: 2.700512647628784, KL: 76.89794921875, Loss: 0.015935570001602173, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15858/20000], Bound: 0.3714827299118042, Entropy: 141.44371032714844, Temp: 2.7005131244659424, KL: 70.33444213867188, Loss: 0.015588182024657726, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15859/20000], Bound: 0.37183278799057007, Entropy: 140.48760986328125, Temp: 2.7005136013031006, KL: 70.18031311035156, Loss: 0.01605863682925701, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15860/20000], Bound: 0.37328335642814636, Entropy: 142.00897216796875, Temp: 2.700514078140259, KL: 70.54071044921875, Loss: 0.0161593034863472, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15861/20000], Bound: 0.3969701826572418, Entropy: 139.21609497070312, Temp: 2.700514316558838, KL: 78.3076171875, Loss: 0.014549757353961468, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15862/20000], Bound: 0.3700982928276062, Entropy: 140.63206481933594, Temp: 2.7005152702331543, KL: 69.8001708984375, Loss: 0.01584629714488983, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15863/20000], Bound: 0.38761553168296814, Entropy: 141.4991912841797, Temp: 2.7005157470703125, KL: 73.77999877929688, Loss: 0.017836302518844604, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15864/20000], Bound: 0.3715071678161621, Entropy: 140.82447814941406, Temp: 2.70051646232605, KL: 69.88668823242188, Loss: 0.01643010787665844, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15865/20000], Bound: 0.3913689851760864, Entropy: 141.27450561523438, Temp: 2.70051646232605, KL: 75.35923767089844, Loss: 0.01694880612194538, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15866/20000], Bound: 0.3660721182823181, Entropy: 143.41065979003906, Temp: 2.700516700744629, KL: 68.68807983398438, Loss: 0.015787344425916672, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15867/20000], Bound: 0.3999955356121063, Entropy: 141.88528442382812, Temp: 2.700516700744629, KL: 77.68275451660156, Loss: 0.017370086163282394, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15868/20000], Bound: 0.3601190447807312, Entropy: 143.9248809814453, Temp: 2.700516700744629, KL: 68.32325744628906, Loss: 0.013353237882256508, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15869/20000], Bound: 0.41484037041664124, Entropy: 142.364013671875, Temp: 2.700517177581787, KL: 81.62037658691406, Loss: 0.018351247534155846, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15870/20000], Bound: 0.3740672171115875, Entropy: 142.52963256835938, Temp: 2.700517416000366, KL: 70.17604064941406, Loss: 0.017250176519155502, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15871/20000], Bound: 0.3946080207824707, Entropy: 142.48191833496094, Temp: 2.700517416000366, KL: 75.81544494628906, Loss: 0.017870543524622917, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15872/20000], Bound: 0.3657001554965973, Entropy: 142.0906982421875, Temp: 2.700517177581787, KL: 68.55255126953125, Loss: 0.01584324613213539, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15873/20000], Bound: 0.387362539768219, Entropy: 142.7511749267578, Temp: 2.700516939163208, KL: 74.54776000976562, Loss: 0.016277985647320747, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15874/20000], Bound: 0.37927672266960144, Entropy: 141.86659240722656, Temp: 2.700516700744629, KL: 72.93231201171875, Loss: 0.014921274967491627, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15875/20000], Bound: 0.3726486265659332, Entropy: 142.5596466064453, Temp: 2.700516700744629, KL: 71.08204650878906, Loss: 0.014820809476077557, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15876/20000], Bound: 0.3678552806377411, Entropy: 139.67835998535156, Temp: 2.700516939163208, KL: 69.16694641113281, Loss: 0.015837229788303375, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15877/20000], Bound: 0.39884498715400696, Entropy: 139.995361328125, Temp: 2.700517177581787, KL: 77.92976379394531, Loss: 0.01627928763628006, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15878/20000], Bound: 0.3810116946697235, Entropy: 141.61505126953125, Temp: 2.700517416000366, KL: 74.78619384765625, Loss: 0.012417389079928398, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15879/20000], Bound: 0.36760029196739197, Entropy: 142.39889526367188, Temp: 2.7005186080932617, KL: 68.99356079101562, Loss: 0.01602422446012497, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15880/20000], Bound: 0.3808766305446625, Entropy: 138.05433654785156, Temp: 2.700519323348999, KL: 72.58221435546875, Loss: 0.016425689682364464, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15881/20000], Bound: 0.3911290168762207, Entropy: 142.17800903320312, Temp: 2.7005200386047363, KL: 73.84086608886719, Loss: 0.0196295864880085, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15882/20000], Bound: 0.3646252751350403, Entropy: 143.69113159179688, Temp: 2.7005200386047363, KL: 66.34848022460938, Loss: 0.019360961392521858, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15883/20000], Bound: 0.39402464032173157, Entropy: 142.9127655029297, Temp: 2.700519323348999, KL: 77.36175537109375, Loss: 0.014688813127577305, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15884/20000], Bound: 0.36516016721725464, Entropy: 141.4087677001953, Temp: 2.70051908493042, KL: 69.24493408203125, Loss: 0.01427830196917057, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15885/20000], Bound: 0.38922369480133057, Entropy: 142.4903564453125, Temp: 2.70051908493042, KL: 75.27084350585938, Loss: 0.015947207808494568, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15886/20000], Bound: 0.409767746925354, Entropy: 142.29331970214844, Temp: 2.700519323348999, KL: 81.28941345214844, Loss: 0.01611672341823578, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15887/20000], Bound: 0.3865388333797455, Entropy: 140.86436462402344, Temp: 2.7005198001861572, KL: 74.55899047851562, Loss: 0.015811961144208908, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15888/20000], Bound: 0.34895986318588257, Entropy: 144.66513061523438, Temp: 2.7005205154418945, KL: 64.04708862304688, Loss: 0.015510966069996357, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15889/20000], Bound: 0.35735803842544556, Entropy: 140.61293029785156, Temp: 2.7005207538604736, KL: 65.68592834472656, Loss: 0.016802847385406494, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15890/20000], Bound: 0.37216421961784363, Entropy: 139.8494110107422, Temp: 2.7005207538604736, KL: 70.51605224609375, Loss: 0.015612407587468624, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15891/20000], Bound: 0.39080610871315, Entropy: 141.8406219482422, Temp: 2.7005207538604736, KL: 76.55471801757812, Loss: 0.014429314061999321, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15892/20000], Bound: 0.38764336705207825, Entropy: 140.72132873535156, Temp: 2.700521230697632, KL: 74.91433715820312, Loss: 0.01575120911002159, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15893/20000], Bound: 0.3706618547439575, Entropy: 141.4304962158203, Temp: 2.70052170753479, KL: 70.175048828125, Loss: 0.01544968131929636, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15894/20000], Bound: 0.38905391097068787, Entropy: 142.85891723632812, Temp: 2.7005221843719482, KL: 74.75387573242188, Loss: 0.016812318935990334, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15895/20000], Bound: 0.39936336874961853, Entropy: 140.5983428955078, Temp: 2.7005226612091064, KL: 76.75791931152344, Loss: 0.018734270706772804, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15896/20000], Bound: 0.3631686866283417, Entropy: 142.782470703125, Temp: 2.7005226612091064, KL: 68.01301574707031, Loss: 0.015517381951212883, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15897/20000], Bound: 0.3614797592163086, Entropy: 142.34112548828125, Temp: 2.7005226612091064, KL: 67.16226196289062, Loss: 0.01621132344007492, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15898/20000], Bound: 0.3925613760948181, Entropy: 140.33897399902344, Temp: 2.7005224227905273, KL: 75.09793090820312, Loss: 0.018081970512866974, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15899/20000], Bound: 0.37168166041374207, Entropy: 139.5322265625, Temp: 2.700521945953369, KL: 71.40910339355469, Loss: 0.013703658245503902, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15900/20000], Bound: 0.39575526118278503, Entropy: 141.10183715820312, Temp: 2.700521945953369, KL: 75.0994873046875, Loss: 0.019823839887976646, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15901/20000], Bound: 0.38024356961250305, Entropy: 139.3856964111328, Temp: 2.700521469116211, KL: 72.14663696289062, Loss: 0.01689312607049942, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15902/20000], Bound: 0.3787948489189148, Entropy: 141.88168334960938, Temp: 2.7005207538604736, KL: 71.63972473144531, Loss: 0.017057038843631744, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15903/20000], Bound: 0.36958110332489014, Entropy: 141.2825164794922, Temp: 2.7005200386047363, KL: 68.83261108398438, Loss: 0.017365023493766785, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15904/20000], Bound: 0.3848349153995514, Entropy: 142.35507202148438, Temp: 2.700518846511841, KL: 74.55226135253906, Loss: 0.014905053190886974, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15905/20000], Bound: 0.3949451744556427, Entropy: 141.7470703125, Temp: 2.7005183696746826, KL: 75.7421875, Loss: 0.018190545961260796, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15906/20000], Bound: 0.40211600065231323, Entropy: 140.5765838623047, Temp: 2.700517416000366, KL: 79.75364685058594, Loss: 0.014706109650433064, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15907/20000], Bound: 0.37711700797080994, Entropy: 141.34861755371094, Temp: 2.700517177581787, KL: 71.25570678710938, Loss: 0.0168728306889534, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15908/20000], Bound: 0.40301594138145447, Entropy: 141.62216186523438, Temp: 2.700516700744629, KL: 76.84574890136719, Loss: 0.020587870851159096, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15909/20000], Bound: 0.39801669120788574, Entropy: 141.2831268310547, Temp: 2.7005157470703125, KL: 75.06268310546875, Loss: 0.021132314577698708, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15910/20000], Bound: 0.40137040615081787, Entropy: 141.75477600097656, Temp: 2.700514316558838, KL: 78.79435729980469, Loss: 0.01607029139995575, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15911/20000], Bound: 0.38004761934280396, Entropy: 140.93435668945312, Temp: 2.7005128860473633, KL: 72.28964233398438, Loss: 0.016523441299796104, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15912/20000], Bound: 0.3850021958351135, Entropy: 140.99203491210938, Temp: 2.700511932373047, KL: 72.7498779296875, Loss: 0.01833227649331093, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15913/20000], Bound: 0.38438889384269714, Entropy: 141.1219940185547, Temp: 2.700510263442993, KL: 73.90548706054688, Loss: 0.015862220898270607, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15914/20000], Bound: 0.3936588764190674, Entropy: 142.10267639160156, Temp: 2.7005090713500977, KL: 76.97575378417969, Loss: 0.015203690156340599, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15915/20000], Bound: 0.3811090588569641, Entropy: 138.69944763183594, Temp: 2.7005085945129395, KL: 73.78999328613281, Loss: 0.014313995838165283, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15916/20000], Bound: 0.3796611726284027, Entropy: 140.3134307861328, Temp: 2.7005083560943604, KL: 71.48602294921875, Loss: 0.017804566770792007, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15917/20000], Bound: 0.41022589802742004, Entropy: 140.1052703857422, Temp: 2.700507640838623, KL: 79.9378662109375, Loss: 0.018875254318118095, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15918/20000], Bound: 0.4049246311187744, Entropy: 140.42630004882812, Temp: 2.7005069255828857, KL: 79.70693969726562, Loss: 0.016348276287317276, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15919/20000], Bound: 0.3924412429332733, Entropy: 140.67098999023438, Temp: 2.7005064487457275, KL: 77.20072937011719, Loss: 0.014123037457466125, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15920/20000], Bound: 0.3950659930706024, Entropy: 140.17465209960938, Temp: 2.7005066871643066, KL: 75.98233032226562, Loss: 0.017811931669712067, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15921/20000], Bound: 0.3759330213069916, Entropy: 141.89138793945312, Temp: 2.7005066871643066, KL: 71.28692626953125, Loss: 0.016184525564312935, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15922/20000], Bound: 0.3742101788520813, Entropy: 141.815673828125, Temp: 2.7005066871643066, KL: 71.38119506835938, Loss: 0.015094582922756672, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15923/20000], Bound: 0.36237606406211853, Entropy: 142.45603942871094, Temp: 2.7005066871643066, KL: 66.83723449707031, Loss: 0.017280416563153267, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15924/20000], Bound: 0.4018568992614746, Entropy: 140.6024627685547, Temp: 2.7005062103271484, KL: 78.42826843261719, Loss: 0.017016729339957237, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15925/20000], Bound: 0.36706721782684326, Entropy: 140.8221435546875, Temp: 2.7005059719085693, KL: 68.74526977539062, Loss: 0.016203731298446655, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15926/20000], Bound: 0.3749854862689972, Entropy: 140.45425415039062, Temp: 2.700505495071411, KL: 70.70582580566406, Loss: 0.01675666868686676, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15927/20000], Bound: 0.38932690024375916, Entropy: 140.66493225097656, Temp: 2.700505256652832, KL: 76.16397094726562, Loss: 0.014349393546581268, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15928/20000], Bound: 0.3943047821521759, Entropy: 144.0900421142578, Temp: 2.700505256652832, KL: 76.60221862792969, Loss: 0.016248006373643875, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15929/20000], Bound: 0.3885064721107483, Entropy: 139.31936645507812, Temp: 2.700505495071411, KL: 75.87408447265625, Loss: 0.014441384933888912, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15930/20000], Bound: 0.3771699070930481, Entropy: 142.23269653320312, Temp: 2.7005059719085693, KL: 72.073974609375, Loss: 0.015385877341032028, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15931/20000], Bound: 0.36116549372673035, Entropy: 142.25784301757812, Temp: 2.7005066871643066, KL: 66.11376953125, Loss: 0.01798872835934162, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15932/20000], Bound: 0.40335384011268616, Entropy: 141.5934295654297, Temp: 2.7005066871643066, KL: 78.32566833496094, Loss: 0.018034813925623894, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15933/20000], Bound: 0.4010714590549469, Entropy: 141.81649780273438, Temp: 2.7005064487457275, KL: 77.39935302734375, Loss: 0.018488068133592606, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15934/20000], Bound: 0.3641000986099243, Entropy: 142.81971740722656, Temp: 2.7005059719085693, KL: 67.75140380859375, Loss: 0.016488535329699516, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15935/20000], Bound: 0.3639269471168518, Entropy: 140.5232391357422, Temp: 2.700505495071411, KL: 67.79875183105469, Loss: 0.016310332342982292, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15936/20000], Bound: 0.3896959722042084, Entropy: 141.8977508544922, Temp: 2.700505018234253, KL: 73.91094970703125, Loss: 0.018721166998147964, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15937/20000], Bound: 0.36030569672584534, Entropy: 141.10726928710938, Temp: 2.7005038261413574, KL: 68.19862365722656, Loss: 0.013680975884199142, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15938/20000], Bound: 0.37365853786468506, Entropy: 141.98008728027344, Temp: 2.70050311088562, KL: 70.95445251464844, Loss: 0.015592031180858612, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15939/20000], Bound: 0.38090577721595764, Entropy: 143.130126953125, Temp: 2.700502872467041, KL: 74.02235412597656, Loss: 0.013774729333817959, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15940/20000], Bound: 0.3620329797267914, Entropy: 140.9209747314453, Temp: 2.700502872467041, KL: 66.96641540527344, Loss: 0.016862208023667336, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15941/20000], Bound: 0.3951634466648102, Entropy: 140.00421142578125, Temp: 2.700502634048462, KL: 74.90437316894531, Loss: 0.01986102946102619, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15942/20000], Bound: 0.4118955731391907, Entropy: 139.62229919433594, Temp: 2.7005016803741455, KL: 80.77845764160156, Loss: 0.018254276365041733, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15943/20000], Bound: 0.3920453190803528, Entropy: 140.49107360839844, Temp: 2.700500726699829, KL: 76.1800537109375, Loss: 0.01579708233475685, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15944/20000], Bound: 0.35165464878082275, Entropy: 141.5135498046875, Temp: 2.700500011444092, KL: 63.66038513183594, Loss: 0.017609497532248497, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15945/20000], Bound: 0.3885124623775482, Entropy: 141.1431427001953, Temp: 2.7004990577697754, KL: 76.15476989746094, Loss: 0.01392489392310381, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15946/20000], Bound: 0.3635159432888031, Entropy: 141.47998046875, Temp: 2.700498580932617, KL: 68.89138793945312, Loss: 0.014072343707084656, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15947/20000], Bound: 0.3928777873516083, Entropy: 141.46400451660156, Temp: 2.700498342514038, KL: 76.16567993164062, Loss: 0.016277283430099487, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15948/20000], Bound: 0.37735408544540405, Entropy: 140.88699340820312, Temp: 2.700498342514038, KL: 70.88626098632812, Loss: 0.017683029174804688, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15949/20000], Bound: 0.38948071002960205, Entropy: 142.4514617919922, Temp: 2.70049786567688, KL: 75.48997497558594, Loss: 0.015680694952607155, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15950/20000], Bound: 0.37722155451774597, Entropy: 140.84544372558594, Temp: 2.700497627258301, KL: 69.03935241699219, Loss: 0.021031970158219337, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15951/20000], Bound: 0.3925287127494812, Entropy: 142.8446502685547, Temp: 2.7004964351654053, KL: 76.32467651367188, Loss: 0.015792591497302055, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15952/20000], Bound: 0.3889165222644806, Entropy: 141.36106872558594, Temp: 2.700495719909668, KL: 73.95353698730469, Loss: 0.018219441175460815, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15953/20000], Bound: 0.3811590373516083, Entropy: 140.23828125, Temp: 2.7004945278167725, KL: 72.9002685546875, Loss: 0.015987953171133995, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15954/20000], Bound: 0.37306538224220276, Entropy: 139.98190307617188, Temp: 2.700493574142456, KL: 71.61087036132812, Loss: 0.014062201604247093, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15955/20000], Bound: 0.38545721769332886, Entropy: 139.91343688964844, Temp: 2.700493335723877, KL: 73.65715026855469, Loss: 0.016897596418857574, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15956/20000], Bound: 0.3842330873012543, Entropy: 141.6376953125, Temp: 2.7004926204681396, KL: 74.365478515625, Loss: 0.014926478266716003, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15957/20000], Bound: 0.37755632400512695, Entropy: 140.01470947265625, Temp: 2.7004926204681396, KL: 72.49320983886719, Loss: 0.01481551956385374, Learning Rate: 4.023205858991889e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15958/20000], Bound: 0.40180641412734985, Entropy: 138.67379760742188, Temp: 2.7004926204681396, KL: 78.67674255371094, Loss: 0.016528675332665443, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15959/20000], Bound: 0.37923431396484375, Entropy: 141.4738006591797, Temp: 2.7004928588867188, KL: 71.113037109375, Loss: 0.01826678402721882, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15960/20000], Bound: 0.3992299735546112, Entropy: 140.75596618652344, Temp: 2.7004926204681396, KL: 77.53184509277344, Loss: 0.017227644100785255, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15961/20000], Bound: 0.391070693731308, Entropy: 142.3458709716797, Temp: 2.7004926204681396, KL: 74.65301513671875, Loss: 0.018093910068273544, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15962/20000], Bound: 0.3750203549861908, Entropy: 142.06251525878906, Temp: 2.7004921436309814, KL: 70.4295654296875, Loss: 0.017286598682403564, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15963/20000], Bound: 0.33557412028312683, Entropy: 142.15463256835938, Temp: 2.700491428375244, KL: 60.805328369140625, Loss: 0.014720210805535316, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15964/20000], Bound: 0.39209744334220886, Entropy: 141.623046875, Temp: 2.700490713119507, KL: 76.21400451660156, Loss: 0.015762493014335632, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15965/20000], Bound: 0.3992207944393158, Entropy: 142.20587158203125, Temp: 2.7004902362823486, KL: 77.40498352050781, Loss: 0.017457449808716774, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15966/20000], Bound: 0.3717529773712158, Entropy: 140.94422912597656, Temp: 2.7004897594451904, KL: 69.11526489257812, Loss: 0.017988184466958046, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15967/20000], Bound: 0.3916078507900238, Entropy: 141.84165954589844, Temp: 2.700489044189453, KL: 74.52909851074219, Loss: 0.018615568056702614, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15968/20000], Bound: 0.36635300517082214, Entropy: 140.9830322265625, Temp: 2.7004878520965576, KL: 68.27301025390625, Loss: 0.016703017055988312, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15969/20000], Bound: 0.3750649094581604, Entropy: 140.44993591308594, Temp: 2.700486421585083, KL: 71.33076477050781, Loss: 0.01564163714647293, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15970/20000], Bound: 0.39324522018432617, Entropy: 141.99241638183594, Temp: 2.7004854679107666, KL: 76.61238098144531, Loss: 0.015650497749447823, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15971/20000], Bound: 0.38146308064460754, Entropy: 143.91317749023438, Temp: 2.7004847526550293, KL: 72.01763916015625, Loss: 0.017785081639885902, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15972/20000], Bound: 0.39126595854759216, Entropy: 142.40321350097656, Temp: 2.700483560562134, KL: 74.77299499511719, Loss: 0.017977913841605186, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15973/20000], Bound: 0.38399919867515564, Entropy: 141.7497100830078, Temp: 2.7004823684692383, KL: 73.08184814453125, Loss: 0.017177145928144455, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15974/20000], Bound: 0.3893262445926666, Entropy: 141.68539428710938, Temp: 2.700481414794922, KL: 75.34396362304688, Loss: 0.015867087990045547, Learning Rate: 4.023205858991889e-06\n",
      "Epoch [15975/20000], Bound: 0.3817068338394165, Entropy: 140.62033081054688, Temp: 2.7004806995391846, KL: 72.60629272460938, Loss: 0.016825858503580093, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15976/20000], Bound: 0.3690249025821686, Entropy: 141.8373565673828, Temp: 2.7004799842834473, KL: 69.89787292480469, Loss: 0.015099252574145794, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15977/20000], Bound: 0.37317126989364624, Entropy: 142.92227172851562, Temp: 2.700479745864868, KL: 72.01847839355469, Loss: 0.013363506644964218, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15978/20000], Bound: 0.3891919255256653, Entropy: 140.219970703125, Temp: 2.700479507446289, KL: 74.6749267578125, Loss: 0.01703295297920704, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15979/20000], Bound: 0.40945348143577576, Entropy: 139.8485107421875, Temp: 2.70047926902771, KL: 81.00613403320312, Loss: 0.016465088352560997, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15980/20000], Bound: 0.37590840458869934, Entropy: 141.5029296875, Temp: 2.70047926902771, KL: 70.72991943359375, Loss: 0.017202526330947876, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15981/20000], Bound: 0.394381582736969, Entropy: 140.84451293945312, Temp: 2.700479030609131, KL: 76.64114379882812, Loss: 0.01621764525771141, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15982/20000], Bound: 0.38008174300193787, Entropy: 140.7490234375, Temp: 2.700479030609131, KL: 72.90934753417969, Loss: 0.015394015237689018, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15983/20000], Bound: 0.3554723262786865, Entropy: 142.7476348876953, Temp: 2.700479030609131, KL: 66.47544860839844, Loss: 0.014364919625222683, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15984/20000], Bound: 0.4029255211353302, Entropy: 142.2627716064453, Temp: 2.70047926902771, KL: 79.18074035644531, Loss: 0.016214197501540184, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15985/20000], Bound: 0.3777036666870117, Entropy: 141.9159393310547, Temp: 2.700479507446289, KL: 71.10452270507812, Loss: 0.01746518164873123, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15986/20000], Bound: 0.40168994665145874, Entropy: 141.3013458251953, Temp: 2.700479507446289, KL: 79.51036071777344, Loss: 0.014920730143785477, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15987/20000], Bound: 0.3629204332828522, Entropy: 143.62060546875, Temp: 2.700479745864868, KL: 69.30804443359375, Loss: 0.012989615090191364, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15988/20000], Bound: 0.3803536891937256, Entropy: 141.51010131835938, Temp: 2.7004804611206055, KL: 72.94125366210938, Loss: 0.0154805276542902, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15989/20000], Bound: 0.3665391206741333, Entropy: 139.0004425048828, Temp: 2.7004811763763428, KL: 69.68367004394531, Loss: 0.014188769273459911, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15990/20000], Bound: 0.3784693777561188, Entropy: 142.0780792236328, Temp: 2.700482130050659, KL: 73.52168273925781, Loss: 0.013398406095802784, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15991/20000], Bound: 0.3918435871601105, Entropy: 141.43150329589844, Temp: 2.7004830837249756, KL: 75.5712890625, Loss: 0.016814183443784714, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15992/20000], Bound: 0.3942753076553345, Entropy: 141.31793212890625, Temp: 2.700484275817871, KL: 78.00736999511719, Loss: 0.013629994355142117, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15993/20000], Bound: 0.4015823006629944, Entropy: 139.9215545654297, Temp: 2.7004857063293457, KL: 78.26905822753906, Loss: 0.01715964451432228, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15994/20000], Bound: 0.4011976420879364, Entropy: 143.2390594482422, Temp: 2.700486898422241, KL: 77.60374450683594, Loss: 0.018179113045334816, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15995/20000], Bound: 0.42734014987945557, Entropy: 140.5165557861328, Temp: 2.7004880905151367, KL: 84.84024047851562, Loss: 0.01950056105852127, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15996/20000], Bound: 0.4043986201286316, Entropy: 137.77325439453125, Temp: 2.700489044189453, KL: 80.0074462890625, Loss: 0.015499839559197426, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15997/20000], Bound: 0.41005435585975647, Entropy: 143.51397705078125, Temp: 2.7004899978637695, KL: 80.97050476074219, Loss: 0.01686714217066765, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15998/20000], Bound: 0.3850351870059967, Entropy: 140.1114959716797, Temp: 2.700491189956665, KL: 75.21131896972656, Loss: 0.01379247847944498, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [15999/20000], Bound: 0.38007494807243347, Entropy: 140.90139770507812, Temp: 2.7004926204681396, KL: 73.00482177734375, Loss: 0.015213750302791595, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16000/20000], Bound: 0.41217881441116333, Entropy: 138.90802001953125, Temp: 2.700493812561035, KL: 81.27899169921875, Loss: 0.017486361786723137, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16001/20000], Bound: 0.40204110741615295, Entropy: 140.3771209716797, Temp: 2.7004952430725098, KL: 78.57334899902344, Loss: 0.016849840059876442, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16002/20000], Bound: 0.41061869263648987, Entropy: 140.22268676757812, Temp: 2.700496196746826, KL: 80.29304504394531, Loss: 0.018437374383211136, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16003/20000], Bound: 0.4119112491607666, Entropy: 140.82925415039062, Temp: 2.7004971504211426, KL: 81.55000305175781, Loss: 0.01683449186384678, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16004/20000], Bound: 0.3888678252696991, Entropy: 139.940673828125, Temp: 2.700498342514038, KL: 75.86834716796875, Loss: 0.014647752046585083, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16005/20000], Bound: 0.40816617012023926, Entropy: 140.36648559570312, Temp: 2.7004992961883545, KL: 80.96502685546875, Loss: 0.015822652727365494, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16006/20000], Bound: 0.3634531497955322, Entropy: 141.87835693359375, Temp: 2.700500726699829, KL: 68.31343078613281, Loss: 0.0151096535846591, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16007/20000], Bound: 0.37763920426368713, Entropy: 143.15647888183594, Temp: 2.7005021572113037, KL: 73.54566955566406, Loss: 0.012911188416182995, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16008/20000], Bound: 0.38782891631126404, Entropy: 142.11170959472656, Temp: 2.7005035877227783, KL: 75.23880004882812, Loss: 0.015250691212713718, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16009/20000], Bound: 0.38423484563827515, Entropy: 141.17327880859375, Temp: 2.700505256652832, KL: 74.41534423828125, Loss: 0.014835218898952007, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16010/20000], Bound: 0.3993777632713318, Entropy: 141.851318359375, Temp: 2.7005066871643066, KL: 78.88555908203125, Loss: 0.01480273436754942, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16011/20000], Bound: 0.38209447264671326, Entropy: 139.6857147216797, Temp: 2.7005085945129395, KL: 73.0928955078125, Loss: 0.016133170574903488, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16012/20000], Bound: 0.3769003450870514, Entropy: 142.55567932128906, Temp: 2.700510263442993, KL: 71.39532470703125, Loss: 0.016498805955052376, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16013/20000], Bound: 0.3861532509326935, Entropy: 141.54522705078125, Temp: 2.7005116939544678, KL: 74.88453674316406, Loss: 0.015000893734395504, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16014/20000], Bound: 0.3853406310081482, Entropy: 139.34109497070312, Temp: 2.7005131244659424, KL: 74.81434631347656, Loss: 0.014692367985844612, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16015/20000], Bound: 0.3850107491016388, Entropy: 139.6779327392578, Temp: 2.700514793395996, KL: 73.70713806152344, Loss: 0.016564523801207542, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16016/20000], Bound: 0.3807365298271179, Entropy: 139.85484313964844, Temp: 2.7005159854888916, KL: 73.30268859863281, Loss: 0.01501666009426117, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16017/20000], Bound: 0.39658376574516296, Entropy: 142.76747131347656, Temp: 2.7005176544189453, KL: 76.79606628417969, Loss: 0.017136530950665474, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16018/20000], Bound: 0.3826928436756134, Entropy: 142.19198608398438, Temp: 2.700518846511841, KL: 72.67222595214844, Loss: 0.01723344251513481, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16019/20000], Bound: 0.3788960576057434, Entropy: 141.5636749267578, Temp: 2.7005200386047363, KL: 71.25796508789062, Loss: 0.017817914485931396, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16020/20000], Bound: 0.39039546251296997, Entropy: 141.75880432128906, Temp: 2.7005207538604736, KL: 74.53834533691406, Loss: 0.017939450219273567, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16021/20000], Bound: 0.3881832957267761, Entropy: 141.6627655029297, Temp: 2.700521230697632, KL: 75.90240478515625, Loss: 0.014214057475328445, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16022/20000], Bound: 0.37914881110191345, Entropy: 140.216552734375, Temp: 2.700521945953369, KL: 71.86090087890625, Loss: 0.01683666929602623, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16023/20000], Bound: 0.37299278378486633, Entropy: 142.38584899902344, Temp: 2.7005226612091064, KL: 71.83688354492188, Loss: 0.013605562970042229, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16024/20000], Bound: 0.37047940492630005, Entropy: 141.2812042236328, Temp: 2.7005233764648438, KL: 70.0933837890625, Loss: 0.015504556708037853, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16025/20000], Bound: 0.38912472128868103, Entropy: 141.64805603027344, Temp: 2.700524091720581, KL: 74.45831298828125, Loss: 0.017397964373230934, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16026/20000], Bound: 0.405654639005661, Entropy: 139.2131805419922, Temp: 2.7005248069763184, KL: 79.25404357910156, Loss: 0.01759243756532669, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16027/20000], Bound: 0.41124197840690613, Entropy: 139.47592163085938, Temp: 2.7005252838134766, KL: 82.79315185546875, Loss: 0.014157874509692192, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16028/20000], Bound: 0.38770046830177307, Entropy: 143.1096954345703, Temp: 2.700525999069214, KL: 73.7769775390625, Loss: 0.01788797415792942, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16029/20000], Bound: 0.3849641680717468, Entropy: 141.11167907714844, Temp: 2.700526714324951, KL: 74.67256164550781, Loss: 0.014752073213458061, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16030/20000], Bound: 0.3822001516819, Entropy: 141.60281372070312, Temp: 2.7005276679992676, KL: 71.21333312988281, Loss: 0.019670045003294945, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16031/20000], Bound: 0.3809677064418793, Entropy: 141.06785583496094, Temp: 2.7005279064178467, KL: 72.73568725585938, Loss: 0.0161904264241457, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16032/20000], Bound: 0.4008370339870453, Entropy: 140.50814819335938, Temp: 2.700528144836426, KL: 78.24758911132812, Loss: 0.016788359731435776, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16033/20000], Bound: 0.3974299430847168, Entropy: 141.87432861328125, Temp: 2.700528383255005, KL: 78.29191589355469, Loss: 0.014831318520009518, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16034/20000], Bound: 0.3825359642505646, Entropy: 140.6654052734375, Temp: 2.700529098510742, KL: 73.97364807128906, Loss: 0.014739681035280228, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16035/20000], Bound: 0.3848576843738556, Entropy: 143.2445068359375, Temp: 2.7005295753479004, KL: 72.59072875976562, Loss: 0.01854919269680977, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16036/20000], Bound: 0.3593442738056183, Entropy: 142.25230407714844, Temp: 2.7005300521850586, KL: 66.369873046875, Loss: 0.01656719110906124, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16037/20000], Bound: 0.3967136740684509, Entropy: 142.24501037597656, Temp: 2.7005302906036377, KL: 77.33551025390625, Loss: 0.016209129244089127, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16038/20000], Bound: 0.3657268285751343, Entropy: 141.4396514892578, Temp: 2.700530529022217, KL: 68.63101196289062, Loss: 0.01571204699575901, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16039/20000], Bound: 0.4044075012207031, Entropy: 139.95318603515625, Temp: 2.700530767440796, KL: 79.67880249023438, Loss: 0.016113681718707085, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16040/20000], Bound: 0.38772711157798767, Entropy: 140.99656677246094, Temp: 2.700531244277954, KL: 73.45411682128906, Loss: 0.018500203266739845, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16041/20000], Bound: 0.3681297302246094, Entropy: 139.74424743652344, Temp: 2.700531244277954, KL: 68.29246520996094, Loss: 0.01760081946849823, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16042/20000], Bound: 0.40076956152915955, Entropy: 141.42127990722656, Temp: 2.700531005859375, KL: 78.40214538574219, Loss: 0.01646500639617443, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16043/20000], Bound: 0.4043193459510803, Entropy: 141.9449920654297, Temp: 2.700530767440796, KL: 80.61848449707031, Loss: 0.014324963092803955, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16044/20000], Bound: 0.38095924258232117, Entropy: 140.06521606445312, Temp: 2.700531244277954, KL: 71.47834777832031, Loss: 0.018513867631554604, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16045/20000], Bound: 0.4107225239276886, Entropy: 140.7585906982422, Temp: 2.700531244277954, KL: 81.59547424316406, Loss: 0.016084423288702965, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16046/20000], Bound: 0.385175883769989, Entropy: 139.23974609375, Temp: 2.700531482696533, KL: 73.62614440917969, Loss: 0.016803694888949394, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16047/20000], Bound: 0.3768828809261322, Entropy: 142.09991455078125, Temp: 2.7005317211151123, KL: 72.63589477539062, Loss: 0.014192761853337288, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16048/20000], Bound: 0.3810062110424042, Entropy: 141.19635009765625, Temp: 2.7005321979522705, KL: 73.00189208984375, Loss: 0.015718212351202965, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16049/20000], Bound: 0.4210107922554016, Entropy: 141.26466369628906, Temp: 2.7005324363708496, KL: 83.37916564941406, Loss: 0.018588334321975708, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16050/20000], Bound: 0.4316502511501312, Entropy: 139.95599365234375, Temp: 2.700532913208008, KL: 85.86958312988281, Loss: 0.02007928304374218, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16051/20000], Bound: 0.37424156069755554, Entropy: 141.28445434570312, Temp: 2.700533151626587, KL: 70.62385559082031, Loss: 0.016513656824827194, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16052/20000], Bound: 0.39261409640312195, Entropy: 139.51927185058594, Temp: 2.700533151626587, KL: 76.60383605957031, Loss: 0.015322615392506123, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16053/20000], Bound: 0.4272698163986206, Entropy: 140.87135314941406, Temp: 2.700533390045166, KL: 87.1837158203125, Loss: 0.015121739357709885, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16054/20000], Bound: 0.39309489727020264, Entropy: 140.84500122070312, Temp: 2.7005341053009033, KL: 76.72120666503906, Loss: 0.015367500483989716, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16055/20000], Bound: 0.40263283252716064, Entropy: 142.339599609375, Temp: 2.7005350589752197, KL: 78.05525207519531, Loss: 0.018136663362383842, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16056/20000], Bound: 0.3904861807823181, Entropy: 139.9361114501953, Temp: 2.700535774230957, KL: 76.65794372558594, Loss: 0.014064452610909939, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16057/20000], Bound: 0.35449618101119995, Entropy: 142.59646606445312, Temp: 2.7005364894866943, KL: 64.94093322753906, Loss: 0.016702378168702126, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16058/20000], Bound: 0.4002971053123474, Entropy: 141.9229736328125, Temp: 2.7005372047424316, KL: 78.37893676757812, Loss: 0.016247501596808434, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16059/20000], Bound: 0.36795681715011597, Entropy: 142.4380645751953, Temp: 2.700537919998169, KL: 69.24755859375, Loss: 0.015741607174277306, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16060/20000], Bound: 0.35616686940193176, Entropy: 140.20315551757812, Temp: 2.7005386352539062, KL: 65.80120849609375, Loss: 0.015972839668393135, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16061/20000], Bound: 0.4060666561126709, Entropy: 139.0455780029297, Temp: 2.7005388736724854, KL: 81.14447021484375, Loss: 0.014321527443826199, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16062/20000], Bound: 0.3923415541648865, Entropy: 140.72496032714844, Temp: 2.7005398273468018, KL: 76.60899353027344, Loss: 0.01516461931169033, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16063/20000], Bound: 0.38863876461982727, Entropy: 140.80593872070312, Temp: 2.700540781021118, KL: 73.98202514648438, Loss: 0.018016530200839043, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16064/20000], Bound: 0.3715263605117798, Entropy: 141.36233520507812, Temp: 2.7005414962768555, KL: 71.11814880371094, Loss: 0.014160455204546452, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16065/20000], Bound: 0.37703436613082886, Entropy: 142.7716522216797, Temp: 2.7005422115325928, KL: 71.95767211914062, Loss: 0.015529297292232513, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16066/20000], Bound: 0.40908119082450867, Entropy: 140.39967346191406, Temp: 2.700543165206909, KL: 81.31246948242188, Loss: 0.015690557658672333, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16067/20000], Bound: 0.4036426544189453, Entropy: 142.13729858398438, Temp: 2.7005443572998047, KL: 78.02589416503906, Loss: 0.01875014416873455, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16068/20000], Bound: 0.37945756316185, Entropy: 139.9800567626953, Temp: 2.700545072555542, KL: 71.93162536621094, Loss: 0.016870953142642975, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16069/20000], Bound: 0.3947010040283203, Entropy: 139.77499389648438, Temp: 2.7005455493927, KL: 76.66000366210938, Loss: 0.01635795272886753, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16070/20000], Bound: 0.3759196996688843, Entropy: 141.05902099609375, Temp: 2.7005460262298584, KL: 71.08901977539062, Loss: 0.016544204205274582, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16071/20000], Bound: 0.4259209930896759, Entropy: 141.15396118164062, Temp: 2.7005465030670166, KL: 84.84516906738281, Loss: 0.018677784129977226, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16072/20000], Bound: 0.38517624139785767, Entropy: 143.1443328857422, Temp: 2.700546979904175, KL: 74.3206787109375, Loss: 0.015518097206950188, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16073/20000], Bound: 0.35550978779792786, Entropy: 143.04391479492188, Temp: 2.700547456741333, KL: 66.20582580566406, Loss: 0.014884051866829395, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16074/20000], Bound: 0.3866259753704071, Entropy: 141.14138793945312, Temp: 2.700547933578491, KL: 74.08720397949219, Loss: 0.016732780262827873, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16075/20000], Bound: 0.39358317852020264, Entropy: 141.8844757080078, Temp: 2.7005484104156494, KL: 76.32569885253906, Loss: 0.016366319730877876, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16076/20000], Bound: 0.41277703642845154, Entropy: 140.61610412597656, Temp: 2.7005486488342285, KL: 82.31004333496094, Loss: 0.015913857147097588, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16077/20000], Bound: 0.3912044167518616, Entropy: 142.94729614257812, Temp: 2.700549364089966, KL: 76.41648864746094, Loss: 0.014902112074196339, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16078/20000], Bound: 0.401856392621994, Entropy: 141.6382598876953, Temp: 2.7005503177642822, KL: 77.65931701660156, Loss: 0.01844058185815811, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16079/20000], Bound: 0.38995710015296936, Entropy: 141.2416534423828, Temp: 2.7005510330200195, KL: 75.06561279296875, Loss: 0.016725439578294754, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16080/20000], Bound: 0.3622540831565857, Entropy: 142.79275512695312, Temp: 2.7005515098571777, KL: 67.71467590332031, Loss: 0.015592521987855434, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16081/20000], Bound: 0.40657931566238403, Entropy: 140.73487854003906, Temp: 2.700551748275757, KL: 78.21206665039062, Loss: 0.020036153495311737, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16082/20000], Bound: 0.38500550389289856, Entropy: 141.6568145751953, Temp: 2.700551748275757, KL: 73.85444641113281, Loss: 0.016289303079247475, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16083/20000], Bound: 0.40613773465156555, Entropy: 140.62557983398438, Temp: 2.700551748275757, KL: 80.02963256835938, Loss: 0.016425276175141335, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16084/20000], Bound: 0.3949519991874695, Entropy: 141.06678771972656, Temp: 2.700551986694336, KL: 78.0821533203125, Loss: 0.013862194493412971, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16085/20000], Bound: 0.3743720054626465, Entropy: 143.05287170410156, Temp: 2.7005527019500732, KL: 70.68927001953125, Loss: 0.01646195352077484, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16086/20000], Bound: 0.35434162616729736, Entropy: 142.5778350830078, Temp: 2.7005531787872314, KL: 65.11331176757812, Loss: 0.016303593292832375, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16087/20000], Bound: 0.38922634720802307, Entropy: 142.94386291503906, Temp: 2.7005534172058105, KL: 74.5614013671875, Loss: 0.017262447625398636, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16088/20000], Bound: 0.38432928919792175, Entropy: 141.52825927734375, Temp: 2.7005536556243896, KL: 73.50343322753906, Loss: 0.0165749229490757, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16089/20000], Bound: 0.407498836517334, Entropy: 140.59608459472656, Temp: 2.7005536556243896, KL: 81.365478515625, Loss: 0.01470976322889328, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16090/20000], Bound: 0.38545042276382446, Entropy: 142.99868774414062, Temp: 2.700554132461548, KL: 74.09782409667969, Loss: 0.01607857272028923, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16091/20000], Bound: 0.40932780504226685, Entropy: 139.92616271972656, Temp: 2.700554609298706, KL: 81.43994140625, Loss: 0.015592479147017002, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16092/20000], Bound: 0.3850260376930237, Entropy: 141.03993225097656, Temp: 2.7005553245544434, KL: 75.0152587890625, Loss: 0.01415120530873537, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16093/20000], Bound: 0.3877403438091278, Entropy: 140.0010528564453, Temp: 2.7005560398101807, KL: 73.98973083496094, Loss: 0.017515888437628746, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16094/20000], Bound: 0.3948483467102051, Entropy: 140.3848419189453, Temp: 2.700556755065918, KL: 76.89433288574219, Loss: 0.01600477285683155, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16095/20000], Bound: 0.38529959321022034, Entropy: 141.7513885498047, Temp: 2.7005577087402344, KL: 73.6927490234375, Loss: 0.01674729399383068, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16096/20000], Bound: 0.37668901681900024, Entropy: 143.36257934570312, Temp: 2.7005584239959717, KL: 71.97517395019531, Loss: 0.015313086099922657, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16097/20000], Bound: 0.3773365020751953, Entropy: 138.65882873535156, Temp: 2.700559139251709, KL: 72.51333618164062, Loss: 0.014661679044365883, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16098/20000], Bound: 0.3782717287540436, Entropy: 138.85826110839844, Temp: 2.7005598545074463, KL: 73.70075988769531, Loss: 0.012962061911821365, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16099/20000], Bound: 0.40843465924263, Entropy: 140.8595733642578, Temp: 2.700561046600342, KL: 80.03718566894531, Loss: 0.01769096404314041, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16100/20000], Bound: 0.4249045252799988, Entropy: 140.152587890625, Temp: 2.700562000274658, KL: 84.81700134277344, Loss: 0.018147969618439674, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16101/20000], Bound: 0.3951287567615509, Entropy: 140.80548095703125, Temp: 2.7005631923675537, KL: 77.42474365234375, Loss: 0.015176135115325451, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16102/20000], Bound: 0.3764093518257141, Entropy: 142.3969268798828, Temp: 2.700564384460449, KL: 72.27830505371094, Loss: 0.014602982439100742, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16103/20000], Bound: 0.39533448219299316, Entropy: 140.7467041015625, Temp: 2.7005655765533447, KL: 76.32731628417969, Loss: 0.01732058636844158, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16104/20000], Bound: 0.38954219222068787, Entropy: 141.3092041015625, Temp: 2.7005667686462402, KL: 74.54409790039062, Loss: 0.017465943470597267, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16105/20000], Bound: 0.39972150325775146, Entropy: 142.6141815185547, Temp: 2.7005677223205566, KL: 79.02503967285156, Loss: 0.014734401367604733, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16106/20000], Bound: 0.4213619530200958, Entropy: 141.66554260253906, Temp: 2.700568914413452, KL: 82.23373413085938, Loss: 0.020909225568175316, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16107/20000], Bound: 0.39221063256263733, Entropy: 141.81057739257812, Temp: 2.7005693912506104, KL: 73.74977111816406, Loss: 0.0203873198479414, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16108/20000], Bound: 0.3908294439315796, Entropy: 142.0540008544922, Temp: 2.7005693912506104, KL: 75.42814636230469, Loss: 0.01652827486395836, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16109/20000], Bound: 0.3994852602481842, Entropy: 140.75328063964844, Temp: 2.7005696296691895, KL: 78.45071411132812, Loss: 0.01566767506301403, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16110/20000], Bound: 0.4023185670375824, Entropy: 140.17367553710938, Temp: 2.7005698680877686, KL: 77.22776794433594, Loss: 0.019495204091072083, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16111/20000], Bound: 0.3907361924648285, Entropy: 141.84202575683594, Temp: 2.7005698680877686, KL: 74.70140075683594, Loss: 0.017823142930865288, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16112/20000], Bound: 0.388032466173172, Entropy: 139.7909393310547, Temp: 2.7005696296691895, KL: 75.20516967773438, Loss: 0.015423806384205818, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16113/20000], Bound: 0.36835309863090515, Entropy: 144.6020050048828, Temp: 2.7005696296691895, KL: 70.67048645019531, Loss: 0.01331583596765995, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16114/20000], Bound: 0.38627710938453674, Entropy: 140.69656372070312, Temp: 2.7005701065063477, KL: 75.01437377929688, Loss: 0.01482794713228941, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16115/20000], Bound: 0.37017253041267395, Entropy: 141.15704345703125, Temp: 2.700570583343506, KL: 69.54719543457031, Loss: 0.016354253515601158, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16116/20000], Bound: 0.3746890425682068, Entropy: 142.43508911132812, Temp: 2.700571060180664, KL: 70.88174438476562, Loss: 0.01627407595515251, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16117/20000], Bound: 0.3949248790740967, Entropy: 142.3022918701172, Temp: 2.700571298599243, KL: 76.26274108886719, Loss: 0.017216144129633904, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16118/20000], Bound: 0.3597525954246521, Entropy: 141.36312866210938, Temp: 2.7005715370178223, KL: 67.48870849609375, Loss: 0.014708212576806545, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16119/20000], Bound: 0.38207876682281494, Entropy: 142.06460571289062, Temp: 2.7005717754364014, KL: 73.71115112304688, Loss: 0.014980592764914036, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16120/20000], Bound: 0.3765408992767334, Entropy: 141.56809997558594, Temp: 2.7005722522735596, KL: 70.35369873046875, Loss: 0.018236428499221802, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16121/20000], Bound: 0.3860641419887543, Entropy: 141.33229064941406, Temp: 2.7005722522735596, KL: 73.79817199707031, Loss: 0.016964737325906754, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16122/20000], Bound: 0.3753885328769684, Entropy: 142.449462890625, Temp: 2.7005722522735596, KL: 72.23887634277344, Loss: 0.014133011922240257, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16123/20000], Bound: 0.3790184259414673, Entropy: 142.42559814453125, Temp: 2.7005722522735596, KL: 71.80586242675781, Loss: 0.016869282349944115, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16124/20000], Bound: 0.4200564920902252, Entropy: 142.5236053466797, Temp: 2.7005724906921387, KL: 85.70954895019531, Loss: 0.013731736689805984, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16125/20000], Bound: 0.387233167886734, Entropy: 140.663818359375, Temp: 2.700573205947876, KL: 74.09294128417969, Loss: 0.017050594091415405, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16126/20000], Bound: 0.3977864384651184, Entropy: 138.72666931152344, Temp: 2.7005739212036133, KL: 76.35914611816406, Loss: 0.018605975434184074, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16127/20000], Bound: 0.3446371257305145, Entropy: 142.40232849121094, Temp: 2.7005741596221924, KL: 62.27320861816406, Loss: 0.016588425263762474, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16128/20000], Bound: 0.379813551902771, Entropy: 139.98609924316406, Temp: 2.7005741596221924, KL: 73.636474609375, Loss: 0.01390511728823185, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16129/20000], Bound: 0.36246082186698914, Entropy: 139.36056518554688, Temp: 2.7005743980407715, KL: 68.29020690917969, Loss: 0.014635011553764343, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16130/20000], Bound: 0.3763901889324188, Entropy: 142.21165466308594, Temp: 2.7005748748779297, KL: 71.88543701171875, Loss: 0.015320224687457085, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16131/20000], Bound: 0.36270132660865784, Entropy: 142.01292419433594, Temp: 2.700575113296509, KL: 67.23234558105469, Loss: 0.0167191531509161, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16132/20000], Bound: 0.3904815912246704, Entropy: 140.23910522460938, Temp: 2.700575351715088, KL: 73.88038635253906, Loss: 0.019204899668693542, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16133/20000], Bound: 0.3998746871948242, Entropy: 140.99932861328125, Temp: 2.700575113296509, KL: 77.65249633789062, Loss: 0.017360052093863487, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16134/20000], Bound: 0.3792392611503601, Entropy: 140.36241149902344, Temp: 2.7005748748779297, KL: 73.72979736328125, Loss: 0.013425222598016262, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16135/20000], Bound: 0.3960303068161011, Entropy: 140.0824737548828, Temp: 2.700575113296509, KL: 77.00411987304688, Loss: 0.016448576003313065, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16136/20000], Bound: 0.386777400970459, Entropy: 139.658935546875, Temp: 2.700575351715088, KL: 74.78230285644531, Loss: 0.015527923591434956, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16137/20000], Bound: 0.3843297064304352, Entropy: 139.60452270507812, Temp: 2.700575828552246, KL: 74.912109375, Loss: 0.013967196457087994, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16138/20000], Bound: 0.3881763219833374, Entropy: 141.550048828125, Temp: 2.7005765438079834, KL: 72.96412658691406, Loss: 0.01965092495083809, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16139/20000], Bound: 0.3899467885494232, Entropy: 140.68722534179688, Temp: 2.7005767822265625, KL: 75.20698547363281, Loss: 0.01645834371447563, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16140/20000], Bound: 0.3758394420146942, Entropy: 140.8260955810547, Temp: 2.7005770206451416, KL: 72.88542175292969, Loss: 0.01317580696195364, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16141/20000], Bound: 0.3855549991130829, Entropy: 140.9811248779297, Temp: 2.700577735900879, KL: 73.41685485839844, Loss: 0.017396006733179092, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16142/20000], Bound: 0.40762388706207275, Entropy: 141.67477416992188, Temp: 2.700578212738037, KL: 81.20701599121094, Loss: 0.015073067508637905, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16143/20000], Bound: 0.38425159454345703, Entropy: 141.72784423828125, Temp: 2.7005786895751953, KL: 73.81645202636719, Loss: 0.01595376431941986, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16144/20000], Bound: 0.3876260221004486, Entropy: 140.2188720703125, Temp: 2.7005794048309326, KL: 75.35041809082031, Loss: 0.014934966340661049, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16145/20000], Bound: 0.3864276707172394, Entropy: 140.80874633789062, Temp: 2.700580358505249, KL: 73.17214965820312, Loss: 0.018320143222808838, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16146/20000], Bound: 0.37140366435050964, Entropy: 142.25030517578125, Temp: 2.7005808353424072, KL: 69.88699340820312, Loss: 0.016375355422496796, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16147/20000], Bound: 0.40526852011680603, Entropy: 139.9984893798828, Temp: 2.7005813121795654, KL: 80.36131286621094, Loss: 0.015328418463468552, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16148/20000], Bound: 0.3618415296077728, Entropy: 142.90342712402344, Temp: 2.7005820274353027, KL: 66.86676025390625, Loss: 0.016947465017437935, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16149/20000], Bound: 0.36959925293922424, Entropy: 141.7767333984375, Temp: 2.700582504272461, KL: 69.65359497070312, Loss: 0.015855012461543083, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16150/20000], Bound: 0.37933316826820374, Entropy: 140.15878295898438, Temp: 2.700582504272461, KL: 71.69996643066406, Loss: 0.017233649268746376, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16151/20000], Bound: 0.38836386799812317, Entropy: 139.5116729736328, Temp: 2.700582504272461, KL: 72.95864868164062, Loss: 0.01976267248392105, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16152/20000], Bound: 0.4105681777000427, Entropy: 142.1556854248047, Temp: 2.7005820274353027, KL: 82.43705749511719, Loss: 0.014440407045185566, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16153/20000], Bound: 0.3848375082015991, Entropy: 142.21385192871094, Temp: 2.700582504272461, KL: 72.39700317382812, Loss: 0.018897414207458496, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16154/20000], Bound: 0.3809248208999634, Entropy: 141.28489685058594, Temp: 2.7005820274353027, KL: 72.67060852050781, Loss: 0.016288403421640396, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16155/20000], Bound: 0.3959807753562927, Entropy: 142.12416076660156, Temp: 2.7005820274353027, KL: 77.180908203125, Loss: 0.01609419286251068, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16156/20000], Bound: 0.3933382034301758, Entropy: 142.31158447265625, Temp: 2.7005820274353027, KL: 76.19259643554688, Loss: 0.0164793748408556, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16157/20000], Bound: 0.37719109654426575, Entropy: 140.3321075439453, Temp: 2.7005820274353027, KL: 70.78147888183594, Loss: 0.017790811136364937, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16158/20000], Bound: 0.36071455478668213, Entropy: 142.98094177246094, Temp: 2.7005817890167236, KL: 66.67202758789062, Loss: 0.016720805317163467, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16159/20000], Bound: 0.3839210569858551, Entropy: 142.22976684570312, Temp: 2.7005813121795654, KL: 74.56365966796875, Loss: 0.014392432756721973, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16160/20000], Bound: 0.3766634464263916, Entropy: 141.94395446777344, Temp: 2.7005810737609863, KL: 71.27534484863281, Loss: 0.016595356166362762, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16161/20000], Bound: 0.3812119960784912, Entropy: 140.41796875, Temp: 2.7005808353424072, KL: 71.62020874023438, Loss: 0.018387053161859512, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16162/20000], Bound: 0.40018972754478455, Entropy: 141.2982940673828, Temp: 2.700580358505249, KL: 77.93911743164062, Loss: 0.017003053799271584, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16163/20000], Bound: 0.3991979658603668, Entropy: 139.66302490234375, Temp: 2.700579881668091, KL: 79.4810791015625, Loss: 0.013601941987872124, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16164/20000], Bound: 0.3831833004951477, Entropy: 141.81851196289062, Temp: 2.70058012008667, KL: 74.2001953125, Loss: 0.014668538235127926, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16165/20000], Bound: 0.3720925450325012, Entropy: 141.63880920410156, Temp: 2.700580596923828, KL: 69.28378295898438, Loss: 0.017856452614068985, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16166/20000], Bound: 0.3805461525917053, Entropy: 140.5334930419922, Temp: 2.700580596923828, KL: 72.34751892089844, Loss: 0.016683755442500114, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16167/20000], Bound: 0.3944531977176666, Entropy: 141.5321807861328, Temp: 2.700580596923828, KL: 76.20124816894531, Loss: 0.01707216538488865, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16168/20000], Bound: 0.4191330075263977, Entropy: 140.5459747314453, Temp: 2.700580596923828, KL: 82.31358337402344, Loss: 0.019495176151394844, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16169/20000], Bound: 0.391916960477829, Entropy: 142.00277709960938, Temp: 2.700580358505249, KL: 74.21492004394531, Loss: 0.019366281107068062, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16170/20000], Bound: 0.39715301990509033, Entropy: 140.9792938232422, Temp: 2.7005796432495117, KL: 77.38987731933594, Loss: 0.016349924728274345, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16171/20000], Bound: 0.36784034967422485, Entropy: 141.83847045898438, Temp: 2.7005791664123535, KL: 68.3643798828125, Loss: 0.01731581799685955, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16172/20000], Bound: 0.39804989099502563, Entropy: 141.79962158203125, Temp: 2.7005786895751953, KL: 76.68226623535156, Loss: 0.01815248467028141, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16173/20000], Bound: 0.42235124111175537, Entropy: 139.9521026611328, Temp: 2.700577974319458, KL: 85.12496948242188, Loss: 0.016119761392474174, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16174/20000], Bound: 0.38732171058654785, Entropy: 142.81507873535156, Temp: 2.700577735900879, KL: 74.67933654785156, Loss: 0.016012847423553467, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16175/20000], Bound: 0.36716461181640625, Entropy: 141.10287475585938, Temp: 2.7005774974823, KL: 68.54458618164062, Loss: 0.016626978293061256, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16176/20000], Bound: 0.3493913412094116, Entropy: 143.7236328125, Temp: 2.7005770206451416, KL: 63.99687194824219, Loss: 0.015825390815734863, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16177/20000], Bound: 0.38193702697753906, Entropy: 140.60470581054688, Temp: 2.7005765438079834, KL: 73.84165954589844, Loss: 0.014662951231002808, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16178/20000], Bound: 0.36987724900245667, Entropy: 144.31309509277344, Temp: 2.7005763053894043, KL: 69.47544860839844, Loss: 0.016331374645233154, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16179/20000], Bound: 0.36848166584968567, Entropy: 142.65623474121094, Temp: 2.700576066970825, KL: 70.0738525390625, Loss: 0.014488198794424534, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16180/20000], Bound: 0.3900528848171234, Entropy: 140.05130004882812, Temp: 2.700576066970825, KL: 75.48912048339844, Loss: 0.015993548557162285, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16181/20000], Bound: 0.40693309903144836, Entropy: 142.0764617919922, Temp: 2.700576066970825, KL: 78.58515930175781, Loss: 0.019542481750249863, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16182/20000], Bound: 0.387751042842865, Entropy: 142.08651733398438, Temp: 2.700575828552246, KL: 75.13804626464844, Loss: 0.015395760536193848, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16183/20000], Bound: 0.4078238010406494, Entropy: 141.007080078125, Temp: 2.700575828552246, KL: 80.37980651855469, Loss: 0.016716083511710167, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16184/20000], Bound: 0.37591633200645447, Entropy: 142.3548583984375, Temp: 2.700575828552246, KL: 71.8138427734375, Loss: 0.015200662426650524, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16185/20000], Bound: 0.40165045857429504, Entropy: 142.19309997558594, Temp: 2.700576066970825, KL: 79.79435729980469, Loss: 0.014374100603163242, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16186/20000], Bound: 0.3826746940612793, Entropy: 142.35858154296875, Temp: 2.7005767822265625, KL: 72.54461669921875, Loss: 0.01746043935418129, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16187/20000], Bound: 0.38349449634552, Entropy: 142.5978546142578, Temp: 2.7005772590637207, KL: 72.64109802246094, Loss: 0.017722412943840027, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16188/20000], Bound: 0.3713516592979431, Entropy: 142.8480682373047, Temp: 2.7005774974823, KL: 69.43360900878906, Loss: 0.01718728058040142, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16189/20000], Bound: 0.3936263918876648, Entropy: 143.45094299316406, Temp: 2.7005774974823, KL: 76.57843017578125, Loss: 0.015922250226140022, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16190/20000], Bound: 0.39399397373199463, Entropy: 140.37063598632812, Temp: 2.7005774974823, KL: 76.24993896484375, Loss: 0.01673111692070961, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16191/20000], Bound: 0.39563336968421936, Entropy: 141.6622772216797, Temp: 2.7005774974823, KL: 74.87214660644531, Loss: 0.020178474485874176, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16192/20000], Bound: 0.36296841502189636, Entropy: 143.89556884765625, Temp: 2.7005770206451416, KL: 68.44996643066406, Loss: 0.014604246243834496, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16193/20000], Bound: 0.38831669092178345, Entropy: 141.6110382080078, Temp: 2.7005767822265625, KL: 75.5108642578125, Loss: 0.015011758543550968, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16194/20000], Bound: 0.3810584247112274, Entropy: 142.06161499023438, Temp: 2.7005767822265625, KL: 70.26177978515625, Loss: 0.02081982046365738, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16195/20000], Bound: 0.37338897585868835, Entropy: 143.98426818847656, Temp: 2.700576066970825, KL: 70.77885437011719, Loss: 0.015774890780448914, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16196/20000], Bound: 0.38805994391441345, Entropy: 139.8419952392578, Temp: 2.700575351715088, KL: 74.31475830078125, Loss: 0.01708727888762951, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16197/20000], Bound: 0.40425488352775574, Entropy: 141.5088653564453, Temp: 2.7005748748779297, KL: 80.35842895507812, Loss: 0.014771153219044209, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16198/20000], Bound: 0.3643264174461365, Entropy: 141.37417602539062, Temp: 2.7005746364593506, KL: 68.28584289550781, Loss: 0.015617985278367996, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16199/20000], Bound: 0.3961690366268158, Entropy: 142.7030029296875, Temp: 2.7005743980407715, KL: 77.01824951171875, Loss: 0.016498401761054993, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16200/20000], Bound: 0.36490389704704285, Entropy: 141.34530639648438, Temp: 2.7005743980407715, KL: 68.07394409179688, Loss: 0.016312599182128906, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16201/20000], Bound: 0.3485080301761627, Entropy: 142.59608459472656, Temp: 2.7005741596221924, KL: 63.12117004394531, Loss: 0.016994353383779526, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16202/20000], Bound: 0.36317408084869385, Entropy: 141.86717224121094, Temp: 2.700573682785034, KL: 68.79267883300781, Loss: 0.014077093452215195, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16203/20000], Bound: 0.3779789209365845, Entropy: 141.127685546875, Temp: 2.700573444366455, KL: 70.25486755371094, Loss: 0.019185887649655342, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16204/20000], Bound: 0.37430229783058167, Entropy: 142.17933654785156, Temp: 2.700572967529297, KL: 72.1072998046875, Loss: 0.013799693435430527, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16205/20000], Bound: 0.37938371300697327, Entropy: 142.4440155029297, Temp: 2.7005722522735596, KL: 71.70199584960938, Loss: 0.01725682243704796, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16206/20000], Bound: 0.3968117833137512, Entropy: 143.79721069335938, Temp: 2.7005720138549805, KL: 76.91157531738281, Loss: 0.017048200592398643, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16207/20000], Bound: 0.3622911870479584, Entropy: 142.7646026611328, Temp: 2.7005715370178223, KL: 67.33242797851562, Loss: 0.01631973870098591, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16208/20000], Bound: 0.386639267206192, Entropy: 142.21780395507812, Temp: 2.700571060180664, KL: 74.57499694824219, Loss: 0.0158370491117239, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16209/20000], Bound: 0.36666005849838257, Entropy: 140.5236358642578, Temp: 2.700570583343506, KL: 68.37619018554688, Loss: 0.016673777252435684, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16210/20000], Bound: 0.38360413908958435, Entropy: 140.12571716308594, Temp: 2.7005701065063477, KL: 74.18898010253906, Loss: 0.014915484935045242, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16211/20000], Bound: 0.38785770535469055, Entropy: 142.22586059570312, Temp: 2.7005698680877686, KL: 76.06929016113281, Loss: 0.013729280792176723, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16212/20000], Bound: 0.3880598843097687, Entropy: 139.2290802001953, Temp: 2.7005701065063477, KL: 73.53034973144531, Loss: 0.01853949949145317, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16213/20000], Bound: 0.3905988037586212, Entropy: 143.4996795654297, Temp: 2.7005698680877686, KL: 74.06932067871094, Loss: 0.01891874521970749, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16214/20000], Bound: 0.3924241065979004, Entropy: 139.4468231201172, Temp: 2.7005693912506104, KL: 75.39234924316406, Loss: 0.017462460324168205, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16215/20000], Bound: 0.37941670417785645, Entropy: 140.00425720214844, Temp: 2.7005691528320312, KL: 72.94075012207031, Loss: 0.014980950392782688, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16216/20000], Bound: 0.3752804398536682, Entropy: 142.44741821289062, Temp: 2.700568914413452, KL: 70.48023986816406, Loss: 0.01733158528804779, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16217/20000], Bound: 0.3701540529727936, Entropy: 142.89273071289062, Temp: 2.700568437576294, KL: 68.92195129394531, Loss: 0.017502138391137123, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16218/20000], Bound: 0.3865601718425751, Entropy: 140.082763671875, Temp: 2.7005677223205566, KL: 74.20602416992188, Loss: 0.016477404162287712, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16219/20000], Bound: 0.3926622271537781, Entropy: 140.92564392089844, Temp: 2.7005670070648193, KL: 75.39778137207031, Loss: 0.017582165077328682, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16220/20000], Bound: 0.392231285572052, Entropy: 140.73976135253906, Temp: 2.700566291809082, KL: 75.87466430664062, Loss: 0.01646439917385578, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16221/20000], Bound: 0.42450037598609924, Entropy: 140.58729553222656, Temp: 2.700565814971924, KL: 86.00408935546875, Loss: 0.015719003975391388, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16222/20000], Bound: 0.3670481741428375, Entropy: 141.1846466064453, Temp: 2.700565814971924, KL: 70.25285339355469, Loss: 0.013402948155999184, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16223/20000], Bound: 0.38476458191871643, Entropy: 142.03216552734375, Temp: 2.700566053390503, KL: 73.28195190429688, Loss: 0.017219556495547295, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16224/20000], Bound: 0.37861448526382446, Entropy: 141.73831176757812, Temp: 2.700566053390503, KL: 72.15310668945312, Loss: 0.016010593622922897, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16225/20000], Bound: 0.3412831425666809, Entropy: 141.2883758544922, Temp: 2.700566053390503, KL: 62.03099060058594, Loss: 0.015333263203501701, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16226/20000], Bound: 0.3756037652492523, Entropy: 141.65931701660156, Temp: 2.700566053390503, KL: 70.10479736328125, Loss: 0.018198566511273384, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16227/20000], Bound: 0.3885214626789093, Entropy: 140.92933654785156, Temp: 2.7005655765533447, KL: 74.17308044433594, Loss: 0.01759943924844265, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16228/20000], Bound: 0.3835624158382416, Entropy: 141.31204223632812, Temp: 2.7005653381347656, KL: 72.22285461425781, Loss: 0.018533233553171158, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16229/20000], Bound: 0.3902193605899811, Entropy: 142.42001342773438, Temp: 2.7005646228790283, KL: 76.31672668457031, Loss: 0.014551571570336819, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16230/20000], Bound: 0.4171786904335022, Entropy: 141.1053924560547, Temp: 2.70056414604187, KL: 81.055419921875, Loss: 0.020717648789286613, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16231/20000], Bound: 0.3843260109424591, Entropy: 141.10768127441406, Temp: 2.700563430786133, KL: 72.39591979980469, Loss: 0.01862375997006893, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16232/20000], Bound: 0.3931560814380646, Entropy: 141.20440673828125, Temp: 2.7005627155303955, KL: 76.87722778320312, Loss: 0.015112249180674553, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16233/20000], Bound: 0.39513662457466125, Entropy: 141.4896697998047, Temp: 2.700562000274658, KL: 76.04856872558594, Loss: 0.017728401347994804, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16234/20000], Bound: 0.3448079526424408, Entropy: 141.29620361328125, Temp: 2.700561285018921, KL: 61.6884765625, Loss: 0.017757883295416832, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16235/20000], Bound: 0.3717840909957886, Entropy: 141.30776977539062, Temp: 2.7005600929260254, KL: 71.25621032714844, Loss: 0.014041264541447163, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16236/20000], Bound: 0.38113120198249817, Entropy: 141.8946990966797, Temp: 2.700559377670288, KL: 74.09466552734375, Loss: 0.013762230984866619, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16237/20000], Bound: 0.3474414646625519, Entropy: 142.72149658203125, Temp: 2.700559377670288, KL: 64.18025207519531, Loss: 0.014487917535007, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16238/20000], Bound: 0.3951931893825531, Entropy: 140.3135528564453, Temp: 2.700559139251709, KL: 76.07986450195312, Loss: 0.017701348289847374, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16239/20000], Bound: 0.378735214471817, Entropy: 141.5906524658203, Temp: 2.700558662414551, KL: 70.90264892578125, Loss: 0.018390154466032982, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16240/20000], Bound: 0.4096021354198456, Entropy: 141.86187744140625, Temp: 2.7005579471588135, KL: 82.74174499511719, Loss: 0.013335573486983776, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16241/20000], Bound: 0.3765205144882202, Entropy: 141.54302978515625, Temp: 2.7005579471588135, KL: 71.30722045898438, Loss: 0.016460055485367775, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16242/20000], Bound: 0.3961462378501892, Entropy: 141.1529998779297, Temp: 2.7005579471588135, KL: 77.77677917480469, Loss: 0.0150813739746809, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16243/20000], Bound: 0.40071937441825867, Entropy: 138.7552947998047, Temp: 2.7005581855773926, KL: 76.53532409667969, Loss: 0.019893981516361237, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16244/20000], Bound: 0.36771219968795776, Entropy: 142.13632202148438, Temp: 2.7005579471588135, KL: 68.15219116210938, Loss: 0.017641164362430573, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16245/20000], Bound: 0.3875384032726288, Entropy: 142.03578186035156, Temp: 2.7005574703216553, KL: 75.02894592285156, Loss: 0.015482555143535137, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16246/20000], Bound: 0.36922886967658997, Entropy: 141.3289337158203, Temp: 2.700557231903076, KL: 68.68641662597656, Loss: 0.017450319603085518, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16247/20000], Bound: 0.37168052792549133, Entropy: 140.62059020996094, Temp: 2.700556755065918, KL: 69.60667419433594, Loss: 0.01704053394496441, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16248/20000], Bound: 0.3852112591266632, Entropy: 143.6331329345703, Temp: 2.7005560398101807, KL: 73.699951171875, Loss: 0.016686296090483665, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16249/20000], Bound: 0.3736766278743744, Entropy: 138.805908203125, Temp: 2.7005555629730225, KL: 71.7359619140625, Loss: 0.014155132696032524, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16250/20000], Bound: 0.3620956540107727, Entropy: 142.93679809570312, Temp: 2.7005553245544434, KL: 67.13264465332031, Loss: 0.01658751256763935, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16251/20000], Bound: 0.38836172223091125, Entropy: 142.83651733398438, Temp: 2.700554847717285, KL: 74.35809326171875, Loss: 0.017170285806059837, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16252/20000], Bound: 0.4151080548763275, Entropy: 140.67361450195312, Temp: 2.700554370880127, KL: 81.47462463378906, Loss: 0.01877235621213913, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16253/20000], Bound: 0.40454408526420593, Entropy: 142.13134765625, Temp: 2.7005538940429688, KL: 78.99884033203125, Loss: 0.01744863949716091, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16254/20000], Bound: 0.3929765522480011, Entropy: 141.10610961914062, Temp: 2.7005534172058105, KL: 75.91880798339844, Loss: 0.01678873412311077, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16255/20000], Bound: 0.40392041206359863, Entropy: 139.9449005126953, Temp: 2.7005529403686523, KL: 79.65228271484375, Loss: 0.015892939642071724, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16256/20000], Bound: 0.3777764141559601, Entropy: 140.80137634277344, Temp: 2.7005527019500732, KL: 72.20233154296875, Loss: 0.015472008846700191, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16257/20000], Bound: 0.3766307532787323, Entropy: 141.46441650390625, Temp: 2.7005527019500732, KL: 71.25227355957031, Loss: 0.016620459035038948, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16258/20000], Bound: 0.4028576612472534, Entropy: 142.3119659423828, Temp: 2.700552463531494, KL: 78.30558776855469, Loss: 0.017797697335481644, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16259/20000], Bound: 0.3845880627632141, Entropy: 142.5023651123047, Temp: 2.700552225112915, KL: 74.08047485351562, Loss: 0.015645885840058327, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16260/20000], Bound: 0.4244963228702545, Entropy: 141.25662231445312, Temp: 2.700552225112915, KL: 84.260009765625, Loss: 0.018945617601275444, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16261/20000], Bound: 0.3841765522956848, Entropy: 140.90328979492188, Temp: 2.700551986694336, KL: 72.21937561035156, Loss: 0.018870050087571144, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16262/20000], Bound: 0.379712849855423, Entropy: 139.2836151123047, Temp: 2.700551748275757, KL: 72.39981079101562, Loss: 0.016140682622790337, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16263/20000], Bound: 0.38468992710113525, Entropy: 141.3755645751953, Temp: 2.7005512714385986, KL: 74.58045959472656, Loss: 0.014775054529309273, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16264/20000], Bound: 0.3978404700756073, Entropy: 140.52114868164062, Temp: 2.7005510330200195, KL: 77.59593200683594, Loss: 0.016345541924238205, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16265/20000], Bound: 0.3759353458881378, Entropy: 141.5284881591797, Temp: 2.7005510330200195, KL: 70.9755859375, Loss: 0.01676255837082863, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16266/20000], Bound: 0.38633155822753906, Entropy: 141.46127319335938, Temp: 2.7005507946014404, KL: 71.72793579101562, Loss: 0.020941907539963722, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16267/20000], Bound: 0.3883344829082489, Entropy: 143.20738220214844, Temp: 2.700550079345703, KL: 76.37445068359375, Loss: 0.013422234915196896, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16268/20000], Bound: 0.3997049629688263, Entropy: 142.94187927246094, Temp: 2.700549840927124, KL: 78.97958374023438, Loss: 0.0148092620074749, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16269/20000], Bound: 0.3937985301017761, Entropy: 141.23744201660156, Temp: 2.700550079345703, KL: 77.27647399902344, Loss: 0.014723566360771656, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16270/20000], Bound: 0.38770854473114014, Entropy: 142.36260986328125, Temp: 2.7005505561828613, KL: 73.6434326171875, Loss: 0.018139783293008804, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16271/20000], Bound: 0.3953302800655365, Entropy: 140.58558654785156, Temp: 2.7005507946014404, KL: 77.21281433105469, Loss: 0.015678703784942627, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16272/20000], Bound: 0.37223753333091736, Entropy: 141.382080078125, Temp: 2.7005512714385986, KL: 70.92022705078125, Loss: 0.014903121627867222, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16273/20000], Bound: 0.36637356877326965, Entropy: 142.02349853515625, Temp: 2.700551748275757, KL: 68.81637573242188, Loss: 0.01570824347436428, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16274/20000], Bound: 0.3956407308578491, Entropy: 141.22079467773438, Temp: 2.700551986694336, KL: 76.94001770019531, Loss: 0.016353700309991837, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16275/20000], Bound: 0.3817828595638275, Entropy: 141.79483032226562, Temp: 2.700552463531494, KL: 73.46694946289062, Loss: 0.015273801051080227, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16276/20000], Bound: 0.3825705051422119, Entropy: 141.03860473632812, Temp: 2.7005529403686523, KL: 74.92373657226562, Loss: 0.01299938652664423, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16277/20000], Bound: 0.36882302165031433, Entropy: 140.3224639892578, Temp: 2.7005538940429688, KL: 68.34725952148438, Loss: 0.017864424735307693, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16278/20000], Bound: 0.3866710066795349, Entropy: 141.5904083251953, Temp: 2.700554370880127, KL: 74.68373107910156, Loss: 0.01565275341272354, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16279/20000], Bound: 0.3643933832645416, Entropy: 143.1947784423828, Temp: 2.7005550861358643, KL: 68.61154174804688, Loss: 0.015049836598336697, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16280/20000], Bound: 0.3880755603313446, Entropy: 140.7233123779297, Temp: 2.7005555629730225, KL: 75.22041320800781, Loss: 0.015418760478496552, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16281/20000], Bound: 0.3998505473136902, Entropy: 139.338623046875, Temp: 2.7005562782287598, KL: 78.18234252929688, Loss: 0.016365600749850273, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16282/20000], Bound: 0.3928247094154358, Entropy: 141.0644073486328, Temp: 2.700556993484497, KL: 75.34117126464844, Loss: 0.01777544990181923, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16283/20000], Bound: 0.37749093770980835, Entropy: 139.45822143554688, Temp: 2.7005574703216553, KL: 70.66220092773438, Loss: 0.01817130297422409, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16284/20000], Bound: 0.37596654891967773, Entropy: 140.58636474609375, Temp: 2.7005577087402344, KL: 70.78190612792969, Loss: 0.0171378031373024, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16285/20000], Bound: 0.39176905155181885, Entropy: 142.72361755371094, Temp: 2.7005577087402344, KL: 74.69235229492188, Loss: 0.018401607871055603, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16286/20000], Bound: 0.4094519317150116, Entropy: 140.48887634277344, Temp: 2.7005574703216553, KL: 79.72198486328125, Loss: 0.01884259656071663, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16287/20000], Bound: 0.38346943259239197, Entropy: 140.755859375, Temp: 2.700556993484497, KL: 73.36317443847656, Loss: 0.016371870413422585, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16288/20000], Bound: 0.3952631652355194, Entropy: 140.317626953125, Temp: 2.700556516647339, KL: 76.29690551757812, Loss: 0.01733781024813652, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16289/20000], Bound: 0.37109991908073425, Entropy: 142.1216583251953, Temp: 2.7005560398101807, KL: 69.62576293945312, Loss: 0.016698308289051056, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16290/20000], Bound: 0.39469555020332336, Entropy: 140.3714141845703, Temp: 2.7005555629730225, KL: 76.83851623535156, Loss: 0.016024580225348473, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16291/20000], Bound: 0.3932604193687439, Entropy: 141.75106811523438, Temp: 2.7005555629730225, KL: 76.22607421875, Loss: 0.01637471467256546, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16292/20000], Bound: 0.3892371356487274, Entropy: 140.64682006835938, Temp: 2.7005553245544434, KL: 73.97402954101562, Loss: 0.01835584081709385, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16293/20000], Bound: 0.37767502665519714, Entropy: 140.8055419921875, Temp: 2.700554847717285, KL: 71.7530517578125, Loss: 0.01624978706240654, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16294/20000], Bound: 0.35750916600227356, Entropy: 140.0007781982422, Temp: 2.700554370880127, KL: 65.54766845703125, Loss: 0.017137371003627777, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16295/20000], Bound: 0.36056357622146606, Entropy: 140.87435913085938, Temp: 2.7005536556243896, KL: 66.51570129394531, Loss: 0.016931457445025444, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16296/20000], Bound: 0.3994976282119751, Entropy: 141.26622009277344, Temp: 2.7005527019500732, KL: 79.68592834472656, Loss: 0.013387362472712994, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16297/20000], Bound: 0.37843334674835205, Entropy: 143.75428771972656, Temp: 2.700552463531494, KL: 71.64883422851562, Loss: 0.0168473981320858, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16298/20000], Bound: 0.3828660547733307, Entropy: 141.70468139648438, Temp: 2.700552225112915, KL: 75.33421325683594, Loss: 0.012398183345794678, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16299/20000], Bound: 0.3868754804134369, Entropy: 140.2877655029297, Temp: 2.700552463531494, KL: 73.48072814941406, Loss: 0.01799052022397518, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16300/20000], Bound: 0.3617044985294342, Entropy: 142.75918579101562, Temp: 2.700552463531494, KL: 66.24948120117188, Loss: 0.01801866479218006, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16301/20000], Bound: 0.3807138204574585, Entropy: 141.22970581054688, Temp: 2.700551986694336, KL: 71.956298828125, Loss: 0.017497627064585686, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16302/20000], Bound: 0.3915554881095886, Entropy: 140.15158081054688, Temp: 2.700551748275757, KL: 76.12185668945312, Loss: 0.015638623386621475, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16303/20000], Bound: 0.3916877508163452, Entropy: 142.43226623535156, Temp: 2.7005515098571777, KL: 74.70553588867188, Loss: 0.01833288185298443, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16304/20000], Bound: 0.39953935146331787, Entropy: 142.57997131347656, Temp: 2.7005510330200195, KL: 78.18643188476562, Loss: 0.01618659496307373, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16305/20000], Bound: 0.40378135442733765, Entropy: 140.15171813964844, Temp: 2.7005507946014404, KL: 79.02571105957031, Loss: 0.016975905746221542, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16306/20000], Bound: 0.3759530782699585, Entropy: 140.41079711914062, Temp: 2.7005505561828613, KL: 71.27505493164062, Loss: 0.016217542812228203, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16307/20000], Bound: 0.3815973103046417, Entropy: 142.62196350097656, Temp: 2.7005503177642822, KL: 73.15744018554688, Loss: 0.01574729196727276, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16308/20000], Bound: 0.37784409523010254, Entropy: 141.5839385986328, Temp: 2.700550079345703, KL: 71.82810974121094, Loss: 0.0162009596824646, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16309/20000], Bound: 0.40139660239219666, Entropy: 140.27398681640625, Temp: 2.700549840927124, KL: 78.17776489257812, Loss: 0.017226729542016983, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16310/20000], Bound: 0.38817623257637024, Entropy: 140.6331024169922, Temp: 2.700549840927124, KL: 75.64424133300781, Loss: 0.014688519760966301, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16311/20000], Bound: 0.39204880595207214, Entropy: 142.4144744873047, Temp: 2.700549840927124, KL: 75.43305969238281, Loss: 0.017182469367980957, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16312/20000], Bound: 0.3872486650943756, Entropy: 141.441650390625, Temp: 2.700549840927124, KL: 74.23724365234375, Loss: 0.016791580244898796, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16313/20000], Bound: 0.39913907647132874, Entropy: 140.88143920898438, Temp: 2.700549840927124, KL: 77.87860107421875, Loss: 0.01653617061674595, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16314/20000], Bound: 0.3697730600833893, Entropy: 141.7501678466797, Temp: 2.700550079345703, KL: 70.71031188964844, Loss: 0.013989956118166447, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16315/20000], Bound: 0.3882945477962494, Entropy: 141.25662231445312, Temp: 2.7005505561828613, KL: 74.52372741699219, Loss: 0.016827179118990898, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16316/20000], Bound: 0.37345200777053833, Entropy: 143.99998474121094, Temp: 2.7005507946014404, KL: 70.564208984375, Loss: 0.016205498948693275, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16317/20000], Bound: 0.365850567817688, Entropy: 142.77418518066406, Temp: 2.7005510330200195, KL: 70.35108947753906, Loss: 0.012592397630214691, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16318/20000], Bound: 0.37185460329055786, Entropy: 144.24427795410156, Temp: 2.700551748275757, KL: 71.68833923339844, Loss: 0.013278407976031303, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16319/20000], Bound: 0.39965784549713135, Entropy: 141.10867309570312, Temp: 2.700552463531494, KL: 78.95132446289062, Loss: 0.014835657551884651, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16320/20000], Bound: 0.39008253812789917, Entropy: 142.6656036376953, Temp: 2.7005536556243896, KL: 76.61091613769531, Loss: 0.013932471163570881, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16321/20000], Bound: 0.3780711889266968, Entropy: 140.02243041992188, Temp: 2.7005550861358643, KL: 71.88343811035156, Loss: 0.0162197295576334, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16322/20000], Bound: 0.39148440957069397, Entropy: 141.12002563476562, Temp: 2.7005562782287598, KL: 76.21188354492188, Loss: 0.01543329656124115, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16323/20000], Bound: 0.374136358499527, Entropy: 142.17385864257812, Temp: 2.7005577087402344, KL: 71.54324340820312, Loss: 0.01475582830607891, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16324/20000], Bound: 0.3574773669242859, Entropy: 142.41209411621094, Temp: 2.700559139251709, KL: 67.448974609375, Loss: 0.013600718230009079, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16325/20000], Bound: 0.35819050669670105, Entropy: 141.35072326660156, Temp: 2.7005603313446045, KL: 67.31489562988281, Loss: 0.014218726195394993, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16326/20000], Bound: 0.4093632400035858, Entropy: 139.86634826660156, Temp: 2.7005615234375, KL: 79.93571472167969, Loss: 0.018397368490695953, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16327/20000], Bound: 0.3723137080669403, Entropy: 142.25653076171875, Temp: 2.7005627155303955, KL: 69.3201904296875, Loss: 0.017905935645103455, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16328/20000], Bound: 0.4114477336406708, Entropy: 141.7128448486328, Temp: 2.700563430786133, KL: 82.52133178710938, Loss: 0.014776896685361862, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16329/20000], Bound: 0.3690044581890106, Entropy: 142.30624389648438, Temp: 2.7005646228790283, KL: 68.95751953125, Loss: 0.0168301984667778, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16330/20000], Bound: 0.3732118308544159, Entropy: 138.97189331054688, Temp: 2.7005653381347656, KL: 70.94349670410156, Loss: 0.015376100316643715, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16331/20000], Bound: 0.3708987534046173, Entropy: 140.7972412109375, Temp: 2.700566291809082, KL: 69.56349182128906, Loss: 0.016707444563508034, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16332/20000], Bound: 0.3682883679866791, Entropy: 141.12454223632812, Temp: 2.7005670070648193, KL: 69.34953308105469, Loss: 0.015727419406175613, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16333/20000], Bound: 0.3783077597618103, Entropy: 142.93197631835938, Temp: 2.7005674839019775, KL: 72.52442932128906, Loss: 0.015159296803176403, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16334/20000], Bound: 0.3859090507030487, Entropy: 141.1471710205078, Temp: 2.700568199157715, KL: 72.83831787109375, Loss: 0.018658122047781944, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16335/20000], Bound: 0.40049296617507935, Entropy: 141.52061462402344, Temp: 2.700568437576294, KL: 79.40286254882812, Loss: 0.01446004118770361, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16336/20000], Bound: 0.4117584526538849, Entropy: 141.32945251464844, Temp: 2.7005691528320312, KL: 82.8623046875, Loss: 0.014319860376417637, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16337/20000], Bound: 0.4137204885482788, Entropy: 140.78302001953125, Temp: 2.7005701065063477, KL: 81.08834838867188, Loss: 0.01870630495250225, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16338/20000], Bound: 0.40496352314949036, Entropy: 140.26292419433594, Temp: 2.700571060180664, KL: 78.59327697753906, Loss: 0.01843244582414627, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16339/20000], Bound: 0.39396846294403076, Entropy: 140.42747497558594, Temp: 2.7005717754364014, KL: 76.79412841796875, Loss: 0.015709590166807175, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16340/20000], Bound: 0.4093375504016876, Entropy: 140.80821228027344, Temp: 2.7005722522735596, KL: 82.13035583496094, Loss: 0.01431982684880495, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16341/20000], Bound: 0.39020058512687683, Entropy: 141.54672241210938, Temp: 2.700573444366455, KL: 74.53373718261719, Loss: 0.017842568457126617, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16342/20000], Bound: 0.3723001182079315, Entropy: 140.72052001953125, Temp: 2.7005743980407715, KL: 69.81513977050781, Loss: 0.01698244735598564, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16343/20000], Bound: 0.3549254834651947, Entropy: 142.60247802734375, Temp: 2.7005748748779297, KL: 66.05897521972656, Loss: 0.014854243956506252, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16344/20000], Bound: 0.3784438371658325, Entropy: 138.01388549804688, Temp: 2.700575351715088, KL: 72.12864685058594, Loss: 0.015964824706315994, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16345/20000], Bound: 0.38197532296180725, Entropy: 140.3104248046875, Temp: 2.700575828552246, KL: 73.52166748046875, Loss: 0.015275968238711357, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16346/20000], Bound: 0.41262996196746826, Entropy: 142.1579132080078, Temp: 2.7005765438079834, KL: 82.63270568847656, Loss: 0.01523414347320795, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16347/20000], Bound: 0.3865436613559723, Entropy: 142.84364318847656, Temp: 2.7005774974823, KL: 74.0289306640625, Loss: 0.016796506941318512, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16348/20000], Bound: 0.37298041582107544, Entropy: 142.22999572753906, Temp: 2.700578451156616, KL: 70.79791259765625, Loss: 0.01552314218133688, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16349/20000], Bound: 0.372248113155365, Entropy: 142.4492950439453, Temp: 2.7005789279937744, KL: 70.35948181152344, Loss: 0.015947148203849792, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16350/20000], Bound: 0.3707178831100464, Entropy: 140.0928955078125, Temp: 2.7005796432495117, KL: 67.74996948242188, Loss: 0.019969671964645386, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16351/20000], Bound: 0.3958260118961334, Entropy: 142.30929565429688, Temp: 2.7005796432495117, KL: 76.63931274414062, Loss: 0.017012156546115875, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16352/20000], Bound: 0.3705729842185974, Entropy: 141.74623107910156, Temp: 2.7005796432495117, KL: 70.61369323730469, Loss: 0.014591142535209656, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16353/20000], Bound: 0.37503281235694885, Entropy: 141.0658721923828, Temp: 2.700579881668091, KL: 70.83897399902344, Loss: 0.016535889357328415, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16354/20000], Bound: 0.37952473759651184, Entropy: 143.7902374267578, Temp: 2.700579881668091, KL: 71.97686767578125, Loss: 0.01682339422404766, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16355/20000], Bound: 0.3689974844455719, Entropy: 142.05252075195312, Temp: 2.700579881668091, KL: 69.74311828613281, Loss: 0.015372171066701412, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16356/20000], Bound: 0.39581114053726196, Entropy: 140.9510955810547, Temp: 2.700579881668091, KL: 78.52725219726562, Loss: 0.01350854616612196, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16357/20000], Bound: 0.40129774808883667, Entropy: 143.4704132080078, Temp: 2.700580358505249, KL: 77.7208251953125, Loss: 0.018018445000052452, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16358/20000], Bound: 0.37062138319015503, Entropy: 141.0722198486328, Temp: 2.700580596923828, KL: 68.7841796875, Loss: 0.0180039182305336, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16359/20000], Bound: 0.3815869390964508, Entropy: 140.25308227539062, Temp: 2.700580596923828, KL: 74.24378967285156, Loss: 0.013730683363974094, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16360/20000], Bound: 0.3895235061645508, Entropy: 140.62010192871094, Temp: 2.7005808353424072, KL: 74.06001281738281, Loss: 0.018352193757891655, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16361/20000], Bound: 0.3778536915779114, Entropy: 141.616455078125, Temp: 2.7005808353424072, KL: 72.74949645996094, Loss: 0.014500418677926064, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16362/20000], Bound: 0.400819331407547, Entropy: 143.1886444091797, Temp: 2.7005810737609863, KL: 78.31867980957031, Loss: 0.016647525131702423, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16363/20000], Bound: 0.40697965025901794, Entropy: 139.68634033203125, Temp: 2.7005813121795654, KL: 81.20143127441406, Loss: 0.014724545180797577, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16364/20000], Bound: 0.3749837875366211, Entropy: 141.2110137939453, Temp: 2.7005820274353027, KL: 70.515625, Loss: 0.01710856705904007, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16365/20000], Bound: 0.36901384592056274, Entropy: 141.0600128173828, Temp: 2.700582504272461, KL: 70.07481384277344, Loss: 0.014766653068363667, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16366/20000], Bound: 0.38584986329078674, Entropy: 141.55250549316406, Temp: 2.70058274269104, KL: 74.35734558105469, Loss: 0.01581387035548687, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16367/20000], Bound: 0.3772217929363251, Entropy: 141.89898681640625, Temp: 2.7005834579467773, KL: 71.51069641113281, Loss: 0.016457105055451393, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16368/20000], Bound: 0.3701239228248596, Entropy: 141.57460021972656, Temp: 2.7005839347839355, KL: 69.9130859375, Loss: 0.015651285648345947, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16369/20000], Bound: 0.40032508969306946, Entropy: 140.16275024414062, Temp: 2.7005844116210938, KL: 77.50465393066406, Loss: 0.01788211241364479, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16370/20000], Bound: 0.36165928840637207, Entropy: 139.88426208496094, Temp: 2.700584650039673, KL: 68.03164672851562, Loss: 0.014695748686790466, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16371/20000], Bound: 0.3688237965106964, Entropy: 142.1222381591797, Temp: 2.700584888458252, KL: 68.76493835449219, Loss: 0.0170917771756649, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16372/20000], Bound: 0.3973191976547241, Entropy: 139.5692596435547, Temp: 2.700584888458252, KL: 78.15277099609375, Loss: 0.01502870861440897, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16373/20000], Bound: 0.38915184140205383, Entropy: 142.64959716796875, Temp: 2.700585126876831, KL: 75.65785217285156, Loss: 0.015192308463156223, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16374/20000], Bound: 0.38894134759902954, Entropy: 140.71315002441406, Temp: 2.7005858421325684, KL: 76.12821960449219, Loss: 0.014207310043275356, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16375/20000], Bound: 0.4003257751464844, Entropy: 139.93386840820312, Temp: 2.7005865573883057, KL: 79.40696716308594, Loss: 0.014360440894961357, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16376/20000], Bound: 0.36142024397850037, Entropy: 141.78407287597656, Temp: 2.700587749481201, KL: 69.01100158691406, Loss: 0.012757952325046062, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16377/20000], Bound: 0.3894919753074646, Entropy: 141.56736755371094, Temp: 2.7005889415740967, KL: 76.08833312988281, Loss: 0.014579805545508862, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16378/20000], Bound: 0.37417957186698914, Entropy: 142.00335693359375, Temp: 2.7005906105041504, KL: 69.95327758789062, Loss: 0.017722802236676216, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16379/20000], Bound: 0.38345828652381897, Entropy: 140.99009704589844, Temp: 2.700591802597046, KL: 73.55726623535156, Loss: 0.01600685529410839, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16380/20000], Bound: 0.38952362537384033, Entropy: 141.29769897460938, Temp: 2.7005927562713623, KL: 76.61602783203125, Loss: 0.013620054349303246, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16381/20000], Bound: 0.3689684271812439, Entropy: 142.5769500732422, Temp: 2.700594186782837, KL: 69.95228576660156, Loss: 0.014969704672694206, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16382/20000], Bound: 0.38571757078170776, Entropy: 143.6617889404297, Temp: 2.7005953788757324, KL: 74.76228332519531, Loss: 0.014992884360253811, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16383/20000], Bound: 0.39077091217041016, Entropy: 138.65292358398438, Temp: 2.700596809387207, KL: 77.99003601074219, Loss: 0.01175351906567812, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16384/20000], Bound: 0.4011538326740265, Entropy: 142.01986694335938, Temp: 2.70059871673584, KL: 78.76287841796875, Loss: 0.016009850427508354, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16385/20000], Bound: 0.376010924577713, Entropy: 142.27191162109375, Temp: 2.7006008625030518, KL: 71.38972473144531, Loss: 0.016036449000239372, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16386/20000], Bound: 0.3726561367511749, Entropy: 139.87310791015625, Temp: 2.7006025314331055, KL: 70.53195190429688, Loss: 0.015844007954001427, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16387/20000], Bound: 0.399371862411499, Entropy: 139.38482666015625, Temp: 2.700604200363159, KL: 77.90298461914062, Loss: 0.016619671136140823, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16388/20000], Bound: 0.3932265043258667, Entropy: 140.78150939941406, Temp: 2.700605630874634, KL: 76.91075134277344, Loss: 0.015089016407728195, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16389/20000], Bound: 0.3990243971347809, Entropy: 141.41085815429688, Temp: 2.7006072998046875, KL: 77.029052734375, Loss: 0.018046490848064423, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16390/20000], Bound: 0.36920011043548584, Entropy: 143.13003540039062, Temp: 2.700608491897583, KL: 68.87062072753906, Loss: 0.017094504088163376, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16391/20000], Bound: 0.3846721947193146, Entropy: 142.4516143798828, Temp: 2.7006094455718994, KL: 73.13197326660156, Loss: 0.017447832971811295, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16392/20000], Bound: 0.3745671510696411, Entropy: 138.45074462890625, Temp: 2.7006101608276367, KL: 71.3238525390625, Loss: 0.015391146764159203, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16393/20000], Bound: 0.3826146721839905, Entropy: 143.0172119140625, Temp: 2.700610876083374, KL: 72.48399353027344, Loss: 0.017540689557790756, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16394/20000], Bound: 0.400759756565094, Entropy: 142.89047241210938, Temp: 2.7006113529205322, KL: 79.31784057617188, Loss: 0.014765055850148201, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16395/20000], Bound: 0.36332106590270996, Entropy: 144.00827026367188, Temp: 2.7006120681762695, KL: 68.89938354492188, Loss: 0.013956674374639988, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16396/20000], Bound: 0.34182485938072205, Entropy: 141.22503662109375, Temp: 2.700613021850586, KL: 59.88023376464844, Loss: 0.019590172916650772, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16397/20000], Bound: 0.38446950912475586, Entropy: 141.54273986816406, Temp: 2.700613021850586, KL: 73.85235595703125, Loss: 0.01600491814315319, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16398/20000], Bound: 0.3901408016681671, Entropy: 141.8396453857422, Temp: 2.700613021850586, KL: 76.59991455078125, Loss: 0.013985075987875462, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16399/20000], Bound: 0.3985673487186432, Entropy: 139.65443420410156, Temp: 2.700613498687744, KL: 77.94148254394531, Loss: 0.01610586792230606, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16400/20000], Bound: 0.3902444839477539, Entropy: 140.2532958984375, Temp: 2.7006142139434814, KL: 75.11566162109375, Loss: 0.0167893934994936, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16401/20000], Bound: 0.3894430994987488, Entropy: 138.7145538330078, Temp: 2.7006146907806396, KL: 74.13345336914062, Loss: 0.01817287690937519, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16402/20000], Bound: 0.36050423979759216, Entropy: 141.8778839111328, Temp: 2.7006149291992188, KL: 66.998046875, Loss: 0.016007978469133377, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16403/20000], Bound: 0.4034438133239746, Entropy: 140.4239501953125, Temp: 2.7006149291992188, KL: 78.4869384765625, Loss: 0.017787056043744087, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16404/20000], Bound: 0.4017331600189209, Entropy: 138.46144104003906, Temp: 2.7006149291992188, KL: 78.83992004394531, Loss: 0.016187310218811035, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16405/20000], Bound: 0.37766602635383606, Entropy: 139.16249084472656, Temp: 2.700615167617798, KL: 70.36991882324219, Loss: 0.01880628615617752, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16406/20000], Bound: 0.3885185420513153, Entropy: 140.86509704589844, Temp: 2.7006149291992188, KL: 73.05775451660156, Loss: 0.01966322772204876, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16407/20000], Bound: 0.37247252464294434, Entropy: 142.48040771484375, Temp: 2.7006142139434814, KL: 70.3121337890625, Loss: 0.016153855249285698, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16408/20000], Bound: 0.3577098250389099, Entropy: 141.44253540039062, Temp: 2.700613498687744, KL: 66.87405395507812, Loss: 0.014786102809011936, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16409/20000], Bound: 0.3707219064235687, Entropy: 141.24322509765625, Temp: 2.700613021850586, KL: 69.72010803222656, Loss: 0.01632443629205227, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16410/20000], Bound: 0.37615516781806946, Entropy: 142.3861083984375, Temp: 2.7006123065948486, KL: 70.63029479980469, Loss: 0.01751929149031639, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16411/20000], Bound: 0.4110547602176666, Entropy: 140.11070251464844, Temp: 2.7006118297576904, KL: 82.46116638183594, Loss: 0.014668588526546955, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16412/20000], Bound: 0.37292057275772095, Entropy: 140.78097534179688, Temp: 2.7006115913391113, KL: 70.2147216796875, Loss: 0.016571437940001488, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16413/20000], Bound: 0.3863068222999573, Entropy: 141.0447235107422, Temp: 2.7006113529205322, KL: 74.45671081542969, Loss: 0.015876850113272667, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16414/20000], Bound: 0.3770825266838074, Entropy: 140.3385772705078, Temp: 2.700611114501953, KL: 71.43946838378906, Loss: 0.016514981165528297, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16415/20000], Bound: 0.381961464881897, Entropy: 141.75682067871094, Temp: 2.700610876083374, KL: 74.14891052246094, Loss: 0.01410755142569542, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16416/20000], Bound: 0.4095449149608612, Entropy: 140.50369262695312, Temp: 2.700610876083374, KL: 81.71572875976562, Loss: 0.01520379539579153, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16417/20000], Bound: 0.36876001954078674, Entropy: 143.22393798828125, Temp: 2.7006113529205322, KL: 68.62535095214844, Loss: 0.017316818237304688, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16418/20000], Bound: 0.41464900970458984, Entropy: 140.30841064453125, Temp: 2.7006115913391113, KL: 82.73184204101562, Loss: 0.016186585649847984, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16419/20000], Bound: 0.4112606942653656, Entropy: 140.44195556640625, Temp: 2.7006118297576904, KL: 81.04682922363281, Loss: 0.0174025297164917, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16420/20000], Bound: 0.4183896481990814, Entropy: 138.39697265625, Temp: 2.7006123065948486, KL: 85.07681274414062, Loss: 0.013958141207695007, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16421/20000], Bound: 0.39788711071014404, Entropy: 140.57264709472656, Temp: 2.700613260269165, KL: 75.43083190917969, Loss: 0.020380325615406036, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16422/20000], Bound: 0.3839271366596222, Entropy: 142.7845458984375, Temp: 2.7006137371063232, KL: 73.90359497070312, Loss: 0.015618054196238518, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16423/20000], Bound: 0.377607136964798, Entropy: 140.51019287109375, Temp: 2.7006142139434814, KL: 69.60546875, Loss: 0.020190205425024033, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16424/20000], Bound: 0.38542652130126953, Entropy: 140.2277069091797, Temp: 2.7006139755249023, KL: 74.80964660644531, Loss: 0.014748351648449898, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16425/20000], Bound: 0.38424623012542725, Entropy: 141.4845428466797, Temp: 2.7006139755249023, KL: 73.59124755859375, Loss: 0.01636810041964054, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16426/20000], Bound: 0.3893696069717407, Entropy: 144.05335998535156, Temp: 2.7006139755249023, KL: 75.01695251464844, Loss: 0.01649726927280426, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16427/20000], Bound: 0.37290170788764954, Entropy: 141.6688232421875, Temp: 2.7006139755249023, KL: 71.44442749023438, Loss: 0.014284756034612656, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16428/20000], Bound: 0.4144803583621979, Entropy: 142.3321990966797, Temp: 2.7006142139434814, KL: 81.8583984375, Loss: 0.017708731815218925, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16429/20000], Bound: 0.37153124809265137, Entropy: 142.64727783203125, Temp: 2.7006144523620605, KL: 69.81295776367188, Loss: 0.01658015139400959, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16430/20000], Bound: 0.3968062400817871, Entropy: 141.5592041015625, Temp: 2.7006146907806396, KL: 76.99391174316406, Loss: 0.01689312420785427, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16431/20000], Bound: 0.38113686442375183, Entropy: 141.22816467285156, Temp: 2.7006149291992188, KL: 72.64846801757812, Loss: 0.01644333079457283, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16432/20000], Bound: 0.38250577449798584, Entropy: 140.75418090820312, Temp: 2.700615167617798, KL: 73.50148010253906, Loss: 0.015598469413816929, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16433/20000], Bound: 0.39322909712791443, Entropy: 141.8102264404297, Temp: 2.700615406036377, KL: 76.60728454589844, Loss: 0.015652377158403397, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16434/20000], Bound: 0.3951420485973358, Entropy: 139.8913116455078, Temp: 2.700615882873535, KL: 76.53208923339844, Loss: 0.016836650669574738, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16435/20000], Bound: 0.36759814620018005, Entropy: 141.52130126953125, Temp: 2.7006161212921143, KL: 68.38560485839844, Loss: 0.017149467021226883, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16436/20000], Bound: 0.38700321316719055, Entropy: 142.2035369873047, Temp: 2.7006161212921143, KL: 74.32453918457031, Loss: 0.016497844830155373, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16437/20000], Bound: 0.37785181403160095, Entropy: 140.77955627441406, Temp: 2.7006161212921143, KL: 72.04937744140625, Loss: 0.015795987099409103, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16438/20000], Bound: 0.3836577534675598, Entropy: 140.20208740234375, Temp: 2.7006161212921143, KL: 73.285400390625, Loss: 0.016617676243185997, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16439/20000], Bound: 0.3804898262023926, Entropy: 141.31980895996094, Temp: 2.7006161212921143, KL: 70.4718017578125, Loss: 0.02012666128575802, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16440/20000], Bound: 0.38436686992645264, Entropy: 139.4478759765625, Temp: 2.700615882873535, KL: 72.90965270996094, Loss: 0.01769501529633999, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16441/20000], Bound: 0.3749561309814453, Entropy: 143.08316040039062, Temp: 2.700615167617798, KL: 71.35464477539062, Loss: 0.015540759079158306, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16442/20000], Bound: 0.3911884129047394, Entropy: 141.60848999023438, Temp: 2.7006146907806396, KL: 75.73898315429688, Loss: 0.01614837348461151, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16443/20000], Bound: 0.4075448215007782, Entropy: 141.4931640625, Temp: 2.7006142139434814, KL: 80.47615051269531, Loss: 0.016382556408643723, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16444/20000], Bound: 0.38470926880836487, Entropy: 141.81625366210938, Temp: 2.7006139755249023, KL: 74.99014282226562, Loss: 0.014027531258761883, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16445/20000], Bound: 0.37393930554389954, Entropy: 141.40769958496094, Temp: 2.7006142139434814, KL: 71.40135192871094, Loss: 0.01491448376327753, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16446/20000], Bound: 0.39575013518333435, Entropy: 138.171875, Temp: 2.7006144523620605, KL: 77.74429321289062, Loss: 0.0149251539260149, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16447/20000], Bound: 0.37716245651245117, Entropy: 142.16307067871094, Temp: 2.700615167617798, KL: 72.61370849609375, Loss: 0.014383623376488686, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16448/20000], Bound: 0.39981478452682495, Entropy: 140.2489776611328, Temp: 2.700615882873535, KL: 79.15447998046875, Loss: 0.01454663835465908, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16449/20000], Bound: 0.3850022256374359, Entropy: 141.37355041503906, Temp: 2.7006168365478516, KL: 73.73382568359375, Loss: 0.016511419788002968, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16450/20000], Bound: 0.407453715801239, Entropy: 141.23489379882812, Temp: 2.700617790222168, KL: 80.45803833007812, Loss: 0.016365353018045425, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16451/20000], Bound: 0.38947856426239014, Entropy: 142.25637817382812, Temp: 2.7006187438964844, KL: 73.96760559082031, Loss: 0.01849919930100441, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16452/20000], Bound: 0.37687718868255615, Entropy: 141.23226928710938, Temp: 2.7006194591522217, KL: 71.96615600585938, Loss: 0.015430537983775139, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16453/20000], Bound: 0.4081098437309265, Entropy: 141.12747192382812, Temp: 2.700620174407959, KL: 78.089599609375, Loss: 0.02111615613102913, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16454/20000], Bound: 0.3750499188899994, Entropy: 142.3280792236328, Temp: 2.700620412826538, KL: 71.21711730957031, Loss: 0.01584523916244507, Learning Rate: 2.816244101294322e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16455/20000], Bound: 0.39029157161712646, Entropy: 140.91563415527344, Temp: 2.700620651245117, KL: 74.52529907226562, Loss: 0.01790805347263813, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16456/20000], Bound: 0.4047328233718872, Entropy: 141.64120483398438, Temp: 2.700620651245117, KL: 79.33277893066406, Loss: 0.01693570986390114, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16457/20000], Bound: 0.35120993852615356, Entropy: 141.39276123046875, Temp: 2.700620651245117, KL: 65.41575622558594, Loss: 0.014131829142570496, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16458/20000], Bound: 0.3890758156776428, Entropy: 139.54005432128906, Temp: 2.700620651245117, KL: 75.21653747558594, Loss: 0.015968473628163338, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16459/20000], Bound: 0.41051924228668213, Entropy: 140.20750427246094, Temp: 2.7006208896636963, KL: 80.77780151367188, Loss: 0.017485421150922775, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16460/20000], Bound: 0.3791520297527313, Entropy: 139.61717224121094, Temp: 2.7006211280822754, KL: 72.42723083496094, Loss: 0.01579068787395954, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16461/20000], Bound: 0.4051932096481323, Entropy: 141.21519470214844, Temp: 2.7006213665008545, KL: 78.96864318847656, Loss: 0.017865484580397606, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16462/20000], Bound: 0.37393298745155334, Entropy: 140.9649200439453, Temp: 2.7006216049194336, KL: 71.23171997070312, Loss: 0.015225269831717014, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16463/20000], Bound: 0.38778963685035706, Entropy: 141.8732452392578, Temp: 2.7006216049194336, KL: 74.48240661621094, Loss: 0.016630994156003, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16464/20000], Bound: 0.3989984095096588, Entropy: 140.29354858398438, Temp: 2.7006218433380127, KL: 78.96195983886719, Loss: 0.014453692361712456, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16465/20000], Bound: 0.39382779598236084, Entropy: 141.80792236328125, Temp: 2.70062255859375, KL: 76.01741027832031, Loss: 0.01707128994166851, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16466/20000], Bound: 0.39865025877952576, Entropy: 142.2415771484375, Temp: 2.700623035430908, KL: 77.45533752441406, Loss: 0.017051629722118378, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16467/20000], Bound: 0.38159120082855225, Entropy: 142.82339477539062, Temp: 2.7006235122680664, KL: 73.91067504882812, Loss: 0.01435011811554432, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16468/20000], Bound: 0.3807588815689087, Entropy: 141.85301208496094, Temp: 2.7006242275238037, KL: 71.21540832519531, Loss: 0.018894094973802567, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16469/20000], Bound: 0.4030616283416748, Entropy: 140.8148651123047, Temp: 2.700624465942383, KL: 79.48304748535156, Loss: 0.01573130488395691, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16470/20000], Bound: 0.3529336154460907, Entropy: 140.7156219482422, Temp: 2.700624942779541, KL: 64.80197143554688, Loss: 0.016154775395989418, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16471/20000], Bound: 0.38967958092689514, Entropy: 140.27137756347656, Temp: 2.700624942779541, KL: 77.52789306640625, Loss: 0.012016773223876953, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16472/20000], Bound: 0.39165830612182617, Entropy: 141.00865173339844, Temp: 2.7006258964538574, KL: 75.4256591796875, Loss: 0.01698424480855465, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16473/20000], Bound: 0.3567582070827484, Entropy: 143.4006805419922, Temp: 2.7006266117095947, KL: 66.32395935058594, Loss: 0.015311650931835175, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16474/20000], Bound: 0.36947906017303467, Entropy: 142.65110778808594, Temp: 2.700627326965332, KL: 71.26107788085938, Loss: 0.012815892696380615, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16475/20000], Bound: 0.3636595904827118, Entropy: 141.49728393554688, Temp: 2.7006280422210693, KL: 68.76205444335938, Loss: 0.014388004317879677, Learning Rate: 2.816244101294322e-06\n",
      "Epoch [16476/20000], Bound: 0.3423525393009186, Entropy: 141.62306213378906, Temp: 2.7006287574768066, KL: 63.68562316894531, Loss: 0.01281255204230547, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16477/20000], Bound: 0.3912477195262909, Entropy: 142.98922729492188, Temp: 2.700629711151123, KL: 73.42936706542969, Loss: 0.020456846803426743, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16478/20000], Bound: 0.3733396530151367, Entropy: 143.66368103027344, Temp: 2.700629949569702, KL: 70.63180541992188, Loss: 0.016021443530917168, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16479/20000], Bound: 0.3716670572757721, Entropy: 141.2843475341797, Temp: 2.7006301879882812, KL: 70.44779968261719, Loss: 0.015476713888347149, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16480/20000], Bound: 0.35879185795783997, Entropy: 142.78085327148438, Temp: 2.7006304264068604, KL: 67.47224426269531, Loss: 0.014240081422030926, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16481/20000], Bound: 0.385428786277771, Entropy: 141.0477294921875, Temp: 2.7006309032440186, KL: 75.38819885253906, Loss: 0.013678612187504768, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16482/20000], Bound: 0.3771965503692627, Entropy: 141.97462463378906, Temp: 2.7006313800811768, KL: 71.81306457519531, Loss: 0.015884215012192726, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16483/20000], Bound: 0.40459343791007996, Entropy: 141.87948608398438, Temp: 2.700632095336914, KL: 78.6214599609375, Loss: 0.018175454810261726, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16484/20000], Bound: 0.38690125942230225, Entropy: 143.20541381835938, Temp: 2.700632333755493, KL: 72.79524230957031, Loss: 0.01927422732114792, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16485/20000], Bound: 0.4137963354587555, Entropy: 140.42578125, Temp: 2.700632333755493, KL: 81.27737426757812, Loss: 0.018399642780423164, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16486/20000], Bound: 0.3763219714164734, Entropy: 140.87164306640625, Temp: 2.700632333755493, KL: 69.97406005859375, Loss: 0.01882319152355194, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16487/20000], Bound: 0.37497785687446594, Entropy: 142.219970703125, Temp: 2.700632095336914, KL: 72.49705505371094, Loss: 0.013437354005873203, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16488/20000], Bound: 0.38468948006629944, Entropy: 142.1518096923828, Temp: 2.700632095336914, KL: 72.99308776855469, Loss: 0.017714448273181915, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16489/20000], Bound: 0.4255446493625641, Entropy: 140.25418090820312, Temp: 2.700632095336914, KL: 85.21589660644531, Loss: 0.01777665689587593, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16490/20000], Bound: 0.38592293858528137, Entropy: 140.314453125, Temp: 2.700632095336914, KL: 73.74073791503906, Loss: 0.016995351761579514, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16491/20000], Bound: 0.39405378699302673, Entropy: 142.5693359375, Temp: 2.700632095336914, KL: 76.88197326660156, Loss: 0.015594125725328922, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16492/20000], Bound: 0.3624178469181061, Entropy: 141.59788513183594, Temp: 2.700632095336914, KL: 67.34303283691406, Loss: 0.01636665314435959, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16493/20000], Bound: 0.41171324253082275, Entropy: 140.08938598632812, Temp: 2.700632095336914, KL: 82.44256591796875, Loss: 0.015072350390255451, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16494/20000], Bound: 0.39969465136528015, Entropy: 140.4972381591797, Temp: 2.700632333755493, KL: 75.43365478515625, Loss: 0.021369457244873047, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16495/20000], Bound: 0.3602219521999359, Entropy: 141.20687866210938, Temp: 2.700632095336914, KL: 68.28585815429688, Loss: 0.013476967811584473, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16496/20000], Bound: 0.3900635242462158, Entropy: 139.4191131591797, Temp: 2.700632095336914, KL: 75.24441528320312, Loss: 0.016452917829155922, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16497/20000], Bound: 0.37426164746284485, Entropy: 141.0909881591797, Temp: 2.700632095336914, KL: 70.41572570800781, Loss: 0.016910476610064507, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16498/20000], Bound: 0.39221230149269104, Entropy: 140.3074493408203, Temp: 2.700632095336914, KL: 77.70840454101562, Loss: 0.01305963471531868, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16499/20000], Bound: 0.39383357763290405, Entropy: 143.16600036621094, Temp: 2.700632333755493, KL: 75.46537780761719, Loss: 0.018096622079610825, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16500/20000], Bound: 0.35536646842956543, Entropy: 141.19049072265625, Temp: 2.7006325721740723, KL: 66.86817932128906, Loss: 0.013584328815340996, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16501/20000], Bound: 0.3733758330345154, Entropy: 142.9908447265625, Temp: 2.7006328105926514, KL: 72.10194396972656, Loss: 0.013318787328898907, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16502/20000], Bound: 0.38774701952934265, Entropy: 139.6157684326172, Temp: 2.7006332874298096, KL: 76.47450256347656, Loss: 0.01291978545486927, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16503/20000], Bound: 0.3789573609828949, Entropy: 140.6805877685547, Temp: 2.700634241104126, KL: 71.56694030761719, Loss: 0.01727953925728798, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16504/20000], Bound: 0.3875405788421631, Entropy: 139.94842529296875, Temp: 2.7006349563598633, KL: 74.18997192382812, Loss: 0.01703776977956295, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16505/20000], Bound: 0.3710962235927582, Entropy: 142.01983642578125, Temp: 2.7006351947784424, KL: 70.45771789550781, Loss: 0.015156666748225689, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16506/20000], Bound: 0.4021436274051666, Entropy: 140.52391052246094, Temp: 2.7006356716156006, KL: 78.09126281738281, Loss: 0.017800407484173775, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16507/20000], Bound: 0.38965484499931335, Entropy: 140.5580596923828, Temp: 2.700636148452759, KL: 74.77096557617188, Loss: 0.017107635736465454, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16508/20000], Bound: 0.4029444754123688, Entropy: 140.58497619628906, Temp: 2.700636386871338, KL: 79.0560302734375, Loss: 0.016457168385386467, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16509/20000], Bound: 0.41365212202072144, Entropy: 140.1508331298828, Temp: 2.700636863708496, KL: 81.26651000976562, Loss: 0.018338659778237343, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16510/20000], Bound: 0.3887365162372589, Entropy: 139.40621948242188, Temp: 2.700637102127075, KL: 75.51753234863281, Loss: 0.015227467752993107, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16511/20000], Bound: 0.36538881063461304, Entropy: 141.40000915527344, Temp: 2.7006375789642334, KL: 66.55973815917969, Loss: 0.019370513036847115, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16512/20000], Bound: 0.38448742032051086, Entropy: 139.98316955566406, Temp: 2.7006375789642334, KL: 72.96870422363281, Loss: 0.017650816589593887, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16513/20000], Bound: 0.40872302651405334, Entropy: 139.7242889404297, Temp: 2.7006373405456543, KL: 78.47879028320312, Loss: 0.02073788270354271, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16514/20000], Bound: 0.38672271370887756, Entropy: 141.47994995117188, Temp: 2.700636863708496, KL: 74.55332946777344, Loss: 0.01592283882200718, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16515/20000], Bound: 0.3851906359195709, Entropy: 140.33067321777344, Temp: 2.700636625289917, KL: 74.197998046875, Loss: 0.015753796324133873, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16516/20000], Bound: 0.3889195919036865, Entropy: 141.2332763671875, Temp: 2.700636386871338, KL: 75.69706726074219, Loss: 0.014994277618825436, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16517/20000], Bound: 0.38492438197135925, Entropy: 142.18267822265625, Temp: 2.700636386871338, KL: 73.41703796386719, Loss: 0.017056141048669815, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16518/20000], Bound: 0.36198726296424866, Entropy: 140.57835388183594, Temp: 2.700636386871338, KL: 67.02922058105469, Loss: 0.016723064705729485, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16519/20000], Bound: 0.3668464422225952, Entropy: 140.79061889648438, Temp: 2.700636148452759, KL: 70.59910583496094, Loss: 0.012656592763960361, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16520/20000], Bound: 0.3843020796775818, Entropy: 141.885986328125, Temp: 2.700636148452759, KL: 75.58576965332031, Loss: 0.012705706991255283, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16521/20000], Bound: 0.3985234498977661, Entropy: 140.73883056640625, Temp: 2.700636625289917, KL: 77.232666015625, Loss: 0.017394250258803368, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16522/20000], Bound: 0.3954663574695587, Entropy: 142.00198364257812, Temp: 2.700637102127075, KL: 76.41082763671875, Loss: 0.017238814383745193, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16523/20000], Bound: 0.3749520778656006, Entropy: 141.75758361816406, Temp: 2.7006373405456543, KL: 71.8170166015625, Loss: 0.014682754874229431, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16524/20000], Bound: 0.3722410202026367, Entropy: 142.03512573242188, Temp: 2.7006378173828125, KL: 70.07943725585938, Loss: 0.016462353989481926, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16525/20000], Bound: 0.3886035978794098, Entropy: 140.25283813476562, Temp: 2.7006378173828125, KL: 76.19618225097656, Loss: 0.013898949138820171, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16526/20000], Bound: 0.35558944940567017, Entropy: 143.48297119140625, Temp: 2.70063853263855, KL: 64.77653503417969, Loss: 0.017572101205587387, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16527/20000], Bound: 0.3926963210105896, Entropy: 142.17214965820312, Temp: 2.70063853263855, KL: 76.36575317382812, Loss: 0.015809230506420135, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16528/20000], Bound: 0.3946446180343628, Entropy: 141.60972595214844, Temp: 2.700639009475708, KL: 77.29655456542969, Loss: 0.015149504877626896, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16529/20000], Bound: 0.4035530090332031, Entropy: 140.7977752685547, Temp: 2.700639247894287, KL: 79.87600708007812, Loss: 0.01527604553848505, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16530/20000], Bound: 0.38489794731140137, Entropy: 142.49781799316406, Temp: 2.7006399631500244, KL: 75.27388000488281, Loss: 0.01360416878014803, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16531/20000], Bound: 0.3689025640487671, Entropy: 141.93557739257812, Temp: 2.700640916824341, KL: 69.31793212890625, Loss: 0.01610986515879631, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16532/20000], Bound: 0.38183900713920593, Entropy: 142.64984130859375, Temp: 2.700641632080078, KL: 72.68679809570312, Loss: 0.016749095171689987, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16533/20000], Bound: 0.36008718609809875, Entropy: 142.3361053466797, Temp: 2.7006421089172363, KL: 68.53904724121094, Loss: 0.012938177213072777, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16534/20000], Bound: 0.3525089621543884, Entropy: 142.96908569335938, Temp: 2.7006428241729736, KL: 65.47482299804688, Loss: 0.014690575189888477, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16535/20000], Bound: 0.38909798860549927, Entropy: 141.73289489746094, Temp: 2.700643539428711, KL: 73.89985656738281, Loss: 0.01841842383146286, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16536/20000], Bound: 0.381547212600708, Entropy: 140.1119842529297, Temp: 2.700644016265869, KL: 73.04266357421875, Loss: 0.01593376137316227, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16537/20000], Bound: 0.3775673508644104, Entropy: 141.87037658691406, Temp: 2.7006444931030273, KL: 71.62799072265625, Loss: 0.016424639150500298, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16538/20000], Bound: 0.36674070358276367, Entropy: 140.491455078125, Temp: 2.7006449699401855, KL: 68.73524475097656, Loss: 0.01605193503201008, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16539/20000], Bound: 0.387575626373291, Entropy: 140.11016845703125, Temp: 2.7006449699401855, KL: 76.17314147949219, Loss: 0.013385155238211155, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16540/20000], Bound: 0.40549278259277344, Entropy: 140.61166381835938, Temp: 2.7006454467773438, KL: 79.80485534667969, Loss: 0.01648390106856823, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16541/20000], Bound: 0.3884817063808441, Entropy: 139.6800537109375, Temp: 2.700646162033081, KL: 73.45318603515625, Loss: 0.018911441788077354, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16542/20000], Bound: 0.3556854724884033, Entropy: 140.8742218017578, Temp: 2.70064640045166, KL: 66.35812377929688, Loss: 0.014693617820739746, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16543/20000], Bound: 0.387813001871109, Entropy: 143.46463012695312, Temp: 2.7006466388702393, KL: 73.69758605957031, Loss: 0.018096888437867165, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16544/20000], Bound: 0.3820877969264984, Entropy: 141.50489807128906, Temp: 2.7006468772888184, KL: 73.91264343261719, Loss: 0.014613114297389984, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16545/20000], Bound: 0.3588847815990448, Entropy: 141.21771240234375, Temp: 2.7006471157073975, KL: 67.02976989746094, Loss: 0.015107638202607632, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16546/20000], Bound: 0.36826056241989136, Entropy: 140.4585418701172, Temp: 2.7006473541259766, KL: 68.41694641113281, Loss: 0.017440075054764748, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16547/20000], Bound: 0.36493733525276184, Entropy: 143.44003295898438, Temp: 2.7006473541259766, KL: 67.45988464355469, Loss: 0.01746753416955471, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16548/20000], Bound: 0.3960627019405365, Entropy: 141.26007080078125, Temp: 2.7006471157073975, KL: 77.34414672851562, Loss: 0.015837468206882477, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16549/20000], Bound: 0.36333411931991577, Entropy: 142.1385955810547, Temp: 2.7006471157073975, KL: 66.28045654296875, Loss: 0.018812503665685654, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16550/20000], Bound: 0.3860100507736206, Entropy: 143.07595825195312, Temp: 2.7006466388702393, KL: 74.943115234375, Loss: 0.014816406182944775, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16551/20000], Bound: 0.40655621886253357, Entropy: 143.197998046875, Temp: 2.70064640045166, KL: 78.89079284667969, Loss: 0.018767517060041428, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16552/20000], Bound: 0.41099169850349426, Entropy: 141.2161102294922, Temp: 2.700646162033081, KL: 81.16294860839844, Loss: 0.01703716441988945, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16553/20000], Bound: 0.3887619376182556, Entropy: 139.959716796875, Temp: 2.700645923614502, KL: 74.22593688964844, Loss: 0.01763259619474411, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16554/20000], Bound: 0.3564969003200531, Entropy: 141.8386688232422, Temp: 2.700645685195923, KL: 65.57846069335938, Loss: 0.01655678264796734, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16555/20000], Bound: 0.37747788429260254, Entropy: 141.4307098388672, Temp: 2.7006452083587646, KL: 70.56498718261719, Loss: 0.018345031887292862, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16556/20000], Bound: 0.4087105989456177, Entropy: 138.772705078125, Temp: 2.7006447315216064, KL: 80.17143249511719, Loss: 0.017597248777747154, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16557/20000], Bound: 0.38228902220726013, Entropy: 140.7089080810547, Temp: 2.7006442546844482, KL: 73.91229248046875, Loss: 0.01472172699868679, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16558/20000], Bound: 0.388054758310318, Entropy: 140.18272399902344, Temp: 2.700644016265869, KL: 75.58961486816406, Loss: 0.014724797569215298, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16559/20000], Bound: 0.40319550037384033, Entropy: 142.39012145996094, Temp: 2.700644016265869, KL: 80.69276428222656, Loss: 0.013565916568040848, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16560/20000], Bound: 0.3998648226261139, Entropy: 142.62135314941406, Temp: 2.7006442546844482, KL: 78.85899353027344, Loss: 0.015121588483452797, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16561/20000], Bound: 0.38883253931999207, Entropy: 141.01632690429688, Temp: 2.7006447315216064, KL: 75.81961059570312, Loss: 0.014720308594405651, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16562/20000], Bound: 0.3837554156780243, Entropy: 143.1559600830078, Temp: 2.7006452083587646, KL: 73.11239624023438, Loss: 0.016990797594189644, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16563/20000], Bound: 0.3860935568809509, Entropy: 140.98829650878906, Temp: 2.700645685195923, KL: 73.90080261230469, Loss: 0.016791202127933502, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16564/20000], Bound: 0.3940502405166626, Entropy: 138.87364196777344, Temp: 2.700646162033081, KL: 74.5538330078125, Loss: 0.019902700558304787, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16565/20000], Bound: 0.38068944215774536, Entropy: 140.60565185546875, Temp: 2.700646162033081, KL: 71.87895202636719, Loss: 0.017628569155931473, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16566/20000], Bound: 0.39317524433135986, Entropy: 138.298828125, Temp: 2.700646162033081, KL: 76.64947509765625, Loss: 0.015545184724032879, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16567/20000], Bound: 0.3600657284259796, Entropy: 140.74053955078125, Temp: 2.700646162033081, KL: 66.17640686035156, Loss: 0.017301272600889206, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16568/20000], Bound: 0.36503031849861145, Entropy: 141.7861328125, Temp: 2.700645923614502, KL: 68.93319702148438, Loss: 0.014788511209189892, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16569/20000], Bound: 0.40122705698013306, Entropy: 141.27174377441406, Temp: 2.700645923614502, KL: 78.17512512207031, Loss: 0.017138924449682236, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16570/20000], Bound: 0.4049234390258789, Entropy: 141.31011962890625, Temp: 2.700645923614502, KL: 79.54296875, Loss: 0.016652630642056465, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16571/20000], Bound: 0.3732224404811859, Entropy: 141.22311401367188, Temp: 2.700645923614502, KL: 70.93624877929688, Loss: 0.015395812690258026, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16572/20000], Bound: 0.40176481008529663, Entropy: 140.5433349609375, Temp: 2.700645923614502, KL: 77.71109008789062, Loss: 0.01829499937593937, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16573/20000], Bound: 0.38889893889427185, Entropy: 140.52232360839844, Temp: 2.700645923614502, KL: 75.10086059570312, Loss: 0.016087012365460396, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16574/20000], Bound: 0.38729292154312134, Entropy: 141.33914184570312, Temp: 2.700645923614502, KL: 74.51284790039062, Loss: 0.016306132078170776, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16575/20000], Bound: 0.3624305725097656, Entropy: 142.0994873046875, Temp: 2.700645923614502, KL: 67.00022888183594, Loss: 0.017008107155561447, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16576/20000], Bound: 0.4183300733566284, Entropy: 141.15023803710938, Temp: 2.700645685195923, KL: 84.03129577636719, Loss: 0.01586046814918518, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16577/20000], Bound: 0.3925732374191284, Entropy: 142.49952697753906, Temp: 2.700645685195923, KL: 76.24043273925781, Loss: 0.015974273905158043, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16578/20000], Bound: 0.38415613770484924, Entropy: 140.6350860595703, Temp: 2.700645923614502, KL: 71.50399780273438, Loss: 0.020184237509965897, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16579/20000], Bound: 0.3778340220451355, Entropy: 142.27992248535156, Temp: 2.700645685195923, KL: 72.29771423339844, Loss: 0.01532694511115551, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16580/20000], Bound: 0.3716833293437958, Entropy: 140.80645751953125, Temp: 2.7006454467773438, KL: 69.29931640625, Loss: 0.01761174388229847, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16581/20000], Bound: 0.3936460018157959, Entropy: 142.21743774414062, Temp: 2.7006452083587646, KL: 76.56884765625, Loss: 0.01595134660601616, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16582/20000], Bound: 0.39605802297592163, Entropy: 139.9772491455078, Temp: 2.7006449699401855, KL: 77.64952087402344, Loss: 0.015269510447978973, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16583/20000], Bound: 0.39616483449935913, Entropy: 142.034423828125, Temp: 2.7006449699401855, KL: 76.45475769042969, Loss: 0.017540041357278824, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16584/20000], Bound: 0.3833090364933014, Entropy: 141.88314819335938, Temp: 2.7006449699401855, KL: 72.34867858886719, Loss: 0.018164658918976784, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16585/20000], Bound: 0.3676838278770447, Entropy: 141.09475708007812, Temp: 2.7006449699401855, KL: 68.55976867675781, Loss: 0.016872303560376167, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16586/20000], Bound: 0.4060624837875366, Entropy: 140.6162872314453, Temp: 2.7006447315216064, KL: 79.14515686035156, Loss: 0.01802190952003002, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16587/20000], Bound: 0.363500714302063, Entropy: 142.562744140625, Temp: 2.7006442546844482, KL: 68.16844177246094, Loss: 0.015404105186462402, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16588/20000], Bound: 0.37882184982299805, Entropy: 140.7103729248047, Temp: 2.700644016265869, KL: 73.41801452636719, Loss: 0.013780097477138042, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16589/20000], Bound: 0.40391311049461365, Entropy: 140.87448120117188, Temp: 2.700644016265869, KL: 79.84852600097656, Loss: 0.01552650984376669, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16590/20000], Bound: 0.4023098051548004, Entropy: 141.10296630859375, Temp: 2.7006442546844482, KL: 77.56117248535156, Loss: 0.018873780965805054, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16591/20000], Bound: 0.3975190818309784, Entropy: 140.46121215820312, Temp: 2.7006442546844482, KL: 78.09587097167969, Loss: 0.01524435356259346, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16592/20000], Bound: 0.38207170367240906, Entropy: 140.94288635253906, Temp: 2.7006444931030273, KL: 73.42527770996094, Loss: 0.01550676766782999, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16593/20000], Bound: 0.3874044716358185, Entropy: 140.20277404785156, Temp: 2.7006447315216064, KL: 73.07940673828125, Loss: 0.019020305946469307, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16594/20000], Bound: 0.3883860409259796, Entropy: 142.5678253173828, Temp: 2.7006447315216064, KL: 74.51748657226562, Loss: 0.016889125108718872, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16595/20000], Bound: 0.3585801422595978, Entropy: 140.44216918945312, Temp: 2.7006447315216064, KL: 66.88955688476562, Loss: 0.015209078788757324, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16596/20000], Bound: 0.4013671576976776, Entropy: 142.13905334472656, Temp: 2.7006447315216064, KL: 80.31718444824219, Loss: 0.013250402174890041, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16597/20000], Bound: 0.37295812368392944, Entropy: 142.0792999267578, Temp: 2.7006449699401855, KL: 71.96733093261719, Loss: 0.013346802443265915, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16598/20000], Bound: 0.36497947573661804, Entropy: 142.45338439941406, Temp: 2.7006454467773438, KL: 66.66285705566406, Loss: 0.018965192139148712, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16599/20000], Bound: 0.3704000413417816, Entropy: 142.26905822753906, Temp: 2.700645685195923, KL: 67.07322692871094, Loss: 0.021055258810520172, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16600/20000], Bound: 0.3761647343635559, Entropy: 141.2080535888672, Temp: 2.7006452083587646, KL: 71.09913635253906, Loss: 0.016656629741191864, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16601/20000], Bound: 0.39085853099823, Entropy: 142.84841918945312, Temp: 2.7006449699401855, KL: 74.3992919921875, Loss: 0.018449651077389717, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16602/20000], Bound: 0.3833128809928894, Entropy: 142.52134704589844, Temp: 2.7006442546844482, KL: 74.28096008300781, Loss: 0.014589277096092701, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16603/20000], Bound: 0.3864321708679199, Entropy: 140.3377227783203, Temp: 2.70064377784729, KL: 73.1954345703125, Loss: 0.018279988318681717, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16604/20000], Bound: 0.38593176007270813, Entropy: 139.16403198242188, Temp: 2.700643301010132, KL: 72.58871459960938, Loss: 0.019133085384964943, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16605/20000], Bound: 0.4047706425189972, Entropy: 142.421875, Temp: 2.7006425857543945, KL: 80.36100769042969, Loss: 0.015053272247314453, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16606/20000], Bound: 0.37587589025497437, Entropy: 140.99520874023438, Temp: 2.7006421089172363, KL: 71.70358276367188, Loss: 0.015383847989141941, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16607/20000], Bound: 0.40935423970222473, Entropy: 140.1953582763672, Temp: 2.7006418704986572, KL: 80.20750427246094, Loss: 0.017889907583594322, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16608/20000], Bound: 0.3957889676094055, Entropy: 141.09425354003906, Temp: 2.700641632080078, KL: 76.68785095214844, Loss: 0.01690259948372841, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16609/20000], Bound: 0.37719056010246277, Entropy: 141.5758514404297, Temp: 2.700641632080078, KL: 72.748291015625, Loss: 0.014149629510939121, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16610/20000], Bound: 0.36659952998161316, Entropy: 143.2484130859375, Temp: 2.700641632080078, KL: 68.66168212890625, Loss: 0.016113972291350365, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16611/20000], Bound: 0.394834041595459, Entropy: 140.35546875, Temp: 2.700641632080078, KL: 76.92503356933594, Loss: 0.015940921381115913, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16612/20000], Bound: 0.37690436840057373, Entropy: 142.58297729492188, Temp: 2.700641632080078, KL: 71.85595703125, Loss: 0.015649201348423958, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16613/20000], Bound: 0.38450685143470764, Entropy: 141.65525817871094, Temp: 2.700641632080078, KL: 73.88887023925781, Loss: 0.015957705676555634, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16614/20000], Bound: 0.3623186945915222, Entropy: 142.57432556152344, Temp: 2.700641632080078, KL: 66.23533630371094, Loss: 0.01836581341922283, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16615/20000], Bound: 0.3901327848434448, Entropy: 141.92369079589844, Temp: 2.70064115524292, KL: 76.38052368164062, Loss: 0.014387203380465508, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16616/20000], Bound: 0.388312965631485, Entropy: 141.36489868164062, Temp: 2.70064115524292, KL: 74.63200378417969, Loss: 0.016637537628412247, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16617/20000], Bound: 0.38813796639442444, Entropy: 140.88925170898438, Temp: 2.70064115524292, KL: 75.27363586425781, Loss: 0.01535480935126543, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16618/20000], Bound: 0.40764400362968445, Entropy: 139.3748016357422, Temp: 2.700641632080078, KL: 81.22238159179688, Loss: 0.015056530013680458, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16619/20000], Bound: 0.37386924028396606, Entropy: 142.68544006347656, Temp: 2.7006418704986572, KL: 70.64604187011719, Loss: 0.016275949776172638, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16620/20000], Bound: 0.39184102416038513, Entropy: 142.48208618164062, Temp: 2.7006421089172363, KL: 74.74420166015625, Loss: 0.018345555290579796, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16621/20000], Bound: 0.3820355236530304, Entropy: 140.70599365234375, Temp: 2.7006423473358154, KL: 72.7169189453125, Loss: 0.016798770055174828, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16622/20000], Bound: 0.3859313428401947, Entropy: 141.07933044433594, Temp: 2.7006423473358154, KL: 75.40219116210938, Loss: 0.013923931866884232, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16623/20000], Bound: 0.408284455537796, Entropy: 141.44248962402344, Temp: 2.7006425857543945, KL: 79.76652526855469, Loss: 0.01810908131301403, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16624/20000], Bound: 0.39208075404167175, Entropy: 141.49021911621094, Temp: 2.7006428241729736, KL: 76.87467956542969, Loss: 0.014531673863530159, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16625/20000], Bound: 0.3882335424423218, Entropy: 141.47808837890625, Temp: 2.700643301010132, KL: 74.30233764648438, Loss: 0.017204878851771355, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16626/20000], Bound: 0.35364124178886414, Entropy: 142.54736328125, Temp: 2.700643539428711, KL: 66.56817626953125, Loss: 0.013249524869024754, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16627/20000], Bound: 0.37025198340415955, Entropy: 141.81326293945312, Temp: 2.700644016265869, KL: 71.11416625976562, Loss: 0.013495657593011856, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16628/20000], Bound: 0.3815896809101105, Entropy: 141.66607666015625, Temp: 2.7006447315216064, KL: 73.57746887207031, Loss: 0.014966375194489956, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16629/20000], Bound: 0.3970324993133545, Entropy: 140.2796630859375, Temp: 2.7006452083587646, KL: 78.26728820800781, Loss: 0.014659969136118889, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16630/20000], Bound: 0.3870238661766052, Entropy: 140.45314025878906, Temp: 2.700646162033081, KL: 74.17347717285156, Loss: 0.016788940876722336, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16631/20000], Bound: 0.376957505941391, Entropy: 143.1727752685547, Temp: 2.7006468772888184, KL: 71.59814453125, Loss: 0.016154881566762924, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16632/20000], Bound: 0.37963616847991943, Entropy: 141.47557067871094, Temp: 2.7006475925445557, KL: 72.93743896484375, Loss: 0.015105154365301132, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16633/20000], Bound: 0.39758065342903137, Entropy: 140.6897430419922, Temp: 2.700648069381714, KL: 77.60577392578125, Loss: 0.0161855760961771, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16634/20000], Bound: 0.39056310057640076, Entropy: 141.52401733398438, Temp: 2.700648784637451, KL: 75.66767883300781, Loss: 0.01594078168272972, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16635/20000], Bound: 0.3887101113796234, Entropy: 140.28695678710938, Temp: 2.7006494998931885, KL: 74.863037109375, Loss: 0.016425006091594696, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16636/20000], Bound: 0.3675004243850708, Entropy: 142.14805603027344, Temp: 2.700650215148926, KL: 69.21043395996094, Loss: 0.015571264550089836, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16637/20000], Bound: 0.4135303497314453, Entropy: 140.60348510742188, Temp: 2.700650691986084, KL: 80.56916809082031, Loss: 0.019561393186450005, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16638/20000], Bound: 0.3980291485786438, Entropy: 141.2281951904297, Temp: 2.7006514072418213, KL: 76.53117370605469, Loss: 0.018421458080410957, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16639/20000], Bound: 0.38994866609573364, Entropy: 142.43215942382812, Temp: 2.7006514072418213, KL: 76.32774353027344, Loss: 0.014385041780769825, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16640/20000], Bound: 0.3920576870441437, Entropy: 140.66317749023438, Temp: 2.7006518840789795, KL: 77.57037353515625, Loss: 0.013231186196208, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16641/20000], Bound: 0.39073628187179565, Entropy: 141.0504913330078, Temp: 2.700652599334717, KL: 76.99513244628906, Loss: 0.013577286154031754, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16642/20000], Bound: 0.37236425280570984, Entropy: 140.62779235839844, Temp: 2.700653553009033, KL: 70.67524719238281, Loss: 0.015424596145749092, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16643/20000], Bound: 0.36778417229652405, Entropy: 141.76756286621094, Temp: 2.7006545066833496, KL: 68.684326171875, Loss: 0.01669451594352722, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16644/20000], Bound: 0.39308497309684753, Entropy: 142.61495971679688, Temp: 2.700654983520508, KL: 77.24629211425781, Loss: 0.014391112141311169, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16645/20000], Bound: 0.3686278462409973, Entropy: 142.78256225585938, Temp: 2.700655937194824, KL: 68.26287841796875, Loss: 0.017918692901730537, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16646/20000], Bound: 0.36844879388809204, Entropy: 141.88067626953125, Temp: 2.7006566524505615, KL: 69.23092651367188, Loss: 0.01603216864168644, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16647/20000], Bound: 0.377765029668808, Entropy: 141.5124969482422, Temp: 2.7006571292877197, KL: 71.78390502929688, Loss: 0.016241535544395447, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16648/20000], Bound: 0.36848700046539307, Entropy: 142.55088806152344, Temp: 2.700657606124878, KL: 70.45025634765625, Loss: 0.01379481516778469, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16649/20000], Bound: 0.3835703432559967, Entropy: 141.91355895996094, Temp: 2.700658082962036, KL: 73.98979187011719, Loss: 0.015266926027834415, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16650/20000], Bound: 0.3954481780529022, Entropy: 140.29408264160156, Temp: 2.7006585597991943, KL: 76.59085083007812, Loss: 0.016895784065127373, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16651/20000], Bound: 0.38565096259117126, Entropy: 143.367919921875, Temp: 2.7006590366363525, KL: 73.92512512207031, Loss: 0.016507450491189957, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16652/20000], Bound: 0.37057244777679443, Entropy: 142.06588745117188, Temp: 2.7006595134735107, KL: 70.09324645996094, Loss: 0.01555506233125925, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16653/20000], Bound: 0.37203994393348694, Entropy: 142.33604431152344, Temp: 2.700659990310669, KL: 69.821533203125, Loss: 0.01683364063501358, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16654/20000], Bound: 0.38077712059020996, Entropy: 142.39828491210938, Temp: 2.700660228729248, KL: 73.44221496582031, Loss: 0.014781396836042404, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16655/20000], Bound: 0.3635493516921997, Entropy: 141.4149932861328, Temp: 2.7006607055664062, KL: 68.17160034179688, Loss: 0.01542380079627037, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16656/20000], Bound: 0.4056997001171112, Entropy: 140.69692993164062, Temp: 2.7006609439849854, KL: 79.85791015625, Loss: 0.016500813886523247, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16657/20000], Bound: 0.3855786919593811, Entropy: 141.82772827148438, Temp: 2.7006616592407227, KL: 74.81666564941406, Loss: 0.014817890711128712, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16658/20000], Bound: 0.4270387589931488, Entropy: 141.8380126953125, Temp: 2.700662136077881, KL: 85.9293212890625, Loss: 0.017313027754426003, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16659/20000], Bound: 0.3921065330505371, Entropy: 140.08985900878906, Temp: 2.700662612915039, KL: 76.7637939453125, Loss: 0.014751188457012177, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16660/20000], Bound: 0.40244436264038086, Entropy: 141.5791473388672, Temp: 2.7006633281707764, KL: 79.20098876953125, Loss: 0.01591240055859089, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16661/20000], Bound: 0.38580262660980225, Entropy: 141.50987243652344, Temp: 2.7006642818450928, KL: 73.51095581054688, Loss: 0.017356110736727715, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16662/20000], Bound: 0.39951685070991516, Entropy: 140.8361053466797, Temp: 2.700664758682251, KL: 77.72938537597656, Loss: 0.017021475359797478, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16663/20000], Bound: 0.42630890011787415, Entropy: 141.30856323242188, Temp: 2.7006654739379883, KL: 85.18984985351562, Loss: 0.01826329343020916, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16664/20000], Bound: 0.38448604941368103, Entropy: 141.0276336669922, Temp: 2.7006661891937256, KL: 71.26747131347656, Loss: 0.020799977704882622, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16665/20000], Bound: 0.3899966776371002, Entropy: 141.86175537109375, Temp: 2.7006661891937256, KL: 76.47573852539062, Loss: 0.014137274585664272, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16666/20000], Bound: 0.3967069089412689, Entropy: 140.96588134765625, Temp: 2.700666666030884, KL: 76.81610107421875, Loss: 0.017168361693620682, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16667/20000], Bound: 0.3670365810394287, Entropy: 141.4055633544922, Temp: 2.700666904449463, KL: 69.63102722167969, Loss: 0.0145490150898695, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16668/20000], Bound: 0.4063749611377716, Entropy: 141.66383361816406, Temp: 2.700667381286621, KL: 79.19053649902344, Loss: 0.018111931160092354, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16669/20000], Bound: 0.41592198610305786, Entropy: 139.45103454589844, Temp: 2.7006676197052, KL: 82.17935180664062, Loss: 0.017927829176187515, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16670/20000], Bound: 0.36496177315711975, Entropy: 141.81785583496094, Temp: 2.7006678581237793, KL: 68.52752685546875, Loss: 0.01550385169684887, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16671/20000], Bound: 0.37670812010765076, Entropy: 142.2315673828125, Temp: 2.7006680965423584, KL: 71.63026428222656, Loss: 0.01596277393400669, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16672/20000], Bound: 0.40075740218162537, Entropy: 143.07664489746094, Temp: 2.7006680965423584, KL: 77.60664367675781, Loss: 0.017932448536157608, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16673/20000], Bound: 0.3806261718273163, Entropy: 142.90528869628906, Temp: 2.7006683349609375, KL: 73.56494140625, Loss: 0.01447339728474617, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16674/20000], Bound: 0.3960103988647461, Entropy: 142.19471740722656, Temp: 2.7006685733795166, KL: 76.3878173828125, Loss: 0.01757955551147461, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16675/20000], Bound: 0.409334659576416, Entropy: 141.71009826660156, Temp: 2.7006688117980957, KL: 80.40892028808594, Loss: 0.017506344243884087, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16676/20000], Bound: 0.38476863503456116, Entropy: 141.98989868164062, Temp: 2.700669050216675, KL: 74.77911376953125, Loss: 0.014450794085860252, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16677/20000], Bound: 0.377872109413147, Entropy: 140.78306579589844, Temp: 2.700669527053833, KL: 73.58555603027344, Loss: 0.012963209301233292, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16678/20000], Bound: 0.39285266399383545, Entropy: 140.40711975097656, Temp: 2.7006702423095703, KL: 73.68951416015625, Loss: 0.020849555730819702, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16679/20000], Bound: 0.39729803800582886, Entropy: 142.28488159179688, Temp: 2.7006704807281494, KL: 78.29255676269531, Loss: 0.014759169891476631, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16680/20000], Bound: 0.395645409822464, Entropy: 142.0697021484375, Temp: 2.7006709575653076, KL: 76.50209045410156, Loss: 0.017168158665299416, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16681/20000], Bound: 0.373227059841156, Entropy: 140.9154052734375, Temp: 2.7006711959838867, KL: 69.62263488769531, Loss: 0.01783050410449505, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16682/20000], Bound: 0.38229626417160034, Entropy: 142.22665405273438, Temp: 2.7006711959838867, KL: 73.52908325195312, Loss: 0.01543533056974411, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16683/20000], Bound: 0.3997691571712494, Entropy: 140.6281280517578, Temp: 2.700671672821045, KL: 78.37741088867188, Loss: 0.015960756689310074, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16684/20000], Bound: 0.3812222182750702, Entropy: 140.1974639892578, Temp: 2.700671911239624, KL: 73.47367858886719, Loss: 0.014961758628487587, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16685/20000], Bound: 0.40791818499565125, Entropy: 139.8720703125, Temp: 2.700672149658203, KL: 79.91799926757812, Loss: 0.0176246277987957, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16686/20000], Bound: 0.3767363727092743, Entropy: 140.4741668701172, Temp: 2.7006726264953613, KL: 71.62112426757812, Loss: 0.015994789078831673, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16687/20000], Bound: 0.3775160312652588, Entropy: 141.66029357910156, Temp: 2.7006728649139404, KL: 71.25419616699219, Loss: 0.01708958111703396, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16688/20000], Bound: 0.3898150324821472, Entropy: 141.90760803222656, Temp: 2.7006731033325195, KL: 75.20912170410156, Loss: 0.01638372614979744, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16689/20000], Bound: 0.37179362773895264, Entropy: 143.45872497558594, Temp: 2.7006733417510986, KL: 69.23338317871094, Loss: 0.017792347818613052, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16690/20000], Bound: 0.3656619191169739, Entropy: 143.20004272460938, Temp: 2.7006733417510986, KL: 68.89373779296875, Loss: 0.015192758291959763, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16691/20000], Bound: 0.3774781823158264, Entropy: 141.56297302246094, Temp: 2.7006733417510986, KL: 72.2049560546875, Loss: 0.015309168957173824, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16692/20000], Bound: 0.3835573196411133, Entropy: 141.69891357421875, Temp: 2.7006733417510986, KL: 73.88252258300781, Loss: 0.01545867882668972, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16693/20000], Bound: 0.4042164385318756, Entropy: 139.29415893554688, Temp: 2.7006735801696777, KL: 79.07118225097656, Loss: 0.017134102061390877, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16694/20000], Bound: 0.37372493743896484, Entropy: 141.2041778564453, Temp: 2.700673818588257, KL: 69.21046447753906, Loss: 0.018857531249523163, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16695/20000], Bound: 0.3754381239414215, Entropy: 139.20399475097656, Temp: 2.7006735801696777, KL: 70.65791320800781, Loss: 0.017087284475564957, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16696/20000], Bound: 0.3700416684150696, Entropy: 141.0452423095703, Temp: 2.7006733417510986, KL: 69.93037414550781, Loss: 0.015576643869280815, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16697/20000], Bound: 0.36647260189056396, Entropy: 140.21669006347656, Temp: 2.7006731033325195, KL: 70.27783203125, Loss: 0.013055462390184402, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16698/20000], Bound: 0.3830738961696625, Entropy: 140.78334045410156, Temp: 2.7006731033325195, KL: 72.12544250488281, Loss: 0.018451770767569542, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16699/20000], Bound: 0.37325507402420044, Entropy: 141.55897521972656, Temp: 2.7006728649139404, KL: 71.99810791015625, Loss: 0.013447409495711327, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16700/20000], Bound: 0.38743191957473755, Entropy: 141.13113403320312, Temp: 2.7006728649139404, KL: 74.42901611328125, Loss: 0.01653674617409706, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16701/20000], Bound: 0.3998488187789917, Entropy: 141.18309020996094, Temp: 2.7006728649139404, KL: 77.11500549316406, Loss: 0.018341830000281334, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16702/20000], Bound: 0.3914249539375305, Entropy: 140.3177947998047, Temp: 2.7006728649139404, KL: 75.58113098144531, Loss: 0.016569864004850388, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16703/20000], Bound: 0.38602250814437866, Entropy: 141.26905822753906, Temp: 2.7006728649139404, KL: 75.10713195800781, Loss: 0.014519700780510902, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16704/20000], Bound: 0.35968127846717834, Entropy: 143.15757751464844, Temp: 2.7006731033325195, KL: 68.07424926757812, Loss: 0.013587902300059795, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16705/20000], Bound: 0.3778455853462219, Entropy: 139.31932067871094, Temp: 2.7006733417510986, KL: 71.82864379882812, Loss: 0.016201810911297798, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16706/20000], Bound: 0.37920406460762024, Entropy: 141.3043975830078, Temp: 2.7006735801696777, KL: 72.85871887207031, Loss: 0.015020121820271015, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16707/20000], Bound: 0.3696797490119934, Entropy: 140.43211364746094, Temp: 2.700673818588257, KL: 69.57487487792969, Loss: 0.01604393869638443, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16708/20000], Bound: 0.3798435926437378, Entropy: 142.58143615722656, Temp: 2.700674295425415, KL: 71.44062805175781, Loss: 0.017987512052059174, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16709/20000], Bound: 0.4014163911342621, Entropy: 141.34913635253906, Temp: 2.700674295425415, KL: 77.69464111328125, Loss: 0.018133269622921944, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16710/20000], Bound: 0.3817502558231354, Entropy: 141.02020263671875, Temp: 2.700674295425415, KL: 73.14187622070312, Loss: 0.01585923321545124, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16711/20000], Bound: 0.39350569248199463, Entropy: 140.58555603027344, Temp: 2.700674295425415, KL: 75.63795471191406, Loss: 0.017598474398255348, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16712/20000], Bound: 0.4021509289741516, Entropy: 140.8138427734375, Temp: 2.700674295425415, KL: 79.30296325683594, Loss: 0.01556147076189518, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16713/20000], Bound: 0.4021753966808319, Entropy: 138.9293670654297, Temp: 2.700674533843994, KL: 78.05424499511719, Loss: 0.017886850982904434, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16714/20000], Bound: 0.36104917526245117, Entropy: 143.6156463623047, Temp: 2.700674533843994, KL: 67.30403137207031, Loss: 0.01572566293179989, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16715/20000], Bound: 0.3899114727973938, Entropy: 140.4056396484375, Temp: 2.700674533843994, KL: 73.70658874511719, Loss: 0.01921785995364189, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16716/20000], Bound: 0.4027805030345917, Entropy: 140.2053680419922, Temp: 2.700674295425415, KL: 78.94154357910156, Loss: 0.016578784212470055, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16717/20000], Bound: 0.3867018222808838, Entropy: 141.02110290527344, Temp: 2.700673818588257, KL: 74.91326904296875, Loss: 0.015245488844811916, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16718/20000], Bound: 0.38706550002098083, Entropy: 141.3163604736328, Temp: 2.700673818588257, KL: 75.19313049316406, Loss: 0.014923949725925922, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16719/20000], Bound: 0.3787436783313751, Entropy: 141.65232849121094, Temp: 2.700674295425415, KL: 72.06802368164062, Loss: 0.016238011419773102, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16720/20000], Bound: 0.3925788700580597, Entropy: 138.95278930664062, Temp: 2.700674533843994, KL: 76.52638244628906, Loss: 0.015448229387402534, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16721/20000], Bound: 0.39599609375, Entropy: 141.30380249023438, Temp: 2.700674533843994, KL: 74.26325988769531, Loss: 0.021505188196897507, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16722/20000], Bound: 0.3788696229457855, Entropy: 140.66574096679688, Temp: 2.700674533843994, KL: 73.86795043945312, Loss: 0.012972921133041382, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16723/20000], Bound: 0.3795280158519745, Entropy: 142.09902954101562, Temp: 2.700674533843994, KL: 73.53684997558594, Loss: 0.013937804847955704, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16724/20000], Bound: 0.3890852928161621, Entropy: 142.5556640625, Temp: 2.7006750106811523, KL: 75.03831481933594, Loss: 0.01630408875644207, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16725/20000], Bound: 0.3738751709461212, Entropy: 139.52552795410156, Temp: 2.7006752490997314, KL: 70.25680541992188, Loss: 0.017000000923871994, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16726/20000], Bound: 0.41113683581352234, Entropy: 140.22801208496094, Temp: 2.7006752490997314, KL: 82.22247314453125, Loss: 0.01515719573944807, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16727/20000], Bound: 0.3640195429325104, Entropy: 141.70297241210938, Temp: 2.7006757259368896, KL: 69.20838928222656, Loss: 0.013750271871685982, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16728/20000], Bound: 0.397786021232605, Entropy: 141.50965881347656, Temp: 2.700676202774048, KL: 75.79716491699219, Loss: 0.01964704319834709, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16729/20000], Bound: 0.3853338658809662, Entropy: 140.07861328125, Temp: 2.700676441192627, KL: 72.86386108398438, Loss: 0.018301384523510933, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16730/20000], Bound: 0.40622538328170776, Entropy: 141.17559814453125, Temp: 2.700676441192627, KL: 79.86485290527344, Loss: 0.01678037829697132, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16731/20000], Bound: 0.3702230453491211, Entropy: 141.7888641357422, Temp: 2.700676441192627, KL: 70.27960205078125, Loss: 0.015025808475911617, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16732/20000], Bound: 0.36937060952186584, Entropy: 142.2605438232422, Temp: 2.700676679611206, KL: 68.69963073730469, Loss: 0.017501432448625565, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16733/20000], Bound: 0.380751371383667, Entropy: 140.0746612548828, Temp: 2.700676679611206, KL: 72.61187744140625, Loss: 0.016305021941661835, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16734/20000], Bound: 0.388546347618103, Entropy: 142.95481872558594, Temp: 2.700676679611206, KL: 75.30410766601562, Loss: 0.015519943088293076, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16735/20000], Bound: 0.395292729139328, Entropy: 140.80914306640625, Temp: 2.700676679611206, KL: 76.83847045898438, Loss: 0.01635241135954857, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16736/20000], Bound: 0.3833330571651459, Entropy: 141.46536254882812, Temp: 2.700676918029785, KL: 73.67095947265625, Loss: 0.01572980172932148, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16737/20000], Bound: 0.3899252116680145, Entropy: 141.37474060058594, Temp: 2.7006771564483643, KL: 75.39283752441406, Loss: 0.016103429719805717, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16738/20000], Bound: 0.40246689319610596, Entropy: 139.8783721923828, Temp: 2.7006773948669434, KL: 78.92312622070312, Loss: 0.016439398750662804, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16739/20000], Bound: 0.37608495354652405, Entropy: 141.3923797607422, Temp: 2.7006776332855225, KL: 72.03256225585938, Loss: 0.014886346645653248, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16740/20000], Bound: 0.39748233556747437, Entropy: 141.17306518554688, Temp: 2.7006778717041016, KL: 76.19189453125, Loss: 0.018749549984931946, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16741/20000], Bound: 0.38046181201934814, Entropy: 141.4110565185547, Temp: 2.7006781101226807, KL: 72.624267578125, Loss: 0.016127048060297966, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16742/20000], Bound: 0.4024660885334015, Entropy: 141.09320068359375, Temp: 2.7006783485412598, KL: 78.28160095214844, Loss: 0.017626723274588585, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16743/20000], Bound: 0.3461821675300598, Entropy: 141.85205078125, Temp: 2.7006783485412598, KL: 62.26924133300781, Loss: 0.017383787781000137, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16744/20000], Bound: 0.3711588382720947, Entropy: 142.61502075195312, Temp: 2.7006781101226807, KL: 71.60652160644531, Loss: 0.01306326687335968, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16745/20000], Bound: 0.39797285199165344, Entropy: 140.0923614501953, Temp: 2.7006781101226807, KL: 77.631591796875, Loss: 0.016353491693735123, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16746/20000], Bound: 0.39001917839050293, Entropy: 141.51214599609375, Temp: 2.7006783485412598, KL: 74.05526733398438, Loss: 0.01863081566989422, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16747/20000], Bound: 0.3989845812320709, Entropy: 139.33555603027344, Temp: 2.7006783485412598, KL: 78.58482360839844, Loss: 0.015144933015108109, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16748/20000], Bound: 0.3734365701675415, Entropy: 139.95045471191406, Temp: 2.700678586959839, KL: 71.48545837402344, Loss: 0.014492778107523918, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16749/20000], Bound: 0.3747260570526123, Entropy: 143.0419464111328, Temp: 2.700678825378418, KL: 69.58656311035156, Loss: 0.01869249902665615, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16750/20000], Bound: 0.3982170522212982, Entropy: 140.471435546875, Temp: 2.700678825378418, KL: 77.65489196777344, Loss: 0.016444547101855278, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16751/20000], Bound: 0.39447012543678284, Entropy: 141.27980041503906, Temp: 2.700678825378418, KL: 77.06440734863281, Loss: 0.015484318137168884, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16752/20000], Bound: 0.41948556900024414, Entropy: 140.24046325683594, Temp: 2.700679063796997, KL: 84.9158935546875, Loss: 0.014878256246447563, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16753/20000], Bound: 0.40708687901496887, Entropy: 140.18081665039062, Temp: 2.7006795406341553, KL: 80.0928955078125, Loss: 0.016837645322084427, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16754/20000], Bound: 0.3954833149909973, Entropy: 141.66600036621094, Temp: 2.7006800174713135, KL: 74.98118591308594, Loss: 0.019895309582352638, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16755/20000], Bound: 0.4007827043533325, Entropy: 141.8825225830078, Temp: 2.7006802558898926, KL: 79.97196960449219, Loss: 0.013567398302257061, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16756/20000], Bound: 0.3796067535877228, Entropy: 140.26547241210938, Temp: 2.700680732727051, KL: 72.44096374511719, Loss: 0.016008872538805008, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16757/20000], Bound: 0.406081885099411, Entropy: 141.2635040283203, Temp: 2.700681209564209, KL: 80.59898376464844, Loss: 0.015341470018029213, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16758/20000], Bound: 0.38488420844078064, Entropy: 141.62998962402344, Temp: 2.700681686401367, KL: 73.37477111816406, Loss: 0.017113149166107178, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16759/20000], Bound: 0.3715081512928009, Entropy: 140.1376495361328, Temp: 2.7006821632385254, KL: 70.07917785644531, Loss: 0.01607559062540531, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16760/20000], Bound: 0.39612430334091187, Entropy: 140.9794464111328, Temp: 2.7006826400756836, KL: 76.7254638671875, Loss: 0.01701698824763298, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16761/20000], Bound: 0.3785334527492523, Entropy: 141.41705322265625, Temp: 2.700683116912842, KL: 72.9632568359375, Loss: 0.014468393288552761, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16762/20000], Bound: 0.36253759264945984, Entropy: 142.80035400390625, Temp: 2.70068359375, KL: 65.66706848144531, Loss: 0.019532401114702225, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16763/20000], Bound: 0.3977072238922119, Entropy: 141.54881286621094, Temp: 2.70068359375, KL: 76.32855224609375, Loss: 0.018620068207383156, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16764/20000], Bound: 0.3870120644569397, Entropy: 140.65625, Temp: 2.700683355331421, KL: 74.74226379394531, Loss: 0.01572989486157894, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16765/20000], Bound: 0.36104339361190796, Entropy: 141.1300506591797, Temp: 2.700683355331421, KL: 67.05235290527344, Loss: 0.01618867553770542, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16766/20000], Bound: 0.40146082639694214, Entropy: 140.3334197998047, Temp: 2.700683116912842, KL: 78.90562438964844, Loss: 0.01591590605676174, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16767/20000], Bound: 0.3782496154308319, Entropy: 143.05262756347656, Temp: 2.700683116912842, KL: 72.78701782226562, Loss: 0.014643160626292229, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16768/20000], Bound: 0.37899690866470337, Entropy: 141.4963836669922, Temp: 2.700683355331421, KL: 72.18463134765625, Loss: 0.01615750417113304, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16769/20000], Bound: 0.3922831118106842, Entropy: 142.25184631347656, Temp: 2.70068359375, KL: 76.42652893066406, Loss: 0.01547202654182911, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16770/20000], Bound: 0.4081973135471344, Entropy: 140.68533325195312, Temp: 2.700683832168579, KL: 79.86286926269531, Loss: 0.017882494255900383, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16771/20000], Bound: 0.39061251282691956, Entropy: 140.5916748046875, Temp: 2.700684070587158, KL: 75.37696838378906, Loss: 0.016506198793649673, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16772/20000], Bound: 0.34841039776802063, Entropy: 143.69786071777344, Temp: 2.700684070587158, KL: 64.64396667480469, Loss: 0.014125811867415905, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16773/20000], Bound: 0.3673774302005768, Entropy: 140.7041473388672, Temp: 2.7006845474243164, KL: 68.92694091796875, Loss: 0.016031788662075996, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16774/20000], Bound: 0.40315189957618713, Entropy: 140.15325927734375, Temp: 2.7006847858428955, KL: 77.95252990722656, Loss: 0.01861545629799366, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16775/20000], Bound: 0.3717249035835266, Entropy: 140.92962646484375, Temp: 2.7006847858428955, KL: 70.37060546875, Loss: 0.01565067656338215, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16776/20000], Bound: 0.4072180986404419, Entropy: 140.38092041015625, Temp: 2.7006847858428955, KL: 80.837158203125, Loss: 0.015532891266047955, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16777/20000], Bound: 0.3912293016910553, Entropy: 139.99041748046875, Temp: 2.7006847858428955, KL: 75.19390869140625, Loss: 0.017180409282445908, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16778/20000], Bound: 0.3923032581806183, Entropy: 140.13607788085938, Temp: 2.7006850242614746, KL: 75.80744934082031, Loss: 0.01662912592291832, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16779/20000], Bound: 0.39041978120803833, Entropy: 141.10667419433594, Temp: 2.700685501098633, KL: 76.6627197265625, Loss: 0.014021048322319984, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16780/20000], Bound: 0.403787225484848, Entropy: 140.3000946044922, Temp: 2.700685739517212, KL: 79.30848693847656, Loss: 0.016457000747323036, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16781/20000], Bound: 0.40991246700286865, Entropy: 140.69970703125, Temp: 2.70068621635437, KL: 81.03643798828125, Loss: 0.016667740419507027, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16782/20000], Bound: 0.37173888087272644, Entropy: 139.6434783935547, Temp: 2.7006866931915283, KL: 71.15190124511719, Loss: 0.014211608096957207, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16783/20000], Bound: 0.4329681098461151, Entropy: 142.57090759277344, Temp: 2.7006874084472656, KL: 88.48133850097656, Loss: 0.016008377075195312, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16784/20000], Bound: 0.3755994439125061, Entropy: 140.6719512939453, Temp: 2.700688123703003, KL: 71.46305847167969, Loss: 0.015682552009820938, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16785/20000], Bound: 0.37875044345855713, Entropy: 143.16575622558594, Temp: 2.7006890773773193, KL: 72.51043701171875, Loss: 0.015422680415213108, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16786/20000], Bound: 0.41076308488845825, Entropy: 139.66586303710938, Temp: 2.7006900310516357, KL: 79.27751159667969, Loss: 0.020400213077664375, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16787/20000], Bound: 0.39238008856773376, Entropy: 141.51535034179688, Temp: 2.700690507888794, KL: 76.33631896972656, Loss: 0.015691902488470078, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16788/20000], Bound: 0.36799606680870056, Entropy: 141.07846069335938, Temp: 2.700690984725952, KL: 68.63395690917969, Loss: 0.016899490728974342, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16789/20000], Bound: 0.38747522234916687, Entropy: 141.32595825195312, Temp: 2.7006914615631104, KL: 74.49555969238281, Loss: 0.01643713377416134, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16790/20000], Bound: 0.36899036169052124, Entropy: 142.47377014160156, Temp: 2.7006914615631104, KL: 69.72325134277344, Loss: 0.015406099148094654, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16791/20000], Bound: 0.3706590533256531, Entropy: 140.18824768066406, Temp: 2.7006916999816895, KL: 70.46607971191406, Loss: 0.014910843223333359, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16792/20000], Bound: 0.3759936988353729, Entropy: 143.3460693359375, Temp: 2.7006921768188477, KL: 72.20440673828125, Loss: 0.014519760385155678, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16793/20000], Bound: 0.3929981589317322, Entropy: 141.5244903564453, Temp: 2.700692653656006, KL: 75.49082946777344, Loss: 0.017594175413250923, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16794/20000], Bound: 0.3849063813686371, Entropy: 143.11436462402344, Temp: 2.700693130493164, KL: 73.65234375, Loss: 0.016611292958259583, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16795/20000], Bound: 0.3761212229728699, Entropy: 142.34652709960938, Temp: 2.700693368911743, KL: 70.63444519042969, Loss: 0.017494212836027145, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16796/20000], Bound: 0.36894041299819946, Entropy: 142.45489501953125, Temp: 2.700693368911743, KL: 69.98255920410156, Loss: 0.014899767003953457, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16797/20000], Bound: 0.41745907068252563, Entropy: 140.00341796875, Temp: 2.7006936073303223, KL: 82.96534729003906, Loss: 0.01734144054353237, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16798/20000], Bound: 0.38431161642074585, Entropy: 142.16616821289062, Temp: 2.7006938457489014, KL: 74.48097229003906, Loss: 0.014756818301975727, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16799/20000], Bound: 0.36552101373672485, Entropy: 141.7790069580078, Temp: 2.7006943225860596, KL: 69.11305236816406, Loss: 0.01471302006393671, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16800/20000], Bound: 0.3837144672870636, Entropy: 141.5189208984375, Temp: 2.7006945610046387, KL: 73.75474548339844, Loss: 0.015779949724674225, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16801/20000], Bound: 0.39400795102119446, Entropy: 141.37689208984375, Temp: 2.700695037841797, KL: 76.41825866699219, Loss: 0.016428213566541672, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16802/20000], Bound: 0.40551087260246277, Entropy: 141.00421142578125, Temp: 2.700695514678955, KL: 79.56069946289062, Loss: 0.016946464776992798, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16803/20000], Bound: 0.39999154210090637, Entropy: 140.2868194580078, Temp: 2.7006959915161133, KL: 74.66403198242188, Loss: 0.022958338260650635, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16804/20000], Bound: 0.3898753523826599, Entropy: 141.33517456054688, Temp: 2.700695753097534, KL: 75.45974731445312, Loss: 0.01595270447432995, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16805/20000], Bound: 0.4113669693470001, Entropy: 141.85911560058594, Temp: 2.700695753097534, KL: 79.28726196289062, Loss: 0.020720576867461205, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16806/20000], Bound: 0.392185777425766, Entropy: 142.86167907714844, Temp: 2.700695276260376, KL: 75.03195190429688, Loss: 0.018000980839133263, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16807/20000], Bound: 0.3912723958492279, Entropy: 141.2365264892578, Temp: 2.700695037841797, KL: 76.56086730957031, Loss: 0.014673212543129921, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16808/20000], Bound: 0.37630683183670044, Entropy: 142.05471801757812, Temp: 2.7006945610046387, KL: 71.76614379882812, Loss: 0.015497778542339802, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16809/20000], Bound: 0.3680342733860016, Entropy: 142.57296752929688, Temp: 2.7006943225860596, KL: 69.17135620117188, Loss: 0.015924684703350067, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16810/20000], Bound: 0.3510383665561676, Entropy: 139.88722229003906, Temp: 2.7006943225860596, KL: 66.03340148925781, Loss: 0.012900769710540771, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16811/20000], Bound: 0.38137054443359375, Entropy: 140.93075561523438, Temp: 2.7006943225860596, KL: 72.49446105957031, Loss: 0.016854409128427505, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16812/20000], Bound: 0.38044682145118713, Entropy: 143.3466339111328, Temp: 2.7006943225860596, KL: 72.65882873535156, Loss: 0.01605517975986004, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16813/20000], Bound: 0.38964977860450745, Entropy: 141.1081085205078, Temp: 2.7006943225860596, KL: 76.12428283691406, Loss: 0.014599928632378578, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16814/20000], Bound: 0.3948184549808502, Entropy: 141.49319458007812, Temp: 2.7006943225860596, KL: 76.88746643066406, Loss: 0.016002440825104713, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16815/20000], Bound: 0.3694624602794647, Entropy: 140.41204833984375, Temp: 2.7006945610046387, KL: 69.96485900878906, Loss: 0.015207584016025066, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16816/20000], Bound: 0.40890413522720337, Entropy: 142.03091430664062, Temp: 2.700695037841797, KL: 80.15695190429688, Loss: 0.017732594162225723, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16817/20000], Bound: 0.40682125091552734, Entropy: 141.7062225341797, Temp: 2.700695037841797, KL: 80.65122985839844, Loss: 0.0156562402844429, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16818/20000], Bound: 0.39016756415367126, Entropy: 140.80189514160156, Temp: 2.700695514678955, KL: 75.48384094238281, Loss: 0.01606673188507557, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16819/20000], Bound: 0.36366868019104004, Entropy: 141.13331604003906, Temp: 2.7006959915161133, KL: 68.61651611328125, Loss: 0.014662721194326878, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16820/20000], Bound: 0.3818953037261963, Entropy: 141.97071838378906, Temp: 2.7006964683532715, KL: 74.95355224609375, Loss: 0.01258315984159708, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16821/20000], Bound: 0.40463656187057495, Entropy: 141.76283264160156, Temp: 2.700697422027588, KL: 79.85771179199219, Loss: 0.015911217778921127, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16822/20000], Bound: 0.4143027067184448, Entropy: 141.70497131347656, Temp: 2.700698137283325, KL: 83.18388366699219, Loss: 0.015155600383877754, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16823/20000], Bound: 0.37503188848495483, Entropy: 140.668701171875, Temp: 2.7006993293762207, KL: 70.26339721679688, Loss: 0.01760200224816799, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16824/20000], Bound: 0.38582345843315125, Entropy: 143.315185546875, Temp: 2.700700283050537, KL: 73.64244079589844, Loss: 0.017124252393841743, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16825/20000], Bound: 0.38201677799224854, Entropy: 139.68943786621094, Temp: 2.7007009983062744, KL: 74.32441711425781, Loss: 0.013813154771924019, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16826/20000], Bound: 0.3828239440917969, Entropy: 140.73487854003906, Temp: 2.7007017135620117, KL: 72.01104736328125, Loss: 0.018529480323195457, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16827/20000], Bound: 0.39157620072364807, Entropy: 141.1904754638672, Temp: 2.70070219039917, KL: 77.50218200683594, Loss: 0.013095845468342304, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16828/20000], Bound: 0.39565426111221313, Entropy: 140.7700958251953, Temp: 2.7007031440734863, KL: 77.36570739746094, Loss: 0.015574422664940357, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16829/20000], Bound: 0.3625078797340393, Entropy: 141.16928100585938, Temp: 2.7007040977478027, KL: 68.56546020507812, Loss: 0.014151033945381641, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16830/20000], Bound: 0.3958275020122528, Entropy: 141.09263610839844, Temp: 2.700705051422119, KL: 76.55485534667969, Loss: 0.017170492559671402, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16831/20000], Bound: 0.38872775435447693, Entropy: 140.96849060058594, Temp: 2.7007057666778564, KL: 75.07014465332031, Loss: 0.016051627695560455, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16832/20000], Bound: 0.42437052726745605, Entropy: 142.2374725341797, Temp: 2.700706720352173, KL: 85.031982421875, Loss: 0.01744610071182251, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16833/20000], Bound: 0.40849360823631287, Entropy: 140.23240661621094, Temp: 2.70070743560791, KL: 78.66484069824219, Loss: 0.020266031846404076, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16834/20000], Bound: 0.4034292995929718, Entropy: 139.4118194580078, Temp: 2.7007079124450684, KL: 77.84553527832031, Loss: 0.018967362120747566, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16835/20000], Bound: 0.37495797872543335, Entropy: 141.9484100341797, Temp: 2.7007081508636475, KL: 69.02867126464844, Loss: 0.019848724827170372, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16836/20000], Bound: 0.3861101567745209, Entropy: 139.50978088378906, Temp: 2.7007079124450684, KL: 72.91903686523438, Loss: 0.01861834153532982, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16837/20000], Bound: 0.39179542660713196, Entropy: 141.8214874267578, Temp: 2.70070743560791, KL: 75.59568786621094, Loss: 0.016744863241910934, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16838/20000], Bound: 0.3852577805519104, Entropy: 140.89724731445312, Temp: 2.700707197189331, KL: 74.15087890625, Loss: 0.015877867117524147, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16839/20000], Bound: 0.41843175888061523, Entropy: 140.25546264648438, Temp: 2.700707197189331, KL: 82.25277709960938, Loss: 0.019211456179618835, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16840/20000], Bound: 0.3972705900669098, Entropy: 139.66607666015625, Temp: 2.700706958770752, KL: 78.04312133789062, Loss: 0.015206295996904373, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16841/20000], Bound: 0.3609647750854492, Entropy: 143.421142578125, Temp: 2.700706958770752, KL: 67.81268310546875, Loss: 0.014740217477083206, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16842/20000], Bound: 0.3895222246646881, Entropy: 140.86781311035156, Temp: 2.700706958770752, KL: 75.54054260253906, Loss: 0.015611519105732441, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16843/20000], Bound: 0.36634454131126404, Entropy: 142.72616577148438, Temp: 2.700706958770752, KL: 69.82414245605469, Loss: 0.013828509487211704, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16844/20000], Bound: 0.4098665416240692, Entropy: 142.3035430908203, Temp: 2.700707197189331, KL: 79.47727966308594, Loss: 0.01952887326478958, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16845/20000], Bound: 0.3923759162425995, Entropy: 141.00868225097656, Temp: 2.700707197189331, KL: 77.90679931640625, Loss: 0.01278226263821125, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16846/20000], Bound: 0.37659090757369995, Entropy: 141.33226013183594, Temp: 2.70070743560791, KL: 69.65653991699219, Loss: 0.019554775208234787, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16847/20000], Bound: 0.35393422842025757, Entropy: 143.30567932128906, Temp: 2.70070743560791, KL: 65.42550659179688, Loss: 0.015516562387347221, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16848/20000], Bound: 0.35810989141464233, Entropy: 142.67176818847656, Temp: 2.70070743560791, KL: 67.30448913574219, Loss: 0.01419739332050085, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16849/20000], Bound: 0.3714636564254761, Entropy: 141.39840698242188, Temp: 2.70070743560791, KL: 69.58857727050781, Loss: 0.016960570588707924, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16850/20000], Bound: 0.3762456774711609, Entropy: 139.45423889160156, Temp: 2.70070743560791, KL: 70.96604919433594, Loss: 0.016946613788604736, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16851/20000], Bound: 0.38271084427833557, Entropy: 141.9308319091797, Temp: 2.700707197189331, KL: 71.49076843261719, Loss: 0.01943199150264263, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16852/20000], Bound: 0.37853553891181946, Entropy: 142.521728515625, Temp: 2.700706958770752, KL: 71.34193420410156, Loss: 0.017471393570303917, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16853/20000], Bound: 0.3485080301761627, Entropy: 143.258056640625, Temp: 2.7007062435150146, KL: 64.56222534179688, Loss: 0.014327283017337322, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16854/20000], Bound: 0.38024234771728516, Entropy: 140.17599487304688, Temp: 2.7007057666778564, KL: 72.96664428710938, Loss: 0.015375926159322262, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16855/20000], Bound: 0.3832951486110687, Entropy: 140.2922821044922, Temp: 2.7007052898406982, KL: 73.73516845703125, Loss: 0.015590825118124485, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16856/20000], Bound: 0.3666255474090576, Entropy: 141.45054626464844, Temp: 2.700705051422119, KL: 68.53453063964844, Loss: 0.016363542526960373, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16857/20000], Bound: 0.3946766257286072, Entropy: 140.109375, Temp: 2.700705051422119, KL: 77.77166748046875, Loss: 0.014288043603301048, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16858/20000], Bound: 0.3827846646308899, Entropy: 140.94505310058594, Temp: 2.700705051422119, KL: 73.67808532714844, Loss: 0.015422115102410316, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16859/20000], Bound: 0.39351511001586914, Entropy: 140.49072265625, Temp: 2.700705051422119, KL: 77.05169677734375, Loss: 0.014986536465585232, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16860/20000], Bound: 0.39400699734687805, Entropy: 139.295166015625, Temp: 2.700705051422119, KL: 76.11457824707031, Loss: 0.016990019008517265, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16861/20000], Bound: 0.39497122168540955, Entropy: 142.6111297607422, Temp: 2.7007052898406982, KL: 74.70541381835938, Loss: 0.020125865936279297, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16862/20000], Bound: 0.36927250027656555, Entropy: 144.01931762695312, Temp: 2.700705051422119, KL: 69.13040161132812, Loss: 0.016652457416057587, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16863/20000], Bound: 0.3863878548145294, Entropy: 141.73109436035156, Temp: 2.700705051422119, KL: 74.15226745605469, Loss: 0.01648510806262493, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16864/20000], Bound: 0.37366244196891785, Entropy: 140.4033203125, Temp: 2.70070481300354, KL: 70.8035888671875, Loss: 0.015875138342380524, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16865/20000], Bound: 0.37735044956207275, Entropy: 140.72120666503906, Temp: 2.700704574584961, KL: 72.341796875, Loss: 0.014988002367317677, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16866/20000], Bound: 0.41237133741378784, Entropy: 140.53411865234375, Temp: 2.700704574584961, KL: 82.33489990234375, Loss: 0.01564171351492405, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16867/20000], Bound: 0.3817189335823059, Entropy: 141.71084594726562, Temp: 2.70070481300354, KL: 73.41024780273438, Loss: 0.01534588634967804, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16868/20000], Bound: 0.3855326771736145, Entropy: 142.36585998535156, Temp: 2.700705051422119, KL: 74.38236999511719, Loss: 0.015597525052726269, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16869/20000], Bound: 0.3934374153614044, Entropy: 141.03440856933594, Temp: 2.700705051422119, KL: 77.44154357910156, Loss: 0.014222363010048866, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16870/20000], Bound: 0.3989589512348175, Entropy: 142.10707092285156, Temp: 2.7007055282592773, KL: 77.39854431152344, Loss: 0.017327332869172096, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16871/20000], Bound: 0.3871077001094818, Entropy: 140.15805053710938, Temp: 2.7007060050964355, KL: 74.75994873046875, Loss: 0.015749040991067886, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16872/20000], Bound: 0.40696966648101807, Entropy: 139.88796997070312, Temp: 2.7007064819335938, KL: 79.88632202148438, Loss: 0.017155123874545097, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16873/20000], Bound: 0.394186407327652, Entropy: 141.4694061279297, Temp: 2.700706958770752, KL: 76.03184509277344, Loss: 0.0172412171959877, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16874/20000], Bound: 0.3986382782459259, Entropy: 139.92330932617188, Temp: 2.700707197189331, KL: 79.01454162597656, Loss: 0.014159140177071095, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16875/20000], Bound: 0.38023385405540466, Entropy: 140.22804260253906, Temp: 2.7007079124450684, KL: 71.99365234375, Loss: 0.01717277429997921, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16876/20000], Bound: 0.38273757696151733, Entropy: 141.3800811767578, Temp: 2.7007086277008057, KL: 73.62873840332031, Loss: 0.015488194301724434, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16877/20000], Bound: 0.3783166706562042, Entropy: 142.00096130371094, Temp: 2.7007088661193848, KL: 71.60548400878906, Loss: 0.016866635531187057, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16878/20000], Bound: 0.41598591208457947, Entropy: 140.48196411132812, Temp: 2.700709342956543, KL: 81.86439514160156, Loss: 0.018547426909208298, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16879/20000], Bound: 0.3676852583885193, Entropy: 142.66554260253906, Temp: 2.700709581375122, KL: 69.19180297851562, Loss: 0.015703396871685982, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16880/20000], Bound: 0.3991158604621887, Entropy: 141.0436553955078, Temp: 2.700709819793701, KL: 78.57949829101562, Loss: 0.015227314084768295, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16881/20000], Bound: 0.38516831398010254, Entropy: 142.52597045898438, Temp: 2.7007102966308594, KL: 72.78387451171875, Loss: 0.01836048625409603, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16882/20000], Bound: 0.3863125443458557, Entropy: 142.34349060058594, Temp: 2.7007105350494385, KL: 75.13740539550781, Loss: 0.014620624482631683, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16883/20000], Bound: 0.3829016089439392, Entropy: 139.4061279296875, Temp: 2.7007107734680176, KL: 73.52853393554688, Loss: 0.0157618448138237, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16884/20000], Bound: 0.41557320952415466, Entropy: 141.4829864501953, Temp: 2.700711250305176, KL: 83.40383911132812, Loss: 0.015464494936168194, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16885/20000], Bound: 0.3792359530925751, Entropy: 140.54833984375, Temp: 2.700711965560913, KL: 72.67001342773438, Loss: 0.015386833809316158, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16886/20000], Bound: 0.38490986824035645, Entropy: 142.0939178466797, Temp: 2.7007126808166504, KL: 74.09799194335938, Loss: 0.015788283199071884, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16887/20000], Bound: 0.3840083181858063, Entropy: 140.40087890625, Temp: 2.7007133960723877, KL: 74.20651245117188, Loss: 0.015101871453225613, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16888/20000], Bound: 0.38870367407798767, Entropy: 141.20492553710938, Temp: 2.700714111328125, KL: 75.87553405761719, Loss: 0.014547599479556084, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16889/20000], Bound: 0.3843570351600647, Entropy: 140.8278350830078, Temp: 2.7007148265838623, KL: 74.56672668457031, Loss: 0.014622685499489307, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16890/20000], Bound: 0.37468111515045166, Entropy: 141.76031494140625, Temp: 2.7007157802581787, KL: 71.22218322753906, Loss: 0.01564079523086548, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16891/20000], Bound: 0.4070674479007721, Entropy: 142.48321533203125, Temp: 2.700716733932495, KL: 78.44392395019531, Loss: 0.01988007314503193, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16892/20000], Bound: 0.3475015461444855, Entropy: 141.0257568359375, Temp: 2.7007172107696533, KL: 63.64356994628906, Loss: 0.015513361431658268, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16893/20000], Bound: 0.3859010636806488, Entropy: 142.41014099121094, Temp: 2.7007176876068115, KL: 73.98641967773438, Loss: 0.016529444605112076, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16894/20000], Bound: 0.37037932872772217, Entropy: 141.14207458496094, Temp: 2.7007181644439697, KL: 71.16494750976562, Loss: 0.013469519093632698, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16895/20000], Bound: 0.3830532133579254, Entropy: 142.23126220703125, Temp: 2.700718402862549, KL: 72.99017333984375, Loss: 0.016840094700455666, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16896/20000], Bound: 0.35727038979530334, Entropy: 143.32546997070312, Temp: 2.700718879699707, KL: 65.32676696777344, Loss: 0.01742379367351532, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16897/20000], Bound: 0.4174950122833252, Entropy: 141.64796447753906, Temp: 2.700719118118286, KL: 84.04058837890625, Loss: 0.015371367335319519, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16898/20000], Bound: 0.3728289008140564, Entropy: 141.02520751953125, Temp: 2.7007195949554443, KL: 69.47908020019531, Loss: 0.01788570173084736, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16899/20000], Bound: 0.37683579325675964, Entropy: 140.80186462402344, Temp: 2.7007198333740234, KL: 71.67367553710938, Loss: 0.01595083251595497, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16900/20000], Bound: 0.39247748255729675, Entropy: 141.00868225097656, Temp: 2.7007200717926025, KL: 76.921142578125, Loss: 0.014662540517747402, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16901/20000], Bound: 0.4017711281776428, Entropy: 141.84275817871094, Temp: 2.7007205486297607, KL: 79.04689025878906, Loss: 0.015826156362891197, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16902/20000], Bound: 0.37426844239234924, Entropy: 139.53163146972656, Temp: 2.70072078704834, KL: 71.77351379394531, Loss: 0.014401041902601719, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16903/20000], Bound: 0.4015059471130371, Entropy: 140.82522583007812, Temp: 2.700721502304077, KL: 79.00102233886719, Loss: 0.015764586627483368, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16904/20000], Bound: 0.3459799289703369, Entropy: 142.18247985839844, Temp: 2.7007222175598145, KL: 62.787628173828125, Loss: 0.01632118411362171, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16905/20000], Bound: 0.358999639749527, Entropy: 142.33255004882812, Temp: 2.7007226943969727, KL: 64.67529296875, Loss: 0.019526872783899307, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16906/20000], Bound: 0.3509906828403473, Entropy: 141.47959899902344, Temp: 2.7007226943969727, KL: 62.92860412597656, Loss: 0.01862460747361183, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16907/20000], Bound: 0.38443273305892944, Entropy: 140.12351989746094, Temp: 2.7007222175598145, KL: 72.4410400390625, Loss: 0.018598977476358414, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16908/20000], Bound: 0.388843297958374, Entropy: 139.1614532470703, Temp: 2.700721502304077, KL: 75.80833435058594, Loss: 0.014747723937034607, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16909/20000], Bound: 0.40806645154953003, Entropy: 141.8140411376953, Temp: 2.700721025466919, KL: 80.51853942871094, Loss: 0.016595982015132904, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16910/20000], Bound: 0.36218130588531494, Entropy: 143.33432006835938, Temp: 2.70072078704834, KL: 68.07661437988281, Loss: 0.014885809272527695, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16911/20000], Bound: 0.36878588795661926, Entropy: 141.26271057128906, Temp: 2.70072078704834, KL: 69.04144287109375, Loss: 0.016560928896069527, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16912/20000], Bound: 0.38370099663734436, Entropy: 141.86605834960938, Temp: 2.7007205486297607, KL: 72.90925598144531, Loss: 0.017338236793875694, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16913/20000], Bound: 0.3668580949306488, Entropy: 140.71705627441406, Temp: 2.7007200717926025, KL: 67.86032104492188, Loss: 0.017733938992023468, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16914/20000], Bound: 0.3863367736339569, Entropy: 142.08103942871094, Temp: 2.7007195949554443, KL: 74.32557678222656, Loss: 0.01613677851855755, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16915/20000], Bound: 0.37543630599975586, Entropy: 139.90151977539062, Temp: 2.700719118118286, KL: 70.25250244140625, Loss: 0.017837245017290115, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16916/20000], Bound: 0.38216742873191833, Entropy: 141.98870849609375, Temp: 2.700718402862549, KL: 72.42012023925781, Loss: 0.0174197256565094, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16917/20000], Bound: 0.3856048285961151, Entropy: 142.1141357421875, Temp: 2.7007176876068115, KL: 73.0274658203125, Loss: 0.018145006150007248, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16918/20000], Bound: 0.3744129538536072, Entropy: 142.06153869628906, Temp: 2.7007172107696533, KL: 70.05380249023438, Loss: 0.017661523073911667, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16919/20000], Bound: 0.37379786372184753, Entropy: 141.28082275390625, Temp: 2.700716257095337, KL: 70.76127624511719, Loss: 0.0160253643989563, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16920/20000], Bound: 0.39156925678253174, Entropy: 142.3568572998047, Temp: 2.7007153034210205, KL: 75.68989562988281, Loss: 0.016447408124804497, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16921/20000], Bound: 0.41456279158592224, Entropy: 140.24464416503906, Temp: 2.7007148265838623, KL: 81.05226135253906, Loss: 0.019248640164732933, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16922/20000], Bound: 0.3896031975746155, Entropy: 142.5975341796875, Temp: 2.700714111328125, KL: 75.33277893066406, Loss: 0.016040217131376266, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16923/20000], Bound: 0.3752877116203308, Entropy: 143.21417236328125, Temp: 2.7007133960723877, KL: 71.78205871582031, Loss: 0.014926466159522533, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16924/20000], Bound: 0.3761342167854309, Entropy: 140.874267578125, Temp: 2.7007129192352295, KL: 71.36862182617188, Loss: 0.016142023727297783, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16925/20000], Bound: 0.4129065275192261, Entropy: 141.8857421875, Temp: 2.7007124423980713, KL: 80.81768798828125, Loss: 0.018751274794340134, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16926/20000], Bound: 0.3727814555168152, Entropy: 139.825439453125, Temp: 2.700711965560913, KL: 70.497802734375, Loss: 0.015974465757608414, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16927/20000], Bound: 0.3743515908718109, Entropy: 141.94390869140625, Temp: 2.700711488723755, KL: 71.05584716796875, Loss: 0.015773748978972435, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16928/20000], Bound: 0.3992483615875244, Entropy: 141.17381286621094, Temp: 2.7007110118865967, KL: 77.52920532226562, Loss: 0.017244746908545494, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16929/20000], Bound: 0.3874824047088623, Entropy: 140.00189208984375, Temp: 2.7007107734680176, KL: 75.7398681640625, Loss: 0.014137537218630314, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16930/20000], Bound: 0.3956982493400574, Entropy: 141.57461547851562, Temp: 2.7007107734680176, KL: 77.13333129882812, Loss: 0.016028815880417824, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16931/20000], Bound: 0.39194637537002563, Entropy: 139.47982788085938, Temp: 2.7007107734680176, KL: 74.57553100585938, Loss: 0.018715757876634598, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16932/20000], Bound: 0.3559317886829376, Entropy: 140.36135864257812, Temp: 2.7007105350494385, KL: 66.63465881347656, Loss: 0.014309492893517017, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16933/20000], Bound: 0.4019647538661957, Entropy: 140.82481384277344, Temp: 2.7007105350494385, KL: 78.84040832519531, Loss: 0.016315331682562828, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16934/20000], Bound: 0.35213005542755127, Entropy: 141.9538116455078, Temp: 2.7007105350494385, KL: 64.90127563476562, Loss: 0.015557984821498394, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16935/20000], Bound: 0.4150662124156952, Entropy: 142.41464233398438, Temp: 2.7007105350494385, KL: 80.94441223144531, Loss: 0.019731920212507248, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16936/20000], Bound: 0.39681366086006165, Entropy: 140.7488250732422, Temp: 2.7007102966308594, KL: 78.16023254394531, Loss: 0.014738821424543858, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16937/20000], Bound: 0.40290072560310364, Entropy: 141.0719451904297, Temp: 2.7007102966308594, KL: 78.01785278320312, Loss: 0.018355736508965492, Learning Rate: 1.9713708709060253e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16938/20000], Bound: 0.366244375705719, Entropy: 142.81304931640625, Temp: 2.7007102966308594, KL: 68.48468017578125, Loss: 0.016255808994174004, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16939/20000], Bound: 0.3679697811603546, Entropy: 142.732421875, Temp: 2.7007100582122803, KL: 68.9041748046875, Loss: 0.01638551615178585, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16940/20000], Bound: 0.39759373664855957, Entropy: 141.4474334716797, Temp: 2.700709819793701, KL: 77.63392639160156, Loss: 0.0161412563174963, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16941/20000], Bound: 0.37526026368141174, Entropy: 142.5175323486328, Temp: 2.700709819793701, KL: 70.04911804199219, Loss: 0.018120167776942253, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16942/20000], Bound: 0.3940066695213318, Entropy: 143.19676208496094, Temp: 2.700709581375122, KL: 76.41752624511719, Loss: 0.016429021954536438, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16943/20000], Bound: 0.4079577922821045, Entropy: 141.8905792236328, Temp: 2.700709342956543, KL: 81.06716918945312, Loss: 0.015519543550908566, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16944/20000], Bound: 0.3757101893424988, Entropy: 142.462890625, Temp: 2.700709342956543, KL: 72.63652038574219, Loss: 0.013569112867116928, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16945/20000], Bound: 0.37672197818756104, Entropy: 142.79348754882812, Temp: 2.700709581375122, KL: 72.04885864257812, Loss: 0.015195516869425774, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16946/20000], Bound: 0.3564641773700714, Entropy: 142.28111267089844, Temp: 2.700709819793701, KL: 65.22259521484375, Loss: 0.017199167981743813, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16947/20000], Bound: 0.40252476930618286, Entropy: 140.68991088867188, Temp: 2.700709819793701, KL: 76.81123352050781, Loss: 0.020381659269332886, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16948/20000], Bound: 0.39738765358924866, Entropy: 141.4999542236328, Temp: 2.700709581375122, KL: 76.50599670410156, Loss: 0.01811634562909603, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16949/20000], Bound: 0.36372724175453186, Entropy: 143.24981689453125, Temp: 2.700709104537964, KL: 68.98171997070312, Loss: 0.014017335139214993, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16950/20000], Bound: 0.3961465358734131, Entropy: 140.68374633789062, Temp: 2.7007088661193848, KL: 75.82292175292969, Loss: 0.018700355663895607, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16951/20000], Bound: 0.38161423802375793, Entropy: 142.2568359375, Temp: 2.7007086277008057, KL: 71.642822265625, Loss: 0.018561888486146927, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16952/20000], Bound: 0.37547290325164795, Entropy: 141.8648681640625, Temp: 2.7007081508636475, KL: 71.69258117675781, Loss: 0.015190508216619492, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16953/20000], Bound: 0.4019390046596527, Entropy: 140.04327392578125, Temp: 2.7007079124450684, KL: 75.68632507324219, Loss: 0.022140376269817352, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16954/20000], Bound: 0.40817582607269287, Entropy: 139.46142578125, Temp: 2.700706958770752, KL: 81.54806518554688, Loss: 0.014750784263014793, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16955/20000], Bound: 0.39948132634162903, Entropy: 140.7130126953125, Temp: 2.7007064819335938, KL: 78.37631225585938, Loss: 0.01580461859703064, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16956/20000], Bound: 0.3904851973056793, Entropy: 141.23268127441406, Temp: 2.7007062435150146, KL: 75.71708679199219, Loss: 0.015807509422302246, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16957/20000], Bound: 0.36788925528526306, Entropy: 141.4852752685547, Temp: 2.7007060050964355, KL: 69.07054138183594, Loss: 0.016035107895731926, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16958/20000], Bound: 0.38350409269332886, Entropy: 142.55789184570312, Temp: 2.7007057666778564, KL: 73.24240112304688, Loss: 0.01661546714603901, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16959/20000], Bound: 0.3738026022911072, Entropy: 141.5807342529297, Temp: 2.7007055282592773, KL: 71.22782897949219, Loss: 0.015164057724177837, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16960/20000], Bound: 0.38757967948913574, Entropy: 139.51324462890625, Temp: 2.7007052898406982, KL: 75.10833740234375, Loss: 0.015359310433268547, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16961/20000], Bound: 0.36789819598197937, Entropy: 141.27049255371094, Temp: 2.7007052898406982, KL: 69.38227844238281, Loss: 0.015462696552276611, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16962/20000], Bound: 0.374467134475708, Entropy: 141.2535858154297, Temp: 2.7007052898406982, KL: 71.37147521972656, Loss: 0.01525068935006857, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16963/20000], Bound: 0.3810696601867676, Entropy: 140.31593322753906, Temp: 2.7007052898406982, KL: 74.37306213378906, Loss: 0.01321522705256939, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16964/20000], Bound: 0.37191614508628845, Entropy: 141.75035095214844, Temp: 2.7007057666778564, KL: 69.8729248046875, Loss: 0.016673387959599495, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16965/20000], Bound: 0.36633065342903137, Entropy: 141.97669982910156, Temp: 2.7007060050964355, KL: 68.60093688964844, Loss: 0.016085822135210037, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16966/20000], Bound: 0.4090663492679596, Entropy: 139.04544067382812, Temp: 2.7007062435150146, KL: 79.88720703125, Loss: 0.018322722986340523, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16967/20000], Bound: 0.38276901841163635, Entropy: 140.87464904785156, Temp: 2.7007062435150146, KL: 72.380126953125, Loss: 0.017816705629229546, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16968/20000], Bound: 0.3788244128227234, Entropy: 142.8083038330078, Temp: 2.7007062435150146, KL: 73.26908874511719, Loss: 0.014057780615985394, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16969/20000], Bound: 0.40406545996665955, Entropy: 139.6768798828125, Temp: 2.7007064819335938, KL: 79.35404968261719, Loss: 0.016527049243450165, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16970/20000], Bound: 0.3902204632759094, Entropy: 142.94483947753906, Temp: 2.700706720352173, KL: 74.89210510253906, Loss: 0.017191093415021896, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16971/20000], Bound: 0.35719040036201477, Entropy: 139.9138641357422, Temp: 2.700706958770752, KL: 67.52388000488281, Loss: 0.013314589858055115, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16972/20000], Bound: 0.40542611479759216, Entropy: 139.62640380859375, Temp: 2.700707197189331, KL: 79.33401489257812, Loss: 0.017319196835160255, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16973/20000], Bound: 0.3945155143737793, Entropy: 140.83995056152344, Temp: 2.700707197189331, KL: 75.22697448730469, Loss: 0.01891115866601467, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16974/20000], Bound: 0.39007049798965454, Entropy: 140.9022979736328, Temp: 2.700707197189331, KL: 75.32203674316406, Loss: 0.016313662752509117, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16975/20000], Bound: 0.35168522596359253, Entropy: 140.6919708251953, Temp: 2.700707197189331, KL: 64.80625915527344, Loss: 0.015505144372582436, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16976/20000], Bound: 0.3844148814678192, Entropy: 141.51673889160156, Temp: 2.700707197189331, KL: 74.50759887695312, Loss: 0.01476326584815979, Learning Rate: 1.9713708709060253e-06\n",
      "Epoch [16977/20000], Bound: 0.3764966130256653, Entropy: 141.34027099609375, Temp: 2.70070743560791, KL: 70.8458251953125, Loss: 0.017302755266427994, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16978/20000], Bound: 0.402545690536499, Entropy: 141.83685302734375, Temp: 2.70070743560791, KL: 79.27580261230469, Loss: 0.015830373391509056, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16979/20000], Bound: 0.377765953540802, Entropy: 141.76112365722656, Temp: 2.7007079124450684, KL: 72.00617980957031, Loss: 0.015830928459763527, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16980/20000], Bound: 0.4170580208301544, Entropy: 141.63284301757812, Temp: 2.7007079124450684, KL: 84.68827819824219, Loss: 0.013925008475780487, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16981/20000], Bound: 0.39911016821861267, Entropy: 140.55979919433594, Temp: 2.7007086277008057, KL: 77.10527038574219, Loss: 0.01795354299247265, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16982/20000], Bound: 0.37386995553970337, Entropy: 142.03575134277344, Temp: 2.7007086277008057, KL: 69.68948364257812, Loss: 0.01804780773818493, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16983/20000], Bound: 0.39294350147247314, Entropy: 140.92410278320312, Temp: 2.7007088661193848, KL: 76.17567443847656, Loss: 0.016296619549393654, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16984/20000], Bound: 0.4017409384250641, Entropy: 140.38925170898438, Temp: 2.700709104537964, KL: 77.98728942871094, Loss: 0.017771050333976746, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16985/20000], Bound: 0.40733468532562256, Entropy: 140.40428161621094, Temp: 2.700709104537964, KL: 78.2225341796875, Loss: 0.02043871395289898, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16986/20000], Bound: 0.39194923639297485, Entropy: 140.55043029785156, Temp: 2.700709104537964, KL: 74.77423095703125, Loss: 0.0183494184166193, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16987/20000], Bound: 0.3834831714630127, Entropy: 140.9774169921875, Temp: 2.7007088661193848, KL: 74.17533874511719, Loss: 0.014877032488584518, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16988/20000], Bound: 0.3784518241882324, Entropy: 140.13929748535156, Temp: 2.7007088661193848, KL: 72.66331481933594, Loss: 0.014980368316173553, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16989/20000], Bound: 0.3982205390930176, Entropy: 142.07073974609375, Temp: 2.7007088661193848, KL: 77.45573425292969, Loss: 0.0168154314160347, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16990/20000], Bound: 0.3924456238746643, Entropy: 140.7431182861328, Temp: 2.7007088661193848, KL: 76.61137390136719, Loss: 0.015218588523566723, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16991/20000], Bound: 0.38326311111450195, Entropy: 141.52317810058594, Temp: 2.700709104537964, KL: 72.54812622070312, Loss: 0.01777123101055622, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16992/20000], Bound: 0.38843047618865967, Entropy: 141.13926696777344, Temp: 2.700709104537964, KL: 75.99139404296875, Loss: 0.014185041189193726, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16993/20000], Bound: 0.39001476764678955, Entropy: 140.04432678222656, Temp: 2.700709342956543, KL: 74.69703674316406, Loss: 0.017440548166632652, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16994/20000], Bound: 0.395175576210022, Entropy: 141.3303985595703, Temp: 2.700709581375122, KL: 77.54948425292969, Loss: 0.014972280710935593, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16995/20000], Bound: 0.3773484528064728, Entropy: 142.9806671142578, Temp: 2.700709819793701, KL: 71.3690185546875, Loss: 0.016787972301244736, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16996/20000], Bound: 0.3876795470714569, Entropy: 139.53488159179688, Temp: 2.7007100582122803, KL: 73.536376953125, Loss: 0.018323641270399094, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16997/20000], Bound: 0.3792947232723236, Entropy: 140.8402557373047, Temp: 2.7007100582122803, KL: 71.12835693359375, Loss: 0.0182724017649889, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16998/20000], Bound: 0.37628209590911865, Entropy: 142.5624542236328, Temp: 2.700709819793701, KL: 71.06195068359375, Loss: 0.016788478940725327, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [16999/20000], Bound: 0.3533632159233093, Entropy: 142.85821533203125, Temp: 2.700709581375122, KL: 65.36869812011719, Loss: 0.01532744150608778, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17000/20000], Bound: 0.3542312979698181, Entropy: 139.910400390625, Temp: 2.700709342956543, KL: 64.83322143554688, Loss: 0.016766339540481567, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17001/20000], Bound: 0.37571221590042114, Entropy: 141.21717834472656, Temp: 2.700709104537964, KL: 72.82341003417969, Loss: 0.013224204070866108, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17002/20000], Bound: 0.3803711235523224, Entropy: 140.04209899902344, Temp: 2.700709104537964, KL: 72.36082458496094, Loss: 0.016566528007388115, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17003/20000], Bound: 0.37030020356178284, Entropy: 143.4696807861328, Temp: 2.700709104537964, KL: 68.89387512207031, Loss: 0.017632273957133293, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17004/20000], Bound: 0.38441938161849976, Entropy: 141.18504333496094, Temp: 2.7007088661193848, KL: 73.31094360351562, Loss: 0.016981152817606926, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17005/20000], Bound: 0.37964579463005066, Entropy: 141.52491760253906, Temp: 2.7007086277008057, KL: 72.64093017578125, Loss: 0.015659790486097336, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17006/20000], Bound: 0.363967627286911, Entropy: 140.2658233642578, Temp: 2.7007086277008057, KL: 67.12152099609375, Loss: 0.01758694089949131, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17007/20000], Bound: 0.38198384642601013, Entropy: 140.6261444091797, Temp: 2.7007079124450684, KL: 73.41976928710938, Loss: 0.015470356680452824, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17008/20000], Bound: 0.38517260551452637, Entropy: 141.69886779785156, Temp: 2.7007079124450684, KL: 74.85581970214844, Loss: 0.014526866376399994, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17009/20000], Bound: 0.3803021013736725, Entropy: 141.76443481445312, Temp: 2.7007079124450684, KL: 72.21891784667969, Loss: 0.016792254522442818, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17010/20000], Bound: 0.3892262876033783, Entropy: 142.19447326660156, Temp: 2.70070743560791, KL: 75.29071044921875, Loss: 0.01591356284916401, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17011/20000], Bound: 0.37648898363113403, Entropy: 142.4339141845703, Temp: 2.70070743560791, KL: 72.59991455078125, Loss: 0.014051232486963272, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17012/20000], Bound: 0.3691103756427765, Entropy: 144.37303161621094, Temp: 2.70070743560791, KL: 67.69931030273438, Loss: 0.01921652816236019, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17013/20000], Bound: 0.35840290784835815, Entropy: 142.34884643554688, Temp: 2.700707197189331, KL: 66.67642211914062, Loss: 0.015512187033891678, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17014/20000], Bound: 0.36490967869758606, Entropy: 144.01390075683594, Temp: 2.700707197189331, KL: 67.05101013183594, Loss: 0.018210452049970627, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17015/20000], Bound: 0.3699501156806946, Entropy: 141.71434020996094, Temp: 2.700706720352173, KL: 68.48577880859375, Loss: 0.018203092738986015, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17016/20000], Bound: 0.37758564949035645, Entropy: 140.9921112060547, Temp: 2.7007062435150146, KL: 70.95378112792969, Loss: 0.017683139070868492, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17017/20000], Bound: 0.38338416814804077, Entropy: 140.56312561035156, Temp: 2.7007055282592773, KL: 72.13999938964844, Loss: 0.01859191618859768, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17018/20000], Bound: 0.37377190589904785, Entropy: 141.76133728027344, Temp: 2.700705051422119, KL: 71.17143249511719, Loss: 0.015252145938575268, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17019/20000], Bound: 0.40989866852760315, Entropy: 139.41603088378906, Temp: 2.700704336166382, KL: 81.07420349121094, Loss: 0.016590287908911705, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17020/20000], Bound: 0.37630775570869446, Entropy: 141.88392639160156, Temp: 2.7007038593292236, KL: 71.01922607421875, Loss: 0.016881188377738, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17021/20000], Bound: 0.3747742176055908, Entropy: 141.46371459960938, Temp: 2.7007033824920654, KL: 71.49467468261719, Loss: 0.015185617841780186, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17022/20000], Bound: 0.4164019227027893, Entropy: 140.6173095703125, Temp: 2.7007029056549072, KL: 83.23161315917969, Loss: 0.01625102572143078, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17023/20000], Bound: 0.37007397413253784, Entropy: 141.4262237548828, Temp: 2.700702667236328, KL: 69.93290710449219, Loss: 0.015589243732392788, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17024/20000], Bound: 0.382005900144577, Entropy: 140.5913848876953, Temp: 2.700702428817749, KL: 73.51658630371094, Loss: 0.01530293095856905, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17025/20000], Bound: 0.38128355145454407, Entropy: 142.88302612304688, Temp: 2.700702428817749, KL: 72.74778747558594, Loss: 0.01633882336318493, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17026/20000], Bound: 0.3710547983646393, Entropy: 142.00738525390625, Temp: 2.700702428817749, KL: 68.41738891601562, Loss: 0.01891276054084301, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17027/20000], Bound: 0.3909156620502472, Entropy: 140.96939086914062, Temp: 2.70070219039917, KL: 75.021484375, Loss: 0.0173292625695467, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17028/20000], Bound: 0.3827325999736786, Entropy: 141.75038146972656, Temp: 2.700701951980591, KL: 73.83094787597656, Loss: 0.015111098065972328, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17029/20000], Bound: 0.4118445813655853, Entropy: 140.35198974609375, Temp: 2.7007017135620117, KL: 81.46574401855469, Loss: 0.01695523038506508, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17030/20000], Bound: 0.38043951988220215, Entropy: 142.04750061035156, Temp: 2.7007014751434326, KL: 72.22232055664062, Loss: 0.016859479248523712, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17031/20000], Bound: 0.4028538465499878, Entropy: 141.48141479492188, Temp: 2.7007012367248535, KL: 79.46650695800781, Loss: 0.015647726133465767, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17032/20000], Bound: 0.3903140425682068, Entropy: 141.63693237304688, Temp: 2.7007012367248535, KL: 74.82052612304688, Loss: 0.017374353483319283, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17033/20000], Bound: 0.36596059799194336, Entropy: 142.35890197753906, Temp: 2.7007012367248535, KL: 67.87542724609375, Loss: 0.017234859988093376, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17034/20000], Bound: 0.40571025013923645, Entropy: 141.00856018066406, Temp: 2.7007012367248535, KL: 78.51678466796875, Loss: 0.018990034237504005, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17035/20000], Bound: 0.37814053893089294, Entropy: 142.554931640625, Temp: 2.7007009983062744, KL: 71.52204895019531, Loss: 0.016927042976021767, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17036/20000], Bound: 0.3679830729961395, Entropy: 141.65347290039062, Temp: 2.7007007598876953, KL: 68.60723876953125, Loss: 0.016942180693149567, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17037/20000], Bound: 0.40383192896842957, Entropy: 139.4101104736328, Temp: 2.700700283050537, KL: 80.41432189941406, Loss: 0.014434587210416794, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17038/20000], Bound: 0.38827139139175415, Entropy: 139.791259765625, Temp: 2.700700044631958, KL: 74.82804870605469, Loss: 0.01625257544219494, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17039/20000], Bound: 0.3798159956932068, Entropy: 140.27166748046875, Temp: 2.700700044631958, KL: 72.85545349121094, Loss: 0.015353593043982983, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17040/20000], Bound: 0.3979668617248535, Entropy: 142.60040283203125, Temp: 2.700700044631958, KL: 76.65756225585938, Loss: 0.018153714016079903, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17041/20000], Bound: 0.38943997025489807, Entropy: 140.16903686523438, Temp: 2.700700044631958, KL: 74.43057250976562, Loss: 0.017621852457523346, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17042/20000], Bound: 0.3528066873550415, Entropy: 142.38119506835938, Temp: 2.700699806213379, KL: 65.072509765625, Loss: 0.015589063055813313, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17043/20000], Bound: 0.40552830696105957, Entropy: 139.59280395507812, Temp: 2.7006995677948, KL: 80.31208801269531, Loss: 0.015565099194645882, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17044/20000], Bound: 0.3628610670566559, Entropy: 140.51841735839844, Temp: 2.7006995677948, KL: 68.00477600097656, Loss: 0.01537340134382248, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17045/20000], Bound: 0.40988901257514954, Entropy: 142.01158142089844, Temp: 2.7006995677948, KL: 82.00712585449219, Loss: 0.014857660979032516, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17046/20000], Bound: 0.3914440870285034, Entropy: 141.16468811035156, Temp: 2.700699806213379, KL: 76.55058288574219, Loss: 0.014785696752369404, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17047/20000], Bound: 0.3922862708568573, Entropy: 140.0771942138672, Temp: 2.700700044631958, KL: 76.49769592285156, Loss: 0.015342162922024727, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17048/20000], Bound: 0.38407859206199646, Entropy: 140.20095825195312, Temp: 2.700700521469116, KL: 71.25540161132812, Loss: 0.020603183656930923, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17049/20000], Bound: 0.3714672923088074, Entropy: 141.04519653320312, Temp: 2.700700521469116, KL: 70.36036682128906, Loss: 0.01553359441459179, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17050/20000], Bound: 0.3566102385520935, Entropy: 140.6354522705078, Temp: 2.700700521469116, KL: 64.74563598632812, Loss: 0.01815771870315075, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17051/20000], Bound: 0.40912434458732605, Entropy: 141.1625213623047, Temp: 2.700700283050537, KL: 81.21633911132812, Loss: 0.015894336625933647, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17052/20000], Bound: 0.3456990718841553, Entropy: 141.54725646972656, Temp: 2.700700283050537, KL: 63.6881103515625, Loss: 0.014510716311633587, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17053/20000], Bound: 0.3713700473308563, Entropy: 141.79698181152344, Temp: 2.700700283050537, KL: 70.61593627929688, Loss: 0.015009013935923576, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17054/20000], Bound: 0.38636669516563416, Entropy: 141.75146484375, Temp: 2.700700283050537, KL: 73.68960571289062, Loss: 0.01733020320534706, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17055/20000], Bound: 0.3817860186100006, Entropy: 141.35797119140625, Temp: 2.700700283050537, KL: 71.92135620117188, Loss: 0.018138322979211807, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17056/20000], Bound: 0.36235958337783813, Entropy: 141.84278869628906, Temp: 2.700700044631958, KL: 67.28717041015625, Loss: 0.01644018664956093, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17057/20000], Bound: 0.40079599618911743, Entropy: 141.7742462158203, Temp: 2.700699806213379, KL: 76.70907592773438, Loss: 0.019615795463323593, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17058/20000], Bound: 0.3807087540626526, Entropy: 141.34893798828125, Temp: 2.7006993293762207, KL: 74.44413757324219, Loss: 0.012890207581222057, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17059/20000], Bound: 0.36749690771102905, Entropy: 141.51986694335938, Temp: 2.7006990909576416, KL: 68.1358642578125, Loss: 0.017559265717864037, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17060/20000], Bound: 0.379567414522171, Entropy: 140.9200897216797, Temp: 2.7006988525390625, KL: 72.16755676269531, Loss: 0.01649417355656624, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17061/20000], Bound: 0.3977968990802765, Entropy: 142.02447509765625, Temp: 2.7006986141204834, KL: 79.02090454101562, Loss: 0.013684887439012527, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17062/20000], Bound: 0.41656506061553955, Entropy: 141.68540954589844, Temp: 2.7006986141204834, KL: 80.5687255859375, Loss: 0.021273106336593628, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17063/20000], Bound: 0.3820599913597107, Entropy: 142.23388671875, Temp: 2.7006983757019043, KL: 72.52365112304688, Loss: 0.017170215025544167, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17064/20000], Bound: 0.37037330865859985, Entropy: 141.5084686279297, Temp: 2.700698137283325, KL: 70.91989135742188, Loss: 0.013919886201620102, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17065/20000], Bound: 0.3815922141075134, Entropy: 142.45274353027344, Temp: 2.700698137283325, KL: 72.67236328125, Loss: 0.01664392277598381, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17066/20000], Bound: 0.38275641202926636, Entropy: 141.32505798339844, Temp: 2.700697898864746, KL: 72.9901123046875, Loss: 0.016680564731359482, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17067/20000], Bound: 0.4083597958087921, Entropy: 141.03475952148438, Temp: 2.700697660446167, KL: 80.26547241210938, Loss: 0.017227908596396446, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17068/20000], Bound: 0.4099567234516144, Entropy: 140.1071014404297, Temp: 2.700697660446167, KL: 80.40113830566406, Loss: 0.01786881312727928, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17069/20000], Bound: 0.3635372519493103, Entropy: 141.58334350585938, Temp: 2.700697660446167, KL: 69.09782409667969, Loss: 0.013702992349863052, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17070/20000], Bound: 0.3996848464012146, Entropy: 140.39939880371094, Temp: 2.700697660446167, KL: 78.00477600097656, Loss: 0.016604451462626457, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17071/20000], Bound: 0.3859207332134247, Entropy: 140.0482940673828, Temp: 2.700697660446167, KL: 71.91201782226562, Loss: 0.020380377769470215, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17072/20000], Bound: 0.39114415645599365, Entropy: 142.50784301757812, Temp: 2.700697660446167, KL: 76.02104187011719, Loss: 0.01560288481414318, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17073/20000], Bound: 0.3723808825016022, Entropy: 141.29800415039062, Temp: 2.700697660446167, KL: 69.17179870605469, Loss: 0.018217233940958977, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17074/20000], Bound: 0.3820015490055084, Entropy: 141.55178833007812, Temp: 2.700697422027588, KL: 73.03578186035156, Loss: 0.016190707683563232, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17075/20000], Bound: 0.3995283246040344, Entropy: 141.3075408935547, Temp: 2.700697183609009, KL: 76.44534301757812, Loss: 0.019405333325266838, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17076/20000], Bound: 0.3933117091655731, Entropy: 140.4870147705078, Temp: 2.7006967067718506, KL: 74.5889892578125, Loss: 0.01943485625088215, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17077/20000], Bound: 0.4129975140094757, Entropy: 141.3876190185547, Temp: 2.7006962299346924, KL: 83.21354675292969, Loss: 0.014366555027663708, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17078/20000], Bound: 0.4079483449459076, Entropy: 143.61058044433594, Temp: 2.7006959915161133, KL: 81.95869445800781, Loss: 0.013863584958016872, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17079/20000], Bound: 0.37171316146850586, Entropy: 141.9280242919922, Temp: 2.7006959915161133, KL: 70.97299194335938, Loss: 0.01452930923551321, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17080/20000], Bound: 0.3785353899002075, Entropy: 141.6772003173828, Temp: 2.7006962299346924, KL: 72.82650756835938, Loss: 0.014722704887390137, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17081/20000], Bound: 0.39759573340415955, Entropy: 142.14854431152344, Temp: 2.7006964683532715, KL: 77.34715270996094, Loss: 0.016673145815730095, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17082/20000], Bound: 0.43499433994293213, Entropy: 140.45150756835938, Temp: 2.7006967067718506, KL: 89.49618530273438, Loss: 0.015305589884519577, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17083/20000], Bound: 0.3799189031124115, Entropy: 142.84434509277344, Temp: 2.700697183609009, KL: 70.911865234375, Loss: 0.019006911665201187, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17084/20000], Bound: 0.3634062707424164, Entropy: 141.96804809570312, Temp: 2.700697422027588, KL: 68.26460266113281, Loss: 0.015177127905189991, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17085/20000], Bound: 0.398902952671051, Entropy: 140.8260040283203, Temp: 2.700697660446167, KL: 77.43930053710938, Loss: 0.01722099632024765, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17086/20000], Bound: 0.37009042501449585, Entropy: 142.06190490722656, Temp: 2.700697660446167, KL: 70.11247253417969, Loss: 0.015265426598489285, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17087/20000], Bound: 0.3855102062225342, Entropy: 142.50711059570312, Temp: 2.700697898864746, KL: 73.96440124511719, Loss: 0.016359183937311172, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17088/20000], Bound: 0.38430753350257874, Entropy: 140.78273010253906, Temp: 2.700698137283325, KL: 73.76162719726562, Loss: 0.016086427494883537, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17089/20000], Bound: 0.35816866159439087, Entropy: 142.94168090820312, Temp: 2.7006983757019043, KL: 67.04081726074219, Loss: 0.014715942554175854, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17090/20000], Bound: 0.39792564511299133, Entropy: 140.70916748046875, Temp: 2.7006986141204834, KL: 77.01039123535156, Loss: 0.017477797344326973, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17091/20000], Bound: 0.39756155014038086, Entropy: 141.73052978515625, Temp: 2.7006988525390625, KL: 77.8441162109375, Loss: 0.015734322369098663, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17092/20000], Bound: 0.3829163610935211, Entropy: 143.92013549804688, Temp: 2.7006990909576416, KL: 74.30657958984375, Loss: 0.014329250901937485, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17093/20000], Bound: 0.4041200578212738, Entropy: 141.0707244873047, Temp: 2.7006995677948, KL: 79.98263549804688, Loss: 0.015393485315144062, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17094/20000], Bound: 0.40185484290122986, Entropy: 140.3754425048828, Temp: 2.700700044631958, KL: 78.18142700195312, Loss: 0.017474496737122536, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17095/20000], Bound: 0.3830235004425049, Entropy: 140.54554748535156, Temp: 2.700700521469116, KL: 72.55256652832031, Loss: 0.017634153366088867, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17096/20000], Bound: 0.375383585691452, Entropy: 141.2271728515625, Temp: 2.7007007598876953, KL: 70.559326171875, Loss: 0.017241043969988823, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17097/20000], Bound: 0.40304458141326904, Entropy: 138.85675048828125, Temp: 2.7007009983062744, KL: 79.71914672851562, Loss: 0.015285560861229897, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17098/20000], Bound: 0.3675215244293213, Entropy: 142.265625, Temp: 2.7007012367248535, KL: 67.86444091796875, Loss: 0.018074702471494675, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17099/20000], Bound: 0.37630367279052734, Entropy: 140.64041137695312, Temp: 2.7007012367248535, KL: 71.08296203613281, Loss: 0.016760965809226036, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17100/20000], Bound: 0.3830702304840088, Entropy: 140.9627227783203, Temp: 2.7007012367248535, KL: 72.05137634277344, Loss: 0.018587172031402588, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17101/20000], Bound: 0.3783920407295227, Entropy: 140.84481811523438, Temp: 2.7007009983062744, KL: 72.61373901367188, Loss: 0.015040172263979912, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17102/20000], Bound: 0.400093674659729, Entropy: 143.5237274169922, Temp: 2.7007009983062744, KL: 79.31161499023438, Loss: 0.014410246163606644, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17103/20000], Bound: 0.36634302139282227, Entropy: 141.4140625, Temp: 2.7007012367248535, KL: 68.30596923828125, Loss: 0.0166383795440197, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17104/20000], Bound: 0.4030514061450958, Entropy: 142.07101440429688, Temp: 2.7007012367248535, KL: 79.08631896972656, Loss: 0.016460919752717018, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17105/20000], Bound: 0.375469833612442, Entropy: 139.97250366210938, Temp: 2.7007012367248535, KL: 70.91777038574219, Loss: 0.01662329211831093, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17106/20000], Bound: 0.36612048745155334, Entropy: 141.83648681640625, Temp: 2.7007012367248535, KL: 68.57843017578125, Loss: 0.016017192974686623, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17107/20000], Bound: 0.4122116267681122, Entropy: 140.07012939453125, Temp: 2.7007012367248535, KL: 80.9888916015625, Loss: 0.018043985590338707, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17108/20000], Bound: 0.4275914132595062, Entropy: 140.34288024902344, Temp: 2.7007012367248535, KL: 85.64002990722656, Loss: 0.018166452646255493, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17109/20000], Bound: 0.3498717248439789, Entropy: 142.25950622558594, Temp: 2.7007012367248535, KL: 65.46565246582031, Loss: 0.013353245332837105, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17110/20000], Bound: 0.3792707324028015, Entropy: 140.8991241455078, Temp: 2.7007012367248535, KL: 72.2467041015625, Loss: 0.016189059242606163, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17111/20000], Bound: 0.3639392554759979, Entropy: 139.81068420410156, Temp: 2.7007012367248535, KL: 67.18167114257812, Loss: 0.01746070384979248, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17112/20000], Bound: 0.3965491056442261, Entropy: 140.61228942871094, Temp: 2.7007012367248535, KL: 76.41902160644531, Loss: 0.0178173016756773, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17113/20000], Bound: 0.3860204219818115, Entropy: 140.9672088623047, Temp: 2.7007012367248535, KL: 73.52560424804688, Loss: 0.017446894198656082, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17114/20000], Bound: 0.4038240909576416, Entropy: 142.25437927246094, Temp: 2.7007009983062744, KL: 77.92532348632812, Loss: 0.019038336351513863, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17115/20000], Bound: 0.3613787889480591, Entropy: 142.09576416015625, Temp: 2.7007007598876953, KL: 66.83056640625, Loss: 0.01677413284778595, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17116/20000], Bound: 0.36389046907424927, Entropy: 142.78622436523438, Temp: 2.700700283050537, KL: 68.16943359375, Loss: 0.01560644619166851, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17117/20000], Bound: 0.40586599707603455, Entropy: 141.39300537109375, Temp: 2.700699806213379, KL: 79.5743408203125, Loss: 0.017118632793426514, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17118/20000], Bound: 0.36573317646980286, Entropy: 140.57928466796875, Temp: 2.7006995677948, KL: 69.5284423828125, Loss: 0.014055244624614716, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17119/20000], Bound: 0.36413106322288513, Entropy: 141.1718292236328, Temp: 2.7006993293762207, KL: 69.56681823730469, Loss: 0.013145205564796925, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17120/20000], Bound: 0.3668375015258789, Entropy: 142.5041961669922, Temp: 2.7006993293762207, KL: 68.3978271484375, Loss: 0.01672784425318241, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17121/20000], Bound: 0.38695698976516724, Entropy: 142.02059936523438, Temp: 2.7006993293762207, KL: 74.94381713867188, Loss: 0.015327118337154388, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17122/20000], Bound: 0.4057040810585022, Entropy: 140.82456970214844, Temp: 2.7006993293762207, KL: 80.30879211425781, Loss: 0.015668900683522224, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17123/20000], Bound: 0.3811308443546295, Entropy: 141.2269744873047, Temp: 2.7006995677948, KL: 73.46321105957031, Loss: 0.01493244618177414, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17124/20000], Bound: 0.36338093876838684, Entropy: 142.9722900390625, Temp: 2.700699806213379, KL: 68.13160705566406, Loss: 0.015410101041197777, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17125/20000], Bound: 0.39335116744041443, Entropy: 141.60511779785156, Temp: 2.700700044631958, KL: 74.04376220703125, Loss: 0.020465832203626633, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17126/20000], Bound: 0.3754788637161255, Entropy: 141.59259033203125, Temp: 2.700700044631958, KL: 70.8231201171875, Loss: 0.01680331490933895, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17127/20000], Bound: 0.38845375180244446, Entropy: 139.9166717529297, Temp: 2.700699806213379, KL: 74.33287048339844, Loss: 0.017268098890781403, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17128/20000], Bound: 0.39977481961250305, Entropy: 140.14488220214844, Temp: 2.7006995677948, KL: 77.38096618652344, Loss: 0.017808936536312103, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17129/20000], Bound: 0.38680505752563477, Entropy: 142.64028930664062, Temp: 2.7006993293762207, KL: 73.68911743164062, Loss: 0.017567915841937065, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17130/20000], Bound: 0.3756294846534729, Entropy: 143.13572692871094, Temp: 2.7006990909576416, KL: 71.81427001953125, Loss: 0.015048407949507236, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17131/20000], Bound: 0.35858550667762756, Entropy: 142.3877716064453, Temp: 2.7006988525390625, KL: 66.83328247070312, Loss: 0.015316457487642765, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17132/20000], Bound: 0.4036436676979065, Entropy: 140.5726318359375, Temp: 2.7006986141204834, KL: 77.90776062011719, Loss: 0.01897084154188633, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17133/20000], Bound: 0.38670048117637634, Entropy: 141.82203674316406, Temp: 2.7006983757019043, KL: 72.87934875488281, Loss: 0.019010573625564575, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17134/20000], Bound: 0.37447431683540344, Entropy: 141.9822998046875, Temp: 2.700697898864746, KL: 71.17066955566406, Loss: 0.01562620885670185, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17135/20000], Bound: 0.39628615975379944, Entropy: 140.24545288085938, Temp: 2.700697660446167, KL: 77.27816772460938, Loss: 0.016082579270005226, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17136/20000], Bound: 0.3963202238082886, Entropy: 141.9924774169922, Temp: 2.700697422027588, KL: 77.24017333984375, Loss: 0.016171539202332497, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17137/20000], Bound: 0.3642685115337372, Entropy: 142.8978729248047, Temp: 2.700697183609009, KL: 68.33753967285156, Loss: 0.015492958948016167, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17138/20000], Bound: 0.39513784646987915, Entropy: 141.1158905029297, Temp: 2.7006969451904297, KL: 77.11973571777344, Loss: 0.01574714109301567, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17139/20000], Bound: 0.36875975131988525, Entropy: 141.74954223632812, Temp: 2.7006969451904297, KL: 69.60755920410156, Loss: 0.01549892034381628, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17140/20000], Bound: 0.398253858089447, Entropy: 141.53538513183594, Temp: 2.7006969451904297, KL: 78.88125610351562, Loss: 0.014194469898939133, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17141/20000], Bound: 0.391313761472702, Entropy: 140.03091430664062, Temp: 2.700697183609009, KL: 76.07162475585938, Loss: 0.015601500868797302, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17142/20000], Bound: 0.3616138696670532, Entropy: 141.14895629882812, Temp: 2.700697422027588, KL: 66.53150939941406, Loss: 0.01745029166340828, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17143/20000], Bound: 0.37852057814598083, Entropy: 141.864501953125, Temp: 2.700697422027588, KL: 72.25762939453125, Loss: 0.015768058598041534, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17144/20000], Bound: 0.403064101934433, Entropy: 140.97616577148438, Temp: 2.700697422027588, KL: 78.8267822265625, Loss: 0.016948411241173744, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17145/20000], Bound: 0.3741656243801117, Entropy: 140.85504150390625, Temp: 2.700697422027588, KL: 69.8702392578125, Loss: 0.017869943752884865, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17146/20000], Bound: 0.37327829003334045, Entropy: 141.13815307617188, Temp: 2.700697422027588, KL: 71.82247924804688, Loss: 0.013785137794911861, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17147/20000], Bound: 0.35804063081741333, Entropy: 142.55401611328125, Temp: 2.700697422027588, KL: 66.75740051269531, Loss: 0.015174245461821556, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17148/20000], Bound: 0.3878241777420044, Entropy: 141.6722412109375, Temp: 2.700697422027588, KL: 74.29611206054688, Loss: 0.016995234414935112, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17149/20000], Bound: 0.3889814615249634, Entropy: 141.09812927246094, Temp: 2.700697422027588, KL: 75.27728271484375, Loss: 0.015805615112185478, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17150/20000], Bound: 0.36418798565864563, Entropy: 142.18795776367188, Temp: 2.700697422027588, KL: 68.1787109375, Loss: 0.015744872391223907, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17151/20000], Bound: 0.3824892044067383, Entropy: 141.30148315429688, Temp: 2.700697422027588, KL: 73.80572509765625, Loss: 0.015027019195258617, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17152/20000], Bound: 0.35339200496673584, Entropy: 140.0644073486328, Temp: 2.700697660446167, KL: 65.8984375, Loss: 0.014361443929374218, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17153/20000], Bound: 0.3633473515510559, Entropy: 142.9518585205078, Temp: 2.700697660446167, KL: 66.21446228027344, Loss: 0.018941931426525116, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17154/20000], Bound: 0.3860640525817871, Entropy: 140.3109130859375, Temp: 2.700697660446167, KL: 74.04638671875, Loss: 0.01650623418390751, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17155/20000], Bound: 0.3693571984767914, Entropy: 141.9635772705078, Temp: 2.700697660446167, KL: 69.02041625976562, Loss: 0.01690063253045082, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17156/20000], Bound: 0.38557109236717224, Entropy: 141.4153289794922, Temp: 2.700697422027588, KL: 72.56082153320312, Loss: 0.01899055764079094, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17157/20000], Bound: 0.3967953026294708, Entropy: 139.35873413085938, Temp: 2.7006969451904297, KL: 78.81198120117188, Loss: 0.013521992601454258, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17158/20000], Bound: 0.37971052527427673, Entropy: 140.4951171875, Temp: 2.7006969451904297, KL: 73.14315795898438, Loss: 0.014764515683054924, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17159/20000], Bound: 0.37112268805503845, Entropy: 142.0714569091797, Temp: 2.7006969451904297, KL: 70.11299133300781, Loss: 0.01580940932035446, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17160/20000], Bound: 0.39606690406799316, Entropy: 140.9855194091797, Temp: 2.7006969451904297, KL: 78.09992980957031, Loss: 0.014441034756600857, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17161/20000], Bound: 0.3893873393535614, Entropy: 140.40087890625, Temp: 2.700697183609009, KL: 76.31784057617188, Loss: 0.01409920398145914, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17162/20000], Bound: 0.3909670114517212, Entropy: 141.8462677001953, Temp: 2.700697660446167, KL: 76.80931091308594, Loss: 0.014047205448150635, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17163/20000], Bound: 0.3950454294681549, Entropy: 141.31646728515625, Temp: 2.700697898864746, KL: 77.55534362792969, Loss: 0.014890131540596485, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17164/20000], Bound: 0.38899505138397217, Entropy: 142.99403381347656, Temp: 2.7006986141204834, KL: 75.36143493652344, Loss: 0.015657169744372368, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17165/20000], Bound: 0.4101901650428772, Entropy: 140.63282775878906, Temp: 2.7006993293762207, KL: 80.5716552734375, Loss: 0.01768367737531662, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17166/20000], Bound: 0.3936547338962555, Entropy: 140.8558807373047, Temp: 2.700700044631958, KL: 76.6591796875, Loss: 0.01578940451145172, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17167/20000], Bound: 0.36139750480651855, Entropy: 142.63412475585938, Temp: 2.7007007598876953, KL: 66.30668640136719, Loss: 0.01775376684963703, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17168/20000], Bound: 0.3745322823524475, Entropy: 143.35092163085938, Temp: 2.7007012367248535, KL: 69.60501098632812, Loss: 0.01855563186109066, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17169/20000], Bound: 0.35624954104423523, Entropy: 140.89845275878906, Temp: 2.7007012367248535, KL: 66.42507934570312, Loss: 0.014861796982586384, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17170/20000], Bound: 0.4197218120098114, Entropy: 139.53182983398438, Temp: 2.7007014751434326, KL: 84.54098510742188, Loss: 0.01570669561624527, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17171/20000], Bound: 0.3769533336162567, Entropy: 142.17697143554688, Temp: 2.7007017135620117, KL: 71.16470336914062, Loss: 0.016955578699707985, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17172/20000], Bound: 0.37937742471694946, Entropy: 139.92977905273438, Temp: 2.700701951980591, KL: 72.5361328125, Loss: 0.015710221603512764, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17173/20000], Bound: 0.3819478750228882, Entropy: 141.0363006591797, Temp: 2.70070219039917, KL: 72.87065124511719, Loss: 0.016467643901705742, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17174/20000], Bound: 0.3684672713279724, Entropy: 141.82716369628906, Temp: 2.700702428817749, KL: 68.38861083984375, Loss: 0.017601698637008667, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17175/20000], Bound: 0.36210083961486816, Entropy: 143.21754455566406, Temp: 2.700702428817749, KL: 67.61972045898438, Loss: 0.015689542517066002, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17176/20000], Bound: 0.3759949803352356, Entropy: 139.79562377929688, Temp: 2.700702428817749, KL: 71.75328063964844, Loss: 0.015355710871517658, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17177/20000], Bound: 0.3859218955039978, Entropy: 140.39869689941406, Temp: 2.700702428817749, KL: 72.283447265625, Loss: 0.019693398848176003, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17178/20000], Bound: 0.3737732470035553, Entropy: 141.25669860839844, Temp: 2.70070219039917, KL: 69.862548828125, Loss: 0.01767606846988201, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17179/20000], Bound: 0.35225290060043335, Entropy: 141.57769775390625, Temp: 2.700701951980591, KL: 65.34468078613281, Loss: 0.01480021607130766, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17180/20000], Bound: 0.38614895939826965, Entropy: 142.97459411621094, Temp: 2.7007017135620117, KL: 72.37586975097656, Loss: 0.019644860178232193, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17181/20000], Bound: 0.40344104170799255, Entropy: 142.3335418701172, Temp: 2.7007012367248535, KL: 78.97024536132812, Loss: 0.016891546547412872, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17182/20000], Bound: 0.3972789943218231, Entropy: 141.58387756347656, Temp: 2.7007009983062744, KL: 77.00114440917969, Loss: 0.01713993214070797, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17183/20000], Bound: 0.39911985397338867, Entropy: 143.4491729736328, Temp: 2.700700521469116, KL: 77.82952880859375, Loss: 0.01661790907382965, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17184/20000], Bound: 0.36327680945396423, Entropy: 140.74761962890625, Temp: 2.700700283050537, KL: 68.08604431152344, Loss: 0.015440104529261589, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17185/20000], Bound: 0.3846288025379181, Entropy: 142.62060546875, Temp: 2.700700044631958, KL: 74.21040344238281, Loss: 0.015428672544658184, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17186/20000], Bound: 0.3533991277217865, Entropy: 141.23899841308594, Temp: 2.700699806213379, KL: 64.39764404296875, Loss: 0.01714363321661949, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17187/20000], Bound: 0.3482447862625122, Entropy: 141.22532653808594, Temp: 2.7006995677948, KL: 62.303680419921875, Loss: 0.018373942002654076, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17188/20000], Bound: 0.3854195177555084, Entropy: 139.44960021972656, Temp: 2.7006988525390625, KL: 74.53271484375, Loss: 0.015258073806762695, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17189/20000], Bound: 0.405393123626709, Entropy: 138.58560180664062, Temp: 2.7006983757019043, KL: 81.71757507324219, Loss: 0.012887933291494846, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17190/20000], Bound: 0.3499371409416199, Entropy: 142.564453125, Temp: 2.7006983757019043, KL: 65.47265625, Loss: 0.01337379403412342, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17191/20000], Bound: 0.39445099234580994, Entropy: 141.24949645996094, Temp: 2.7006983757019043, KL: 76.91792297363281, Loss: 0.015745241194963455, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17192/20000], Bound: 0.39016491174697876, Entropy: 141.8226776123047, Temp: 2.7006986141204834, KL: 77.13423156738281, Loss: 0.013009808026254177, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17193/20000], Bound: 0.3792290687561035, Entropy: 142.1463165283203, Temp: 2.7006990909576416, KL: 73.38734436035156, Loss: 0.014055000618100166, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17194/20000], Bound: 0.3807389736175537, Entropy: 140.80755615234375, Temp: 2.7006995677948, KL: 72.99168395996094, Loss: 0.015595445409417152, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17195/20000], Bound: 0.37618288397789, Entropy: 141.43313598632812, Temp: 2.700700044631958, KL: 71.54011535644531, Loss: 0.015850333496928215, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17196/20000], Bound: 0.3907621502876282, Entropy: 142.5216522216797, Temp: 2.700700521469116, KL: 73.97134399414062, Loss: 0.019189991056919098, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17197/20000], Bound: 0.4127222001552582, Entropy: 142.23182678222656, Temp: 2.7007007598876953, KL: 81.03187561035156, Loss: 0.01825103536248207, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17198/20000], Bound: 0.37197479605674744, Entropy: 142.89828491210938, Temp: 2.7007009983062744, KL: 71.37139892578125, Loss: 0.013930130749940872, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17199/20000], Bound: 0.4106118679046631, Entropy: 141.53562927246094, Temp: 2.7007012367248535, KL: 81.20378112792969, Loss: 0.01674945279955864, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17200/20000], Bound: 0.3811146318912506, Entropy: 141.37322998046875, Temp: 2.7007012367248535, KL: 73.54641723632812, Loss: 0.014769721776247025, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17201/20000], Bound: 0.39339110255241394, Entropy: 141.72491455078125, Temp: 2.7007017135620117, KL: 74.96662902832031, Loss: 0.018779082223773003, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17202/20000], Bound: 0.385664701461792, Entropy: 141.53546142578125, Temp: 2.700701951980591, KL: 74.48129272460938, Loss: 0.015485584735870361, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17203/20000], Bound: 0.39594054222106934, Entropy: 138.5701446533203, Temp: 2.70070219039917, KL: 76.26948547363281, Loss: 0.017760686576366425, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17204/20000], Bound: 0.3789674639701843, Entropy: 141.96580505371094, Temp: 2.700702428817749, KL: 72.1939697265625, Loss: 0.016124626621603966, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17205/20000], Bound: 0.3956014811992645, Entropy: 140.89730834960938, Temp: 2.700702667236328, KL: 78.0675048828125, Loss: 0.014246269129216671, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17206/20000], Bound: 0.3755345642566681, Entropy: 140.40122985839844, Temp: 2.7007031440734863, KL: 70.87124633789062, Loss: 0.01674385368824005, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17207/20000], Bound: 0.38861948251724243, Entropy: 141.20556640625, Temp: 2.7007033824920654, KL: 74.56718444824219, Loss: 0.01692410372197628, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17208/20000], Bound: 0.39334210753440857, Entropy: 140.17349243164062, Temp: 2.7007036209106445, KL: 77.10060119628906, Loss: 0.014801585115492344, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17209/20000], Bound: 0.3839333951473236, Entropy: 140.98147583007812, Temp: 2.7007040977478027, KL: 74.17031860351562, Loss: 0.015128440223634243, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17210/20000], Bound: 0.34879159927368164, Entropy: 142.43994140625, Temp: 2.700704574584961, KL: 63.7294921875, Loss: 0.01601412333548069, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17211/20000], Bound: 0.37242740392684937, Entropy: 142.57643127441406, Temp: 2.70070481300354, KL: 69.44000244140625, Loss: 0.017745383083820343, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17212/20000], Bound: 0.39062070846557617, Entropy: 141.32398986816406, Temp: 2.70070481300354, KL: 76.22373962402344, Loss: 0.01494315080344677, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17213/20000], Bound: 0.4105927348136902, Entropy: 140.546630859375, Temp: 2.700705051422119, KL: 81.26911926269531, Loss: 0.0166177935898304, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17214/20000], Bound: 0.39617395401000977, Entropy: 139.90025329589844, Temp: 2.700705051422119, KL: 76.517578125, Loss: 0.01742926985025406, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17215/20000], Bound: 0.3632736802101135, Entropy: 140.92308044433594, Temp: 2.7007052898406982, KL: 67.947265625, Loss: 0.01569545641541481, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17216/20000], Bound: 0.4034719169139862, Entropy: 140.34225463867188, Temp: 2.7007055282592773, KL: 79.10362243652344, Loss: 0.01666177436709404, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17217/20000], Bound: 0.3640836775302887, Entropy: 142.71292114257812, Temp: 2.7007057666778564, KL: 67.99049377441406, Loss: 0.016038803383708, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17218/20000], Bound: 0.40123069286346436, Entropy: 141.06640625, Temp: 2.7007060050964355, KL: 79.77281188964844, Loss: 0.014183602295815945, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17219/20000], Bound: 0.3726993501186371, Entropy: 141.55357360839844, Temp: 2.7007064819335938, KL: 69.82130432128906, Loss: 0.01718340814113617, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17220/20000], Bound: 0.41153061389923096, Entropy: 140.0911407470703, Temp: 2.700706720352173, KL: 80.41331481933594, Loss: 0.01872764155268669, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17221/20000], Bound: 0.3840821385383606, Entropy: 140.33847045898438, Temp: 2.700706958770752, KL: 72.71914672851562, Loss: 0.0178951695561409, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17222/20000], Bound: 0.3689049780368805, Entropy: 141.5467987060547, Temp: 2.700706958770752, KL: 68.74272155761719, Loss: 0.017176596447825432, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17223/20000], Bound: 0.3796495497226715, Entropy: 141.58163452148438, Temp: 2.700706720352173, KL: 71.13351440429688, Loss: 0.018452579155564308, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17224/20000], Bound: 0.37512025237083435, Entropy: 141.32379150390625, Temp: 2.7007064819335938, KL: 70.87684631347656, Loss: 0.016513271257281303, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17225/20000], Bound: 0.40471509099006653, Entropy: 142.807861328125, Temp: 2.7007062435150146, KL: 78.02279663085938, Loss: 0.01935197226703167, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17226/20000], Bound: 0.3779783844947815, Entropy: 143.02337646484375, Temp: 2.7007057666778564, KL: 72.01728820800781, Loss: 0.015923671424388885, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17227/20000], Bound: 0.3815344572067261, Entropy: 141.7078094482422, Temp: 2.7007052898406982, KL: 72.99726867675781, Loss: 0.016011489555239677, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17228/20000], Bound: 0.38046547770500183, Entropy: 141.20579528808594, Temp: 2.700705051422119, KL: 71.95143127441406, Loss: 0.017374925315380096, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17229/20000], Bound: 0.39340469241142273, Entropy: 141.6018524169922, Temp: 2.70070481300354, KL: 77.36175537109375, Loss: 0.014352254569530487, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17230/20000], Bound: 0.3926440179347992, Entropy: 142.52525329589844, Temp: 2.700704574584961, KL: 74.35896301269531, Loss: 0.019496681168675423, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17231/20000], Bound: 0.39923784136772156, Entropy: 141.7646484375, Temp: 2.700704336166382, KL: 78.39872741699219, Loss: 0.015629082918167114, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17232/20000], Bound: 0.4135406017303467, Entropy: 140.7622528076172, Temp: 2.7007040977478027, KL: 81.73854064941406, Loss: 0.01740271970629692, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17233/20000], Bound: 0.36851009726524353, Entropy: 141.97935485839844, Temp: 2.7007038593292236, KL: 69.35127258300781, Loss: 0.015841996297240257, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17234/20000], Bound: 0.38624879717826843, Entropy: 140.5673065185547, Temp: 2.7007036209106445, KL: 74.92628479003906, Loss: 0.014977010898292065, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17235/20000], Bound: 0.3971063494682312, Entropy: 141.22610473632812, Temp: 2.7007036209106445, KL: 77.46253967285156, Loss: 0.016190972179174423, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17236/20000], Bound: 0.38003671169281006, Entropy: 142.32943725585938, Temp: 2.7007036209106445, KL: 72.62591552734375, Loss: 0.01589667610824108, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17237/20000], Bound: 0.37781965732574463, Entropy: 141.8585662841797, Temp: 2.7007036209106445, KL: 72.35244750976562, Loss: 0.015218489803373814, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17238/20000], Bound: 0.3836258053779602, Entropy: 143.1746826171875, Temp: 2.7007038593292236, KL: 71.0858154296875, Loss: 0.02067352645099163, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17239/20000], Bound: 0.38277900218963623, Entropy: 141.7487335205078, Temp: 2.7007036209106445, KL: 74.2850341796875, Loss: 0.014295377768576145, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17240/20000], Bound: 0.3816748857498169, Entropy: 141.40966796875, Temp: 2.7007036209106445, KL: 73.09524536132812, Loss: 0.01590540260076523, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17241/20000], Bound: 0.3706679344177246, Entropy: 140.32350158691406, Temp: 2.7007036209106445, KL: 67.54592895507812, Loss: 0.020321859046816826, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17242/20000], Bound: 0.3563464283943176, Entropy: 143.02670288085938, Temp: 2.7007031440734863, KL: 63.30741882324219, Loss: 0.02068386599421501, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17243/20000], Bound: 0.3641059994697571, Entropy: 142.47775268554688, Temp: 2.700702428817749, KL: 68.33833312988281, Loss: 0.015406477265059948, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17244/20000], Bound: 0.3655737042427063, Entropy: 141.9767303466797, Temp: 2.7007017135620117, KL: 68.8004150390625, Loss: 0.015319501981139183, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17245/20000], Bound: 0.3633347451686859, Entropy: 141.42864990234375, Temp: 2.7007012367248535, KL: 67.91984558105469, Loss: 0.0157780759036541, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17246/20000], Bound: 0.3581349849700928, Entropy: 140.5138397216797, Temp: 2.7007007598876953, KL: 65.55131530761719, Loss: 0.017456132918596268, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17247/20000], Bound: 0.38414135575294495, Entropy: 141.68528747558594, Temp: 2.700700044631958, KL: 73.30337524414062, Loss: 0.016845375299453735, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17248/20000], Bound: 0.37269583344459534, Entropy: 140.10267639160156, Temp: 2.7006993293762207, KL: 71.46017456054688, Loss: 0.01414735522121191, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17249/20000], Bound: 0.38123273849487305, Entropy: 140.59681701660156, Temp: 2.7006988525390625, KL: 73.07368469238281, Loss: 0.01570817269384861, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17250/20000], Bound: 0.38235238194465637, Entropy: 142.1046905517578, Temp: 2.7006983757019043, KL: 73.18339538574219, Loss: 0.016105739399790764, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17251/20000], Bound: 0.3921954035758972, Entropy: 141.0393524169922, Temp: 2.700698137283325, KL: 74.837890625, Loss: 0.018365560099482536, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17252/20000], Bound: 0.39483028650283813, Entropy: 141.26536560058594, Temp: 2.700697660446167, KL: 77.11630249023438, Loss: 0.01558527909219265, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17253/20000], Bound: 0.3830353915691376, Entropy: 142.0885772705078, Temp: 2.700697660446167, KL: 72.92945861816406, Loss: 0.016942758113145828, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17254/20000], Bound: 0.3973952531814575, Entropy: 141.6703338623047, Temp: 2.700697422027588, KL: 77.06840515136719, Loss: 0.017079168930649757, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17255/20000], Bound: 0.4118015468120575, Entropy: 140.871337890625, Temp: 2.700697183609009, KL: 82.88725280761719, Loss: 0.014299280010163784, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17256/20000], Bound: 0.40927836298942566, Entropy: 141.0469970703125, Temp: 2.700697183609009, KL: 80.18858337402344, Loss: 0.017883090302348137, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17257/20000], Bound: 0.3535010516643524, Entropy: 142.9228973388672, Temp: 2.700697183609009, KL: 65.30326843261719, Loss: 0.015519487671554089, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17258/20000], Bound: 0.3969714045524597, Entropy: 142.60438537597656, Temp: 2.700697183609009, KL: 76.08786010742188, Loss: 0.01866193301975727, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17259/20000], Bound: 0.3713854253292084, Entropy: 141.28463745117188, Temp: 2.7006969451904297, KL: 70.36788940429688, Loss: 0.01547637116163969, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17260/20000], Bound: 0.3766394257545471, Entropy: 141.7749481201172, Temp: 2.7006967067718506, KL: 72.56430053710938, Loss: 0.014197220094501972, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17261/20000], Bound: 0.37210482358932495, Entropy: 143.2627716064453, Temp: 2.7006967067718506, KL: 69.75784301757812, Loss: 0.016986161470413208, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17262/20000], Bound: 0.3957747519016266, Entropy: 141.9813995361328, Temp: 2.7006967067718506, KL: 77.90464782714844, Loss: 0.014642547816038132, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17263/20000], Bound: 0.4133896231651306, Entropy: 141.48760986328125, Temp: 2.7006967067718506, KL: 82.81877136230469, Loss: 0.01531785074621439, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17264/20000], Bound: 0.38784387707710266, Entropy: 141.1250762939453, Temp: 2.7006969451904297, KL: 75.43344116210938, Loss: 0.014900271780788898, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17265/20000], Bound: 0.3722904622554779, Entropy: 143.3816375732422, Temp: 2.700697422027588, KL: 71.06941223144531, Loss: 0.014656203798949718, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17266/20000], Bound: 0.3846922516822815, Entropy: 139.0782012939453, Temp: 2.700697660446167, KL: 74.37226867675781, Loss: 0.015163110569119453, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17267/20000], Bound: 0.41626667976379395, Entropy: 139.58642578125, Temp: 2.700698137283325, KL: 82.67231750488281, Loss: 0.01721005327999592, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17268/20000], Bound: 0.3623628318309784, Entropy: 142.78158569335938, Temp: 2.7006986141204834, KL: 68.45574951171875, Loss: 0.014278371818363667, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17269/20000], Bound: 0.37994009256362915, Entropy: 140.48236083984375, Temp: 2.7006990909576416, KL: 72.6806640625, Loss: 0.015743587166070938, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17270/20000], Bound: 0.36700132489204407, Entropy: 142.3746795654297, Temp: 2.7006995677948, KL: 69.01168823242188, Loss: 0.015677370131015778, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17271/20000], Bound: 0.39526069164276123, Entropy: 142.72549438476562, Temp: 2.700700044631958, KL: 78.50701904296875, Loss: 0.013245987705886364, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17272/20000], Bound: 0.3813354969024658, Entropy: 141.5424041748047, Temp: 2.7007007598876953, KL: 71.95823669433594, Loss: 0.01782842166721821, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17273/20000], Bound: 0.36759161949157715, Entropy: 141.48248291015625, Temp: 2.7007012367248535, KL: 68.78337097167969, Loss: 0.016410278156399727, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17274/20000], Bound: 0.3917458653450012, Entropy: 140.39488220214844, Temp: 2.7007014751434326, KL: 75.12068176269531, Loss: 0.01759723201394081, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17275/20000], Bound: 0.41272515058517456, Entropy: 140.38262939453125, Temp: 2.7007017135620117, KL: 79.32502746582031, Loss: 0.02141270600259304, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17276/20000], Bound: 0.36560678482055664, Entropy: 142.70834350585938, Temp: 2.7007017135620117, KL: 69.48379516601562, Loss: 0.014071649871766567, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17277/20000], Bound: 0.4048250615596771, Entropy: 140.36679077148438, Temp: 2.7007017135620117, KL: 78.31275939941406, Loss: 0.01887614093720913, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17278/20000], Bound: 0.39373570680618286, Entropy: 140.95298767089844, Temp: 2.7007017135620117, KL: 75.35304260253906, Loss: 0.018251750618219376, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17279/20000], Bound: 0.3883858621120453, Entropy: 141.23468017578125, Temp: 2.7007014751434326, KL: 75.95610046386719, Loss: 0.014226130209863186, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17280/20000], Bound: 0.39005908370018005, Entropy: 142.24400329589844, Temp: 2.7007014751434326, KL: 74.86622619628906, Loss: 0.017151271924376488, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17281/20000], Bound: 0.36286333203315735, Entropy: 139.63580322265625, Temp: 2.7007014751434326, KL: 66.94305419921875, Loss: 0.01734023354947567, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17282/20000], Bound: 0.3884826600551605, Entropy: 140.78175354003906, Temp: 2.7007012367248535, KL: 74.98930358886719, Loss: 0.01606845296919346, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17283/20000], Bound: 0.3768851161003113, Entropy: 144.3734588623047, Temp: 2.7007012367248535, KL: 72.5869140625, Loss: 0.014286203309893608, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17284/20000], Bound: 0.3940877914428711, Entropy: 140.0281982421875, Temp: 2.7007012367248535, KL: 77.47941589355469, Loss: 0.014507298357784748, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17285/20000], Bound: 0.36066389083862305, Entropy: 142.3649444580078, Temp: 2.7007014751434326, KL: 67.55093383789062, Loss: 0.015068144537508488, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17286/20000], Bound: 0.3860633373260498, Entropy: 141.6405792236328, Temp: 2.7007017135620117, KL: 73.71488952636719, Loss: 0.01711958460509777, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17287/20000], Bound: 0.3499779999256134, Entropy: 142.03466796875, Temp: 2.700701951980591, KL: 63.507781982421875, Loss: 0.01703251153230667, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17288/20000], Bound: 0.38079118728637695, Entropy: 141.24642944335938, Temp: 2.700701951980591, KL: 71.76930236816406, Loss: 0.017886517569422722, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17289/20000], Bound: 0.3892796039581299, Entropy: 141.1058349609375, Temp: 2.7007017135620117, KL: 74.12110900878906, Loss: 0.01810779981315136, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17290/20000], Bound: 0.4051636755466461, Entropy: 139.79299926757812, Temp: 2.7007014751434326, KL: 79.44390869140625, Loss: 0.016969935968518257, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17291/20000], Bound: 0.372027188539505, Entropy: 143.1766357421875, Temp: 2.7007012367248535, KL: 70.35517883300781, Loss: 0.01583925262093544, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17292/20000], Bound: 0.3987557590007782, Entropy: 140.6857452392578, Temp: 2.7007012367248535, KL: 78.44285583496094, Loss: 0.01528210286051035, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17293/20000], Bound: 0.39893046021461487, Entropy: 142.1376953125, Temp: 2.7007012367248535, KL: 77.190185546875, Loss: 0.017697351053357124, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17294/20000], Bound: 0.3895817995071411, Entropy: 141.27523803710938, Temp: 2.7007012367248535, KL: 74.93089294433594, Loss: 0.01677251234650612, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17295/20000], Bound: 0.375270277261734, Entropy: 142.77593994140625, Temp: 2.7007012367248535, KL: 70.18722534179688, Loss: 0.01786973513662815, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17296/20000], Bound: 0.3973735570907593, Entropy: 142.7158966064453, Temp: 2.7007009983062744, KL: 76.29240417480469, Loss: 0.018503960222005844, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17297/20000], Bound: 0.3809061646461487, Entropy: 139.39419555664062, Temp: 2.7007007598876953, KL: 73.74708557128906, Loss: 0.014286472462117672, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17298/20000], Bound: 0.38029053807258606, Entropy: 142.1129608154297, Temp: 2.7007007598876953, KL: 71.52154541015625, Loss: 0.01807708665728569, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17299/20000], Bound: 0.3723197281360626, Entropy: 141.12513732910156, Temp: 2.700700521469116, KL: 67.99493408203125, Loss: 0.020363699644804, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17300/20000], Bound: 0.3827727735042572, Entropy: 140.46092224121094, Temp: 2.700700044631958, KL: 73.61860656738281, Loss: 0.015525798313319683, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17301/20000], Bound: 0.3839581310749054, Entropy: 142.0799102783203, Temp: 2.7006995677948, KL: 74.42985534667969, Loss: 0.014661216177046299, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17302/20000], Bound: 0.37917181849479675, Entropy: 141.12835693359375, Temp: 2.7006993293762207, KL: 71.89852905273438, Loss: 0.016780778765678406, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17303/20000], Bound: 0.3640929162502289, Entropy: 142.2315673828125, Temp: 2.7006990909576416, KL: 67.9141845703125, Loss: 0.016184883192181587, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17304/20000], Bound: 0.38905882835388184, Entropy: 139.49168395996094, Temp: 2.7006988525390625, KL: 75.9547119140625, Loss: 0.014593360014259815, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17305/20000], Bound: 0.37642011046409607, Entropy: 142.3177032470703, Temp: 2.7006986141204834, KL: 71.18748474121094, Loss: 0.01662944257259369, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17306/20000], Bound: 0.4149090647697449, Entropy: 139.72518920898438, Temp: 2.7006983757019043, KL: 82.91268920898438, Loss: 0.015999235212802887, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17307/20000], Bound: 0.3640134632587433, Entropy: 141.83558654785156, Temp: 2.7006983757019043, KL: 68.82975769042969, Loss: 0.014448258094489574, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17308/20000], Bound: 0.37530702352523804, Entropy: 142.35958862304688, Temp: 2.7006983757019043, KL: 70.42120361328125, Loss: 0.0174560584127903, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17309/20000], Bound: 0.3673144280910492, Entropy: 141.337646484375, Temp: 2.7006983757019043, KL: 69.38333129882812, Loss: 0.015153825283050537, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17310/20000], Bound: 0.36603549122810364, Entropy: 142.06459045410156, Temp: 2.7006983757019043, KL: 68.81285095214844, Loss: 0.0155385946854949, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17311/20000], Bound: 0.38933247327804565, Entropy: 141.1011505126953, Temp: 2.7006983757019043, KL: 75.81927490234375, Loss: 0.01499252300709486, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17312/20000], Bound: 0.3701154291629791, Entropy: 142.87684631347656, Temp: 2.7006983757019043, KL: 68.47978210449219, Loss: 0.018301352858543396, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17313/20000], Bound: 0.3751840889453888, Entropy: 141.49671936035156, Temp: 2.700698137283325, KL: 69.54910278320312, Loss: 0.019005315378308296, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17314/20000], Bound: 0.3968232572078705, Entropy: 141.62147521972656, Temp: 2.700697898864746, KL: 77.37985229492188, Loss: 0.0161887314170599, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17315/20000], Bound: 0.35955846309661865, Entropy: 138.6355438232422, Temp: 2.700697660446167, KL: 67.5899658203125, Loss: 0.014420894905924797, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17316/20000], Bound: 0.3964119553565979, Entropy: 140.683837890625, Temp: 2.700697660446167, KL: 76.365966796875, Loss: 0.017840320244431496, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17317/20000], Bound: 0.3934433162212372, Entropy: 140.01551818847656, Temp: 2.700697422027588, KL: 77.42529296875, Loss: 0.014255618676543236, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17318/20000], Bound: 0.4044201076030731, Entropy: 140.2474822998047, Temp: 2.700697422027588, KL: 80.26567077636719, Loss: 0.01503582950681448, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17319/20000], Bound: 0.4236552119255066, Entropy: 139.7198028564453, Temp: 2.700697660446167, KL: 86.50398254394531, Loss: 0.014312033541500568, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17320/20000], Bound: 0.35543617606163025, Entropy: 140.65237426757812, Temp: 2.700697898864746, KL: 66.16563415527344, Loss: 0.014921551570296288, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17321/20000], Bound: 0.39513543248176575, Entropy: 141.63999938964844, Temp: 2.7006983757019043, KL: 76.19120788574219, Loss: 0.017464887350797653, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17322/20000], Bound: 0.39283135533332825, Entropy: 141.54986572265625, Temp: 2.7006986141204834, KL: 76.35107421875, Loss: 0.015910645946860313, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17323/20000], Bound: 0.37012824416160583, Entropy: 141.09124755859375, Temp: 2.7006990909576416, KL: 69.9793701171875, Loss: 0.015531839802861214, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17324/20000], Bound: 0.41319677233695984, Entropy: 143.2292022705078, Temp: 2.7006995677948, KL: 82.48114013671875, Loss: 0.015834543853998184, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17325/20000], Bound: 0.3794669210910797, Entropy: 139.97665405273438, Temp: 2.700700044631958, KL: 70.23056030273438, Loss: 0.020026572048664093, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17326/20000], Bound: 0.38103628158569336, Entropy: 142.7743682861328, Temp: 2.700700283050537, KL: 72.27513122558594, Loss: 0.017081337049603462, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17327/20000], Bound: 0.3637906610965729, Entropy: 142.59939575195312, Temp: 2.700700283050537, KL: 66.65640258789062, Loss: 0.018355483189225197, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17328/20000], Bound: 0.3781603276729584, Entropy: 140.97190856933594, Temp: 2.700700044631958, KL: 72.45230102539062, Loss: 0.01521533727645874, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17329/20000], Bound: 0.3725840747356415, Entropy: 142.14505004882812, Temp: 2.700700044631958, KL: 70.57783508300781, Loss: 0.015721706673502922, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17330/20000], Bound: 0.3630374073982239, Entropy: 143.1156768798828, Temp: 2.700700044631958, KL: 69.50750732421875, Loss: 0.0126833850517869, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17331/20000], Bound: 0.3794120252132416, Entropy: 142.8535919189453, Temp: 2.700700283050537, KL: 72.13859558105469, Loss: 0.016464734449982643, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17332/20000], Bound: 0.37818825244903564, Entropy: 141.13401794433594, Temp: 2.700700283050537, KL: 71.98860168457031, Loss: 0.01608869433403015, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17333/20000], Bound: 0.39196500182151794, Entropy: 141.92584228515625, Temp: 2.700700283050537, KL: 76.18399047851562, Loss: 0.015747951343655586, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17334/20000], Bound: 0.38790422677993774, Entropy: 139.17393493652344, Temp: 2.700700521469116, KL: 74.50653076171875, Loss: 0.01664905808866024, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17335/20000], Bound: 0.4168357849121094, Entropy: 142.4038848876953, Temp: 2.7007007598876953, KL: 84.14572143554688, Loss: 0.014803755097091198, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17336/20000], Bound: 0.39272642135620117, Entropy: 142.13641357421875, Temp: 2.7007012367248535, KL: 76.73921203613281, Loss: 0.015134863555431366, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17337/20000], Bound: 0.3917718231678009, Entropy: 140.61453247070312, Temp: 2.7007014751434326, KL: 74.91995239257812, Loss: 0.01798296347260475, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17338/20000], Bound: 0.39955684542655945, Entropy: 139.64556884765625, Temp: 2.7007017135620117, KL: 79.16728210449219, Loss: 0.014381777495145798, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17339/20000], Bound: 0.3841155469417572, Entropy: 139.8614501953125, Temp: 2.70070219039917, KL: 74.69891357421875, Loss: 0.014247851446270943, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17340/20000], Bound: 0.4060043692588806, Entropy: 140.71319580078125, Temp: 2.7007029056549072, KL: 80.47920227050781, Loss: 0.015520364977419376, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17341/20000], Bound: 0.3566143810749054, Entropy: 141.5462646484375, Temp: 2.7007036209106445, KL: 65.84503173828125, Loss: 0.016124460846185684, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17342/20000], Bound: 0.3622806668281555, Entropy: 143.17124938964844, Temp: 2.7007040977478027, KL: 67.07780456542969, Loss: 0.016786642372608185, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17343/20000], Bound: 0.38556718826293945, Entropy: 143.17518615722656, Temp: 2.700704574584961, KL: 73.23004150390625, Loss: 0.01774953491985798, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17344/20000], Bound: 0.39032596349716187, Entropy: 140.99978637695312, Temp: 2.70070481300354, KL: 75.09849548339844, Loss: 0.016866253688931465, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17345/20000], Bound: 0.3963782787322998, Entropy: 140.5233917236328, Temp: 2.700705051422119, KL: 77.38078308105469, Loss: 0.015943121165037155, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17346/20000], Bound: 0.41975852847099304, Entropy: 140.69456481933594, Temp: 2.700705051422119, KL: 85.48167419433594, Loss: 0.01398596540093422, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17347/20000], Bound: 0.411044180393219, Entropy: 143.24143981933594, Temp: 2.7007055282592773, KL: 82.81941223144531, Loss: 0.014000455848872662, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17348/20000], Bound: 0.38108503818511963, Entropy: 142.09304809570312, Temp: 2.7007062435150146, KL: 72.46516418457031, Loss: 0.016755713149905205, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17349/20000], Bound: 0.3912755250930786, Entropy: 141.387939453125, Temp: 2.700706958770752, KL: 76.38397216796875, Loss: 0.015002512373030186, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17350/20000], Bound: 0.40033456683158875, Entropy: 141.32884216308594, Temp: 2.70070743560791, KL: 78.67955017089844, Loss: 0.01571330614387989, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17351/20000], Bound: 0.3864712119102478, Entropy: 143.59825134277344, Temp: 2.7007081508636475, KL: 75.93449401855469, Loss: 0.013230618089437485, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17352/20000], Bound: 0.38564860820770264, Entropy: 142.73280334472656, Temp: 2.700709104537964, KL: 74.23289489746094, Loss: 0.0159368384629488, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17353/20000], Bound: 0.3702031672000885, Entropy: 142.3427276611328, Temp: 2.7007100582122803, KL: 67.85098266601562, Loss: 0.0195118747651577, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17354/20000], Bound: 0.39608463644981384, Entropy: 141.74630737304688, Temp: 2.7007105350494385, KL: 78.50566101074219, Loss: 0.013699723407626152, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17355/20000], Bound: 0.3717040717601776, Entropy: 141.59115600585938, Temp: 2.7007110118865967, KL: 71.44737243652344, Loss: 0.013646375387907028, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17356/20000], Bound: 0.39079225063323975, Entropy: 141.8223114013672, Temp: 2.700711727142334, KL: 75.73468017578125, Loss: 0.01594187691807747, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17357/20000], Bound: 0.3980976939201355, Entropy: 141.15020751953125, Temp: 2.7007124423980713, KL: 77.67326354980469, Loss: 0.016345230862498283, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17358/20000], Bound: 0.35811230540275574, Entropy: 141.84640502929688, Temp: 2.7007131576538086, KL: 68.36790466308594, Loss: 0.012229904532432556, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17359/20000], Bound: 0.41150128841400146, Entropy: 142.91737365722656, Temp: 2.700714111328125, KL: 81.37004089355469, Loss: 0.016940051689743996, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17360/20000], Bound: 0.38692188262939453, Entropy: 141.58460998535156, Temp: 2.7007148265838623, KL: 72.77999877929688, Loss: 0.019314289093017578, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17361/20000], Bound: 0.3834019899368286, Entropy: 140.1681365966797, Temp: 2.7007153034210205, KL: 74.48190307617188, Loss: 0.0142658157274127, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17362/20000], Bound: 0.3874431550502777, Entropy: 141.1876983642578, Temp: 2.700716018676758, KL: 74.54534912109375, Loss: 0.016327865421772003, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17363/20000], Bound: 0.3886898458003998, Entropy: 141.55828857421875, Temp: 2.700716733932495, KL: 76.48600769042969, Loss: 0.01340994331985712, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17364/20000], Bound: 0.387264221906662, Entropy: 141.95913696289062, Temp: 2.7007174491882324, KL: 75.09815979003906, Loss: 0.015207640826702118, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17365/20000], Bound: 0.40802207589149475, Entropy: 140.26670837402344, Temp: 2.7007181644439697, KL: 80.234619140625, Loss: 0.017096830531954765, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17366/20000], Bound: 0.40119868516921997, Entropy: 140.8847198486328, Temp: 2.700718641281128, KL: 76.45295715332031, Loss: 0.020312318578362465, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17367/20000], Bound: 0.39678502082824707, Entropy: 140.0124053955078, Temp: 2.700719118118286, KL: 79.04670715332031, Loss: 0.013082007877528667, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17368/20000], Bound: 0.39301520586013794, Entropy: 141.50540161132812, Temp: 2.7007198333740234, KL: 78.19587707519531, Loss: 0.012595673091709614, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17369/20000], Bound: 0.42091038823127747, Entropy: 138.9056854248047, Temp: 2.70072078704834, KL: 85.47761535644531, Loss: 0.014648178592324257, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17370/20000], Bound: 0.37690290808677673, Entropy: 142.36080932617188, Temp: 2.7007217407226562, KL: 70.07817077636719, Loss: 0.018940456211566925, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17371/20000], Bound: 0.3751963675022125, Entropy: 140.23306274414062, Temp: 2.7007224559783936, KL: 70.41529846191406, Loss: 0.01740836538374424, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17372/20000], Bound: 0.3452965319156647, Entropy: 143.62994384765625, Temp: 2.700723171234131, KL: 61.739501953125, Loss: 0.01791328564286232, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17373/20000], Bound: 0.38759374618530273, Entropy: 139.08111572265625, Temp: 2.70072340965271, KL: 73.67462158203125, Loss: 0.01802140288054943, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17374/20000], Bound: 0.34778663516044617, Entropy: 142.7038116455078, Temp: 2.700723648071289, KL: 63.13179016113281, Loss: 0.016606636345386505, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17375/20000], Bound: 0.3859215974807739, Entropy: 139.777587890625, Temp: 2.700723648071289, KL: 74.04478454589844, Loss: 0.016432512551546097, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17376/20000], Bound: 0.4108729958534241, Entropy: 140.0107421875, Temp: 2.700723648071289, KL: 81.30534362792969, Loss: 0.016707850620150566, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17377/20000], Bound: 0.37110260128974915, Entropy: 140.7002716064453, Temp: 2.700723648071289, KL: 70.46110534667969, Loss: 0.015154566615819931, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17378/20000], Bound: 0.35945063829421997, Entropy: 142.244384765625, Temp: 2.700723648071289, KL: 66.96525573730469, Loss: 0.015521638095378876, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17379/20000], Bound: 0.37288784980773926, Entropy: 143.4131317138672, Temp: 2.700723648071289, KL: 70.54623413085938, Loss: 0.015941260382533073, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17380/20000], Bound: 0.3894783556461334, Entropy: 141.71514892578125, Temp: 2.700723648071289, KL: 74.93070983886719, Loss: 0.01671692356467247, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17381/20000], Bound: 0.39211928844451904, Entropy: 142.19024658203125, Temp: 2.700723648071289, KL: 77.11299133300781, Loss: 0.014112300239503384, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17382/20000], Bound: 0.3934817314147949, Entropy: 142.42636108398438, Temp: 2.700723886489868, KL: 76.83943176269531, Loss: 0.015361501835286617, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17383/20000], Bound: 0.3839161694049835, Entropy: 141.71690368652344, Temp: 2.7007241249084473, KL: 75.18215942382812, Loss: 0.013246073387563229, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17384/20000], Bound: 0.4064687192440033, Entropy: 143.1943359375, Temp: 2.7007243633270264, KL: 80.29095458984375, Loss: 0.01612737588584423, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17385/20000], Bound: 0.3955943286418915, Entropy: 140.22442626953125, Temp: 2.7007248401641846, KL: 78.37319946289062, Loss: 0.013676630333065987, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17386/20000], Bound: 0.36062946915626526, Entropy: 142.55841064453125, Temp: 2.700725555419922, KL: 66.76002502441406, Loss: 0.01651465892791748, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17387/20000], Bound: 0.3993989825248718, Entropy: 138.1423797607422, Temp: 2.70072603225708, KL: 76.95936584472656, Loss: 0.018382739275693893, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17388/20000], Bound: 0.376473605632782, Entropy: 142.52056884765625, Temp: 2.7007265090942383, KL: 72.44924926757812, Loss: 0.014322146773338318, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17389/20000], Bound: 0.3703666925430298, Entropy: 140.99278259277344, Temp: 2.7007269859313965, KL: 69.80838012695312, Loss: 0.01597443036735058, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17390/20000], Bound: 0.37960323691368103, Entropy: 142.88900756835938, Temp: 2.7007274627685547, KL: 71.66409301757812, Loss: 0.01744568720459938, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17391/20000], Bound: 0.37316784262657166, Entropy: 142.74998474121094, Temp: 2.700727701187134, KL: 68.68228149414062, Loss: 0.01954049989581108, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17392/20000], Bound: 0.3710775077342987, Entropy: 142.1591033935547, Temp: 2.700727701187134, KL: 69.65060424804688, Loss: 0.016641857102513313, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17393/20000], Bound: 0.3975151479244232, Entropy: 140.88385009765625, Temp: 2.700727701187134, KL: 76.2882080078125, Loss: 0.018589695915579796, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17394/20000], Bound: 0.3934710919857025, Entropy: 140.47048950195312, Temp: 2.7007274627685547, KL: 76.70724487304688, Loss: 0.015600442886352539, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17395/20000], Bound: 0.36385801434516907, Entropy: 143.80972290039062, Temp: 2.7007274627685547, KL: 65.88282775878906, Loss: 0.019823014736175537, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17396/20000], Bound: 0.40809911489486694, Entropy: 142.74082946777344, Temp: 2.7007269859313965, KL: 82.05181884765625, Loss: 0.013775622472167015, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17397/20000], Bound: 0.3998558819293976, Entropy: 140.23995971679688, Temp: 2.7007269859313965, KL: 79.42292785644531, Loss: 0.014073461294174194, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17398/20000], Bound: 0.38481080532073975, Entropy: 138.79042053222656, Temp: 2.7007272243499756, KL: 74.61712646484375, Loss: 0.014773978851735592, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17399/20000], Bound: 0.4016354978084564, Entropy: 140.2842559814453, Temp: 2.7007274627685547, KL: 79.63491821289062, Loss: 0.014662632718682289, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17400/20000], Bound: 0.3832652270793915, Entropy: 142.37252807617188, Temp: 2.700727939605713, KL: 74.01336669921875, Loss: 0.015059847384691238, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17401/20000], Bound: 0.39224541187286377, Entropy: 140.55209350585938, Temp: 2.700728416442871, KL: 75.79005432128906, Loss: 0.01663023605942726, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17402/20000], Bound: 0.3815079927444458, Entropy: 138.54638671875, Temp: 2.70072865486145, KL: 72.76217651367188, Loss: 0.01643276959657669, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17403/20000], Bound: 0.4005051851272583, Entropy: 140.86651611328125, Temp: 2.7007291316986084, KL: 78.17250061035156, Loss: 0.016746317967772484, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17404/20000], Bound: 0.3670161962509155, Entropy: 142.78810119628906, Temp: 2.7007296085357666, KL: 68.84457397460938, Loss: 0.015994813293218613, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17405/20000], Bound: 0.3802356719970703, Entropy: 143.48594665527344, Temp: 2.7007298469543457, KL: 71.97354125976562, Loss: 0.017211152240633965, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17406/20000], Bound: 0.39815568923950195, Entropy: 140.59495544433594, Temp: 2.700730085372925, KL: 77.07144165039062, Loss: 0.01749148592352867, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17407/20000], Bound: 0.4110397398471832, Entropy: 142.6865234375, Temp: 2.700730323791504, KL: 82.53579711914062, Loss: 0.014523348771035671, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17408/20000], Bound: 0.3763520121574402, Entropy: 141.1999969482422, Temp: 2.700730562210083, KL: 72.35786437988281, Loss: 0.01442667655646801, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17409/20000], Bound: 0.38615700602531433, Entropy: 142.79031372070312, Temp: 2.700731039047241, KL: 75.56103515625, Loss: 0.013752548024058342, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17410/20000], Bound: 0.3977280259132385, Entropy: 141.00900268554688, Temp: 2.7007317543029785, KL: 77.35517883300781, Loss: 0.016731269657611847, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17411/20000], Bound: 0.39642855525016785, Entropy: 140.72557067871094, Temp: 2.7007322311401367, KL: 75.83602905273438, Loss: 0.01883084885776043, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17412/20000], Bound: 0.3982936143875122, Entropy: 141.23565673828125, Temp: 2.700732707977295, KL: 76.54203796386719, Loss: 0.018547391518950462, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17413/20000], Bound: 0.38703790307044983, Entropy: 140.9792938232422, Temp: 2.700732946395874, KL: 71.68099975585938, Loss: 0.021411795169115067, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17414/20000], Bound: 0.37112581729888916, Entropy: 143.05747985839844, Temp: 2.700732707977295, KL: 69.56285095214844, Loss: 0.01682986132800579, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17415/20000], Bound: 0.37466609477996826, Entropy: 142.41648864746094, Temp: 2.700732469558716, KL: 71.412841796875, Loss: 0.015279962681233883, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17416/20000], Bound: 0.40397706627845764, Entropy: 138.60191345214844, Temp: 2.7007322311401367, KL: 80.32334899902344, Loss: 0.014683793298900127, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17417/20000], Bound: 0.35999399423599243, Entropy: 140.62222290039062, Temp: 2.7007322311401367, KL: 66.44302368164062, Loss: 0.016770994290709496, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17418/20000], Bound: 0.3824911117553711, Entropy: 141.44296264648438, Temp: 2.7007322311401367, KL: 72.88127136230469, Loss: 0.01673985831439495, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17419/20000], Bound: 0.3959037661552429, Entropy: 141.75009155273438, Temp: 2.7007322311401367, KL: 77.35783386230469, Loss: 0.01572592183947563, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17420/20000], Bound: 0.3899596333503723, Entropy: 142.62828063964844, Temp: 2.7007322311401367, KL: 75.24806213378906, Loss: 0.01639067381620407, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17421/20000], Bound: 0.40134647488594055, Entropy: 143.2933349609375, Temp: 2.7007322311401367, KL: 76.97517395019531, Loss: 0.019427215680480003, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17422/20000], Bound: 0.3855390250682831, Entropy: 141.86895751953125, Temp: 2.7007319927215576, KL: 74.64170837402344, Loss: 0.015121090225875378, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17423/20000], Bound: 0.39423149824142456, Entropy: 140.82606506347656, Temp: 2.7007319927215576, KL: 75.30299377441406, Loss: 0.01861543394625187, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17424/20000], Bound: 0.4017931818962097, Entropy: 140.77923583984375, Temp: 2.7007317543029785, KL: 77.81259155273438, Loss: 0.01812358945608139, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17425/20000], Bound: 0.3728539049625397, Entropy: 141.60955810546875, Temp: 2.7007317543029785, KL: 69.61637878417969, Loss: 0.017644841223955154, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17426/20000], Bound: 0.3810635209083557, Entropy: 142.07577514648438, Temp: 2.7007312774658203, KL: 72.81101989746094, Loss: 0.016104092821478844, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17427/20000], Bound: 0.37808242440223694, Entropy: 141.75852966308594, Temp: 2.700731039047241, KL: 71.55487060546875, Loss: 0.016835501417517662, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17428/20000], Bound: 0.3899837136268616, Entropy: 141.92945861816406, Temp: 2.700731039047241, KL: 74.76400756835938, Loss: 0.017299843952059746, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17429/20000], Bound: 0.3748141825199127, Entropy: 142.6768341064453, Temp: 2.700730562210083, KL: 69.94706726074219, Loss: 0.018072225153446198, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17430/20000], Bound: 0.38200485706329346, Entropy: 141.23757934570312, Temp: 2.700730323791504, KL: 73.66023254394531, Loss: 0.01503670308738947, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17431/20000], Bound: 0.4061988294124603, Entropy: 140.15260314941406, Temp: 2.700730085372925, KL: 79.11442565917969, Loss: 0.018155455589294434, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17432/20000], Bound: 0.38825133442878723, Entropy: 142.40478515625, Temp: 2.7007298469543457, KL: 74.98374938964844, Loss: 0.01595371402800083, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17433/20000], Bound: 0.4026571810245514, Entropy: 140.8867950439453, Temp: 2.7007296085357666, KL: 79.24151611328125, Loss: 0.01595574989914894, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17434/20000], Bound: 0.41000115871429443, Entropy: 139.64344787597656, Temp: 2.7007296085357666, KL: 81.75859069824219, Loss: 0.01538082305341959, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17435/20000], Bound: 0.3695806860923767, Entropy: 143.55003356933594, Temp: 2.7007296085357666, KL: 68.78273010253906, Loss: 0.0174587182700634, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17436/20000], Bound: 0.41488775610923767, Entropy: 139.28173828125, Temp: 2.7007296085357666, KL: 82.88067626953125, Loss: 0.016046838834881783, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17437/20000], Bound: 0.3850950598716736, Entropy: 142.55593872070312, Temp: 2.7007296085357666, KL: 73.432373046875, Loss: 0.01712058298289776, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17438/20000], Bound: 0.37793153524398804, Entropy: 143.00131225585938, Temp: 2.7007296085357666, KL: 72.13313293457031, Loss: 0.015684444457292557, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17439/20000], Bound: 0.3912366032600403, Entropy: 139.32200622558594, Temp: 2.7007296085357666, KL: 75.05989074707031, Loss: 0.017432915046811104, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17440/20000], Bound: 0.4002489447593689, Entropy: 142.76515197753906, Temp: 2.7007296085357666, KL: 79.5374755859375, Loss: 0.014077989384531975, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17441/20000], Bound: 0.40785151720046997, Entropy: 141.0602264404297, Temp: 2.7007298469543457, KL: 79.29541015625, Loss: 0.01874062791466713, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17442/20000], Bound: 0.3709565997123718, Entropy: 140.7013397216797, Temp: 2.700730085372925, KL: 70.18980407714844, Loss: 0.015579752624034882, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17443/20000], Bound: 0.3765738606452942, Entropy: 140.02061462402344, Temp: 2.700730323791504, KL: 71.714599609375, Loss: 0.01573568396270275, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17444/20000], Bound: 0.3911709487438202, Entropy: 139.68995666503906, Temp: 2.700730562210083, KL: 76.3087158203125, Loss: 0.015085205435752869, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17445/20000], Bound: 0.38176026940345764, Entropy: 142.5878448486328, Temp: 2.700730562210083, KL: 73.43504333496094, Loss: 0.015322363004088402, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17446/20000], Bound: 0.38153010606765747, Entropy: 142.71189880371094, Temp: 2.700731039047241, KL: 73.34712219238281, Loss: 0.015361684374511242, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17447/20000], Bound: 0.3806016445159912, Entropy: 142.425537109375, Temp: 2.700731039047241, KL: 73.24781799316406, Loss: 0.01504795253276825, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17448/20000], Bound: 0.39133238792419434, Entropy: 142.59751892089844, Temp: 2.7007317543029785, KL: 76.76478576660156, Loss: 0.014328684657812119, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17449/20000], Bound: 0.3771805167198181, Entropy: 140.73074340820312, Temp: 2.7007319927215576, KL: 71.73358154296875, Loss: 0.016023695468902588, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17450/20000], Bound: 0.37357690930366516, Entropy: 140.84765625, Temp: 2.700732469558716, KL: 72.34162902832031, Loss: 0.012982570566236973, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17451/20000], Bound: 0.40709832310676575, Entropy: 143.09654235839844, Temp: 2.700733184814453, KL: 78.90768432617188, Loss: 0.019038816913962364, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17452/20000], Bound: 0.35487908124923706, Entropy: 142.80593872070312, Temp: 2.7007336616516113, KL: 65.9862060546875, Loss: 0.014966282062232494, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17453/20000], Bound: 0.3950428068637848, Entropy: 140.20310974121094, Temp: 2.7007341384887695, KL: 76.76321411132812, Loss: 0.01635556109249592, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17454/20000], Bound: 0.3906495273113251, Entropy: 140.40086364746094, Temp: 2.7007343769073486, KL: 75.24990844726562, Loss: 0.01676200143992901, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17455/20000], Bound: 0.4126307964324951, Entropy: 139.39305114746094, Temp: 2.700734853744507, KL: 81.36944580078125, Loss: 0.0175750944763422, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17456/20000], Bound: 0.41360387206077576, Entropy: 141.5069580078125, Temp: 2.700735330581665, KL: 82.87913513183594, Loss: 0.015326982364058495, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17457/20000], Bound: 0.3785787522792816, Entropy: 140.67384338378906, Temp: 2.7007358074188232, KL: 72.31388854980469, Loss: 0.015695257112383842, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17458/20000], Bound: 0.4149269461631775, Entropy: 139.49951171875, Temp: 2.7007362842559814, KL: 83.92491149902344, Loss: 0.014135697856545448, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17459/20000], Bound: 0.38999611139297485, Entropy: 140.46107482910156, Temp: 2.7007369995117188, KL: 73.02769470214844, Loss: 0.020521176978945732, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17460/20000], Bound: 0.3677457869052887, Entropy: 142.43162536621094, Temp: 2.700737476348877, KL: 69.47325134277344, Loss: 0.01521438080817461, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17461/20000], Bound: 0.38593029975891113, Entropy: 140.29241943359375, Temp: 2.700737953186035, KL: 74.22972106933594, Loss: 0.01609494723379612, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17462/20000], Bound: 0.35135284066200256, Entropy: 142.43080139160156, Temp: 2.700737953186035, KL: 64.68968200683594, Loss: 0.015550350770354271, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17463/20000], Bound: 0.42747634649276733, Entropy: 139.11790466308594, Temp: 2.7007381916046143, KL: 87.05601501464844, Loss: 0.015479309484362602, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17464/20000], Bound: 0.38959813117980957, Entropy: 140.84213256835938, Temp: 2.7007386684417725, KL: 73.25555419921875, Loss: 0.01988333649933338, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17465/20000], Bound: 0.4002705216407776, Entropy: 141.10353088378906, Temp: 2.7007389068603516, KL: 77.64308166503906, Loss: 0.017597176134586334, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17466/20000], Bound: 0.3864534795284271, Entropy: 140.7647247314453, Temp: 2.7007391452789307, KL: 73.42198181152344, Loss: 0.017872869968414307, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17467/20000], Bound: 0.4032057523727417, Entropy: 140.3662567138672, Temp: 2.7007391452789307, KL: 79.07728576660156, Loss: 0.01656348444521427, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17468/20000], Bound: 0.39194825291633606, Entropy: 140.33181762695312, Temp: 2.7007391452789307, KL: 77.10906982421875, Loss: 0.014026550576090813, Learning Rate: 1.3799596096342177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17469/20000], Bound: 0.38603222370147705, Entropy: 141.63650512695312, Temp: 2.7007393836975098, KL: 74.87092590332031, Loss: 0.014962887391448021, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17470/20000], Bound: 0.3784223198890686, Entropy: 141.01686096191406, Temp: 2.700739860534668, KL: 72.20811462402344, Loss: 0.01580762304365635, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17471/20000], Bound: 0.405130535364151, Entropy: 143.23858642578125, Temp: 2.700740098953247, KL: 79.32449340820312, Loss: 0.017173008993268013, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17472/20000], Bound: 0.4055922329425812, Entropy: 140.28167724609375, Temp: 2.700740337371826, KL: 78.83120727539062, Loss: 0.01834266632795334, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17473/20000], Bound: 0.3835301101207733, Entropy: 141.19512939453125, Temp: 2.7007405757904053, KL: 73.88119506835938, Loss: 0.015447108075022697, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17474/20000], Bound: 0.39746469259262085, Entropy: 143.59408569335938, Temp: 2.7007408142089844, KL: 77.06886291503906, Loss: 0.0171168502420187, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17475/20000], Bound: 0.39543166756629944, Entropy: 141.66827392578125, Temp: 2.7007408142089844, KL: 76.69313049316406, Loss: 0.01669813133776188, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17476/20000], Bound: 0.35939908027648926, Entropy: 142.97055053710938, Temp: 2.7007412910461426, KL: 67.15957641601562, Loss: 0.015135222114622593, Learning Rate: 1.3799596096342177e-06\n",
      "Epoch [17477/20000], Bound: 0.40628573298454285, Entropy: 141.90721130371094, Temp: 2.7007412910461426, KL: 79.67721557617188, Loss: 0.01716199330985546, Learning Rate: 1.3799596096342177e-06\n",
      "10964 [tensor(0.3254, device='cuda:0'), tensor(0.0176, device='cuda:0'), tensor(56.4850, device='cuda:0'), tensor(2.6913, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "\n",
    "C=1.\n",
    "R=.5\n",
    "\n",
    "#sample, = ax.scatter([],[],color='red',alpha=0.07)\n",
    "#fig.canvas.draw()\n",
    "    \n",
    "    \n",
    "'''\n",
    "def show(GeN,n,alpha=0.07):\n",
    "    Z=GeN(n).detach().clone().cpu()\n",
    "    plt.pcolormesh(grid_x.numpy(),grid_y.numpy(),p.exp().numpy())\n",
    "    plt.scatter(Z[:,0],Z[:,1],color='red',alpha=alpha) \n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "''' \n",
    "\n",
    "def show(GeN,n):\n",
    "    #Z=GeN(200).detach()\n",
    "    #fig=setup.makePlot(Z,device)\n",
    "    #plt.show()\n",
    "    return\n",
    "    \n",
    "#lr =.03 for lat_dim 5\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    optimizer = GeNPAC(loss,logprior,n_data_samples,C,R,\n",
    "\t\t                                    0, 100, 1000, 50, 50,\n",
    "\t\t                                    20000, .03, .000001, 500, .7,\n",
    "\t\t                                    device, True, temp_dir, save_best=True)\n",
    "    best_epoch, scores=optimizer.run(GeN,show)\n",
    "print(best_epoch,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.3254, device='cuda:0'),\n",
       " tensor(0.0176, device='cuda:0'),\n",
       " tensor(56.4850, device='cuda:0'),\n",
       " tensor(2.6913, device='cuda:0')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6096, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(1/(R*2*scores[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd1565e1b10>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "y=optimizer.score_temp\n",
    "time=torch.arange(0,len(y),1.)\n",
    "plt.title('Catoni temperature C')\n",
    "plt.plot(time, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd156073610>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "y=optimizer.score_entropy\n",
    "time=torch.arange(0,len(y),1.)\n",
    "plt.title('KL divergence')\n",
    "plt.plot(time, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd1563273d0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "y=torch.as_tensor(optimizer.score_elbo)\n",
    "time=torch.arange(0,len(y),1.)\n",
    "plt.title('average tanh(R*sqloss)')\n",
    "plt.plot(time, y, label='tanhloss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0ecb19590>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "y=torch.as_tensor(optimizer.score_elbo)\n",
    "time=torch.arange(0,len(y),1.)\n",
    "plt.title('average sqloss')\n",
    "yy=(1/R)*0.5*(torch.log1p(y)-torch.log1p(-y))\n",
    "plt.plot(time,yy, label='squared loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd15616a810>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "y=optimizer.score_lr\n",
    "time=torch.arange(0,len(y),1.)\n",
    "plt.plot(time, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0.3254, device='cuda:0'),\n",
       " tensor(0.0176, device='cuda:0'),\n",
       " tensor(56.4850, device='cuda:0'),\n",
       " tensor(2.6913, device='cuda:0')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def show(GeN,n):\n",
    "    Z=GeN(n).detach()\n",
    "    fig=setup.makePlot(Z,device)\n",
    "    plt.title('C='+str(scores[-1].cpu().float().numpy())+', lat_dim='+str(lat_dim))\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "\n",
    "show(GeN,1000)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nLPP_train, nLPP_validation, nLPP_test, RSE_train, RSE_validation, RSE_test=setup.evaluate_metrics(GeN(1000).detach(),'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nLPP_valid: (tensor(-0.6451), tensor(0.4865))\n",
      "SE_valid: (tensor(0.0146), tensor(0.0207))\n",
      "nLPP_test: (tensor(2.0849), tensor(2.8231))\n",
      "SE_test: (tensor(2.3422), tensor(3.6776))\n"
     ]
    }
   ],
   "source": [
    "print('nLPP_valid: '+str(nLPP_validation))\n",
    "print('SE_valid: '+str(RSE_validation))\n",
    "print('nLPP_test: '+str(nLPP_test))\n",
    "print('SE_test: '+str(RSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un choix de points $x_0,...,x_{n-1}$, on définit:\n",
    "$$\n",
    "d(\\theta,\\theta')=\\frac{1}{n}\\sum_{i<n}\\vert f_\\theta(x_i)-f_{\\theta'}(x_i)\\vert\n",
    "$$\n",
    "ou\n",
    "$$\n",
    "d_2(\\theta,\\theta')=\\biggl(\\frac{1}{n}\\sum_{i<n}\\vert f_\\theta(x_i)-f_{\\theta'}(x_i)\\vert^2\\biggr)^{\\frac{1}{2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(f\\in A)=P(\\{\\theta \\mid f_\\theta\\in A\\})$\n",
    "\n",
    "$\\theta \\mapsto f_\\theta$ (is it continuous?)\n",
    "\n",
    "relation entre $d(\\theta,\\theta')$ et $d(f_\\theta,f_\\theta')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
