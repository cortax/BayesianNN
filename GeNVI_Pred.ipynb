{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses\n",
    "from Inference.GeNVI_predictive import GeNPredVI, GeNetEns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Experiments.boston import Setup\n",
    "setup=Setup(device)#, layerwidth=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target density #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprior=setup.logPredPrior\n",
    "loglikelihood=setup.loglikelihood\n",
    "projection=setup.projection\n",
    "size_sample=setup.n_train_samples\n",
    "param_count=setup.param_count\n",
    "model=setup._model√®;\n",
    "\n",
    "size_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activation = nn.Tanh()#nn.ReLU()\n",
    "init_b = .001\n",
    "lat_dim=5\n",
    "GeN = GeNetEns(1, lat_dim, 50, param_count, activation, 0.2, init_b, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20000], Loss: 1308225.5, Entropy 1564.958984375, Learning Rate: 0.05\n",
      "Epoch [1/20000], Loss: 23345332.0, Entropy 1907.46142578125, Learning Rate: 0.05\n",
      "Epoch [2/20000], Loss: 8618723.0, Entropy 1432.6884765625, Learning Rate: 0.05\n",
      "Epoch [3/20000], Loss: 3605894.0, Entropy 1158.24365234375, Learning Rate: 0.05\n",
      "Epoch [4/20000], Loss: 2740021.0, Entropy 1085.28759765625, Learning Rate: 0.05\n",
      "Epoch [5/20000], Loss: 818921.6875, Entropy 928.4599609375, Learning Rate: 0.05\n",
      "Epoch [6/20000], Loss: 1232181.625, Entropy 889.197265625, Learning Rate: 0.05\n",
      "Epoch [7/20000], Loss: 1015265.5625, Entropy 900.58349609375, Learning Rate: 0.05\n",
      "Epoch [8/20000], Loss: 2486869.75, Entropy 994.947265625, Learning Rate: 0.05\n",
      "Epoch [9/20000], Loss: 1616295.5, Entropy 934.95947265625, Learning Rate: 0.05\n",
      "Epoch [10/20000], Loss: 1936615.0, Entropy 985.00341796875, Learning Rate: 0.05\n",
      "Epoch [11/20000], Loss: 3474036.5, Entropy 1042.548828125, Learning Rate: 0.05\n",
      "Epoch [12/20000], Loss: 16031630.0, Entropy 1034.39501953125, Learning Rate: 0.05\n",
      "Epoch [13/20000], Loss: 1418637.5, Entropy 890.5078125, Learning Rate: 0.05\n",
      "Epoch [14/20000], Loss: 1983268.25, Entropy 814.67333984375, Learning Rate: 0.05\n",
      "Epoch [15/20000], Loss: 1752548.625, Entropy 913.93603515625, Learning Rate: 0.05\n",
      "Epoch [16/20000], Loss: 3398643.75, Entropy 849.33251953125, Learning Rate: 0.05\n",
      "Epoch [17/20000], Loss: 2621551.0, Entropy 786.70947265625, Learning Rate: 0.05\n",
      "Epoch [18/20000], Loss: 1440665.125, Entropy 820.99755859375, Learning Rate: 0.05\n",
      "Epoch [19/20000], Loss: 1601468.875, Entropy 717.02294921875, Learning Rate: 0.05\n",
      "Epoch [20/20000], Loss: 924218.8125, Entropy 763.3994140625, Learning Rate: 0.05\n",
      "Epoch [21/20000], Loss: 1200886.75, Entropy 737.6787109375, Learning Rate: 0.05\n",
      "Epoch [22/20000], Loss: 1381728.0, Entropy 693.1181640625, Learning Rate: 0.05\n",
      "Epoch [23/20000], Loss: 582691.75, Entropy 581.54931640625, Learning Rate: 0.05\n",
      "Epoch [24/20000], Loss: 336812.75, Entropy 351.72998046875, Learning Rate: 0.05\n",
      "Epoch [25/20000], Loss: 266872.3125, Entropy 344.79931640625, Learning Rate: 0.05\n",
      "Epoch [26/20000], Loss: 731076.8125, Entropy 435.49853515625, Learning Rate: 0.05\n",
      "Epoch [27/20000], Loss: 437263.53125, Entropy 345.62548828125, Learning Rate: 0.05\n",
      "Epoch [28/20000], Loss: 454094.03125, Entropy 270.13525390625, Learning Rate: 0.05\n",
      "Epoch [29/20000], Loss: 331989.125, Entropy 264.7607421875, Learning Rate: 0.05\n",
      "Epoch [30/20000], Loss: 267521.59375, Entropy 143.46630859375, Learning Rate: 0.05\n",
      "Epoch [31/20000], Loss: 102608.734375, Entropy 79.95166015625, Learning Rate: 0.05\n",
      "Epoch [32/20000], Loss: 185175.859375, Entropy 34.91015625, Learning Rate: 0.05\n",
      "Epoch [33/20000], Loss: 295663.21875, Entropy 159.8349609375, Learning Rate: 0.05\n",
      "Epoch [34/20000], Loss: 205253.484375, Entropy 186.85107421875, Learning Rate: 0.05\n",
      "Epoch [35/20000], Loss: 133335.71875, Entropy 72.98876953125, Learning Rate: 0.05\n",
      "Epoch [36/20000], Loss: 85476.3359375, Entropy -26.66748046875, Learning Rate: 0.05\n",
      "Epoch [37/20000], Loss: 92061.9140625, Entropy 40.38623046875, Learning Rate: 0.05\n",
      "Epoch [38/20000], Loss: 136978.03125, Entropy 8.70263671875, Learning Rate: 0.05\n",
      "Epoch [39/20000], Loss: 95993.9453125, Entropy -46.328125, Learning Rate: 0.05\n",
      "Epoch [40/20000], Loss: 60390.96875, Entropy -126.28857421875, Learning Rate: 0.05\n",
      "Epoch [41/20000], Loss: 56263.015625, Entropy -178.8203125, Learning Rate: 0.05\n",
      "Epoch [42/20000], Loss: 58773.8828125, Entropy -179.728515625, Learning Rate: 0.05\n",
      "Epoch [43/20000], Loss: 53164.10546875, Entropy -202.5947265625, Learning Rate: 0.05\n",
      "Epoch [44/20000], Loss: 56420.9609375, Entropy -194.52685546875, Learning Rate: 0.05\n",
      "Epoch [45/20000], Loss: 52505.1953125, Entropy -260.12451171875, Learning Rate: 0.05\n",
      "Epoch [46/20000], Loss: 49853.7421875, Entropy -160.58544921875, Learning Rate: 0.05\n",
      "Epoch [47/20000], Loss: 79755.71875, Entropy -237.5341796875, Learning Rate: 0.05\n",
      "Epoch [48/20000], Loss: 40351.9609375, Entropy -322.9541015625, Learning Rate: 0.05\n",
      "Epoch [49/20000], Loss: 27878.40625, Entropy -332.50341796875, Learning Rate: 0.05\n",
      "Epoch [50/20000], Loss: 23696.734375, Entropy -341.93212890625, Learning Rate: 0.05\n",
      "Epoch [51/20000], Loss: 30965.990234375, Entropy -262.36865234375, Learning Rate: 0.05\n",
      "Epoch [52/20000], Loss: 21247.578125, Entropy -405.75927734375, Learning Rate: 0.05\n",
      "Epoch [53/20000], Loss: 24240.091796875, Entropy -385.8798828125, Learning Rate: 0.05\n",
      "Epoch [54/20000], Loss: 27578.466796875, Entropy -485.986328125, Learning Rate: 0.05\n",
      "Epoch [55/20000], Loss: 21020.0859375, Entropy -465.32177734375, Learning Rate: 0.05\n",
      "Epoch [56/20000], Loss: 22146.6171875, Entropy -566.42333984375, Learning Rate: 0.05\n",
      "Epoch [57/20000], Loss: 20125.20703125, Entropy -520.63037109375, Learning Rate: 0.05\n",
      "Epoch [58/20000], Loss: 24190.359375, Entropy -572.40771484375, Learning Rate: 0.05\n",
      "Epoch [59/20000], Loss: 16898.267578125, Entropy -374.96728515625, Learning Rate: 0.05\n",
      "Epoch [60/20000], Loss: 15313.1962890625, Entropy -566.08447265625, Learning Rate: 0.05\n",
      "Epoch [61/20000], Loss: 15146.677734375, Entropy -561.4892578125, Learning Rate: 0.05\n",
      "Epoch [62/20000], Loss: 15330.111328125, Entropy -570.91552734375, Learning Rate: 0.05\n",
      "Epoch [63/20000], Loss: 14110.763671875, Entropy -544.609375, Learning Rate: 0.05\n",
      "Epoch [64/20000], Loss: 17954.169921875, Entropy -688.31787109375, Learning Rate: 0.05\n",
      "Epoch [65/20000], Loss: 13295.99609375, Entropy -581.46533203125, Learning Rate: 0.05\n",
      "Epoch [66/20000], Loss: 13232.291015625, Entropy -631.9130859375, Learning Rate: 0.05\n",
      "Epoch [67/20000], Loss: 13346.751953125, Entropy -578.71630859375, Learning Rate: 0.05\n",
      "Epoch [68/20000], Loss: 10257.3251953125, Entropy -712.27001953125, Learning Rate: 0.05\n",
      "Epoch [69/20000], Loss: 12263.12109375, Entropy -701.9677734375, Learning Rate: 0.05\n",
      "Epoch [70/20000], Loss: 8931.40625, Entropy -751.7978515625, Learning Rate: 0.05\n",
      "Epoch [71/20000], Loss: 11351.044921875, Entropy -956.63623046875, Learning Rate: 0.05\n",
      "Epoch [72/20000], Loss: 10557.9814453125, Entropy -733.79736328125, Learning Rate: 0.05\n",
      "Epoch [73/20000], Loss: 8541.6689453125, Entropy -840.09521484375, Learning Rate: 0.05\n",
      "Epoch [74/20000], Loss: 9056.8818359375, Entropy -689.455078125, Learning Rate: 0.05\n",
      "Epoch [75/20000], Loss: 10352.521484375, Entropy -761.4326171875, Learning Rate: 0.05\n",
      "Epoch [76/20000], Loss: 8459.462890625, Entropy -823.1884765625, Learning Rate: 0.05\n",
      "Epoch [77/20000], Loss: 10161.1376953125, Entropy -799.4892578125, Learning Rate: 0.05\n",
      "Epoch [78/20000], Loss: 8249.73828125, Entropy -786.40771484375, Learning Rate: 0.05\n",
      "Epoch [79/20000], Loss: 7124.6142578125, Entropy -889.56396484375, Learning Rate: 0.05\n",
      "Epoch [80/20000], Loss: 9575.556640625, Entropy -949.9306640625, Learning Rate: 0.05\n",
      "Epoch [81/20000], Loss: 7784.62255859375, Entropy -787.30224609375, Learning Rate: 0.05\n",
      "Epoch [82/20000], Loss: 9284.65625, Entropy -919.42333984375, Learning Rate: 0.05\n",
      "Epoch [83/20000], Loss: 8185.63720703125, Entropy -885.0537109375, Learning Rate: 0.05\n",
      "Epoch [84/20000], Loss: 7213.27099609375, Entropy -991.62548828125, Learning Rate: 0.05\n",
      "Epoch [85/20000], Loss: 7679.9716796875, Entropy -913.740234375, Learning Rate: 0.05\n",
      "Epoch [86/20000], Loss: 7672.6796875, Entropy -958.5986328125, Learning Rate: 0.05\n",
      "Epoch [87/20000], Loss: 7416.796875, Entropy -920.12255859375, Learning Rate: 0.05\n",
      "Epoch [88/20000], Loss: 7205.1240234375, Entropy -943.634765625, Learning Rate: 0.05\n",
      "Epoch [89/20000], Loss: 8444.849609375, Entropy -1031.8701171875, Learning Rate: 0.05\n",
      "Epoch [90/20000], Loss: 7455.1318359375, Entropy -1030.19287109375, Learning Rate: 0.05\n",
      "Epoch [91/20000], Loss: 6220.27490234375, Entropy -986.97802734375, Learning Rate: 0.05\n",
      "Epoch [92/20000], Loss: 6422.56005859375, Entropy -998.9580078125, Learning Rate: 0.05\n",
      "Epoch [93/20000], Loss: 6264.6650390625, Entropy -1052.95751953125, Learning Rate: 0.05\n",
      "Epoch [94/20000], Loss: 7086.81494140625, Entropy -1002.46435546875, Learning Rate: 0.05\n",
      "Epoch [95/20000], Loss: 6227.42138671875, Entropy -849.8916015625, Learning Rate: 0.05\n",
      "Epoch [96/20000], Loss: 5749.177734375, Entropy -1182.20751953125, Learning Rate: 0.05\n",
      "Epoch [97/20000], Loss: 5950.31884765625, Entropy -999.169921875, Learning Rate: 0.05\n",
      "Epoch [98/20000], Loss: 6205.09033203125, Entropy -1075.26416015625, Learning Rate: 0.05\n",
      "Epoch [99/20000], Loss: 7141.06640625, Entropy -1060.57861328125, Learning Rate: 0.05\n",
      "Epoch [100/20000], Loss: 7195.23291015625, Entropy -1003.50439453125, Learning Rate: 0.05\n",
      "Epoch [101/20000], Loss: 6597.859375, Entropy -1029.39501953125, Learning Rate: 0.05\n",
      "Epoch [102/20000], Loss: 5811.93603515625, Entropy -1095.43505859375, Learning Rate: 0.05\n",
      "Epoch [103/20000], Loss: 5938.17578125, Entropy -1048.3369140625, Learning Rate: 0.05\n",
      "Epoch [104/20000], Loss: 6508.6455078125, Entropy -1020.8486328125, Learning Rate: 0.05\n",
      "Epoch [105/20000], Loss: 6743.6240234375, Entropy -1129.71728515625, Learning Rate: 0.05\n",
      "Epoch [106/20000], Loss: 6517.4873046875, Entropy -1045.15771484375, Learning Rate: 0.05\n",
      "Epoch [107/20000], Loss: 6296.73974609375, Entropy -1081.06103515625, Learning Rate: 0.05\n",
      "Epoch [108/20000], Loss: 6724.82421875, Entropy -1020.75244140625, Learning Rate: 0.05\n",
      "Epoch [109/20000], Loss: 5925.419921875, Entropy -1094.78662109375, Learning Rate: 0.05\n",
      "Epoch [110/20000], Loss: 5740.61181640625, Entropy -1066.875, Learning Rate: 0.05\n",
      "Epoch [111/20000], Loss: 5461.7822265625, Entropy -1067.65087890625, Learning Rate: 0.05\n",
      "Epoch [112/20000], Loss: 5849.9619140625, Entropy -1118.59765625, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [113/20000], Loss: 5467.380859375, Entropy -1065.970703125, Learning Rate: 0.05\n",
      "Epoch [114/20000], Loss: 6004.0986328125, Entropy -1048.6611328125, Learning Rate: 0.05\n",
      "Epoch [115/20000], Loss: 5532.4970703125, Entropy -1108.541015625, Learning Rate: 0.05\n",
      "Epoch [116/20000], Loss: 5477.26953125, Entropy -1137.32177734375, Learning Rate: 0.05\n",
      "Epoch [117/20000], Loss: 5503.1650390625, Entropy -1102.04736328125, Learning Rate: 0.05\n",
      "Epoch [118/20000], Loss: 5807.994140625, Entropy -1180.16796875, Learning Rate: 0.05\n",
      "Epoch [119/20000], Loss: 5676.892578125, Entropy -1100.67529296875, Learning Rate: 0.05\n",
      "Epoch [120/20000], Loss: 5943.73876953125, Entropy -1016.95654296875, Learning Rate: 0.05\n",
      "Epoch [121/20000], Loss: 5126.3125, Entropy -1219.4892578125, Learning Rate: 0.05\n",
      "Epoch [122/20000], Loss: 5364.12939453125, Entropy -1104.7900390625, Learning Rate: 0.05\n",
      "Epoch [123/20000], Loss: 5866.2109375, Entropy -1000.29541015625, Learning Rate: 0.05\n",
      "Epoch [124/20000], Loss: 5519.4208984375, Entropy -1165.4541015625, Learning Rate: 0.05\n",
      "Epoch [125/20000], Loss: 5464.9091796875, Entropy -1163.39208984375, Learning Rate: 0.05\n",
      "Epoch [126/20000], Loss: 6007.6279296875, Entropy -1057.701171875, Learning Rate: 0.05\n",
      "Epoch [127/20000], Loss: 5288.685546875, Entropy -1300.31640625, Learning Rate: 0.05\n",
      "Epoch [128/20000], Loss: 5810.6591796875, Entropy -1217.875, Learning Rate: 0.05\n",
      "Epoch [129/20000], Loss: 6200.8056640625, Entropy -1167.20361328125, Learning Rate: 0.05\n",
      "Epoch [130/20000], Loss: 5682.716796875, Entropy -1161.63720703125, Learning Rate: 0.05\n",
      "Epoch [131/20000], Loss: 5756.4228515625, Entropy -1200.67724609375, Learning Rate: 0.05\n",
      "Epoch [132/20000], Loss: 5834.39794921875, Entropy -1219.69873046875, Learning Rate: 0.05\n",
      "Epoch [133/20000], Loss: 5608.21484375, Entropy -1225.74169921875, Learning Rate: 0.05\n",
      "Epoch [134/20000], Loss: 5477.6669921875, Entropy -1266.67626953125, Learning Rate: 0.05\n",
      "Epoch [135/20000], Loss: 5467.0771484375, Entropy -1257.51904296875, Learning Rate: 0.05\n",
      "Epoch [136/20000], Loss: 5455.453125, Entropy -1103.06298828125, Learning Rate: 0.05\n",
      "Epoch [137/20000], Loss: 5780.34130859375, Entropy -1254.72802734375, Learning Rate: 0.05\n",
      "Epoch [138/20000], Loss: 5019.10498046875, Entropy -1230.2978515625, Learning Rate: 0.05\n",
      "Epoch [139/20000], Loss: 5551.0537109375, Entropy -1209.3740234375, Learning Rate: 0.05\n",
      "Epoch [140/20000], Loss: 5527.54052734375, Entropy -1180.5087890625, Learning Rate: 0.05\n",
      "Epoch [141/20000], Loss: 5155.921875, Entropy -1110.57080078125, Learning Rate: 0.05\n",
      "Epoch [142/20000], Loss: 5146.369140625, Entropy -1212.85595703125, Learning Rate: 0.05\n",
      "Epoch [143/20000], Loss: 5572.96435546875, Entropy -1349.248046875, Learning Rate: 0.05\n",
      "Epoch [144/20000], Loss: 5218.69677734375, Entropy -1223.9111328125, Learning Rate: 0.05\n",
      "Epoch [145/20000], Loss: 5168.8916015625, Entropy -1254.72314453125, Learning Rate: 0.05\n",
      "Epoch [146/20000], Loss: 5172.6748046875, Entropy -1207.30859375, Learning Rate: 0.05\n",
      "Epoch [147/20000], Loss: 5208.0986328125, Entropy -1228.79345703125, Learning Rate: 0.05\n",
      "Epoch [148/20000], Loss: 5242.28857421875, Entropy -1158.91259765625, Learning Rate: 0.05\n",
      "Epoch [149/20000], Loss: 5032.138671875, Entropy -1238.83349609375, Learning Rate: 0.05\n",
      "Epoch [150/20000], Loss: 5832.220703125, Entropy -1223.9013671875, Learning Rate: 0.05\n",
      "Epoch [151/20000], Loss: 5522.3173828125, Entropy -1380.03271484375, Learning Rate: 0.05\n",
      "Epoch [152/20000], Loss: 5470.6083984375, Entropy -1228.20166015625, Learning Rate: 0.05\n",
      "Epoch [153/20000], Loss: 5278.849609375, Entropy -1362.82373046875, Learning Rate: 0.05\n",
      "Epoch [154/20000], Loss: 5110.12451171875, Entropy -1342.71630859375, Learning Rate: 0.05\n",
      "Epoch [155/20000], Loss: 5246.251953125, Entropy -1314.0576171875, Learning Rate: 0.05\n",
      "Epoch [156/20000], Loss: 5012.8564453125, Entropy -1290.80517578125, Learning Rate: 0.05\n",
      "Epoch [157/20000], Loss: 5036.3916015625, Entropy -1313.54931640625, Learning Rate: 0.05\n",
      "Epoch [158/20000], Loss: 5676.31689453125, Entropy -1197.13427734375, Learning Rate: 0.05\n",
      "Epoch [159/20000], Loss: 4772.03125, Entropy -1351.7763671875, Learning Rate: 0.05\n",
      "Epoch [160/20000], Loss: 5496.80029296875, Entropy -1431.7822265625, Learning Rate: 0.05\n",
      "Epoch [161/20000], Loss: 5246.798828125, Entropy -1189.13818359375, Learning Rate: 0.05\n",
      "Epoch [162/20000], Loss: 4998.5478515625, Entropy -1411.57373046875, Learning Rate: 0.05\n",
      "Epoch [163/20000], Loss: 5110.49755859375, Entropy -1339.66357421875, Learning Rate: 0.05\n",
      "Epoch [164/20000], Loss: 5205.2763671875, Entropy -1246.63720703125, Learning Rate: 0.05\n",
      "Epoch [165/20000], Loss: 5238.4384765625, Entropy -1243.31201171875, Learning Rate: 0.05\n",
      "Epoch [166/20000], Loss: 4996.0146484375, Entropy -1217.75732421875, Learning Rate: 0.05\n",
      "Epoch [167/20000], Loss: 4976.11767578125, Entropy -1291.185546875, Learning Rate: 0.05\n",
      "Epoch [168/20000], Loss: 4915.98974609375, Entropy -1287.0478515625, Learning Rate: 0.05\n",
      "Epoch [169/20000], Loss: 5034.44921875, Entropy -1295.92431640625, Learning Rate: 0.05\n",
      "Epoch [170/20000], Loss: 6662.537109375, Entropy -1282.4697265625, Learning Rate: 0.05\n",
      "Epoch [171/20000], Loss: 4798.2900390625, Entropy -1195.05859375, Learning Rate: 0.05\n",
      "Epoch [172/20000], Loss: 5007.53173828125, Entropy -1164.046875, Learning Rate: 0.05\n",
      "Epoch [173/20000], Loss: 5114.791015625, Entropy -1304.06298828125, Learning Rate: 0.05\n",
      "Epoch [174/20000], Loss: 5202.7021484375, Entropy -1316.44970703125, Learning Rate: 0.05\n",
      "Epoch [175/20000], Loss: 5365.1220703125, Entropy -1440.734375, Learning Rate: 0.05\n",
      "Epoch [176/20000], Loss: 4837.6865234375, Entropy -1313.20068359375, Learning Rate: 0.05\n",
      "Epoch [177/20000], Loss: 6042.15576171875, Entropy -1364.4521484375, Learning Rate: 0.05\n",
      "Epoch [178/20000], Loss: 5097.0107421875, Entropy -1448.61767578125, Learning Rate: 0.05\n",
      "Epoch [179/20000], Loss: 5244.8515625, Entropy -1378.04931640625, Learning Rate: 0.05\n",
      "Epoch [180/20000], Loss: 5328.88720703125, Entropy -1356.58447265625, Learning Rate: 0.05\n",
      "Epoch [181/20000], Loss: 5288.126953125, Entropy -1361.9033203125, Learning Rate: 0.05\n",
      "Epoch [182/20000], Loss: 4889.388671875, Entropy -1345.1337890625, Learning Rate: 0.05\n",
      "Epoch [183/20000], Loss: 5551.0419921875, Entropy -1269.2939453125, Learning Rate: 0.05\n",
      "Epoch [184/20000], Loss: 4847.5751953125, Entropy -1266.77099609375, Learning Rate: 0.05\n",
      "Epoch [185/20000], Loss: 4979.87890625, Entropy -1307.02001953125, Learning Rate: 0.05\n",
      "Epoch [186/20000], Loss: 5151.96435546875, Entropy -1359.3564453125, Learning Rate: 0.05\n",
      "Epoch [187/20000], Loss: 5491.49755859375, Entropy -1496.0, Learning Rate: 0.05\n",
      "Epoch [188/20000], Loss: 5095.345703125, Entropy -1272.28271484375, Learning Rate: 0.05\n",
      "Epoch [189/20000], Loss: 5064.2900390625, Entropy -1331.35205078125, Learning Rate: 0.05\n",
      "Epoch [190/20000], Loss: 4996.529296875, Entropy -1387.0498046875, Learning Rate: 0.05\n",
      "Epoch [191/20000], Loss: 4873.291015625, Entropy -1357.03515625, Learning Rate: 0.05\n",
      "Epoch [192/20000], Loss: 4714.64794921875, Entropy -1288.64501953125, Learning Rate: 0.05\n",
      "Epoch [193/20000], Loss: 5165.17431640625, Entropy -1392.01171875, Learning Rate: 0.05\n",
      "Epoch [194/20000], Loss: 4831.16259765625, Entropy -1421.15625, Learning Rate: 0.05\n",
      "Epoch [195/20000], Loss: 4924.0595703125, Entropy -1447.93359375, Learning Rate: 0.05\n",
      "Epoch [196/20000], Loss: 4660.873046875, Entropy -1409.99462890625, Learning Rate: 0.05\n",
      "Epoch [197/20000], Loss: 4777.525390625, Entropy -1313.86962890625, Learning Rate: 0.05\n",
      "Epoch [198/20000], Loss: 4921.44140625, Entropy -1317.2314453125, Learning Rate: 0.05\n",
      "Epoch [199/20000], Loss: 4776.32177734375, Entropy -1246.83740234375, Learning Rate: 0.05\n",
      "Epoch [200/20000], Loss: 4834.6875, Entropy -1244.560546875, Learning Rate: 0.05\n",
      "Epoch [201/20000], Loss: 5028.46533203125, Entropy -1492.3134765625, Learning Rate: 0.05\n",
      "Epoch [202/20000], Loss: 4880.0654296875, Entropy -1411.24853515625, Learning Rate: 0.05\n",
      "Epoch [203/20000], Loss: 4740.771484375, Entropy -1405.3994140625, Learning Rate: 0.05\n",
      "Epoch [204/20000], Loss: 5019.5107421875, Entropy -1489.38671875, Learning Rate: 0.05\n",
      "Epoch [205/20000], Loss: 4798.70361328125, Entropy -1431.176025390625, Learning Rate: 0.05\n",
      "Epoch [206/20000], Loss: 4930.66064453125, Entropy -1413.354248046875, Learning Rate: 0.05\n",
      "Epoch [207/20000], Loss: 4852.962890625, Entropy -1436.410888671875, Learning Rate: 0.05\n",
      "Epoch [208/20000], Loss: 4982.365234375, Entropy -1331.5478515625, Learning Rate: 0.05\n",
      "Epoch [209/20000], Loss: 5008.07763671875, Entropy -1533.621337890625, Learning Rate: 0.05\n",
      "Epoch [210/20000], Loss: 5202.14794921875, Entropy -1314.37255859375, Learning Rate: 0.05\n",
      "Epoch [211/20000], Loss: 4821.71875, Entropy -1487.114990234375, Learning Rate: 0.05\n",
      "Epoch [212/20000], Loss: 4673.81298828125, Entropy -1478.215087890625, Learning Rate: 0.05\n",
      "Epoch [213/20000], Loss: 4881.82568359375, Entropy -1406.10595703125, Learning Rate: 0.05\n",
      "Epoch [214/20000], Loss: 4792.970703125, Entropy -1506.33837890625, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [215/20000], Loss: 4876.1494140625, Entropy -1475.437744140625, Learning Rate: 0.05\n",
      "Epoch [216/20000], Loss: 5272.84765625, Entropy -1340.34375, Learning Rate: 0.05\n",
      "Epoch [217/20000], Loss: 4764.28955078125, Entropy -1418.46435546875, Learning Rate: 0.05\n",
      "Epoch [218/20000], Loss: 4907.568359375, Entropy -1298.51513671875, Learning Rate: 0.05\n",
      "Epoch [219/20000], Loss: 4748.74560546875, Entropy -1476.103271484375, Learning Rate: 0.05\n",
      "Epoch [220/20000], Loss: 4880.82275390625, Entropy -1378.77978515625, Learning Rate: 0.05\n",
      "Epoch [221/20000], Loss: 4775.927734375, Entropy -1490.63427734375, Learning Rate: 0.05\n",
      "Epoch [222/20000], Loss: 4753.654296875, Entropy -1412.0576171875, Learning Rate: 0.05\n",
      "Epoch [223/20000], Loss: 4889.298828125, Entropy -1494.857421875, Learning Rate: 0.05\n",
      "Epoch [224/20000], Loss: 4941.83984375, Entropy -1463.679931640625, Learning Rate: 0.05\n",
      "Epoch [225/20000], Loss: 4474.96728515625, Entropy -1321.240234375, Learning Rate: 0.05\n",
      "Epoch [226/20000], Loss: 4782.474609375, Entropy -1416.48681640625, Learning Rate: 0.05\n",
      "Epoch [227/20000], Loss: 5030.7041015625, Entropy -1473.445556640625, Learning Rate: 0.05\n",
      "Epoch [228/20000], Loss: 4806.28857421875, Entropy -1418.400634765625, Learning Rate: 0.05\n",
      "Epoch [229/20000], Loss: 4772.1328125, Entropy -1335.59423828125, Learning Rate: 0.05\n",
      "Epoch [230/20000], Loss: 4512.01220703125, Entropy -1417.0556640625, Learning Rate: 0.05\n",
      "Epoch [231/20000], Loss: 5102.47021484375, Entropy -1408.3447265625, Learning Rate: 0.05\n",
      "Epoch [232/20000], Loss: 4879.0263671875, Entropy -1458.7392578125, Learning Rate: 0.05\n",
      "Epoch [233/20000], Loss: 4734.76708984375, Entropy -1491.73095703125, Learning Rate: 0.05\n",
      "Epoch [234/20000], Loss: 4679.48046875, Entropy -1363.00634765625, Learning Rate: 0.05\n",
      "Epoch [235/20000], Loss: 4783.60009765625, Entropy -1404.048583984375, Learning Rate: 0.05\n",
      "Epoch [236/20000], Loss: 4784.013671875, Entropy -1482.046630859375, Learning Rate: 0.05\n",
      "Epoch [237/20000], Loss: 4621.115234375, Entropy -1370.20556640625, Learning Rate: 0.05\n",
      "Epoch [238/20000], Loss: 4696.88623046875, Entropy -1406.03515625, Learning Rate: 0.05\n",
      "Epoch [239/20000], Loss: 5064.13232421875, Entropy -1271.54150390625, Learning Rate: 0.05\n",
      "Epoch [240/20000], Loss: 4666.2001953125, Entropy -1515.265625, Learning Rate: 0.05\n",
      "Epoch [241/20000], Loss: 4865.42724609375, Entropy -1476.24267578125, Learning Rate: 0.05\n",
      "Epoch [242/20000], Loss: 4593.5146484375, Entropy -1329.17236328125, Learning Rate: 0.05\n",
      "Epoch [243/20000], Loss: 4938.068359375, Entropy -1479.9501953125, Learning Rate: 0.05\n",
      "Epoch [244/20000], Loss: 4754.0458984375, Entropy -1475.49853515625, Learning Rate: 0.05\n",
      "Epoch [245/20000], Loss: 4728.3251953125, Entropy -1300.5029296875, Learning Rate: 0.05\n",
      "Epoch [246/20000], Loss: 4877.6396484375, Entropy -1430.56787109375, Learning Rate: 0.05\n",
      "Epoch [247/20000], Loss: 4856.48828125, Entropy -1426.2294921875, Learning Rate: 0.05\n",
      "Epoch [248/20000], Loss: 4691.55224609375, Entropy -1541.443603515625, Learning Rate: 0.05\n",
      "Epoch [249/20000], Loss: 4812.3779296875, Entropy -1403.576416015625, Learning Rate: 0.05\n",
      "Epoch [250/20000], Loss: 4693.7939453125, Entropy -1564.61376953125, Learning Rate: 0.05\n",
      "Epoch [251/20000], Loss: 4740.4326171875, Entropy -1431.46435546875, Learning Rate: 0.05\n",
      "Epoch [252/20000], Loss: 4577.7724609375, Entropy -1429.34130859375, Learning Rate: 0.05\n",
      "Epoch [253/20000], Loss: 4628.529296875, Entropy -1411.74267578125, Learning Rate: 0.05\n",
      "Epoch [254/20000], Loss: 4690.74462890625, Entropy -1388.50439453125, Learning Rate: 0.05\n",
      "Epoch [255/20000], Loss: 4738.5537109375, Entropy -1426.89453125, Learning Rate: 0.05\n",
      "Epoch [256/20000], Loss: 4874.037109375, Entropy -1536.988037109375, Learning Rate: 0.05\n",
      "Epoch [257/20000], Loss: 4877.02734375, Entropy -1462.9560546875, Learning Rate: 0.05\n",
      "Epoch [258/20000], Loss: 4652.6025390625, Entropy -1396.5263671875, Learning Rate: 0.05\n",
      "Epoch [259/20000], Loss: 4679.6982421875, Entropy -1422.238525390625, Learning Rate: 0.05\n",
      "Epoch [260/20000], Loss: 5101.46923828125, Entropy -1396.22607421875, Learning Rate: 0.05\n",
      "Epoch [261/20000], Loss: 5423.70751953125, Entropy -1385.02490234375, Learning Rate: 0.05\n",
      "Epoch [262/20000], Loss: 4689.48486328125, Entropy -1455.77734375, Learning Rate: 0.05\n",
      "Epoch [263/20000], Loss: 4544.78857421875, Entropy -1486.54296875, Learning Rate: 0.05\n",
      "Epoch [264/20000], Loss: 4687.8515625, Entropy -1412.73486328125, Learning Rate: 0.05\n",
      "Epoch [265/20000], Loss: 4679.236328125, Entropy -1478.263671875, Learning Rate: 0.05\n",
      "Epoch [266/20000], Loss: 4954.59130859375, Entropy -1470.637939453125, Learning Rate: 0.05\n",
      "Epoch [267/20000], Loss: 4649.13623046875, Entropy -1378.53564453125, Learning Rate: 0.05\n",
      "Epoch [268/20000], Loss: 4721.77197265625, Entropy -1357.46826171875, Learning Rate: 0.05\n",
      "Epoch [269/20000], Loss: 4538.04248046875, Entropy -1508.181884765625, Learning Rate: 0.05\n",
      "Epoch [270/20000], Loss: 4740.15966796875, Entropy -1448.8515625, Learning Rate: 0.05\n",
      "Epoch [271/20000], Loss: 4731.34814453125, Entropy -1475.9931640625, Learning Rate: 0.05\n",
      "Epoch [272/20000], Loss: 5187.18212890625, Entropy -1390.01171875, Learning Rate: 0.05\n",
      "Epoch [273/20000], Loss: 4782.4794921875, Entropy -1547.59130859375, Learning Rate: 0.05\n",
      "Epoch [274/20000], Loss: 4703.7001953125, Entropy -1617.21337890625, Learning Rate: 0.05\n",
      "Epoch [275/20000], Loss: 4753.75732421875, Entropy -1433.368408203125, Learning Rate: 0.05\n",
      "Epoch [276/20000], Loss: 4789.35205078125, Entropy -1525.334228515625, Learning Rate: 0.05\n",
      "Epoch [277/20000], Loss: 4677.29150390625, Entropy -1528.994873046875, Learning Rate: 0.05\n",
      "Epoch [278/20000], Loss: 5165.7705078125, Entropy -1430.4970703125, Learning Rate: 0.05\n",
      "Epoch [279/20000], Loss: 5202.50341796875, Entropy -1518.86328125, Learning Rate: 0.05\n",
      "Epoch [280/20000], Loss: 4686.66650390625, Entropy -1466.98876953125, Learning Rate: 0.05\n",
      "Epoch [281/20000], Loss: 4771.1552734375, Entropy -1381.78125, Learning Rate: 0.05\n",
      "Epoch [282/20000], Loss: 4728.7626953125, Entropy -1340.34521484375, Learning Rate: 0.05\n",
      "Epoch [283/20000], Loss: 4780.3017578125, Entropy -1509.785888671875, Learning Rate: 0.05\n",
      "Epoch [284/20000], Loss: 4798.2373046875, Entropy -1496.416015625, Learning Rate: 0.05\n",
      "Epoch [285/20000], Loss: 4736.43408203125, Entropy -1497.22509765625, Learning Rate: 0.05\n",
      "Epoch [286/20000], Loss: 4601.04052734375, Entropy -1471.630859375, Learning Rate: 0.05\n",
      "Epoch [287/20000], Loss: 4561.94189453125, Entropy -1511.7451171875, Learning Rate: 0.05\n",
      "Epoch [288/20000], Loss: 4554.1669921875, Entropy -1452.77099609375, Learning Rate: 0.05\n",
      "Epoch [289/20000], Loss: 4764.265625, Entropy -1274.25146484375, Learning Rate: 0.05\n",
      "Epoch [290/20000], Loss: 4715.2392578125, Entropy -1469.619140625, Learning Rate: 0.05\n",
      "Epoch [291/20000], Loss: 4751.8857421875, Entropy -1576.51123046875, Learning Rate: 0.05\n",
      "Epoch [292/20000], Loss: 4656.7099609375, Entropy -1535.19482421875, Learning Rate: 0.05\n",
      "Epoch [293/20000], Loss: 4840.884765625, Entropy -1558.09619140625, Learning Rate: 0.05\n",
      "Epoch [294/20000], Loss: 4792.13720703125, Entropy -1460.861083984375, Learning Rate: 0.05\n",
      "Epoch [295/20000], Loss: 4579.4560546875, Entropy -1433.46435546875, Learning Rate: 0.05\n",
      "Epoch [296/20000], Loss: 4739.1484375, Entropy -1667.43896484375, Learning Rate: 0.05\n",
      "Epoch [297/20000], Loss: 4728.4921875, Entropy -1527.32666015625, Learning Rate: 0.05\n",
      "Epoch [298/20000], Loss: 4612.49609375, Entropy -1454.82666015625, Learning Rate: 0.05\n",
      "Epoch [299/20000], Loss: 4773.9150390625, Entropy -1528.0, Learning Rate: 0.05\n",
      "Epoch [300/20000], Loss: 4637.888671875, Entropy -1439.826416015625, Learning Rate: 0.05\n",
      "Epoch [301/20000], Loss: 4659.38134765625, Entropy -1534.8798828125, Learning Rate: 0.05\n",
      "Epoch [302/20000], Loss: 4770.29638671875, Entropy -1690.556396484375, Learning Rate: 0.05\n",
      "Epoch [303/20000], Loss: 4932.0478515625, Entropy -1438.2587890625, Learning Rate: 0.05\n",
      "Epoch [304/20000], Loss: 4547.3271484375, Entropy -1432.01416015625, Learning Rate: 0.05\n",
      "Epoch [305/20000], Loss: 4529.478515625, Entropy -1438.7646484375, Learning Rate: 0.05\n",
      "Epoch [306/20000], Loss: 4594.3486328125, Entropy -1448.321044921875, Learning Rate: 0.05\n",
      "Epoch [307/20000], Loss: 4778.87646484375, Entropy -1420.45068359375, Learning Rate: 0.05\n",
      "Epoch [308/20000], Loss: 4572.8505859375, Entropy -1440.60302734375, Learning Rate: 0.05\n",
      "Epoch [309/20000], Loss: 4850.21826171875, Entropy -1560.10595703125, Learning Rate: 0.05\n",
      "Epoch [310/20000], Loss: 4739.1279296875, Entropy -1491.671875, Learning Rate: 0.05\n",
      "Epoch [311/20000], Loss: 4741.32080078125, Entropy -1471.345947265625, Learning Rate: 0.05\n",
      "Epoch [312/20000], Loss: 4738.18603515625, Entropy -1490.59326171875, Learning Rate: 0.05\n",
      "Epoch [313/20000], Loss: 4824.47509765625, Entropy -1545.84423828125, Learning Rate: 0.05\n",
      "Epoch [314/20000], Loss: 4548.29345703125, Entropy -1376.47412109375, Learning Rate: 0.05\n",
      "Epoch [315/20000], Loss: 4722.82421875, Entropy -1416.890625, Learning Rate: 0.05\n",
      "Epoch [316/20000], Loss: 4623.9072265625, Entropy -1414.564697265625, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [317/20000], Loss: 4499.32421875, Entropy -1338.1982421875, Learning Rate: 0.05\n",
      "Epoch [318/20000], Loss: 4752.8486328125, Entropy -1551.1884765625, Learning Rate: 0.05\n",
      "Epoch [319/20000], Loss: 4547.81103515625, Entropy -1521.380859375, Learning Rate: 0.05\n",
      "Epoch [320/20000], Loss: 4871.66943359375, Entropy -1481.945068359375, Learning Rate: 0.05\n",
      "Epoch [321/20000], Loss: 4625.107421875, Entropy -1355.41650390625, Learning Rate: 0.05\n",
      "Epoch [322/20000], Loss: 4668.25634765625, Entropy -1576.954833984375, Learning Rate: 0.05\n",
      "Epoch [323/20000], Loss: 4722.9755859375, Entropy -1565.140869140625, Learning Rate: 0.05\n",
      "Epoch [324/20000], Loss: 4771.02197265625, Entropy -1466.150634765625, Learning Rate: 0.05\n",
      "Epoch [325/20000], Loss: 4688.8779296875, Entropy -1588.43212890625, Learning Rate: 0.05\n",
      "Epoch [326/20000], Loss: 4853.140625, Entropy -1515.5244140625, Learning Rate: 0.05\n",
      "Epoch [327/20000], Loss: 4638.74267578125, Entropy -1553.379638671875, Learning Rate: 0.05\n",
      "Epoch [328/20000], Loss: 4652.1787109375, Entropy -1568.35888671875, Learning Rate: 0.05\n",
      "Epoch [329/20000], Loss: 4646.4013671875, Entropy -1552.667724609375, Learning Rate: 0.05\n",
      "Epoch [330/20000], Loss: 4764.46142578125, Entropy -1563.238037109375, Learning Rate: 0.05\n",
      "Epoch [331/20000], Loss: 4900.2724609375, Entropy -1399.214599609375, Learning Rate: 0.05\n",
      "Epoch [332/20000], Loss: 4780.48291015625, Entropy -1510.46533203125, Learning Rate: 0.05\n",
      "Epoch [333/20000], Loss: 4750.1455078125, Entropy -1480.3515625, Learning Rate: 0.05\n",
      "Epoch [334/20000], Loss: 4722.958984375, Entropy -1547.250244140625, Learning Rate: 0.05\n",
      "Epoch [335/20000], Loss: 4616.3984375, Entropy -1467.58251953125, Learning Rate: 0.05\n",
      "Epoch [336/20000], Loss: 4684.40966796875, Entropy -1558.69873046875, Learning Rate: 0.05\n",
      "Epoch [337/20000], Loss: 4563.48583984375, Entropy -1613.84814453125, Learning Rate: 0.05\n",
      "Epoch [338/20000], Loss: 4788.736328125, Entropy -1563.435791015625, Learning Rate: 0.05\n",
      "Epoch [339/20000], Loss: 4573.5703125, Entropy -1500.70703125, Learning Rate: 0.05\n",
      "Epoch [340/20000], Loss: 4471.60009765625, Entropy -1495.103271484375, Learning Rate: 0.05\n",
      "Epoch [341/20000], Loss: 4552.1171875, Entropy -1345.00048828125, Learning Rate: 0.05\n",
      "Epoch [342/20000], Loss: 4718.6015625, Entropy -1508.58544921875, Learning Rate: 0.05\n",
      "Epoch [343/20000], Loss: 4638.57373046875, Entropy -1421.91357421875, Learning Rate: 0.05\n",
      "Epoch [344/20000], Loss: 4695.279296875, Entropy -1610.54296875, Learning Rate: 0.05\n",
      "Epoch [345/20000], Loss: 4715.63037109375, Entropy -1599.08447265625, Learning Rate: 0.05\n",
      "Epoch [346/20000], Loss: 4586.11279296875, Entropy -1498.61962890625, Learning Rate: 0.05\n",
      "Epoch [347/20000], Loss: 4609.990234375, Entropy -1550.363525390625, Learning Rate: 0.05\n",
      "Epoch [348/20000], Loss: 4484.6982421875, Entropy -1567.76318359375, Learning Rate: 0.05\n",
      "Epoch [349/20000], Loss: 4582.361328125, Entropy -1641.807373046875, Learning Rate: 0.05\n",
      "Epoch [350/20000], Loss: 4536.75146484375, Entropy -1451.4208984375, Learning Rate: 0.05\n",
      "Epoch [351/20000], Loss: 4569.02099609375, Entropy -1446.619384765625, Learning Rate: 0.05\n",
      "Epoch [352/20000], Loss: 4490.40185546875, Entropy -1423.85302734375, Learning Rate: 0.05\n",
      "Epoch [353/20000], Loss: 4729.48486328125, Entropy -1391.48583984375, Learning Rate: 0.05\n",
      "Epoch [354/20000], Loss: 4539.4599609375, Entropy -1553.92236328125, Learning Rate: 0.05\n",
      "Epoch [355/20000], Loss: 4494.60009765625, Entropy -1464.67626953125, Learning Rate: 0.05\n",
      "Epoch [356/20000], Loss: 4559.4345703125, Entropy -1454.0595703125, Learning Rate: 0.05\n",
      "Epoch [357/20000], Loss: 4656.1611328125, Entropy -1563.12451171875, Learning Rate: 0.05\n",
      "Epoch [358/20000], Loss: 4633.0263671875, Entropy -1591.778076171875, Learning Rate: 0.05\n",
      "Epoch [359/20000], Loss: 4614.28564453125, Entropy -1353.13720703125, Learning Rate: 0.05\n",
      "Epoch [360/20000], Loss: 4627.6416015625, Entropy -1599.077880859375, Learning Rate: 0.05\n",
      "Epoch [361/20000], Loss: 4524.26904296875, Entropy -1350.4814453125, Learning Rate: 0.05\n",
      "Epoch [362/20000], Loss: 4464.228515625, Entropy -1412.3486328125, Learning Rate: 0.05\n",
      "Epoch [363/20000], Loss: 4593.69677734375, Entropy -1519.96826171875, Learning Rate: 0.05\n",
      "Epoch [364/20000], Loss: 4861.44921875, Entropy -1583.2763671875, Learning Rate: 0.05\n",
      "Epoch [365/20000], Loss: 4602.81396484375, Entropy -1508.252685546875, Learning Rate: 0.05\n",
      "Epoch [366/20000], Loss: 4626.84716796875, Entropy -1417.45458984375, Learning Rate: 0.05\n",
      "Epoch [367/20000], Loss: 4543.171875, Entropy -1580.189453125, Learning Rate: 0.05\n",
      "Epoch [368/20000], Loss: 4679.3173828125, Entropy -1421.3115234375, Learning Rate: 0.05\n",
      "Epoch [369/20000], Loss: 4671.1767578125, Entropy -1471.18603515625, Learning Rate: 0.05\n",
      "Epoch [370/20000], Loss: 4500.701171875, Entropy -1542.69482421875, Learning Rate: 0.05\n",
      "Epoch [371/20000], Loss: 4566.53271484375, Entropy -1558.033203125, Learning Rate: 0.05\n",
      "Epoch [372/20000], Loss: 4663.25537109375, Entropy -1425.08642578125, Learning Rate: 0.05\n",
      "Epoch [373/20000], Loss: 4611.8447265625, Entropy -1463.14599609375, Learning Rate: 0.05\n",
      "Epoch [374/20000], Loss: 4610.62060546875, Entropy -1605.3759765625, Learning Rate: 0.05\n",
      "Epoch [375/20000], Loss: 4452.99462890625, Entropy -1489.2646484375, Learning Rate: 0.05\n",
      "Epoch [376/20000], Loss: 4528.26904296875, Entropy -1488.228271484375, Learning Rate: 0.05\n",
      "Epoch [377/20000], Loss: 4613.49609375, Entropy -1405.72021484375, Learning Rate: 0.05\n",
      "Epoch [378/20000], Loss: 4599.93701171875, Entropy -1509.563720703125, Learning Rate: 0.05\n",
      "Epoch [379/20000], Loss: 4616.4462890625, Entropy -1476.00830078125, Learning Rate: 0.05\n",
      "Epoch [380/20000], Loss: 4520.6123046875, Entropy -1483.783203125, Learning Rate: 0.05\n",
      "Epoch [381/20000], Loss: 4503.6015625, Entropy -1492.994873046875, Learning Rate: 0.05\n",
      "Epoch [382/20000], Loss: 4717.560546875, Entropy -1402.403076171875, Learning Rate: 0.05\n",
      "Epoch [383/20000], Loss: 4773.16796875, Entropy -1551.646240234375, Learning Rate: 0.05\n",
      "Epoch [384/20000], Loss: 4615.74365234375, Entropy -1532.36376953125, Learning Rate: 0.05\n",
      "Epoch [385/20000], Loss: 4681.82373046875, Entropy -1690.03125, Learning Rate: 0.05\n",
      "Epoch [386/20000], Loss: 4492.51416015625, Entropy -1557.940185546875, Learning Rate: 0.05\n",
      "Epoch [387/20000], Loss: 4551.58349609375, Entropy -1510.56298828125, Learning Rate: 0.05\n",
      "Epoch [388/20000], Loss: 4490.6640625, Entropy -1380.26611328125, Learning Rate: 0.05\n",
      "Epoch [389/20000], Loss: 4658.15478515625, Entropy -1453.43896484375, Learning Rate: 0.05\n",
      "Epoch [390/20000], Loss: 4552.13623046875, Entropy -1468.717529296875, Learning Rate: 0.05\n",
      "Epoch [391/20000], Loss: 4551.25830078125, Entropy -1474.2255859375, Learning Rate: 0.05\n",
      "Epoch [392/20000], Loss: 4594.44970703125, Entropy -1542.85791015625, Learning Rate: 0.05\n",
      "Epoch [393/20000], Loss: 4712.23046875, Entropy -1524.6474609375, Learning Rate: 0.05\n",
      "Epoch [394/20000], Loss: 4640.48193359375, Entropy -1399.158447265625, Learning Rate: 0.05\n",
      "Epoch [395/20000], Loss: 4514.8310546875, Entropy -1509.55517578125, Learning Rate: 0.05\n",
      "Epoch [396/20000], Loss: 4650.734375, Entropy -1466.62744140625, Learning Rate: 0.05\n",
      "Epoch [397/20000], Loss: 4487.12646484375, Entropy -1440.291259765625, Learning Rate: 0.05\n",
      "Epoch [398/20000], Loss: 4553.06298828125, Entropy -1494.6923828125, Learning Rate: 0.05\n",
      "Epoch [399/20000], Loss: 4510.7294921875, Entropy -1601.675537109375, Learning Rate: 0.05\n",
      "Epoch [400/20000], Loss: 4825.45751953125, Entropy -1428.17138671875, Learning Rate: 0.05\n",
      "Epoch [401/20000], Loss: 4547.1298828125, Entropy -1658.212646484375, Learning Rate: 0.05\n",
      "Epoch [402/20000], Loss: 4642.4716796875, Entropy -1426.75341796875, Learning Rate: 0.05\n",
      "Epoch [403/20000], Loss: 4548.4091796875, Entropy -1547.367431640625, Learning Rate: 0.05\n",
      "Epoch [404/20000], Loss: 4566.62109375, Entropy -1419.391845703125, Learning Rate: 0.05\n",
      "Epoch [405/20000], Loss: 4461.544921875, Entropy -1505.501220703125, Learning Rate: 0.05\n",
      "Epoch [406/20000], Loss: 4523.162109375, Entropy -1498.123291015625, Learning Rate: 0.05\n",
      "Epoch [407/20000], Loss: 4556.0341796875, Entropy -1603.539794921875, Learning Rate: 0.05\n",
      "Epoch [408/20000], Loss: 4590.5966796875, Entropy -1528.87890625, Learning Rate: 0.05\n",
      "Epoch [409/20000], Loss: 4736.64453125, Entropy -1739.849609375, Learning Rate: 0.05\n",
      "Epoch [410/20000], Loss: 4480.11962890625, Entropy -1501.78466796875, Learning Rate: 0.05\n",
      "Epoch [411/20000], Loss: 4615.4765625, Entropy -1536.65283203125, Learning Rate: 0.05\n",
      "Epoch [412/20000], Loss: 4668.45166015625, Entropy -1430.873046875, Learning Rate: 0.05\n",
      "Epoch [413/20000], Loss: 4617.9921875, Entropy -1517.2724609375, Learning Rate: 0.05\n",
      "Epoch [414/20000], Loss: 4514.3515625, Entropy -1543.045166015625, Learning Rate: 0.05\n",
      "Epoch [415/20000], Loss: 4551.64208984375, Entropy -1524.12060546875, Learning Rate: 0.05\n",
      "Epoch [416/20000], Loss: 4450.86767578125, Entropy -1586.604736328125, Learning Rate: 0.05\n",
      "Epoch [417/20000], Loss: 4571.56005859375, Entropy -1527.4677734375, Learning Rate: 0.05\n",
      "Epoch [418/20000], Loss: 4520.72216796875, Entropy -1394.5390625, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [419/20000], Loss: 4702.76953125, Entropy -1514.965087890625, Learning Rate: 0.05\n",
      "Epoch [420/20000], Loss: 4641.79248046875, Entropy -1698.807861328125, Learning Rate: 0.05\n",
      "Epoch [421/20000], Loss: 4507.853515625, Entropy -1494.18798828125, Learning Rate: 0.05\n",
      "Epoch [422/20000], Loss: 4667.18359375, Entropy -1520.654296875, Learning Rate: 0.05\n",
      "Epoch [423/20000], Loss: 4539.4384765625, Entropy -1533.814453125, Learning Rate: 0.05\n",
      "Epoch [424/20000], Loss: 4477.380859375, Entropy -1495.200927734375, Learning Rate: 0.05\n",
      "Epoch [425/20000], Loss: 4844.5390625, Entropy -1493.4248046875, Learning Rate: 0.05\n",
      "Epoch [426/20000], Loss: 4564.25732421875, Entropy -1597.63818359375, Learning Rate: 0.05\n",
      "Epoch [427/20000], Loss: 4601.27001953125, Entropy -1606.13671875, Learning Rate: 0.05\n",
      "Epoch [428/20000], Loss: 4588.65625, Entropy -1525.2626953125, Learning Rate: 0.05\n",
      "Epoch [429/20000], Loss: 4529.7529296875, Entropy -1616.0400390625, Learning Rate: 0.05\n",
      "Epoch [430/20000], Loss: 4536.72900390625, Entropy -1453.460205078125, Learning Rate: 0.05\n",
      "Epoch [431/20000], Loss: 4594.06640625, Entropy -1420.332763671875, Learning Rate: 0.05\n",
      "Epoch [432/20000], Loss: 4527.37451171875, Entropy -1433.128173828125, Learning Rate: 0.05\n",
      "Epoch [433/20000], Loss: 4616.328125, Entropy -1542.7998046875, Learning Rate: 0.05\n",
      "Epoch [434/20000], Loss: 4628.8076171875, Entropy -1601.433837890625, Learning Rate: 0.05\n",
      "Epoch [435/20000], Loss: 4485.45361328125, Entropy -1500.32373046875, Learning Rate: 0.05\n",
      "Epoch [436/20000], Loss: 4703.77197265625, Entropy -1587.02587890625, Learning Rate: 0.05\n",
      "Epoch [437/20000], Loss: 4497.10888671875, Entropy -1467.314453125, Learning Rate: 0.05\n",
      "Epoch [438/20000], Loss: 4604.0078125, Entropy -1489.904296875, Learning Rate: 0.05\n",
      "Epoch [439/20000], Loss: 4767.32177734375, Entropy -1577.0869140625, Learning Rate: 0.05\n",
      "Epoch [440/20000], Loss: 4556.169921875, Entropy -1528.71728515625, Learning Rate: 0.05\n",
      "Epoch [441/20000], Loss: 4528.6796875, Entropy -1578.45166015625, Learning Rate: 0.05\n",
      "Epoch [442/20000], Loss: 4538.0185546875, Entropy -1530.42626953125, Learning Rate: 0.05\n",
      "Epoch [443/20000], Loss: 4652.7734375, Entropy -1631.25244140625, Learning Rate: 0.05\n",
      "Epoch [444/20000], Loss: 4442.44287109375, Entropy -1567.086669921875, Learning Rate: 0.05\n",
      "Epoch [445/20000], Loss: 4656.86376953125, Entropy -1448.98876953125, Learning Rate: 0.05\n",
      "Epoch [446/20000], Loss: 4663.7470703125, Entropy -1511.750244140625, Learning Rate: 0.05\n",
      "Epoch [447/20000], Loss: 4578.39599609375, Entropy -1542.197998046875, Learning Rate: 0.05\n",
      "Epoch [448/20000], Loss: 4566.9296875, Entropy -1509.368408203125, Learning Rate: 0.05\n",
      "Epoch [449/20000], Loss: 4382.296875, Entropy -1469.977294921875, Learning Rate: 0.05\n",
      "Epoch [450/20000], Loss: 4469.9814453125, Entropy -1489.494140625, Learning Rate: 0.05\n",
      "Epoch [451/20000], Loss: 4620.11083984375, Entropy -1454.494140625, Learning Rate: 0.05\n",
      "Epoch [452/20000], Loss: 4549.11181640625, Entropy -1542.44287109375, Learning Rate: 0.05\n",
      "Epoch [453/20000], Loss: 4726.591796875, Entropy -1449.7685546875, Learning Rate: 0.05\n",
      "Epoch [454/20000], Loss: 4530.595703125, Entropy -1624.974853515625, Learning Rate: 0.05\n",
      "Epoch [455/20000], Loss: 4458.5576171875, Entropy -1542.96630859375, Learning Rate: 0.05\n",
      "Epoch [456/20000], Loss: 4504.81201171875, Entropy -1567.58740234375, Learning Rate: 0.05\n",
      "Epoch [457/20000], Loss: 4599.79833984375, Entropy -1664.445068359375, Learning Rate: 0.05\n",
      "Epoch [458/20000], Loss: 4565.02734375, Entropy -1540.99169921875, Learning Rate: 0.05\n",
      "Epoch [459/20000], Loss: 4453.41943359375, Entropy -1503.389404296875, Learning Rate: 0.05\n",
      "Epoch [460/20000], Loss: 4541.84228515625, Entropy -1460.138427734375, Learning Rate: 0.05\n",
      "Epoch [461/20000], Loss: 4524.9228515625, Entropy -1649.023193359375, Learning Rate: 0.05\n",
      "Epoch [462/20000], Loss: 4617.88818359375, Entropy -1525.901611328125, Learning Rate: 0.05\n",
      "Epoch [463/20000], Loss: 4495.19482421875, Entropy -1530.23876953125, Learning Rate: 0.05\n",
      "Epoch [464/20000], Loss: 4433.44189453125, Entropy -1434.076171875, Learning Rate: 0.05\n",
      "Epoch [465/20000], Loss: 4545.71630859375, Entropy -1698.447265625, Learning Rate: 0.05\n",
      "Epoch [466/20000], Loss: 4816.4140625, Entropy -1578.76953125, Learning Rate: 0.05\n",
      "Epoch [467/20000], Loss: 4506.31591796875, Entropy -1424.13818359375, Learning Rate: 0.05\n",
      "Epoch [468/20000], Loss: 4606.359375, Entropy -1579.95458984375, Learning Rate: 0.05\n",
      "Epoch [469/20000], Loss: 4536.3916015625, Entropy -1420.74169921875, Learning Rate: 0.05\n",
      "Epoch [470/20000], Loss: 4553.458984375, Entropy -1582.9833984375, Learning Rate: 0.05\n",
      "Epoch [471/20000], Loss: 4586.5205078125, Entropy -1615.72998046875, Learning Rate: 0.05\n",
      "Epoch [472/20000], Loss: 4445.1982421875, Entropy -1627.82861328125, Learning Rate: 0.05\n",
      "Epoch [473/20000], Loss: 4478.416015625, Entropy -1444.2080078125, Learning Rate: 0.05\n",
      "Epoch [474/20000], Loss: 4535.7568359375, Entropy -1484.572021484375, Learning Rate: 0.05\n",
      "Epoch [475/20000], Loss: 4624.7509765625, Entropy -1497.11865234375, Learning Rate: 0.05\n",
      "Epoch [476/20000], Loss: 4566.13134765625, Entropy -1584.84912109375, Learning Rate: 0.05\n",
      "Epoch [477/20000], Loss: 4509.658203125, Entropy -1586.017822265625, Learning Rate: 0.05\n",
      "Epoch [478/20000], Loss: 4528.6279296875, Entropy -1597.384765625, Learning Rate: 0.05\n",
      "Epoch [479/20000], Loss: 4536.9912109375, Entropy -1538.91015625, Learning Rate: 0.05\n",
      "Epoch [480/20000], Loss: 4531.517578125, Entropy -1566.800048828125, Learning Rate: 0.05\n",
      "Epoch [481/20000], Loss: 4811.66259765625, Entropy -1566.35498046875, Learning Rate: 0.05\n",
      "Epoch [482/20000], Loss: 4485.06982421875, Entropy -1687.86279296875, Learning Rate: 0.05\n",
      "Epoch [483/20000], Loss: 4603.22802734375, Entropy -1524.591796875, Learning Rate: 0.05\n",
      "Epoch [484/20000], Loss: 4424.9521484375, Entropy -1392.749755859375, Learning Rate: 0.05\n",
      "Epoch [485/20000], Loss: 4647.54931640625, Entropy -1581.37158203125, Learning Rate: 0.05\n",
      "Epoch [486/20000], Loss: 4524.2080078125, Entropy -1651.29052734375, Learning Rate: 0.05\n",
      "Epoch [487/20000], Loss: 4582.93212890625, Entropy -1575.098388671875, Learning Rate: 0.05\n",
      "Epoch [488/20000], Loss: 4544.7705078125, Entropy -1384.529296875, Learning Rate: 0.05\n",
      "Epoch [489/20000], Loss: 4559.4609375, Entropy -1630.55908203125, Learning Rate: 0.05\n",
      "Epoch [490/20000], Loss: 4468.6962890625, Entropy -1513.079345703125, Learning Rate: 0.05\n",
      "Epoch [491/20000], Loss: 4609.3720703125, Entropy -1462.467529296875, Learning Rate: 0.05\n",
      "Epoch [492/20000], Loss: 4528.6259765625, Entropy -1557.78564453125, Learning Rate: 0.05\n",
      "Epoch [493/20000], Loss: 4459.083984375, Entropy -1438.941162109375, Learning Rate: 0.05\n",
      "Epoch [494/20000], Loss: 4535.927734375, Entropy -1479.43798828125, Learning Rate: 0.05\n",
      "Epoch [495/20000], Loss: 4439.9443359375, Entropy -1423.51953125, Learning Rate: 0.05\n",
      "Epoch [496/20000], Loss: 4658.375, Entropy -1543.624755859375, Learning Rate: 0.05\n",
      "Epoch [497/20000], Loss: 4497.02734375, Entropy -1487.3271484375, Learning Rate: 0.05\n",
      "Epoch [498/20000], Loss: 4728.98095703125, Entropy -1644.873779296875, Learning Rate: 0.05\n",
      "Epoch [499/20000], Loss: 4527.65380859375, Entropy -1439.43115234375, Learning Rate: 0.05\n",
      "Epoch [500/20000], Loss: 4554.93359375, Entropy -1442.8046875, Learning Rate: 0.05\n",
      "Epoch [501/20000], Loss: 4476.85302734375, Entropy -1518.6552734375, Learning Rate: 0.05\n",
      "Epoch [502/20000], Loss: 4407.10107421875, Entropy -1410.24853515625, Learning Rate: 0.05\n",
      "Epoch [503/20000], Loss: 4514.97802734375, Entropy -1429.25830078125, Learning Rate: 0.05\n",
      "Epoch [504/20000], Loss: 4477.3134765625, Entropy -1477.68505859375, Learning Rate: 0.05\n",
      "Epoch [505/20000], Loss: 4523.27734375, Entropy -1570.6865234375, Learning Rate: 0.05\n",
      "Epoch [506/20000], Loss: 4515.359375, Entropy -1491.149169921875, Learning Rate: 0.05\n",
      "Epoch [507/20000], Loss: 4535.994140625, Entropy -1516.42333984375, Learning Rate: 0.05\n",
      "Epoch [508/20000], Loss: 4447.54833984375, Entropy -1369.115234375, Learning Rate: 0.05\n",
      "Epoch [509/20000], Loss: 4586.43701171875, Entropy -1648.642578125, Learning Rate: 0.05\n",
      "Epoch [510/20000], Loss: 4566.04638671875, Entropy -1620.2060546875, Learning Rate: 0.05\n",
      "Epoch [511/20000], Loss: 4750.51953125, Entropy -1693.39208984375, Learning Rate: 0.05\n",
      "Epoch [512/20000], Loss: 4610.2314453125, Entropy -1540.8466796875, Learning Rate: 0.05\n",
      "Epoch [513/20000], Loss: 4422.2802734375, Entropy -1566.550048828125, Learning Rate: 0.05\n",
      "Epoch [514/20000], Loss: 4505.0390625, Entropy -1566.84765625, Learning Rate: 0.05\n",
      "Epoch [515/20000], Loss: 4450.7158203125, Entropy -1511.43994140625, Learning Rate: 0.05\n",
      "Epoch [516/20000], Loss: 4402.494140625, Entropy -1472.578125, Learning Rate: 0.05\n",
      "Epoch [517/20000], Loss: 4611.25146484375, Entropy -1598.69384765625, Learning Rate: 0.05\n",
      "Epoch [518/20000], Loss: 4556.1806640625, Entropy -1599.92626953125, Learning Rate: 0.05\n",
      "Epoch [519/20000], Loss: 4750.19287109375, Entropy -1512.135498046875, Learning Rate: 0.05\n",
      "Epoch [520/20000], Loss: 4482.3447265625, Entropy -1555.71533203125, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [521/20000], Loss: 4689.26220703125, Entropy -1620.975830078125, Learning Rate: 0.05\n",
      "Epoch [522/20000], Loss: 4486.03857421875, Entropy -1535.2314453125, Learning Rate: 0.05\n",
      "Epoch [523/20000], Loss: 4628.279296875, Entropy -1559.988525390625, Learning Rate: 0.05\n",
      "Epoch [524/20000], Loss: 4519.181640625, Entropy -1616.01953125, Learning Rate: 0.05\n",
      "Epoch [525/20000], Loss: 4540.412109375, Entropy -1627.74169921875, Learning Rate: 0.05\n",
      "Epoch [526/20000], Loss: 4431.19873046875, Entropy -1472.62939453125, Learning Rate: 0.05\n",
      "Epoch [527/20000], Loss: 4604.13720703125, Entropy -1682.44384765625, Learning Rate: 0.05\n",
      "Epoch [528/20000], Loss: 4430.73388671875, Entropy -1468.158203125, Learning Rate: 0.05\n",
      "Epoch [529/20000], Loss: 4427.9013671875, Entropy -1493.2880859375, Learning Rate: 0.05\n",
      "Epoch [530/20000], Loss: 4520.12646484375, Entropy -1459.7626953125, Learning Rate: 0.05\n",
      "Epoch [531/20000], Loss: 4582.32275390625, Entropy -1621.96142578125, Learning Rate: 0.05\n",
      "Epoch [532/20000], Loss: 4426.62744140625, Entropy -1449.5869140625, Learning Rate: 0.05\n",
      "Epoch [533/20000], Loss: 4403.45751953125, Entropy -1462.625244140625, Learning Rate: 0.05\n",
      "Epoch [534/20000], Loss: 4491.6826171875, Entropy -1607.671630859375, Learning Rate: 0.05\n",
      "Epoch [535/20000], Loss: 4610.3515625, Entropy -1648.794677734375, Learning Rate: 0.05\n",
      "Epoch [536/20000], Loss: 4475.4111328125, Entropy -1474.55029296875, Learning Rate: 0.05\n",
      "Epoch [537/20000], Loss: 4584.44775390625, Entropy -1585.185302734375, Learning Rate: 0.05\n",
      "Epoch [538/20000], Loss: 4525.15576171875, Entropy -1584.88818359375, Learning Rate: 0.05\n",
      "Epoch [539/20000], Loss: 4652.7236328125, Entropy -1471.95263671875, Learning Rate: 0.05\n",
      "Epoch [540/20000], Loss: 4487.74169921875, Entropy -1491.087890625, Learning Rate: 0.05\n",
      "Epoch [541/20000], Loss: 4498.9990234375, Entropy -1535.972412109375, Learning Rate: 0.05\n",
      "Epoch [542/20000], Loss: 4406.0302734375, Entropy -1452.826171875, Learning Rate: 0.05\n",
      "Epoch [543/20000], Loss: 4426.509765625, Entropy -1448.148681640625, Learning Rate: 0.05\n",
      "Epoch [544/20000], Loss: 4429.421875, Entropy -1466.249267578125, Learning Rate: 0.05\n",
      "Epoch [545/20000], Loss: 4592.05712890625, Entropy -1491.22216796875, Learning Rate: 0.05\n",
      "Epoch [546/20000], Loss: 4556.92333984375, Entropy -1606.873291015625, Learning Rate: 0.05\n",
      "Epoch [547/20000], Loss: 4533.79638671875, Entropy -1413.1044921875, Learning Rate: 0.05\n",
      "Epoch [548/20000], Loss: 4583.07958984375, Entropy -1682.34619140625, Learning Rate: 0.05\n",
      "Epoch [549/20000], Loss: 4486.6875, Entropy -1417.77587890625, Learning Rate: 0.05\n",
      "Epoch [550/20000], Loss: 4437.08984375, Entropy -1506.50390625, Learning Rate: 0.05\n",
      "Epoch [551/20000], Loss: 4457.16015625, Entropy -1580.115234375, Learning Rate: 0.05\n",
      "Epoch [552/20000], Loss: 4417.04541015625, Entropy -1469.623291015625, Learning Rate: 0.05\n",
      "Epoch [553/20000], Loss: 4431.73779296875, Entropy -1450.1337890625, Learning Rate: 0.05\n",
      "Epoch [554/20000], Loss: 4424.96875, Entropy -1554.6787109375, Learning Rate: 0.05\n",
      "Epoch [555/20000], Loss: 4385.73876953125, Entropy -1480.55322265625, Learning Rate: 0.05\n",
      "Epoch [556/20000], Loss: 4414.89013671875, Entropy -1557.809326171875, Learning Rate: 0.05\n",
      "Epoch [557/20000], Loss: 4566.32666015625, Entropy -1555.666015625, Learning Rate: 0.05\n",
      "Epoch [558/20000], Loss: 4499.67578125, Entropy -1633.362548828125, Learning Rate: 0.05\n",
      "Epoch [559/20000], Loss: 4499.123046875, Entropy -1513.63623046875, Learning Rate: 0.05\n",
      "Epoch [560/20000], Loss: 4576.65576171875, Entropy -1499.76025390625, Learning Rate: 0.05\n",
      "Epoch [561/20000], Loss: 4618.1875, Entropy -1550.851806640625, Learning Rate: 0.05\n",
      "Epoch [562/20000], Loss: 4588.5517578125, Entropy -1424.1455078125, Learning Rate: 0.05\n",
      "Epoch [563/20000], Loss: 4371.51904296875, Entropy -1509.0390625, Learning Rate: 0.05\n",
      "Epoch [564/20000], Loss: 4482.3974609375, Entropy -1590.462890625, Learning Rate: 0.05\n",
      "Epoch [565/20000], Loss: 4554.171875, Entropy -1637.595947265625, Learning Rate: 0.05\n",
      "Epoch [566/20000], Loss: 4483.73486328125, Entropy -1546.031982421875, Learning Rate: 0.05\n",
      "Epoch [567/20000], Loss: 4500.47216796875, Entropy -1429.41064453125, Learning Rate: 0.05\n",
      "Epoch [568/20000], Loss: 4488.79638671875, Entropy -1391.6005859375, Learning Rate: 0.05\n",
      "Epoch [569/20000], Loss: 4408.2880859375, Entropy -1475.185546875, Learning Rate: 0.05\n",
      "Epoch [570/20000], Loss: 4517.091796875, Entropy -1533.089111328125, Learning Rate: 0.05\n",
      "Epoch [571/20000], Loss: 4691.32080078125, Entropy -1533.14208984375, Learning Rate: 0.05\n",
      "Epoch [572/20000], Loss: 4504.8115234375, Entropy -1518.558837890625, Learning Rate: 0.05\n",
      "Epoch [573/20000], Loss: 4464.43212890625, Entropy -1528.009765625, Learning Rate: 0.05\n",
      "Epoch [574/20000], Loss: 4590.03564453125, Entropy -1337.26806640625, Learning Rate: 0.05\n",
      "Epoch [575/20000], Loss: 4424.52197265625, Entropy -1501.47412109375, Learning Rate: 0.05\n",
      "Epoch [576/20000], Loss: 4438.00244140625, Entropy -1470.92041015625, Learning Rate: 0.05\n",
      "Epoch [577/20000], Loss: 4380.98388671875, Entropy -1435.418212890625, Learning Rate: 0.05\n",
      "Epoch [578/20000], Loss: 4745.79052734375, Entropy -1591.9990234375, Learning Rate: 0.05\n",
      "Epoch [579/20000], Loss: 4389.3330078125, Entropy -1457.68212890625, Learning Rate: 0.05\n",
      "Epoch [580/20000], Loss: 4488.43994140625, Entropy -1433.40966796875, Learning Rate: 0.05\n",
      "Epoch [581/20000], Loss: 4440.80126953125, Entropy -1510.919921875, Learning Rate: 0.05\n",
      "Epoch [582/20000], Loss: 4525.25, Entropy -1488.529296875, Learning Rate: 0.05\n",
      "Epoch [583/20000], Loss: 4584.74609375, Entropy -1694.89892578125, Learning Rate: 0.05\n",
      "Epoch [584/20000], Loss: 4516.5205078125, Entropy -1591.8408203125, Learning Rate: 0.05\n",
      "Epoch [585/20000], Loss: 4504.02587890625, Entropy -1553.3798828125, Learning Rate: 0.05\n",
      "Epoch [586/20000], Loss: 4512.29638671875, Entropy -1425.88671875, Learning Rate: 0.05\n",
      "Epoch [587/20000], Loss: 4440.6943359375, Entropy -1601.80322265625, Learning Rate: 0.05\n",
      "Epoch [588/20000], Loss: 4467.3994140625, Entropy -1435.8125, Learning Rate: 0.05\n",
      "Epoch [589/20000], Loss: 4455.57861328125, Entropy -1509.632080078125, Learning Rate: 0.05\n",
      "Epoch [590/20000], Loss: 4554.64794921875, Entropy -1598.39990234375, Learning Rate: 0.05\n",
      "Epoch [591/20000], Loss: 4493.38330078125, Entropy -1553.1904296875, Learning Rate: 0.05\n",
      "Epoch [592/20000], Loss: 4469.64453125, Entropy -1636.880859375, Learning Rate: 0.05\n",
      "Epoch [593/20000], Loss: 4438.77880859375, Entropy -1498.509033203125, Learning Rate: 0.05\n",
      "Epoch [594/20000], Loss: 4456.38720703125, Entropy -1465.364501953125, Learning Rate: 0.05\n",
      "Epoch [595/20000], Loss: 4425.6552734375, Entropy -1516.86865234375, Learning Rate: 0.05\n",
      "Epoch [596/20000], Loss: 4708.73583984375, Entropy -1493.90380859375, Learning Rate: 0.05\n",
      "Epoch [597/20000], Loss: 4496.74658203125, Entropy -1652.833984375, Learning Rate: 0.05\n",
      "Epoch [598/20000], Loss: 4538.36474609375, Entropy -1561.82861328125, Learning Rate: 0.05\n",
      "Epoch [599/20000], Loss: 4522.81005859375, Entropy -1470.7080078125, Learning Rate: 0.05\n",
      "Epoch [600/20000], Loss: 4477.63671875, Entropy -1524.75, Learning Rate: 0.05\n",
      "Epoch [601/20000], Loss: 4477.26708984375, Entropy -1629.925048828125, Learning Rate: 0.05\n",
      "Epoch [602/20000], Loss: 4490.6259765625, Entropy -1570.988525390625, Learning Rate: 0.05\n",
      "Epoch [603/20000], Loss: 4466.1728515625, Entropy -1520.9990234375, Learning Rate: 0.05\n",
      "Epoch [604/20000], Loss: 4535.08740234375, Entropy -1576.822265625, Learning Rate: 0.05\n",
      "Epoch [605/20000], Loss: 4422.77685546875, Entropy -1320.62158203125, Learning Rate: 0.05\n",
      "Epoch [606/20000], Loss: 4501.28759765625, Entropy -1669.3671875, Learning Rate: 0.05\n",
      "Epoch [607/20000], Loss: 4432.9521484375, Entropy -1449.82861328125, Learning Rate: 0.05\n",
      "Epoch [608/20000], Loss: 4773.669921875, Entropy -1566.058837890625, Learning Rate: 0.05\n",
      "Epoch [609/20000], Loss: 4608.11279296875, Entropy -1550.338623046875, Learning Rate: 0.05\n",
      "Epoch [610/20000], Loss: 4431.9765625, Entropy -1480.508544921875, Learning Rate: 0.05\n",
      "Epoch [611/20000], Loss: 4505.8994140625, Entropy -1666.04443359375, Learning Rate: 0.05\n",
      "Epoch [612/20000], Loss: 4572.00830078125, Entropy -1604.99853515625, Learning Rate: 0.05\n",
      "Epoch [613/20000], Loss: 4481.27197265625, Entropy -1513.93603515625, Learning Rate: 0.05\n",
      "Epoch [614/20000], Loss: 4472.0390625, Entropy -1552.11181640625, Learning Rate: 0.05\n",
      "Epoch [615/20000], Loss: 4339.11669921875, Entropy -1424.21337890625, Learning Rate: 0.05\n",
      "Epoch [616/20000], Loss: 4530.28125, Entropy -1564.6240234375, Learning Rate: 0.05\n",
      "Epoch [617/20000], Loss: 4533.2490234375, Entropy -1581.464111328125, Learning Rate: 0.05\n",
      "Epoch [618/20000], Loss: 4463.7421875, Entropy -1718.9208984375, Learning Rate: 0.05\n",
      "Epoch [619/20000], Loss: 4614.0517578125, Entropy -1593.81005859375, Learning Rate: 0.05\n",
      "Epoch [620/20000], Loss: 4447.90966796875, Entropy -1531.86962890625, Learning Rate: 0.05\n",
      "Epoch [621/20000], Loss: 4484.64111328125, Entropy -1588.30078125, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [622/20000], Loss: 4458.025390625, Entropy -1478.25146484375, Learning Rate: 0.05\n",
      "Epoch [623/20000], Loss: 4474.66064453125, Entropy -1596.734130859375, Learning Rate: 0.05\n",
      "Epoch [624/20000], Loss: 4492.4755859375, Entropy -1384.58642578125, Learning Rate: 0.05\n",
      "Epoch [625/20000], Loss: 4499.236328125, Entropy -1558.16943359375, Learning Rate: 0.05\n",
      "Epoch [626/20000], Loss: 4547.9375, Entropy -1593.452880859375, Learning Rate: 0.05\n",
      "Epoch [627/20000], Loss: 4462.2822265625, Entropy -1517.4111328125, Learning Rate: 0.05\n",
      "Epoch [628/20000], Loss: 4469.9697265625, Entropy -1552.320068359375, Learning Rate: 0.05\n",
      "Epoch [629/20000], Loss: 4506.40380859375, Entropy -1492.229736328125, Learning Rate: 0.05\n",
      "Epoch [630/20000], Loss: 4404.41162109375, Entropy -1426.079833984375, Learning Rate: 0.05\n",
      "Epoch [631/20000], Loss: 4473.75146484375, Entropy -1555.669677734375, Learning Rate: 0.05\n",
      "Epoch [632/20000], Loss: 4556.23291015625, Entropy -1625.6396484375, Learning Rate: 0.05\n",
      "Epoch [633/20000], Loss: 4551.2578125, Entropy -1664.578369140625, Learning Rate: 0.05\n",
      "Epoch [634/20000], Loss: 4570.3603515625, Entropy -1568.81201171875, Learning Rate: 0.05\n",
      "Epoch [635/20000], Loss: 4453.220703125, Entropy -1533.9462890625, Learning Rate: 0.05\n",
      "Epoch [636/20000], Loss: 4353.236328125, Entropy -1364.9306640625, Learning Rate: 0.05\n",
      "Epoch [637/20000], Loss: 4484.78466796875, Entropy -1555.755859375, Learning Rate: 0.05\n",
      "Epoch [638/20000], Loss: 4506.6796875, Entropy -1528.05078125, Learning Rate: 0.05\n",
      "Epoch [639/20000], Loss: 4495.50390625, Entropy -1423.69287109375, Learning Rate: 0.05\n",
      "Epoch [640/20000], Loss: 4443.84521484375, Entropy -1486.28369140625, Learning Rate: 0.05\n",
      "Epoch [641/20000], Loss: 4464.27783203125, Entropy -1529.38720703125, Learning Rate: 0.05\n",
      "Epoch [642/20000], Loss: 4413.9619140625, Entropy -1494.33154296875, Learning Rate: 0.05\n",
      "Epoch [643/20000], Loss: 4461.96142578125, Entropy -1545.36328125, Learning Rate: 0.05\n",
      "Epoch [644/20000], Loss: 4443.22119140625, Entropy -1539.6357421875, Learning Rate: 0.05\n",
      "Epoch [645/20000], Loss: 4494.64990234375, Entropy -1473.86181640625, Learning Rate: 0.05\n",
      "Epoch [646/20000], Loss: 4604.0234375, Entropy -1646.6416015625, Learning Rate: 0.05\n",
      "Epoch [647/20000], Loss: 4443.12158203125, Entropy -1569.91162109375, Learning Rate: 0.05\n",
      "Epoch [648/20000], Loss: 4479.80322265625, Entropy -1513.224609375, Learning Rate: 0.05\n",
      "Epoch [649/20000], Loss: 4425.58544921875, Entropy -1536.19580078125, Learning Rate: 0.05\n",
      "Epoch [650/20000], Loss: 4507.85888671875, Entropy -1441.798828125, Learning Rate: 0.05\n",
      "Epoch [651/20000], Loss: 4557.51416015625, Entropy -1549.5009765625, Learning Rate: 0.05\n",
      "Epoch [652/20000], Loss: 4536.08349609375, Entropy -1516.3095703125, Learning Rate: 0.05\n",
      "Epoch [653/20000], Loss: 4595.638671875, Entropy -1731.34716796875, Learning Rate: 0.05\n",
      "Epoch [654/20000], Loss: 4421.486328125, Entropy -1484.95849609375, Learning Rate: 0.05\n",
      "Epoch [655/20000], Loss: 4425.54443359375, Entropy -1612.521484375, Learning Rate: 0.05\n",
      "Epoch [656/20000], Loss: 4462.458984375, Entropy -1555.46044921875, Learning Rate: 0.05\n",
      "Epoch [657/20000], Loss: 4667.27783203125, Entropy -1371.67333984375, Learning Rate: 0.05\n",
      "Epoch [658/20000], Loss: 4474.4931640625, Entropy -1562.61767578125, Learning Rate: 0.05\n",
      "Epoch [659/20000], Loss: 4466.9580078125, Entropy -1407.0234375, Learning Rate: 0.05\n",
      "Epoch [660/20000], Loss: 4494.888671875, Entropy -1606.43017578125, Learning Rate: 0.05\n",
      "Epoch [661/20000], Loss: 4469.5947265625, Entropy -1526.9580078125, Learning Rate: 0.05\n",
      "Epoch [662/20000], Loss: 4529.8369140625, Entropy -1556.41748046875, Learning Rate: 0.05\n",
      "Epoch [663/20000], Loss: 4457.06591796875, Entropy -1424.121826171875, Learning Rate: 0.05\n",
      "Epoch [664/20000], Loss: 4399.96630859375, Entropy -1425.3798828125, Learning Rate: 0.05\n",
      "Epoch [665/20000], Loss: 4548.7666015625, Entropy -1627.425048828125, Learning Rate: 0.05\n",
      "Epoch [666/20000], Loss: 4393.79931640625, Entropy -1424.5283203125, Learning Rate: 0.05\n",
      "Epoch [667/20000], Loss: 4479.263671875, Entropy -1326.5615234375, Learning Rate: 0.05\n",
      "Epoch [668/20000], Loss: 4414.39990234375, Entropy -1455.99853515625, Learning Rate: 0.05\n",
      "Epoch [669/20000], Loss: 4433.3310546875, Entropy -1581.475341796875, Learning Rate: 0.05\n",
      "Epoch [670/20000], Loss: 4492.60107421875, Entropy -1540.4921875, Learning Rate: 0.05\n",
      "Epoch [671/20000], Loss: 4417.78125, Entropy -1457.74072265625, Learning Rate: 0.05\n",
      "Epoch [672/20000], Loss: 4464.1123046875, Entropy -1566.35888671875, Learning Rate: 0.05\n",
      "Epoch [673/20000], Loss: 4465.896484375, Entropy -1525.556640625, Learning Rate: 0.05\n",
      "Epoch [674/20000], Loss: 5091.7451171875, Entropy -1533.578125, Learning Rate: 0.05\n",
      "Epoch [675/20000], Loss: 4498.09716796875, Entropy -1568.472412109375, Learning Rate: 0.05\n",
      "Epoch [676/20000], Loss: 4422.01171875, Entropy -1456.617919921875, Learning Rate: 0.05\n",
      "Epoch [677/20000], Loss: 4477.72265625, Entropy -1553.20947265625, Learning Rate: 0.05\n",
      "Epoch [678/20000], Loss: 4471.68994140625, Entropy -1582.395751953125, Learning Rate: 0.05\n",
      "Epoch [679/20000], Loss: 4544.40673828125, Entropy -1589.082763671875, Learning Rate: 0.05\n",
      "Epoch [680/20000], Loss: 4439.1953125, Entropy -1606.2080078125, Learning Rate: 0.05\n",
      "Epoch [681/20000], Loss: 4484.1396484375, Entropy -1529.83349609375, Learning Rate: 0.05\n",
      "Epoch [682/20000], Loss: 4420.8984375, Entropy -1424.358642578125, Learning Rate: 0.05\n",
      "Epoch [683/20000], Loss: 4421.24609375, Entropy -1465.4189453125, Learning Rate: 0.05\n",
      "Epoch [684/20000], Loss: 4389.2158203125, Entropy -1446.30615234375, Learning Rate: 0.05\n",
      "Epoch [685/20000], Loss: 4534.62109375, Entropy -1492.77490234375, Learning Rate: 0.05\n",
      "Epoch [686/20000], Loss: 4470.31640625, Entropy -1599.685546875, Learning Rate: 0.05\n",
      "Epoch [687/20000], Loss: 4500.87451171875, Entropy -1448.49072265625, Learning Rate: 0.05\n",
      "Epoch [688/20000], Loss: 4530.740234375, Entropy -1544.22802734375, Learning Rate: 0.05\n",
      "Epoch [689/20000], Loss: 4466.052734375, Entropy -1442.20361328125, Learning Rate: 0.05\n",
      "Epoch [690/20000], Loss: 4568.10107421875, Entropy -1519.5712890625, Learning Rate: 0.05\n",
      "Epoch [691/20000], Loss: 4468.46875, Entropy -1462.99755859375, Learning Rate: 0.05\n",
      "Epoch [692/20000], Loss: 4406.7080078125, Entropy -1355.12939453125, Learning Rate: 0.05\n",
      "Epoch [693/20000], Loss: 4399.2158203125, Entropy -1414.126708984375, Learning Rate: 0.05\n",
      "Epoch [694/20000], Loss: 4416.623046875, Entropy -1520.05615234375, Learning Rate: 0.05\n",
      "Epoch [695/20000], Loss: 4440.43115234375, Entropy -1521.654296875, Learning Rate: 0.05\n",
      "Epoch [696/20000], Loss: 4397.4091796875, Entropy -1382.3916015625, Learning Rate: 0.05\n",
      "Epoch [697/20000], Loss: 4394.88623046875, Entropy -1307.841796875, Learning Rate: 0.05\n",
      "Epoch [698/20000], Loss: 4493.9306640625, Entropy -1498.733642578125, Learning Rate: 0.05\n",
      "Epoch [699/20000], Loss: 4422.26416015625, Entropy -1573.03662109375, Learning Rate: 0.05\n",
      "Epoch [700/20000], Loss: 4372.54150390625, Entropy -1416.58642578125, Learning Rate: 0.05\n",
      "Epoch [701/20000], Loss: 4483.0556640625, Entropy -1502.521728515625, Learning Rate: 0.05\n",
      "Epoch [702/20000], Loss: 4405.29638671875, Entropy -1494.62646484375, Learning Rate: 0.05\n",
      "Epoch [703/20000], Loss: 4478.48193359375, Entropy -1609.038818359375, Learning Rate: 0.05\n",
      "Epoch [704/20000], Loss: 4604.18212890625, Entropy -1533.94873046875, Learning Rate: 0.05\n",
      "Epoch [705/20000], Loss: 4398.3251953125, Entropy -1476.08642578125, Learning Rate: 0.05\n",
      "Epoch [706/20000], Loss: 4474.28759765625, Entropy -1400.919921875, Learning Rate: 0.05\n",
      "Epoch [707/20000], Loss: 4472.51708984375, Entropy -1588.2978515625, Learning Rate: 0.05\n",
      "Epoch [708/20000], Loss: 4428.50439453125, Entropy -1573.12841796875, Learning Rate: 0.05\n",
      "Epoch [709/20000], Loss: 4532.33349609375, Entropy -1639.541748046875, Learning Rate: 0.05\n",
      "Epoch [710/20000], Loss: 4490.24560546875, Entropy -1559.0732421875, Learning Rate: 0.05\n",
      "Epoch [711/20000], Loss: 4421.77099609375, Entropy -1565.52197265625, Learning Rate: 0.05\n",
      "Epoch [712/20000], Loss: 4455.0322265625, Entropy -1665.234375, Learning Rate: 0.05\n",
      "Epoch [713/20000], Loss: 4503.7138671875, Entropy -1527.890380859375, Learning Rate: 0.05\n",
      "Epoch [714/20000], Loss: 4405.16748046875, Entropy -1465.061767578125, Learning Rate: 0.05\n",
      "Epoch [715/20000], Loss: 4643.771484375, Entropy -1642.112548828125, Learning Rate: 0.05\n",
      "Epoch [716/20000], Loss: 4417.9892578125, Entropy -1536.4306640625, Learning Rate: 0.05\n",
      "Epoch [717/20000], Loss: 4576.03369140625, Entropy -1713.62353515625, Learning Rate: 0.05\n",
      "Epoch [718/20000], Loss: 4401.03125, Entropy -1512.141845703125, Learning Rate: 0.05\n",
      "Epoch [719/20000], Loss: 4446.07470703125, Entropy -1425.129150390625, Learning Rate: 0.05\n",
      "Epoch [720/20000], Loss: 4425.92626953125, Entropy -1495.0771484375, Learning Rate: 0.05\n",
      "Epoch [721/20000], Loss: 4411.01904296875, Entropy -1478.870361328125, Learning Rate: 0.05\n",
      "Epoch [722/20000], Loss: 4457.64697265625, Entropy -1595.912353515625, Learning Rate: 0.05\n",
      "Epoch [723/20000], Loss: 4465.69921875, Entropy -1541.441162109375, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [724/20000], Loss: 4411.30908203125, Entropy -1549.25634765625, Learning Rate: 0.05\n",
      "Epoch [725/20000], Loss: 4412.6494140625, Entropy -1315.45166015625, Learning Rate: 0.05\n",
      "Epoch [726/20000], Loss: 4391.04296875, Entropy -1484.123291015625, Learning Rate: 0.05\n",
      "Epoch [727/20000], Loss: 4451.11279296875, Entropy -1533.27587890625, Learning Rate: 0.05\n",
      "Epoch [728/20000], Loss: 4419.798828125, Entropy -1412.7626953125, Learning Rate: 0.05\n",
      "Epoch [729/20000], Loss: 4460.85888671875, Entropy -1477.859619140625, Learning Rate: 0.05\n",
      "Epoch [730/20000], Loss: 4430.96484375, Entropy -1549.90478515625, Learning Rate: 0.05\n",
      "Epoch [731/20000], Loss: 4461.80908203125, Entropy -1442.001708984375, Learning Rate: 0.05\n",
      "Epoch [732/20000], Loss: 4483.77294921875, Entropy -1606.767333984375, Learning Rate: 0.05\n",
      "Epoch [733/20000], Loss: 4398.66943359375, Entropy -1585.62255859375, Learning Rate: 0.05\n",
      "Epoch [734/20000], Loss: 4282.76220703125, Entropy -1362.29443359375, Learning Rate: 0.05\n",
      "Epoch [735/20000], Loss: 4495.31591796875, Entropy -1536.64404296875, Learning Rate: 0.05\n",
      "Epoch [736/20000], Loss: 4501.22802734375, Entropy -1506.251953125, Learning Rate: 0.05\n",
      "Epoch [737/20000], Loss: 4448.77197265625, Entropy -1511.8330078125, Learning Rate: 0.05\n",
      "Epoch [738/20000], Loss: 4530.77392578125, Entropy -1421.3271484375, Learning Rate: 0.05\n",
      "Epoch [739/20000], Loss: 4575.23974609375, Entropy -1582.394287109375, Learning Rate: 0.05\n",
      "Epoch [740/20000], Loss: 4432.68017578125, Entropy -1515.510986328125, Learning Rate: 0.05\n",
      "Epoch [741/20000], Loss: 4538.080078125, Entropy -1473.232177734375, Learning Rate: 0.05\n",
      "Epoch [742/20000], Loss: 4468.361328125, Entropy -1484.3818359375, Learning Rate: 0.05\n",
      "Epoch [743/20000], Loss: 4405.5966796875, Entropy -1382.2978515625, Learning Rate: 0.05\n",
      "Epoch [744/20000], Loss: 4435.03564453125, Entropy -1520.3486328125, Learning Rate: 0.05\n",
      "Epoch [745/20000], Loss: 4551.50146484375, Entropy -1557.153076171875, Learning Rate: 0.05\n",
      "Epoch [746/20000], Loss: 4537.23583984375, Entropy -1660.132568359375, Learning Rate: 0.05\n",
      "Epoch [747/20000], Loss: 4409.77294921875, Entropy -1549.57177734375, Learning Rate: 0.05\n",
      "Epoch [748/20000], Loss: 4411.05615234375, Entropy -1482.0517578125, Learning Rate: 0.05\n",
      "Epoch [749/20000], Loss: 4461.140625, Entropy -1484.038818359375, Learning Rate: 0.05\n",
      "Epoch [750/20000], Loss: 4390.96337890625, Entropy -1533.238037109375, Learning Rate: 0.05\n",
      "Epoch [751/20000], Loss: 4377.9228515625, Entropy -1323.1728515625, Learning Rate: 0.05\n",
      "Epoch [752/20000], Loss: 4519.126953125, Entropy -1502.9150390625, Learning Rate: 0.05\n",
      "Epoch [753/20000], Loss: 4450.1787109375, Entropy -1501.65185546875, Learning Rate: 0.05\n",
      "Epoch [754/20000], Loss: 4380.669921875, Entropy -1333.98193359375, Learning Rate: 0.05\n",
      "Epoch [755/20000], Loss: 4375.8740234375, Entropy -1295.03076171875, Learning Rate: 0.05\n",
      "Epoch [756/20000], Loss: 4429.41455078125, Entropy -1289.34912109375, Learning Rate: 0.05\n",
      "Epoch [757/20000], Loss: 4416.3837890625, Entropy -1513.682373046875, Learning Rate: 0.05\n",
      "Epoch [758/20000], Loss: 4564.67431640625, Entropy -1553.41845703125, Learning Rate: 0.05\n",
      "Epoch [759/20000], Loss: 4421.88232421875, Entropy -1523.687255859375, Learning Rate: 0.05\n",
      "Epoch [760/20000], Loss: 4374.80908203125, Entropy -1442.233154296875, Learning Rate: 0.05\n",
      "Epoch [761/20000], Loss: 4350.33935546875, Entropy -1462.515380859375, Learning Rate: 0.05\n",
      "Epoch [762/20000], Loss: 4462.8896484375, Entropy -1478.42333984375, Learning Rate: 0.05\n",
      "Epoch [763/20000], Loss: 4415.193359375, Entropy -1455.456298828125, Learning Rate: 0.05\n",
      "Epoch [764/20000], Loss: 4417.3076171875, Entropy -1398.130615234375, Learning Rate: 0.05\n",
      "Epoch [765/20000], Loss: 4534.5673828125, Entropy -1401.7568359375, Learning Rate: 0.05\n",
      "Epoch [766/20000], Loss: 4440.27587890625, Entropy -1470.580322265625, Learning Rate: 0.05\n",
      "Epoch [767/20000], Loss: 4553.95361328125, Entropy -1533.2958984375, Learning Rate: 0.05\n",
      "Epoch [768/20000], Loss: 4352.01611328125, Entropy -1495.994140625, Learning Rate: 0.05\n",
      "Epoch [769/20000], Loss: 4489.6123046875, Entropy -1412.60888671875, Learning Rate: 0.05\n",
      "Epoch [770/20000], Loss: 4422.45947265625, Entropy -1528.048583984375, Learning Rate: 0.05\n",
      "Epoch [771/20000], Loss: 4466.853515625, Entropy -1492.71728515625, Learning Rate: 0.05\n",
      "Epoch [772/20000], Loss: 4418.29833984375, Entropy -1491.6640625, Learning Rate: 0.05\n",
      "Epoch [773/20000], Loss: 4389.52001953125, Entropy -1561.4306640625, Learning Rate: 0.05\n",
      "Epoch [774/20000], Loss: 4442.6142578125, Entropy -1427.172119140625, Learning Rate: 0.05\n",
      "Epoch [775/20000], Loss: 4429.93603515625, Entropy -1508.498291015625, Learning Rate: 0.05\n",
      "Epoch [776/20000], Loss: 4386.4580078125, Entropy -1454.04248046875, Learning Rate: 0.05\n",
      "Epoch [777/20000], Loss: 4386.4306640625, Entropy -1493.26416015625, Learning Rate: 0.05\n",
      "Epoch [778/20000], Loss: 4413.09326171875, Entropy -1578.96728515625, Learning Rate: 0.05\n",
      "Epoch [779/20000], Loss: 4493.98046875, Entropy -1549.14453125, Learning Rate: 0.05\n",
      "Epoch [780/20000], Loss: 4352.650390625, Entropy -1417.8916015625, Learning Rate: 0.05\n",
      "Epoch [781/20000], Loss: 4352.00634765625, Entropy -1507.199462890625, Learning Rate: 0.05\n",
      "Epoch [782/20000], Loss: 4436.64501953125, Entropy -1507.324951171875, Learning Rate: 0.05\n",
      "Epoch [783/20000], Loss: 4329.74755859375, Entropy -1488.256591796875, Learning Rate: 0.05\n",
      "Epoch [784/20000], Loss: 4395.0146484375, Entropy -1587.571533203125, Learning Rate: 0.05\n",
      "Epoch [785/20000], Loss: 4378.330078125, Entropy -1493.2353515625, Learning Rate: 0.05\n",
      "Epoch [786/20000], Loss: 4538.4189453125, Entropy -1556.04052734375, Learning Rate: 0.05\n",
      "Epoch [787/20000], Loss: 4493.1513671875, Entropy -1497.68505859375, Learning Rate: 0.05\n",
      "Epoch [788/20000], Loss: 4560.9013671875, Entropy -1544.112060546875, Learning Rate: 0.05\n",
      "Epoch [789/20000], Loss: 4420.5126953125, Entropy -1437.8076171875, Learning Rate: 0.05\n",
      "Epoch [790/20000], Loss: 4684.90625, Entropy -1479.20458984375, Learning Rate: 0.05\n",
      "Epoch [791/20000], Loss: 4477.5673828125, Entropy -1486.9384765625, Learning Rate: 0.05\n",
      "Epoch [792/20000], Loss: 4395.1689453125, Entropy -1553.528564453125, Learning Rate: 0.05\n",
      "Epoch [793/20000], Loss: 4499.51904296875, Entropy -1595.85498046875, Learning Rate: 0.05\n",
      "Epoch [794/20000], Loss: 4631.24658203125, Entropy -1385.30419921875, Learning Rate: 0.05\n",
      "Epoch [795/20000], Loss: 4522.2373046875, Entropy -1555.26611328125, Learning Rate: 0.05\n",
      "Epoch [796/20000], Loss: 4367.986328125, Entropy -1525.60986328125, Learning Rate: 0.05\n",
      "Epoch [797/20000], Loss: 4462.923828125, Entropy -1439.268310546875, Learning Rate: 0.05\n",
      "Epoch [798/20000], Loss: 4499.67626953125, Entropy -1576.1025390625, Learning Rate: 0.05\n",
      "Epoch [799/20000], Loss: 4405.88427734375, Entropy -1540.80810546875, Learning Rate: 0.05\n",
      "Epoch [800/20000], Loss: 4562.55615234375, Entropy -1630.288818359375, Learning Rate: 0.05\n",
      "Epoch [801/20000], Loss: 4461.43408203125, Entropy -1657.53173828125, Learning Rate: 0.05\n",
      "Epoch [802/20000], Loss: 4379.958984375, Entropy -1393.973876953125, Learning Rate: 0.05\n",
      "Epoch [803/20000], Loss: 4427.32177734375, Entropy -1383.9921875, Learning Rate: 0.05\n",
      "Epoch [804/20000], Loss: 4464.70361328125, Entropy -1425.16015625, Learning Rate: 0.05\n",
      "Epoch [805/20000], Loss: 4345.6171875, Entropy -1467.76513671875, Learning Rate: 0.05\n",
      "Epoch [806/20000], Loss: 4481.8779296875, Entropy -1472.82763671875, Learning Rate: 0.05\n",
      "Epoch [807/20000], Loss: 4403.810546875, Entropy -1464.17138671875, Learning Rate: 0.05\n",
      "Epoch [808/20000], Loss: 4356.9091796875, Entropy -1421.9697265625, Learning Rate: 0.05\n",
      "Epoch [809/20000], Loss: 4381.45166015625, Entropy -1500.812744140625, Learning Rate: 0.05\n",
      "Epoch [810/20000], Loss: 4442.4130859375, Entropy -1527.19384765625, Learning Rate: 0.05\n",
      "Epoch [811/20000], Loss: 4483.35498046875, Entropy -1614.25537109375, Learning Rate: 0.05\n",
      "Epoch [812/20000], Loss: 4405.197265625, Entropy -1392.423828125, Learning Rate: 0.05\n",
      "Epoch [813/20000], Loss: 4408.0830078125, Entropy -1537.70849609375, Learning Rate: 0.05\n",
      "Epoch [814/20000], Loss: 4410.6142578125, Entropy -1565.19140625, Learning Rate: 0.05\n",
      "Epoch [815/20000], Loss: 4445.27978515625, Entropy -1447.728515625, Learning Rate: 0.05\n",
      "Epoch [816/20000], Loss: 4381.31396484375, Entropy -1478.85498046875, Learning Rate: 0.05\n",
      "Epoch [817/20000], Loss: 4343.93505859375, Entropy -1266.28173828125, Learning Rate: 0.05\n",
      "Epoch [818/20000], Loss: 4354.76416015625, Entropy -1497.8681640625, Learning Rate: 0.05\n",
      "Epoch [819/20000], Loss: 4407.8671875, Entropy -1558.45751953125, Learning Rate: 0.05\n",
      "Epoch [820/20000], Loss: 4394.68994140625, Entropy -1330.09228515625, Learning Rate: 0.05\n",
      "Epoch [821/20000], Loss: 4391.1650390625, Entropy -1483.1650390625, Learning Rate: 0.05\n",
      "Epoch [822/20000], Loss: 4379.052734375, Entropy -1486.54736328125, Learning Rate: 0.05\n",
      "Epoch [823/20000], Loss: 4326.591796875, Entropy -1412.611328125, Learning Rate: 0.05\n",
      "Epoch [824/20000], Loss: 4467.6689453125, Entropy -1508.01953125, Learning Rate: 0.05\n",
      "Epoch [825/20000], Loss: 4382.087890625, Entropy -1507.36279296875, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [826/20000], Loss: 4445.90087890625, Entropy -1425.3583984375, Learning Rate: 0.05\n",
      "Epoch [827/20000], Loss: 4498.4306640625, Entropy -1569.125, Learning Rate: 0.05\n",
      "Epoch [828/20000], Loss: 4412.09130859375, Entropy -1414.51416015625, Learning Rate: 0.05\n",
      "Epoch [829/20000], Loss: 4382.87109375, Entropy -1430.760009765625, Learning Rate: 0.05\n",
      "Epoch [830/20000], Loss: 4394.4287109375, Entropy -1529.582275390625, Learning Rate: 0.05\n",
      "Epoch [831/20000], Loss: 4443.5068359375, Entropy -1551.69140625, Learning Rate: 0.05\n",
      "Epoch [832/20000], Loss: 4451.66357421875, Entropy -1587.96142578125, Learning Rate: 0.05\n",
      "Epoch [833/20000], Loss: 4342.00341796875, Entropy -1574.521240234375, Learning Rate: 0.05\n",
      "Epoch [834/20000], Loss: 4355.94921875, Entropy -1490.786865234375, Learning Rate: 0.05\n",
      "Epoch [835/20000], Loss: 4414.30419921875, Entropy -1593.916015625, Learning Rate: 0.05\n",
      "Epoch [836/20000], Loss: 4431.78466796875, Entropy -1381.88037109375, Learning Rate: 0.05\n",
      "Epoch [837/20000], Loss: 4366.23291015625, Entropy -1473.23046875, Learning Rate: 0.05\n",
      "Epoch [838/20000], Loss: 4446.9267578125, Entropy -1483.07568359375, Learning Rate: 0.05\n",
      "Epoch [839/20000], Loss: 4426.732421875, Entropy -1529.534423828125, Learning Rate: 0.05\n",
      "Epoch [840/20000], Loss: 4444.74365234375, Entropy -1512.9990234375, Learning Rate: 0.05\n",
      "Epoch [841/20000], Loss: 4413.17333984375, Entropy -1460.7216796875, Learning Rate: 0.05\n",
      "Epoch [842/20000], Loss: 4415.85546875, Entropy -1518.392578125, Learning Rate: 0.05\n",
      "Epoch [843/20000], Loss: 4416.47607421875, Entropy -1464.51171875, Learning Rate: 0.05\n",
      "Epoch [844/20000], Loss: 4441.46923828125, Entropy -1475.293701171875, Learning Rate: 0.05\n",
      "Epoch [845/20000], Loss: 4488.5322265625, Entropy -1534.344970703125, Learning Rate: 0.05\n",
      "Epoch [846/20000], Loss: 4511.3505859375, Entropy -1418.449462890625, Learning Rate: 0.05\n",
      "Epoch [847/20000], Loss: 4384.5498046875, Entropy -1559.37939453125, Learning Rate: 0.05\n",
      "Epoch [848/20000], Loss: 4397.41748046875, Entropy -1477.23388671875, Learning Rate: 0.05\n",
      "Epoch [849/20000], Loss: 4418.72802734375, Entropy -1434.58984375, Learning Rate: 0.05\n",
      "Epoch [850/20000], Loss: 4444.5615234375, Entropy -1531.110107421875, Learning Rate: 0.05\n",
      "Epoch [851/20000], Loss: 4522.89306640625, Entropy -1432.139404296875, Learning Rate: 0.05\n",
      "Epoch [852/20000], Loss: 4399.4765625, Entropy -1445.061767578125, Learning Rate: 0.05\n",
      "Epoch [853/20000], Loss: 4396.27587890625, Entropy -1543.068359375, Learning Rate: 0.05\n",
      "Epoch [854/20000], Loss: 4459.5361328125, Entropy -1492.4638671875, Learning Rate: 0.05\n",
      "Epoch [855/20000], Loss: 4396.458984375, Entropy -1440.216796875, Learning Rate: 0.05\n",
      "Epoch [856/20000], Loss: 4446.884765625, Entropy -1543.05615234375, Learning Rate: 0.05\n",
      "Epoch [857/20000], Loss: 4326.9951171875, Entropy -1443.0830078125, Learning Rate: 0.05\n",
      "Epoch [858/20000], Loss: 4565.5078125, Entropy -1559.853515625, Learning Rate: 0.05\n",
      "Epoch [859/20000], Loss: 4330.05810546875, Entropy -1396.646484375, Learning Rate: 0.05\n",
      "Epoch [860/20000], Loss: 4359.6337890625, Entropy -1430.97119140625, Learning Rate: 0.05\n",
      "Epoch [861/20000], Loss: 4427.68017578125, Entropy -1500.303466796875, Learning Rate: 0.05\n",
      "Epoch [862/20000], Loss: 4320.1494140625, Entropy -1287.77880859375, Learning Rate: 0.05\n",
      "Epoch [863/20000], Loss: 4365.6162109375, Entropy -1555.765625, Learning Rate: 0.05\n",
      "Epoch [864/20000], Loss: 4407.42529296875, Entropy -1404.5927734375, Learning Rate: 0.05\n",
      "Epoch [865/20000], Loss: 4437.92236328125, Entropy -1597.53759765625, Learning Rate: 0.05\n",
      "Epoch [866/20000], Loss: 4405.41845703125, Entropy -1557.54150390625, Learning Rate: 0.05\n",
      "Epoch [867/20000], Loss: 4372.32373046875, Entropy -1417.617919921875, Learning Rate: 0.05\n",
      "Epoch [868/20000], Loss: 4503.67333984375, Entropy -1544.39013671875, Learning Rate: 0.05\n",
      "Epoch [869/20000], Loss: 4306.515625, Entropy -1410.05322265625, Learning Rate: 0.05\n",
      "Epoch [870/20000], Loss: 4332.08251953125, Entropy -1467.136962890625, Learning Rate: 0.05\n",
      "Epoch [871/20000], Loss: 4322.03076171875, Entropy -1482.492431640625, Learning Rate: 0.05\n",
      "Epoch [872/20000], Loss: 4395.03515625, Entropy -1413.04150390625, Learning Rate: 0.05\n",
      "Epoch [873/20000], Loss: 4427.3115234375, Entropy -1483.53662109375, Learning Rate: 0.05\n",
      "Epoch [874/20000], Loss: 4360.2578125, Entropy -1490.7998046875, Learning Rate: 0.05\n",
      "Epoch [875/20000], Loss: 4398.84375, Entropy -1447.139404296875, Learning Rate: 0.05\n",
      "Epoch [876/20000], Loss: 4341.3330078125, Entropy -1492.722900390625, Learning Rate: 0.05\n",
      "Epoch [877/20000], Loss: 4441.14697265625, Entropy -1526.43994140625, Learning Rate: 0.05\n",
      "Epoch [878/20000], Loss: 4363.181640625, Entropy -1377.3056640625, Learning Rate: 0.05\n",
      "Epoch [879/20000], Loss: 4433.96240234375, Entropy -1504.4873046875, Learning Rate: 0.05\n",
      "Epoch [880/20000], Loss: 4361.13232421875, Entropy -1445.6796875, Learning Rate: 0.05\n",
      "Epoch [881/20000], Loss: 4378.197265625, Entropy -1553.279296875, Learning Rate: 0.05\n",
      "Epoch [882/20000], Loss: 4435.02099609375, Entropy -1540.1630859375, Learning Rate: 0.05\n",
      "Epoch [883/20000], Loss: 4422.625, Entropy -1659.77685546875, Learning Rate: 0.05\n",
      "Epoch [884/20000], Loss: 4428.24462890625, Entropy -1477.00146484375, Learning Rate: 0.05\n",
      "Epoch [885/20000], Loss: 4399.46875, Entropy -1331.2158203125, Learning Rate: 0.05\n",
      "Epoch [886/20000], Loss: 4465.72607421875, Entropy -1565.36328125, Learning Rate: 0.05\n",
      "Epoch [887/20000], Loss: 4431.0703125, Entropy -1550.041259765625, Learning Rate: 0.05\n",
      "Epoch [888/20000], Loss: 4380.78271484375, Entropy -1389.9873046875, Learning Rate: 0.05\n",
      "Epoch [889/20000], Loss: 4482.3251953125, Entropy -1469.83447265625, Learning Rate: 0.05\n",
      "Epoch [890/20000], Loss: 4332.357421875, Entropy -1443.705078125, Learning Rate: 0.05\n",
      "Epoch [891/20000], Loss: 4282.67626953125, Entropy -1430.144775390625, Learning Rate: 0.05\n",
      "Epoch [892/20000], Loss: 4419.14990234375, Entropy -1579.692138671875, Learning Rate: 0.05\n",
      "Epoch [893/20000], Loss: 4359.12158203125, Entropy -1514.47314453125, Learning Rate: 0.05\n",
      "Epoch [894/20000], Loss: 4398.60498046875, Entropy -1431.874755859375, Learning Rate: 0.05\n",
      "Epoch [895/20000], Loss: 4291.4970703125, Entropy -1367.51953125, Learning Rate: 0.05\n",
      "Epoch [896/20000], Loss: 4359.3583984375, Entropy -1453.7177734375, Learning Rate: 0.05\n",
      "Epoch [897/20000], Loss: 4366.9931640625, Entropy -1383.79052734375, Learning Rate: 0.05\n",
      "Epoch [898/20000], Loss: 4347.6708984375, Entropy -1464.19384765625, Learning Rate: 0.05\n",
      "Epoch [899/20000], Loss: 4375.77587890625, Entropy -1573.56689453125, Learning Rate: 0.05\n",
      "Epoch [900/20000], Loss: 4383.3642578125, Entropy -1531.5888671875, Learning Rate: 0.05\n",
      "Epoch [901/20000], Loss: 4402.67919921875, Entropy -1346.98974609375, Learning Rate: 0.05\n",
      "Epoch [902/20000], Loss: 4327.77197265625, Entropy -1400.43115234375, Learning Rate: 0.05\n",
      "Epoch [903/20000], Loss: 4347.134765625, Entropy -1461.206787109375, Learning Rate: 0.05\n",
      "Epoch [904/20000], Loss: 4342.06884765625, Entropy -1427.658203125, Learning Rate: 0.05\n",
      "Epoch [905/20000], Loss: 4372.33935546875, Entropy -1363.31103515625, Learning Rate: 0.05\n",
      "Epoch [906/20000], Loss: 4440.48876953125, Entropy -1529.572021484375, Learning Rate: 0.05\n",
      "Epoch [907/20000], Loss: 4415.1435546875, Entropy -1542.18603515625, Learning Rate: 0.05\n",
      "Epoch [908/20000], Loss: 4360.97265625, Entropy -1479.89990234375, Learning Rate: 0.05\n",
      "Epoch [909/20000], Loss: 4383.689453125, Entropy -1394.64794921875, Learning Rate: 0.05\n",
      "Epoch [910/20000], Loss: 4382.1533203125, Entropy -1480.350830078125, Learning Rate: 0.05\n",
      "Epoch [911/20000], Loss: 4411.68017578125, Entropy -1391.09326171875, Learning Rate: 0.05\n",
      "Epoch [912/20000], Loss: 4374.25732421875, Entropy -1535.49755859375, Learning Rate: 0.05\n",
      "Epoch [913/20000], Loss: 4391.1572265625, Entropy -1560.56884765625, Learning Rate: 0.05\n",
      "Epoch [914/20000], Loss: 4405.1923828125, Entropy -1623.282958984375, Learning Rate: 0.05\n",
      "Epoch [915/20000], Loss: 4376.4423828125, Entropy -1421.91845703125, Learning Rate: 0.05\n",
      "Epoch [916/20000], Loss: 4418.54248046875, Entropy -1515.32421875, Learning Rate: 0.05\n",
      "Epoch [917/20000], Loss: 4512.109375, Entropy -1598.658203125, Learning Rate: 0.05\n",
      "Epoch [918/20000], Loss: 4290.5439453125, Entropy -1351.974609375, Learning Rate: 0.05\n",
      "Epoch [919/20000], Loss: 4450.33642578125, Entropy -1487.295654296875, Learning Rate: 0.05\n",
      "Epoch [920/20000], Loss: 4277.908203125, Entropy -1451.3935546875, Learning Rate: 0.05\n",
      "Epoch [921/20000], Loss: 4397.9482421875, Entropy -1538.025390625, Learning Rate: 0.05\n",
      "Epoch [922/20000], Loss: 4340.61474609375, Entropy -1470.244140625, Learning Rate: 0.05\n",
      "Epoch [923/20000], Loss: 4442.232421875, Entropy -1471.11962890625, Learning Rate: 0.05\n",
      "Epoch [924/20000], Loss: 4376.26123046875, Entropy -1462.44580078125, Learning Rate: 0.05\n",
      "Epoch [925/20000], Loss: 4375.2197265625, Entropy -1538.0185546875, Learning Rate: 0.05\n",
      "Epoch [926/20000], Loss: 4374.1396484375, Entropy -1466.23388671875, Learning Rate: 0.05\n",
      "Epoch [927/20000], Loss: 4416.2802734375, Entropy -1541.7685546875, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [928/20000], Loss: 4344.4462890625, Entropy -1306.52294921875, Learning Rate: 0.05\n",
      "Epoch [929/20000], Loss: 4403.9296875, Entropy -1435.5810546875, Learning Rate: 0.05\n",
      "Epoch [930/20000], Loss: 4364.10009765625, Entropy -1404.96630859375, Learning Rate: 0.05\n",
      "Epoch [931/20000], Loss: 4361.65625, Entropy -1391.60400390625, Learning Rate: 0.05\n",
      "Epoch [932/20000], Loss: 4383.7724609375, Entropy -1639.95849609375, Learning Rate: 0.05\n",
      "Epoch [933/20000], Loss: 4423.5419921875, Entropy -1581.629150390625, Learning Rate: 0.05\n",
      "Epoch [934/20000], Loss: 4492.416015625, Entropy -1493.959716796875, Learning Rate: 0.05\n",
      "Epoch [935/20000], Loss: 4385.09716796875, Entropy -1590.801513671875, Learning Rate: 0.05\n",
      "Epoch [936/20000], Loss: 4410.9775390625, Entropy -1378.955078125, Learning Rate: 0.05\n",
      "Epoch [937/20000], Loss: 4374.97119140625, Entropy -1464.10302734375, Learning Rate: 0.05\n",
      "Epoch [938/20000], Loss: 4316.44677734375, Entropy -1440.34814453125, Learning Rate: 0.05\n",
      "Epoch [939/20000], Loss: 4315.40869140625, Entropy -1445.87841796875, Learning Rate: 0.05\n",
      "Epoch [940/20000], Loss: 4385.74951171875, Entropy -1399.3515625, Learning Rate: 0.05\n",
      "Epoch [941/20000], Loss: 4384.41796875, Entropy -1363.3955078125, Learning Rate: 0.05\n",
      "Epoch [942/20000], Loss: 4470.72314453125, Entropy -1474.300048828125, Learning Rate: 0.05\n",
      "Epoch [943/20000], Loss: 4424.67529296875, Entropy -1493.8564453125, Learning Rate: 0.05\n",
      "Epoch [944/20000], Loss: 4354.46826171875, Entropy -1475.7880859375, Learning Rate: 0.05\n",
      "Epoch [945/20000], Loss: 4304.2431640625, Entropy -1409.883544921875, Learning Rate: 0.05\n",
      "Epoch [946/20000], Loss: 4359.53564453125, Entropy -1314.7509765625, Learning Rate: 0.05\n",
      "Epoch [947/20000], Loss: 4334.1953125, Entropy -1304.9267578125, Learning Rate: 0.05\n",
      "Epoch [948/20000], Loss: 4299.7431640625, Entropy -1243.33740234375, Learning Rate: 0.05\n",
      "Epoch [949/20000], Loss: 4406.591796875, Entropy -1442.01123046875, Learning Rate: 0.05\n",
      "Epoch [950/20000], Loss: 4346.97021484375, Entropy -1381.72119140625, Learning Rate: 0.05\n",
      "Epoch [951/20000], Loss: 4565.4189453125, Entropy -1428.962158203125, Learning Rate: 0.05\n",
      "Epoch [952/20000], Loss: 4363.5830078125, Entropy -1441.3740234375, Learning Rate: 0.05\n",
      "Epoch [953/20000], Loss: 4275.69189453125, Entropy -1367.84912109375, Learning Rate: 0.05\n",
      "Epoch [954/20000], Loss: 4349.74169921875, Entropy -1582.395751953125, Learning Rate: 0.05\n",
      "Epoch [955/20000], Loss: 4302.6083984375, Entropy -1493.07421875, Learning Rate: 0.05\n",
      "Epoch [956/20000], Loss: 4394.3720703125, Entropy -1430.222900390625, Learning Rate: 0.05\n",
      "Epoch [957/20000], Loss: 4377.43994140625, Entropy -1551.065185546875, Learning Rate: 0.05\n",
      "Epoch [958/20000], Loss: 4365.1220703125, Entropy -1484.737060546875, Learning Rate: 0.05\n",
      "Epoch [959/20000], Loss: 4343.357421875, Entropy -1488.9921875, Learning Rate: 0.05\n",
      "Epoch [960/20000], Loss: 4421.2861328125, Entropy -1496.975830078125, Learning Rate: 0.05\n",
      "Epoch [961/20000], Loss: 4398.3525390625, Entropy -1458.942626953125, Learning Rate: 0.05\n",
      "Epoch [962/20000], Loss: 4404.388671875, Entropy -1420.7265625, Learning Rate: 0.05\n",
      "Epoch [963/20000], Loss: 4391.3203125, Entropy -1530.47314453125, Learning Rate: 0.05\n",
      "Epoch [964/20000], Loss: 4373.21826171875, Entropy -1443.6376953125, Learning Rate: 0.05\n",
      "Epoch [965/20000], Loss: 4308.400390625, Entropy -1464.8671875, Learning Rate: 0.05\n",
      "Epoch [966/20000], Loss: 4257.75390625, Entropy -1356.2705078125, Learning Rate: 0.05\n",
      "Epoch [967/20000], Loss: 4356.1796875, Entropy -1373.150390625, Learning Rate: 0.05\n",
      "Epoch [968/20000], Loss: 4364.53076171875, Entropy -1486.42041015625, Learning Rate: 0.05\n",
      "Epoch [969/20000], Loss: 4392.00537109375, Entropy -1443.4814453125, Learning Rate: 0.05\n",
      "Epoch [970/20000], Loss: 4318.7841796875, Entropy -1419.663818359375, Learning Rate: 0.05\n",
      "Epoch [971/20000], Loss: 4357.2509765625, Entropy -1492.970947265625, Learning Rate: 0.05\n",
      "Epoch [972/20000], Loss: 4323.68701171875, Entropy -1394.25244140625, Learning Rate: 0.05\n",
      "Epoch [973/20000], Loss: 4299.32177734375, Entropy -1484.080322265625, Learning Rate: 0.05\n",
      "Epoch [974/20000], Loss: 4378.279296875, Entropy -1483.16259765625, Learning Rate: 0.05\n",
      "Epoch [975/20000], Loss: 4325.9326171875, Entropy -1559.45556640625, Learning Rate: 0.05\n",
      "Epoch [976/20000], Loss: 4373.81982421875, Entropy -1478.251220703125, Learning Rate: 0.05\n",
      "Epoch [977/20000], Loss: 4325.54052734375, Entropy -1499.168212890625, Learning Rate: 0.05\n",
      "Epoch [978/20000], Loss: 4346.2548828125, Entropy -1422.66845703125, Learning Rate: 0.05\n",
      "Epoch [979/20000], Loss: 4390.775390625, Entropy -1495.308349609375, Learning Rate: 0.05\n",
      "Epoch [980/20000], Loss: 4370.38037109375, Entropy -1523.657958984375, Learning Rate: 0.05\n",
      "Epoch [981/20000], Loss: 4258.45166015625, Entropy -1387.40673828125, Learning Rate: 0.05\n",
      "Epoch [982/20000], Loss: 4355.4404296875, Entropy -1467.2431640625, Learning Rate: 0.05\n",
      "Epoch [983/20000], Loss: 4354.33935546875, Entropy -1524.51806640625, Learning Rate: 0.05\n",
      "Epoch [984/20000], Loss: 4384.13232421875, Entropy -1473.626953125, Learning Rate: 0.05\n",
      "Epoch [985/20000], Loss: 4313.99609375, Entropy -1443.667724609375, Learning Rate: 0.05\n",
      "Epoch [986/20000], Loss: 4343.94873046875, Entropy -1408.609130859375, Learning Rate: 0.05\n",
      "Epoch [987/20000], Loss: 4420.39013671875, Entropy -1482.82568359375, Learning Rate: 0.05\n",
      "Epoch [988/20000], Loss: 4335.56884765625, Entropy -1486.631591796875, Learning Rate: 0.05\n",
      "Epoch [989/20000], Loss: 4332.2783203125, Entropy -1409.904541015625, Learning Rate: 0.05\n",
      "Epoch [990/20000], Loss: 4361.88623046875, Entropy -1574.9287109375, Learning Rate: 0.05\n",
      "Epoch [991/20000], Loss: 4419.9462890625, Entropy -1465.923583984375, Learning Rate: 0.05\n",
      "Epoch [992/20000], Loss: 4299.24169921875, Entropy -1435.3828125, Learning Rate: 0.05\n",
      "Epoch [993/20000], Loss: 4330.8603515625, Entropy -1452.708984375, Learning Rate: 0.05\n",
      "Epoch [994/20000], Loss: 4391.779296875, Entropy -1409.01171875, Learning Rate: 0.05\n",
      "Epoch [995/20000], Loss: 4325.27880859375, Entropy -1489.10498046875, Learning Rate: 0.05\n",
      "Epoch [996/20000], Loss: 4386.24609375, Entropy -1549.091796875, Learning Rate: 0.05\n",
      "Epoch [997/20000], Loss: 4442.767578125, Entropy -1744.8037109375, Learning Rate: 0.05\n",
      "Epoch [998/20000], Loss: 4382.82568359375, Entropy -1541.30419921875, Learning Rate: 0.05\n",
      "Epoch [999/20000], Loss: 4479.5693359375, Entropy -1528.55908203125, Learning Rate: 0.05\n",
      "Epoch [1000/20000], Loss: 4388.58837890625, Entropy -1450.17626953125, Learning Rate: 0.05\n",
      "Epoch [1001/20000], Loss: 4316.017578125, Entropy -1417.40576171875, Learning Rate: 0.05\n",
      "Epoch [1002/20000], Loss: 4510.85400390625, Entropy -1513.87841796875, Learning Rate: 0.05\n",
      "Epoch [1003/20000], Loss: 4455.8779296875, Entropy -1545.18994140625, Learning Rate: 0.05\n",
      "Epoch [1004/20000], Loss: 4273.6796875, Entropy -1376.74951171875, Learning Rate: 0.05\n",
      "Epoch [1005/20000], Loss: 4404.50537109375, Entropy -1440.43115234375, Learning Rate: 0.05\n",
      "Epoch [1006/20000], Loss: 4326.14794921875, Entropy -1441.20703125, Learning Rate: 0.05\n",
      "Epoch [1007/20000], Loss: 4373.40380859375, Entropy -1502.39697265625, Learning Rate: 0.05\n",
      "Epoch [1008/20000], Loss: 4380.283203125, Entropy -1487.807373046875, Learning Rate: 0.05\n",
      "Epoch [1009/20000], Loss: 4350.6806640625, Entropy -1468.710205078125, Learning Rate: 0.05\n",
      "Epoch [1010/20000], Loss: 4368.3798828125, Entropy -1488.017333984375, Learning Rate: 0.05\n",
      "Epoch [1011/20000], Loss: 4368.30908203125, Entropy -1494.18896484375, Learning Rate: 0.05\n",
      "Epoch [1012/20000], Loss: 4332.4375, Entropy -1464.67724609375, Learning Rate: 0.05\n",
      "Epoch [1013/20000], Loss: 4388.62841796875, Entropy -1399.817626953125, Learning Rate: 0.05\n",
      "Epoch [1014/20000], Loss: 4387.21630859375, Entropy -1452.759765625, Learning Rate: 0.05\n",
      "Epoch [1015/20000], Loss: 4360.4033203125, Entropy -1601.20849609375, Learning Rate: 0.05\n",
      "Epoch [1016/20000], Loss: 4312.783203125, Entropy -1401.47705078125, Learning Rate: 0.05\n",
      "Epoch [1017/20000], Loss: 4356.8671875, Entropy -1452.21337890625, Learning Rate: 0.05\n",
      "Epoch [1018/20000], Loss: 4442.4423828125, Entropy -1484.70166015625, Learning Rate: 0.05\n",
      "Epoch [1019/20000], Loss: 4352.00390625, Entropy -1435.578369140625, Learning Rate: 0.05\n",
      "Epoch [1020/20000], Loss: 4274.18798828125, Entropy -1409.776123046875, Learning Rate: 0.05\n",
      "Epoch [1021/20000], Loss: 4412.6220703125, Entropy -1415.8203125, Learning Rate: 0.05\n",
      "Epoch [1022/20000], Loss: 4408.3134765625, Entropy -1456.21044921875, Learning Rate: 0.05\n",
      "Epoch [1023/20000], Loss: 4368.09765625, Entropy -1360.85986328125, Learning Rate: 0.05\n",
      "Epoch [1024/20000], Loss: 4334.3505859375, Entropy -1368.310546875, Learning Rate: 0.05\n",
      "Epoch [1025/20000], Loss: 4337.19482421875, Entropy -1401.1669921875, Learning Rate: 0.05\n",
      "Epoch [1026/20000], Loss: 4350.7080078125, Entropy -1512.361328125, Learning Rate: 0.05\n",
      "Epoch [1027/20000], Loss: 4268.7529296875, Entropy -1361.224609375, Learning Rate: 0.05\n",
      "Epoch [1028/20000], Loss: 4264.4560546875, Entropy -1452.357421875, Learning Rate: 0.05\n",
      "Epoch [1029/20000], Loss: 4378.00537109375, Entropy -1500.35302734375, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1030/20000], Loss: 4363.29248046875, Entropy -1500.75537109375, Learning Rate: 0.05\n",
      "Epoch [1031/20000], Loss: 4272.193359375, Entropy -1440.33740234375, Learning Rate: 0.05\n",
      "Epoch [1032/20000], Loss: 4363.228515625, Entropy -1433.085205078125, Learning Rate: 0.05\n",
      "Epoch [1033/20000], Loss: 4297.263671875, Entropy -1326.81201171875, Learning Rate: 0.05\n",
      "Epoch [1034/20000], Loss: 4322.59912109375, Entropy -1469.1943359375, Learning Rate: 0.05\n",
      "Epoch [1035/20000], Loss: 4429.53759765625, Entropy -1329.3671875, Learning Rate: 0.05\n",
      "Epoch [1036/20000], Loss: 4394.70361328125, Entropy -1514.232421875, Learning Rate: 0.05\n",
      "Epoch [1037/20000], Loss: 4380.8876953125, Entropy -1502.10986328125, Learning Rate: 0.05\n",
      "Epoch [1038/20000], Loss: 4226.17138671875, Entropy -1287.80078125, Learning Rate: 0.05\n",
      "Epoch [1039/20000], Loss: 4365.7587890625, Entropy -1456.55029296875, Learning Rate: 0.05\n",
      "Epoch [1040/20000], Loss: 4283.88134765625, Entropy -1364.220703125, Learning Rate: 0.05\n",
      "Epoch [1041/20000], Loss: 4393.994140625, Entropy -1579.4423828125, Learning Rate: 0.05\n",
      "Epoch [1042/20000], Loss: 4350.015625, Entropy -1571.91015625, Learning Rate: 0.05\n",
      "Epoch [1043/20000], Loss: 4339.67333984375, Entropy -1541.12451171875, Learning Rate: 0.05\n",
      "Epoch [1044/20000], Loss: 4269.59423828125, Entropy -1343.88037109375, Learning Rate: 0.05\n",
      "Epoch [1045/20000], Loss: 4354.30322265625, Entropy -1466.89013671875, Learning Rate: 0.05\n",
      "Epoch [1046/20000], Loss: 4313.97119140625, Entropy -1513.81787109375, Learning Rate: 0.05\n",
      "Epoch [1047/20000], Loss: 4359.10791015625, Entropy -1403.20947265625, Learning Rate: 0.05\n",
      "Epoch [1048/20000], Loss: 4274.35693359375, Entropy -1425.89501953125, Learning Rate: 0.05\n",
      "Epoch [1049/20000], Loss: 4356.51220703125, Entropy -1546.630126953125, Learning Rate: 0.05\n",
      "Epoch [1050/20000], Loss: 4396.09521484375, Entropy -1405.7177734375, Learning Rate: 0.05\n",
      "Epoch [1051/20000], Loss: 4426.39111328125, Entropy -1570.0556640625, Learning Rate: 0.05\n",
      "Epoch [1052/20000], Loss: 4463.25927734375, Entropy -1515.060546875, Learning Rate: 0.05\n",
      "Epoch [1053/20000], Loss: 4320.3095703125, Entropy -1408.13623046875, Learning Rate: 0.05\n",
      "Epoch [1054/20000], Loss: 4427.39990234375, Entropy -1537.6025390625, Learning Rate: 0.05\n",
      "Epoch [1055/20000], Loss: 4344.34814453125, Entropy -1401.861083984375, Learning Rate: 0.05\n",
      "Epoch [1056/20000], Loss: 4311.9990234375, Entropy -1423.22021484375, Learning Rate: 0.05\n",
      "Epoch [1057/20000], Loss: 4339.3291015625, Entropy -1446.35498046875, Learning Rate: 0.05\n",
      "Epoch [1058/20000], Loss: 4316.78955078125, Entropy -1535.174072265625, Learning Rate: 0.05\n",
      "Epoch [1059/20000], Loss: 4300.8017578125, Entropy -1408.150390625, Learning Rate: 0.05\n",
      "Epoch [1060/20000], Loss: 4292.22412109375, Entropy -1418.63916015625, Learning Rate: 0.05\n",
      "Epoch [1061/20000], Loss: 4371.9794921875, Entropy -1522.67236328125, Learning Rate: 0.05\n",
      "Epoch [1062/20000], Loss: 4303.97900390625, Entropy -1427.5517578125, Learning Rate: 0.05\n",
      "Epoch [1063/20000], Loss: 4323.80859375, Entropy -1475.962646484375, Learning Rate: 0.05\n",
      "Epoch [1064/20000], Loss: 4318.365234375, Entropy -1341.21875, Learning Rate: 0.05\n",
      "Epoch [1065/20000], Loss: 4370.02197265625, Entropy -1469.1259765625, Learning Rate: 0.05\n",
      "Epoch [1066/20000], Loss: 4299.349609375, Entropy -1446.5302734375, Learning Rate: 0.05\n",
      "Epoch [1067/20000], Loss: 4331.53173828125, Entropy -1473.03125, Learning Rate: 0.05\n",
      "Epoch [1068/20000], Loss: 4365.6357421875, Entropy -1494.613525390625, Learning Rate: 0.05\n",
      "Epoch [1069/20000], Loss: 4292.33544921875, Entropy -1384.63037109375, Learning Rate: 0.05\n",
      "Epoch [1070/20000], Loss: 4339.16162109375, Entropy -1469.40576171875, Learning Rate: 0.05\n",
      "Epoch [1071/20000], Loss: 4262.22216796875, Entropy -1312.1103515625, Learning Rate: 0.05\n",
      "Epoch [1072/20000], Loss: 4306.9248046875, Entropy -1440.31005859375, Learning Rate: 0.05\n",
      "Epoch [1073/20000], Loss: 4284.7060546875, Entropy -1350.28564453125, Learning Rate: 0.05\n",
      "Epoch [1074/20000], Loss: 4351.58251953125, Entropy -1376.369140625, Learning Rate: 0.05\n",
      "Epoch [1075/20000], Loss: 4344.77392578125, Entropy -1506.8701171875, Learning Rate: 0.05\n",
      "Epoch [1076/20000], Loss: 4364.97412109375, Entropy -1584.015625, Learning Rate: 0.05\n",
      "Epoch [1077/20000], Loss: 4346.28759765625, Entropy -1506.75244140625, Learning Rate: 0.05\n",
      "Epoch [1078/20000], Loss: 4340.10888671875, Entropy -1436.37451171875, Learning Rate: 0.05\n",
      "Epoch [1079/20000], Loss: 4333.44921875, Entropy -1342.46728515625, Learning Rate: 0.05\n",
      "Epoch [1080/20000], Loss: 4280.40478515625, Entropy -1446.62548828125, Learning Rate: 0.05\n",
      "Epoch [1081/20000], Loss: 4336.724609375, Entropy -1431.70751953125, Learning Rate: 0.05\n",
      "Epoch [1082/20000], Loss: 4395.60498046875, Entropy -1294.462890625, Learning Rate: 0.05\n",
      "Epoch [1083/20000], Loss: 4310.423828125, Entropy -1470.0263671875, Learning Rate: 0.05\n",
      "Epoch [1084/20000], Loss: 4334.599609375, Entropy -1386.32666015625, Learning Rate: 0.05\n",
      "Epoch [1085/20000], Loss: 4271.70751953125, Entropy -1416.5625, Learning Rate: 0.05\n",
      "Epoch [1086/20000], Loss: 4305.8212890625, Entropy -1479.430908203125, Learning Rate: 0.05\n",
      "Epoch [1087/20000], Loss: 4342.380859375, Entropy -1337.7578125, Learning Rate: 0.05\n",
      "Epoch [1088/20000], Loss: 4357.95556640625, Entropy -1535.208251953125, Learning Rate: 0.05\n",
      "Epoch [1089/20000], Loss: 4287.54345703125, Entropy -1460.04443359375, Learning Rate: 0.05\n",
      "Epoch [1090/20000], Loss: 4335.53759765625, Entropy -1350.42724609375, Learning Rate: 0.05\n",
      "Epoch [1091/20000], Loss: 4307.4033203125, Entropy -1510.762451171875, Learning Rate: 0.05\n",
      "Epoch [1092/20000], Loss: 4322.93359375, Entropy -1393.3125, Learning Rate: 0.05\n",
      "Epoch [1093/20000], Loss: 4292.21240234375, Entropy -1324.640625, Learning Rate: 0.05\n",
      "Epoch [1094/20000], Loss: 4389.55078125, Entropy -1451.419189453125, Learning Rate: 0.05\n",
      "Epoch [1095/20000], Loss: 4334.13623046875, Entropy -1503.473388671875, Learning Rate: 0.05\n",
      "Epoch [1096/20000], Loss: 4416.3173828125, Entropy -1491.556640625, Learning Rate: 0.05\n",
      "Epoch [1097/20000], Loss: 4312.0859375, Entropy -1388.3046875, Learning Rate: 0.05\n",
      "Epoch [1098/20000], Loss: 4351.2783203125, Entropy -1420.107177734375, Learning Rate: 0.05\n",
      "Epoch [1099/20000], Loss: 4350.62255859375, Entropy -1475.99560546875, Learning Rate: 0.05\n",
      "Epoch [1100/20000], Loss: 4371.22900390625, Entropy -1510.499755859375, Learning Rate: 0.05\n",
      "Epoch [1101/20000], Loss: 4357.41064453125, Entropy -1472.03857421875, Learning Rate: 0.05\n",
      "Epoch [1102/20000], Loss: 4306.0751953125, Entropy -1581.90869140625, Learning Rate: 0.05\n",
      "Epoch [1103/20000], Loss: 4271.3583984375, Entropy -1452.99853515625, Learning Rate: 0.05\n",
      "Epoch [1104/20000], Loss: 4290.13427734375, Entropy -1369.1396484375, Learning Rate: 0.05\n",
      "Epoch [1105/20000], Loss: 4344.90380859375, Entropy -1447.0439453125, Learning Rate: 0.05\n",
      "Epoch [1106/20000], Loss: 4330.16259765625, Entropy -1518.3203125, Learning Rate: 0.05\n",
      "Epoch [1107/20000], Loss: 4371.7958984375, Entropy -1562.91015625, Learning Rate: 0.05\n",
      "Epoch [1108/20000], Loss: 4351.6240234375, Entropy -1353.50634765625, Learning Rate: 0.05\n",
      "Epoch [1109/20000], Loss: 4248.20068359375, Entropy -1367.11962890625, Learning Rate: 0.05\n",
      "Epoch [1110/20000], Loss: 4324.39404296875, Entropy -1348.9775390625, Learning Rate: 0.05\n",
      "Epoch [1111/20000], Loss: 4381.50341796875, Entropy -1400.70947265625, Learning Rate: 0.05\n",
      "Epoch [1112/20000], Loss: 4350.70458984375, Entropy -1375.3505859375, Learning Rate: 0.05\n",
      "Epoch [1113/20000], Loss: 4366.84130859375, Entropy -1478.56884765625, Learning Rate: 0.05\n",
      "Epoch [1114/20000], Loss: 4333.31640625, Entropy -1426.458251953125, Learning Rate: 0.05\n",
      "Epoch [1115/20000], Loss: 4360.89453125, Entropy -1398.13427734375, Learning Rate: 0.05\n",
      "Epoch [1116/20000], Loss: 4372.783203125, Entropy -1481.21142578125, Learning Rate: 0.05\n",
      "Epoch [1117/20000], Loss: 4369.68896484375, Entropy -1399.368408203125, Learning Rate: 0.05\n",
      "Epoch [1118/20000], Loss: 4209.408203125, Entropy -1359.9267578125, Learning Rate: 0.05\n",
      "Epoch [1119/20000], Loss: 4340.97900390625, Entropy -1386.20263671875, Learning Rate: 0.05\n",
      "Epoch [1120/20000], Loss: 4310.13671875, Entropy -1470.392822265625, Learning Rate: 0.05\n",
      "Epoch [1121/20000], Loss: 4294.3037109375, Entropy -1297.03173828125, Learning Rate: 0.05\n",
      "Epoch [1122/20000], Loss: 4325.29345703125, Entropy -1512.3671875, Learning Rate: 0.05\n",
      "Epoch [1123/20000], Loss: 4374.1689453125, Entropy -1473.410400390625, Learning Rate: 0.05\n",
      "Epoch [1124/20000], Loss: 4331.75146484375, Entropy -1462.95849609375, Learning Rate: 0.05\n",
      "Epoch [1125/20000], Loss: 4414.33837890625, Entropy -1455.021484375, Learning Rate: 0.05\n",
      "Epoch [1126/20000], Loss: 4307.6025390625, Entropy -1294.8505859375, Learning Rate: 0.05\n",
      "Epoch [1127/20000], Loss: 4388.123046875, Entropy -1388.32861328125, Learning Rate: 0.05\n",
      "Epoch [1128/20000], Loss: 4259.85205078125, Entropy -1335.3076171875, Learning Rate: 0.05\n",
      "Epoch [1129/20000], Loss: 4297.8828125, Entropy -1411.09033203125, Learning Rate: 0.05\n",
      "Epoch [1130/20000], Loss: 4299.19189453125, Entropy -1425.5830078125, Learning Rate: 0.05\n",
      "Epoch [1131/20000], Loss: 4314.6611328125, Entropy -1371.673828125, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1132/20000], Loss: 4276.951171875, Entropy -1381.8408203125, Learning Rate: 0.05\n",
      "Epoch [1133/20000], Loss: 4374.61181640625, Entropy -1393.75634765625, Learning Rate: 0.05\n",
      "Epoch [1134/20000], Loss: 4267.60888671875, Entropy -1388.4345703125, Learning Rate: 0.05\n",
      "Epoch [1135/20000], Loss: 4419.80615234375, Entropy -1536.7939453125, Learning Rate: 0.05\n",
      "Epoch [1136/20000], Loss: 4405.4384765625, Entropy -1612.64453125, Learning Rate: 0.05\n",
      "Epoch [1137/20000], Loss: 4267.8466796875, Entropy -1340.11572265625, Learning Rate: 0.05\n",
      "Epoch [1138/20000], Loss: 4326.96142578125, Entropy -1492.390869140625, Learning Rate: 0.05\n",
      "Epoch [1139/20000], Loss: 4275.08935546875, Entropy -1417.21337890625, Learning Rate: 0.05\n",
      "Epoch [1140/20000], Loss: 4320.10009765625, Entropy -1400.4140625, Learning Rate: 0.05\n",
      "Epoch [1141/20000], Loss: 4346.49658203125, Entropy -1431.9111328125, Learning Rate: 0.05\n",
      "Epoch [1142/20000], Loss: 4348.83349609375, Entropy -1466.381103515625, Learning Rate: 0.05\n",
      "Epoch [1143/20000], Loss: 4314.298828125, Entropy -1510.16796875, Learning Rate: 0.05\n",
      "Epoch [1144/20000], Loss: 4323.61376953125, Entropy -1485.76904296875, Learning Rate: 0.05\n",
      "Epoch [1145/20000], Loss: 4386.43310546875, Entropy -1534.68896484375, Learning Rate: 0.05\n",
      "Epoch [1146/20000], Loss: 4352.6806640625, Entropy -1433.43408203125, Learning Rate: 0.05\n",
      "Epoch [1147/20000], Loss: 4232.55859375, Entropy -1355.20654296875, Learning Rate: 0.05\n",
      "Epoch [1148/20000], Loss: 4277.8984375, Entropy -1390.1904296875, Learning Rate: 0.05\n",
      "Epoch [1149/20000], Loss: 4299.4892578125, Entropy -1343.451171875, Learning Rate: 0.05\n",
      "Epoch [1150/20000], Loss: 4248.4873046875, Entropy -1357.21240234375, Learning Rate: 0.05\n",
      "Epoch [1151/20000], Loss: 4287.0107421875, Entropy -1456.9921875, Learning Rate: 0.05\n",
      "Epoch [1152/20000], Loss: 4275.15087890625, Entropy -1484.492919921875, Learning Rate: 0.05\n",
      "Epoch [1153/20000], Loss: 4319.50537109375, Entropy -1485.89599609375, Learning Rate: 0.05\n",
      "Epoch [1154/20000], Loss: 4265.16796875, Entropy -1427.099609375, Learning Rate: 0.05\n",
      "Epoch [1155/20000], Loss: 4330.9326171875, Entropy -1432.3310546875, Learning Rate: 0.05\n",
      "Epoch [1156/20000], Loss: 4265.74755859375, Entropy -1381.66943359375, Learning Rate: 0.05\n",
      "Epoch [1157/20000], Loss: 4395.22314453125, Entropy -1399.138916015625, Learning Rate: 0.05\n",
      "Epoch [1158/20000], Loss: 4373.18994140625, Entropy -1399.902099609375, Learning Rate: 0.05\n",
      "Epoch [1159/20000], Loss: 4417.302734375, Entropy -1503.4443359375, Learning Rate: 0.05\n",
      "Epoch [1160/20000], Loss: 4308.94384765625, Entropy -1319.8046875, Learning Rate: 0.05\n",
      "Epoch [1161/20000], Loss: 4327.19140625, Entropy -1427.678955078125, Learning Rate: 0.05\n",
      "Epoch [1162/20000], Loss: 4279.94091796875, Entropy -1380.04541015625, Learning Rate: 0.05\n",
      "Epoch [1163/20000], Loss: 4315.404296875, Entropy -1406.9677734375, Learning Rate: 0.05\n",
      "Epoch [1164/20000], Loss: 4298.56884765625, Entropy -1346.11767578125, Learning Rate: 0.05\n",
      "Epoch [1165/20000], Loss: 4362.7255859375, Entropy -1591.9892578125, Learning Rate: 0.05\n",
      "Epoch [1166/20000], Loss: 4325.2158203125, Entropy -1373.8671875, Learning Rate: 0.05\n",
      "Epoch [1167/20000], Loss: 4259.263671875, Entropy -1356.73876953125, Learning Rate: 0.05\n",
      "Epoch [1168/20000], Loss: 4335.9736328125, Entropy -1463.73388671875, Learning Rate: 0.05\n",
      "Epoch [1169/20000], Loss: 4311.2001953125, Entropy -1386.2822265625, Learning Rate: 0.05\n",
      "Epoch [1170/20000], Loss: 4517.71728515625, Entropy -1645.7734375, Learning Rate: 0.05\n",
      "Epoch [1171/20000], Loss: 4228.2021484375, Entropy -1308.109375, Learning Rate: 0.05\n",
      "Epoch [1172/20000], Loss: 4326.75146484375, Entropy -1562.2578125, Learning Rate: 0.05\n",
      "Epoch [1173/20000], Loss: 4298.01025390625, Entropy -1395.02099609375, Learning Rate: 0.05\n",
      "Epoch [1174/20000], Loss: 4390.892578125, Entropy -1553.88427734375, Learning Rate: 0.05\n",
      "Epoch [1175/20000], Loss: 4443.68408203125, Entropy -1462.54541015625, Learning Rate: 0.05\n",
      "Epoch [1176/20000], Loss: 4234.02001953125, Entropy -1351.3759765625, Learning Rate: 0.05\n",
      "Epoch [1177/20000], Loss: 4320.6376953125, Entropy -1412.061279296875, Learning Rate: 0.05\n",
      "Epoch [1178/20000], Loss: 4313.626953125, Entropy -1326.53857421875, Learning Rate: 0.05\n",
      "Epoch [1179/20000], Loss: 4348.931640625, Entropy -1474.962646484375, Learning Rate: 0.05\n",
      "Epoch [1180/20000], Loss: 4329.4931640625, Entropy -1523.98828125, Learning Rate: 0.05\n",
      "Epoch [1181/20000], Loss: 4236.03369140625, Entropy -1381.61865234375, Learning Rate: 0.05\n",
      "Epoch [1182/20000], Loss: 4432.31884765625, Entropy -1506.01708984375, Learning Rate: 0.05\n",
      "Epoch [1183/20000], Loss: 4343.39306640625, Entropy -1511.8798828125, Learning Rate: 0.05\n",
      "Epoch [1184/20000], Loss: 4319.71728515625, Entropy -1441.82421875, Learning Rate: 0.05\n",
      "Epoch [1185/20000], Loss: 4437.42724609375, Entropy -1534.90966796875, Learning Rate: 0.05\n",
      "Epoch [1186/20000], Loss: 4279.583984375, Entropy -1421.072265625, Learning Rate: 0.05\n",
      "Epoch [1187/20000], Loss: 4351.49072265625, Entropy -1440.17333984375, Learning Rate: 0.05\n",
      "Epoch [1188/20000], Loss: 4383.23681640625, Entropy -1438.988525390625, Learning Rate: 0.05\n",
      "Epoch [1189/20000], Loss: 4299.79052734375, Entropy -1424.6767578125, Learning Rate: 0.05\n",
      "Epoch [1190/20000], Loss: 4294.33740234375, Entropy -1417.421875, Learning Rate: 0.05\n",
      "Epoch [1191/20000], Loss: 4310.80419921875, Entropy -1330.35595703125, Learning Rate: 0.05\n",
      "Epoch [1192/20000], Loss: 4269.59326171875, Entropy -1337.578125, Learning Rate: 0.05\n",
      "Epoch [1193/20000], Loss: 4337.11865234375, Entropy -1408.513916015625, Learning Rate: 0.05\n",
      "Epoch [1194/20000], Loss: 4306.453125, Entropy -1520.45361328125, Learning Rate: 0.05\n",
      "Epoch [1195/20000], Loss: 4246.22216796875, Entropy -1357.90185546875, Learning Rate: 0.05\n",
      "Epoch [1196/20000], Loss: 4329.12353515625, Entropy -1356.03466796875, Learning Rate: 0.05\n",
      "Epoch [1197/20000], Loss: 4306.62451171875, Entropy -1431.40185546875, Learning Rate: 0.05\n",
      "Epoch [1198/20000], Loss: 4314.27978515625, Entropy -1453.73681640625, Learning Rate: 0.05\n",
      "Epoch [1199/20000], Loss: 4309.048828125, Entropy -1381.8212890625, Learning Rate: 0.05\n",
      "Epoch [1200/20000], Loss: 4276.0302734375, Entropy -1353.73876953125, Learning Rate: 0.05\n",
      "Epoch [1201/20000], Loss: 4313.29638671875, Entropy -1427.87109375, Learning Rate: 0.05\n",
      "Epoch [1202/20000], Loss: 4302.669921875, Entropy -1322.83642578125, Learning Rate: 0.05\n",
      "Epoch [1203/20000], Loss: 4295.9765625, Entropy -1446.76611328125, Learning Rate: 0.05\n",
      "Epoch [1204/20000], Loss: 4297.4462890625, Entropy -1440.15087890625, Learning Rate: 0.05\n",
      "Epoch [1205/20000], Loss: 4311.1259765625, Entropy -1425.3203125, Learning Rate: 0.05\n",
      "Epoch [1206/20000], Loss: 4310.68505859375, Entropy -1427.635009765625, Learning Rate: 0.05\n",
      "Epoch [1207/20000], Loss: 4366.12353515625, Entropy -1444.63134765625, Learning Rate: 0.05\n",
      "Epoch [1208/20000], Loss: 4294.20458984375, Entropy -1497.960693359375, Learning Rate: 0.05\n",
      "Epoch [1209/20000], Loss: 4296.59033203125, Entropy -1372.98095703125, Learning Rate: 0.05\n",
      "Epoch [1210/20000], Loss: 4325.87841796875, Entropy -1460.719970703125, Learning Rate: 0.05\n",
      "Epoch [1211/20000], Loss: 4284.8671875, Entropy -1326.47607421875, Learning Rate: 0.05\n",
      "Epoch [1212/20000], Loss: 4365.548828125, Entropy -1484.951171875, Learning Rate: 0.05\n",
      "Epoch [1213/20000], Loss: 4284.5693359375, Entropy -1356.244140625, Learning Rate: 0.05\n",
      "Epoch [1214/20000], Loss: 4264.70556640625, Entropy -1438.76123046875, Learning Rate: 0.05\n",
      "Epoch [1215/20000], Loss: 4262.35595703125, Entropy -1417.86376953125, Learning Rate: 0.05\n",
      "Epoch [1216/20000], Loss: 4323.0087890625, Entropy -1307.60888671875, Learning Rate: 0.05\n",
      "Epoch [1217/20000], Loss: 4318.0595703125, Entropy -1404.319580078125, Learning Rate: 0.05\n",
      "Epoch [1218/20000], Loss: 4294.2626953125, Entropy -1404.248046875, Learning Rate: 0.05\n",
      "Epoch [1219/20000], Loss: 4318.107421875, Entropy -1355.919921875, Learning Rate: 0.05\n",
      "Epoch [1220/20000], Loss: 4285.67626953125, Entropy -1437.23828125, Learning Rate: 0.05\n",
      "Epoch [1221/20000], Loss: 4303.10595703125, Entropy -1498.217041015625, Learning Rate: 0.05\n",
      "Epoch [1222/20000], Loss: 4340.6005859375, Entropy -1537.78515625, Learning Rate: 0.05\n",
      "Epoch [1223/20000], Loss: 4299.33837890625, Entropy -1442.2197265625, Learning Rate: 0.05\n",
      "Epoch [1224/20000], Loss: 4282.044921875, Entropy -1372.6259765625, Learning Rate: 0.05\n",
      "Epoch [1225/20000], Loss: 4318.24609375, Entropy -1438.953369140625, Learning Rate: 0.05\n",
      "Epoch [1226/20000], Loss: 4316.42822265625, Entropy -1512.51171875, Learning Rate: 0.05\n",
      "Epoch [1227/20000], Loss: 4291.2333984375, Entropy -1245.16552734375, Learning Rate: 0.05\n",
      "Epoch [1228/20000], Loss: 4276.95654296875, Entropy -1450.64208984375, Learning Rate: 0.05\n",
      "Epoch [1229/20000], Loss: 4298.03662109375, Entropy -1503.50830078125, Learning Rate: 0.05\n",
      "Epoch [1230/20000], Loss: 4276.1767578125, Entropy -1385.82568359375, Learning Rate: 0.05\n",
      "Epoch [1231/20000], Loss: 4317.73291015625, Entropy -1349.484375, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1232/20000], Loss: 4318.50146484375, Entropy -1444.4091796875, Learning Rate: 0.05\n",
      "Epoch [1233/20000], Loss: 4298.55908203125, Entropy -1370.83984375, Learning Rate: 0.05\n",
      "Epoch [1234/20000], Loss: 4344.9912109375, Entropy -1425.5419921875, Learning Rate: 0.05\n",
      "Epoch [1235/20000], Loss: 4302.75732421875, Entropy -1226.4345703125, Learning Rate: 0.05\n",
      "Epoch [1236/20000], Loss: 4293.6435546875, Entropy -1463.74755859375, Learning Rate: 0.05\n",
      "Epoch [1237/20000], Loss: 4325.95654296875, Entropy -1339.802734375, Learning Rate: 0.05\n",
      "Epoch [1238/20000], Loss: 4346.23486328125, Entropy -1449.022705078125, Learning Rate: 0.05\n",
      "Epoch [1239/20000], Loss: 4293.048828125, Entropy -1424.063232421875, Learning Rate: 0.05\n",
      "Epoch [1240/20000], Loss: 4351.64794921875, Entropy -1330.33349609375, Learning Rate: 0.05\n",
      "Epoch [1241/20000], Loss: 4311.06103515625, Entropy -1461.25, Learning Rate: 0.05\n",
      "Epoch [1242/20000], Loss: 4314.5830078125, Entropy -1321.92529296875, Learning Rate: 0.05\n",
      "Epoch [1243/20000], Loss: 4349.79541015625, Entropy -1515.219482421875, Learning Rate: 0.05\n",
      "Epoch [1244/20000], Loss: 4265.93017578125, Entropy -1461.513671875, Learning Rate: 0.05\n",
      "Epoch [1245/20000], Loss: 4391.39111328125, Entropy -1487.42724609375, Learning Rate: 0.05\n",
      "Epoch [1246/20000], Loss: 4236.5009765625, Entropy -1387.62744140625, Learning Rate: 0.05\n",
      "Epoch [1247/20000], Loss: 4290.3837890625, Entropy -1356.57958984375, Learning Rate: 0.05\n",
      "Epoch [1248/20000], Loss: 4243.82421875, Entropy -1406.14453125, Learning Rate: 0.05\n",
      "Epoch [1249/20000], Loss: 4336.69091796875, Entropy -1538.793212890625, Learning Rate: 0.05\n",
      "Epoch [1250/20000], Loss: 4348.84130859375, Entropy -1433.462646484375, Learning Rate: 0.05\n",
      "Epoch [1251/20000], Loss: 4181.14013671875, Entropy -1219.49462890625, Learning Rate: 0.05\n",
      "Epoch [1252/20000], Loss: 4283.88330078125, Entropy -1444.909423828125, Learning Rate: 0.05\n",
      "Epoch [1253/20000], Loss: 4277.9248046875, Entropy -1446.037353515625, Learning Rate: 0.05\n",
      "Epoch [1254/20000], Loss: 4309.50634765625, Entropy -1424.36328125, Learning Rate: 0.05\n",
      "Epoch [1255/20000], Loss: 4289.91845703125, Entropy -1521.032470703125, Learning Rate: 0.05\n",
      "Epoch [1256/20000], Loss: 4321.57177734375, Entropy -1581.1787109375, Learning Rate: 0.05\n",
      "Epoch [1257/20000], Loss: 4312.3837890625, Entropy -1343.01025390625, Learning Rate: 0.05\n",
      "Epoch [1258/20000], Loss: 4325.45068359375, Entropy -1477.94677734375, Learning Rate: 0.05\n",
      "Epoch [1259/20000], Loss: 4212.77783203125, Entropy -1346.07763671875, Learning Rate: 0.05\n",
      "Epoch [1260/20000], Loss: 4255.11767578125, Entropy -1432.822998046875, Learning Rate: 0.05\n",
      "Epoch [1261/20000], Loss: 4320.15380859375, Entropy -1427.45947265625, Learning Rate: 0.05\n",
      "Epoch [1262/20000], Loss: 4325.37109375, Entropy -1413.443115234375, Learning Rate: 0.05\n",
      "Epoch [1263/20000], Loss: 4265.6962890625, Entropy -1331.935546875, Learning Rate: 0.05\n",
      "Epoch [1264/20000], Loss: 4288.8828125, Entropy -1380.54443359375, Learning Rate: 0.05\n",
      "Epoch [1265/20000], Loss: 4339.69091796875, Entropy -1337.388671875, Learning Rate: 0.05\n",
      "Epoch [1266/20000], Loss: 4248.8037109375, Entropy -1407.626708984375, Learning Rate: 0.05\n",
      "Epoch [1267/20000], Loss: 4331.01953125, Entropy -1516.1640625, Learning Rate: 0.05\n",
      "Epoch [1268/20000], Loss: 4278.3720703125, Entropy -1342.3134765625, Learning Rate: 0.05\n",
      "Epoch [1269/20000], Loss: 4269.640625, Entropy -1376.83642578125, Learning Rate: 0.05\n",
      "Epoch [1270/20000], Loss: 4297.91015625, Entropy -1382.568359375, Learning Rate: 0.05\n",
      "Epoch [1271/20000], Loss: 4241.73291015625, Entropy -1399.4091796875, Learning Rate: 0.05\n",
      "Epoch [1272/20000], Loss: 4306.89892578125, Entropy -1315.5166015625, Learning Rate: 0.05\n",
      "Epoch [1273/20000], Loss: 4304.0859375, Entropy -1547.77392578125, Learning Rate: 0.05\n",
      "Epoch [1274/20000], Loss: 4254.59130859375, Entropy -1400.716064453125, Learning Rate: 0.05\n",
      "Epoch [1275/20000], Loss: 4294.39501953125, Entropy -1413.368896484375, Learning Rate: 0.05\n",
      "Epoch [1276/20000], Loss: 4299.72314453125, Entropy -1420.44482421875, Learning Rate: 0.05\n",
      "Epoch [1277/20000], Loss: 4284.7255859375, Entropy -1465.83740234375, Learning Rate: 0.05\n",
      "Epoch [1278/20000], Loss: 4287.52294921875, Entropy -1396.80078125, Learning Rate: 0.05\n",
      "Epoch [1279/20000], Loss: 4259.994140625, Entropy -1390.65283203125, Learning Rate: 0.05\n",
      "Epoch [1280/20000], Loss: 4437.3994140625, Entropy -1430.72265625, Learning Rate: 0.05\n",
      "Epoch [1281/20000], Loss: 4385.64990234375, Entropy -1476.20068359375, Learning Rate: 0.05\n",
      "Epoch [1282/20000], Loss: 4256.2099609375, Entropy -1396.9052734375, Learning Rate: 0.05\n",
      "Epoch [1283/20000], Loss: 4211.658203125, Entropy -1234.841796875, Learning Rate: 0.05\n",
      "Epoch [1284/20000], Loss: 4303.65869140625, Entropy -1445.265869140625, Learning Rate: 0.05\n",
      "Epoch [1285/20000], Loss: 4372.16748046875, Entropy -1480.58642578125, Learning Rate: 0.05\n",
      "Epoch [1286/20000], Loss: 4338.4951171875, Entropy -1454.285400390625, Learning Rate: 0.05\n",
      "Epoch [1287/20000], Loss: 4285.84375, Entropy -1416.755126953125, Learning Rate: 0.05\n",
      "Epoch [1288/20000], Loss: 4299.591796875, Entropy -1514.33740234375, Learning Rate: 0.05\n",
      "Epoch [1289/20000], Loss: 4318.37890625, Entropy -1441.7109375, Learning Rate: 0.05\n",
      "Epoch [1290/20000], Loss: 4212.39111328125, Entropy -1384.0341796875, Learning Rate: 0.05\n",
      "Epoch [1291/20000], Loss: 4317.0263671875, Entropy -1275.62646484375, Learning Rate: 0.05\n",
      "Epoch [1292/20000], Loss: 4292.4921875, Entropy -1448.92578125, Learning Rate: 0.05\n",
      "Epoch [1293/20000], Loss: 4300.36376953125, Entropy -1355.22900390625, Learning Rate: 0.05\n",
      "Epoch [1294/20000], Loss: 4288.50634765625, Entropy -1246.2744140625, Learning Rate: 0.05\n",
      "Epoch [1295/20000], Loss: 4246.947265625, Entropy -1424.0234375, Learning Rate: 0.05\n",
      "Epoch [1296/20000], Loss: 4356.32763671875, Entropy -1304.56591796875, Learning Rate: 0.05\n",
      "Epoch [1297/20000], Loss: 4327.96533203125, Entropy -1447.24365234375, Learning Rate: 0.05\n",
      "Epoch [1298/20000], Loss: 4313.23876953125, Entropy -1359.60791015625, Learning Rate: 0.05\n",
      "Epoch [1299/20000], Loss: 4324.70849609375, Entropy -1443.48583984375, Learning Rate: 0.05\n",
      "Epoch [1300/20000], Loss: 4318.01953125, Entropy -1430.615234375, Learning Rate: 0.05\n",
      "Epoch [1301/20000], Loss: 4379.97705078125, Entropy -1532.9833984375, Learning Rate: 0.05\n",
      "Epoch [1302/20000], Loss: 4273.68994140625, Entropy -1288.00830078125, Learning Rate: 0.05\n",
      "Epoch [1303/20000], Loss: 4370.64111328125, Entropy -1346.00927734375, Learning Rate: 0.05\n",
      "Epoch [1304/20000], Loss: 4311.6708984375, Entropy -1399.8232421875, Learning Rate: 0.05\n",
      "Epoch [1305/20000], Loss: 4320.01904296875, Entropy -1536.859375, Learning Rate: 0.05\n",
      "Epoch [1306/20000], Loss: 4310.1591796875, Entropy -1387.36962890625, Learning Rate: 0.05\n",
      "Epoch [1307/20000], Loss: 4280.17578125, Entropy -1441.5400390625, Learning Rate: 0.05\n",
      "Epoch [1308/20000], Loss: 4248.6474609375, Entropy -1422.59765625, Learning Rate: 0.05\n",
      "Epoch [1309/20000], Loss: 4281.4306640625, Entropy -1398.45556640625, Learning Rate: 0.05\n",
      "Epoch [1310/20000], Loss: 4244.9189453125, Entropy -1276.72265625, Learning Rate: 0.05\n",
      "Epoch [1311/20000], Loss: 4277.71728515625, Entropy -1488.783447265625, Learning Rate: 0.05\n",
      "Epoch [1312/20000], Loss: 4224.05908203125, Entropy -1304.05908203125, Learning Rate: 0.05\n",
      "Epoch [1313/20000], Loss: 4273.3046875, Entropy -1421.21044921875, Learning Rate: 0.05\n",
      "Epoch [1314/20000], Loss: 4254.1220703125, Entropy -1313.2919921875, Learning Rate: 0.05\n",
      "Epoch [1315/20000], Loss: 4311.537109375, Entropy -1457.1796875, Learning Rate: 0.05\n",
      "Epoch [1316/20000], Loss: 4274.0537109375, Entropy -1436.228759765625, Learning Rate: 0.05\n",
      "Epoch [1317/20000], Loss: 4256.99169921875, Entropy -1334.97607421875, Learning Rate: 0.05\n",
      "Epoch [1318/20000], Loss: 4253.8193359375, Entropy -1350.7607421875, Learning Rate: 0.05\n",
      "Epoch [1319/20000], Loss: 4247.7216796875, Entropy -1521.939208984375, Learning Rate: 0.05\n",
      "Epoch [1320/20000], Loss: 4281.287109375, Entropy -1532.86083984375, Learning Rate: 0.05\n",
      "Epoch [1321/20000], Loss: 4280.17041015625, Entropy -1427.107666015625, Learning Rate: 0.05\n",
      "Epoch [1322/20000], Loss: 4305.90234375, Entropy -1435.33154296875, Learning Rate: 0.05\n",
      "Epoch [1323/20000], Loss: 4261.56103515625, Entropy -1340.97607421875, Learning Rate: 0.05\n",
      "Epoch [1324/20000], Loss: 4290.625, Entropy -1369.36865234375, Learning Rate: 0.05\n",
      "Epoch [1325/20000], Loss: 4248.48193359375, Entropy -1388.31787109375, Learning Rate: 0.05\n",
      "Epoch [1326/20000], Loss: 4305.98486328125, Entropy -1396.23681640625, Learning Rate: 0.05\n",
      "Epoch [1327/20000], Loss: 4233.93994140625, Entropy -1384.208984375, Learning Rate: 0.05\n",
      "Epoch [1328/20000], Loss: 4298.59326171875, Entropy -1612.7109375, Learning Rate: 0.05\n",
      "Epoch [1329/20000], Loss: 4256.3095703125, Entropy -1444.60693359375, Learning Rate: 0.05\n",
      "Epoch [1330/20000], Loss: 4313.29296875, Entropy -1424.324462890625, Learning Rate: 0.05\n",
      "Epoch [1331/20000], Loss: 4460.39892578125, Entropy -1388.919921875, Learning Rate: 0.05\n",
      "Epoch [1332/20000], Loss: 4293.47705078125, Entropy -1399.6865234375, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1333/20000], Loss: 4222.17529296875, Entropy -1314.19091796875, Learning Rate: 0.05\n",
      "Epoch [1334/20000], Loss: 4231.109375, Entropy -1420.247314453125, Learning Rate: 0.05\n",
      "Epoch [1335/20000], Loss: 4290.83935546875, Entropy -1458.36328125, Learning Rate: 0.05\n",
      "Epoch [1336/20000], Loss: 4256.35693359375, Entropy -1449.00927734375, Learning Rate: 0.05\n",
      "Epoch [1337/20000], Loss: 4251.29443359375, Entropy -1425.931640625, Learning Rate: 0.05\n",
      "Epoch [1338/20000], Loss: 4327.66943359375, Entropy -1363.50634765625, Learning Rate: 0.05\n",
      "Epoch [1339/20000], Loss: 4324.51611328125, Entropy -1386.63818359375, Learning Rate: 0.05\n",
      "Epoch [1340/20000], Loss: 4206.11376953125, Entropy -1383.75390625, Learning Rate: 0.05\n",
      "Epoch [1341/20000], Loss: 4339.53857421875, Entropy -1508.85791015625, Learning Rate: 0.05\n",
      "Epoch [1342/20000], Loss: 4282.9794921875, Entropy -1336.740234375, Learning Rate: 0.05\n",
      "Epoch [1343/20000], Loss: 4210.09375, Entropy -1412.80712890625, Learning Rate: 0.05\n",
      "Epoch [1344/20000], Loss: 4286.5576171875, Entropy -1360.57861328125, Learning Rate: 0.05\n",
      "Epoch [1345/20000], Loss: 4231.00341796875, Entropy -1393.08984375, Learning Rate: 0.05\n",
      "Epoch [1346/20000], Loss: 4300.9462890625, Entropy -1444.845703125, Learning Rate: 0.05\n",
      "Epoch [1347/20000], Loss: 4260.744140625, Entropy -1453.829833984375, Learning Rate: 0.05\n",
      "Epoch [1348/20000], Loss: 4290.79150390625, Entropy -1320.2294921875, Learning Rate: 0.05\n",
      "Epoch [1349/20000], Loss: 4245.001953125, Entropy -1268.7294921875, Learning Rate: 0.05\n",
      "Epoch [1350/20000], Loss: 4314.6962890625, Entropy -1386.3623046875, Learning Rate: 0.05\n",
      "Epoch [1351/20000], Loss: 4293.787109375, Entropy -1445.963623046875, Learning Rate: 0.05\n",
      "Epoch [1352/20000], Loss: 4275.46728515625, Entropy -1380.54736328125, Learning Rate: 0.05\n",
      "Epoch [1353/20000], Loss: 4317.1728515625, Entropy -1467.89990234375, Learning Rate: 0.05\n",
      "Epoch [1354/20000], Loss: 4237.412109375, Entropy -1430.37548828125, Learning Rate: 0.05\n",
      "Epoch [1355/20000], Loss: 4266.759765625, Entropy -1231.73681640625, Learning Rate: 0.05\n",
      "Epoch [1356/20000], Loss: 4332.20849609375, Entropy -1375.12841796875, Learning Rate: 0.05\n",
      "Epoch [1357/20000], Loss: 4362.25341796875, Entropy -1466.33349609375, Learning Rate: 0.05\n",
      "Epoch [1358/20000], Loss: 4330.58984375, Entropy -1410.008056640625, Learning Rate: 0.05\n",
      "Epoch [1359/20000], Loss: 4293.876953125, Entropy -1314.1328125, Learning Rate: 0.05\n",
      "Epoch [1360/20000], Loss: 4225.8740234375, Entropy -1343.87353515625, Learning Rate: 0.05\n",
      "Epoch [1361/20000], Loss: 4310.3994140625, Entropy -1441.03125, Learning Rate: 0.05\n",
      "Epoch [1362/20000], Loss: 4314.986328125, Entropy -1282.02783203125, Learning Rate: 0.05\n",
      "Epoch [1363/20000], Loss: 4256.6025390625, Entropy -1321.22119140625, Learning Rate: 0.05\n",
      "Epoch [1364/20000], Loss: 4222.4404296875, Entropy -1324.74951171875, Learning Rate: 0.05\n",
      "Epoch [1365/20000], Loss: 4276.17333984375, Entropy -1561.179931640625, Learning Rate: 0.05\n",
      "Epoch [1366/20000], Loss: 4260.767578125, Entropy -1330.57470703125, Learning Rate: 0.05\n",
      "Epoch [1367/20000], Loss: 4265.86669921875, Entropy -1387.22021484375, Learning Rate: 0.05\n",
      "Epoch [1368/20000], Loss: 4221.07275390625, Entropy -1353.3974609375, Learning Rate: 0.05\n",
      "Epoch [1369/20000], Loss: 4264.0712890625, Entropy -1421.419189453125, Learning Rate: 0.05\n",
      "Epoch [1370/20000], Loss: 4248.134765625, Entropy -1317.9775390625, Learning Rate: 0.05\n",
      "Epoch [1371/20000], Loss: 4212.5791015625, Entropy -1409.89599609375, Learning Rate: 0.05\n",
      "Epoch [1372/20000], Loss: 4269.96142578125, Entropy -1327.07763671875, Learning Rate: 0.05\n",
      "Epoch [1373/20000], Loss: 4281.2294921875, Entropy -1477.73876953125, Learning Rate: 0.05\n",
      "Epoch [1374/20000], Loss: 4270.2861328125, Entropy -1304.86767578125, Learning Rate: 0.05\n",
      "Epoch [1375/20000], Loss: 4269.67333984375, Entropy -1421.35400390625, Learning Rate: 0.05\n",
      "Epoch [1376/20000], Loss: 4329.7890625, Entropy -1432.02392578125, Learning Rate: 0.05\n",
      "Epoch [1377/20000], Loss: 4239.89208984375, Entropy -1401.75927734375, Learning Rate: 0.05\n",
      "Epoch [1378/20000], Loss: 4319.2080078125, Entropy -1432.705810546875, Learning Rate: 0.05\n",
      "Epoch [1379/20000], Loss: 4230.0751953125, Entropy -1384.68798828125, Learning Rate: 0.05\n",
      "Epoch [1380/20000], Loss: 4235.927734375, Entropy -1386.06884765625, Learning Rate: 0.05\n",
      "Epoch [1381/20000], Loss: 4247.07861328125, Entropy -1391.11328125, Learning Rate: 0.05\n",
      "Epoch [1382/20000], Loss: 4307.662109375, Entropy -1438.846923828125, Learning Rate: 0.05\n",
      "Epoch [1383/20000], Loss: 4267.6962890625, Entropy -1400.4658203125, Learning Rate: 0.05\n",
      "Epoch [1384/20000], Loss: 4273.841796875, Entropy -1440.823486328125, Learning Rate: 0.05\n",
      "Epoch [1385/20000], Loss: 4288.05419921875, Entropy -1459.44287109375, Learning Rate: 0.05\n",
      "Epoch [1386/20000], Loss: 4250.87060546875, Entropy -1415.83154296875, Learning Rate: 0.05\n",
      "Epoch [1387/20000], Loss: 4275.94970703125, Entropy -1437.39013671875, Learning Rate: 0.05\n",
      "Epoch [1388/20000], Loss: 4230.5341796875, Entropy -1336.58935546875, Learning Rate: 0.05\n",
      "Epoch [1389/20000], Loss: 4250.38232421875, Entropy -1482.06689453125, Learning Rate: 0.05\n",
      "Epoch [1390/20000], Loss: 4193.775390625, Entropy -1316.443359375, Learning Rate: 0.05\n",
      "Epoch [1391/20000], Loss: 4248.19189453125, Entropy -1359.19287109375, Learning Rate: 0.05\n",
      "Epoch [1392/20000], Loss: 4262.0478515625, Entropy -1422.54150390625, Learning Rate: 0.05\n",
      "Epoch [1393/20000], Loss: 4189.89501953125, Entropy -1324.78271484375, Learning Rate: 0.05\n",
      "Epoch [1394/20000], Loss: 4330.94140625, Entropy -1442.4755859375, Learning Rate: 0.05\n",
      "Epoch [1395/20000], Loss: 4273.6015625, Entropy -1359.2529296875, Learning Rate: 0.05\n",
      "Epoch [1396/20000], Loss: 4234.8173828125, Entropy -1398.818115234375, Learning Rate: 0.05\n",
      "Epoch [1397/20000], Loss: 4255.4384765625, Entropy -1365.93505859375, Learning Rate: 0.05\n",
      "Epoch [1398/20000], Loss: 4242.7841796875, Entropy -1278.12646484375, Learning Rate: 0.05\n",
      "Epoch [1399/20000], Loss: 4271.4921875, Entropy -1340.75244140625, Learning Rate: 0.05\n",
      "Epoch [1400/20000], Loss: 4332.345703125, Entropy -1405.2099609375, Learning Rate: 0.05\n",
      "Epoch [1401/20000], Loss: 4272.11181640625, Entropy -1375.423828125, Learning Rate: 0.05\n",
      "Epoch [1402/20000], Loss: 4185.41552734375, Entropy -1364.142578125, Learning Rate: 0.05\n",
      "Epoch [1403/20000], Loss: 4271.3828125, Entropy -1401.30615234375, Learning Rate: 0.05\n",
      "Epoch [1404/20000], Loss: 4367.00048828125, Entropy -1441.082275390625, Learning Rate: 0.05\n",
      "Epoch [1405/20000], Loss: 4437.35546875, Entropy -1390.4091796875, Learning Rate: 0.05\n",
      "Epoch [1406/20000], Loss: 4212.5439453125, Entropy -1389.34423828125, Learning Rate: 0.05\n",
      "Epoch [1407/20000], Loss: 4316.83447265625, Entropy -1463.82421875, Learning Rate: 0.05\n",
      "Epoch [1408/20000], Loss: 4296.5927734375, Entropy -1403.783203125, Learning Rate: 0.05\n",
      "Epoch [1409/20000], Loss: 4241.62548828125, Entropy -1389.64697265625, Learning Rate: 0.05\n",
      "Epoch [1410/20000], Loss: 4240.1533203125, Entropy -1312.91650390625, Learning Rate: 0.05\n",
      "Epoch [1411/20000], Loss: 4265.29296875, Entropy -1412.86181640625, Learning Rate: 0.05\n",
      "Epoch [1412/20000], Loss: 4210.375, Entropy -1351.884765625, Learning Rate: 0.05\n",
      "Epoch [1413/20000], Loss: 4303.73583984375, Entropy -1433.5107421875, Learning Rate: 0.05\n",
      "Epoch [1414/20000], Loss: 4289.23828125, Entropy -1395.79345703125, Learning Rate: 0.05\n",
      "Epoch [1415/20000], Loss: 4234.41162109375, Entropy -1298.7177734375, Learning Rate: 0.05\n",
      "Epoch [1416/20000], Loss: 4217.39013671875, Entropy -1405.90869140625, Learning Rate: 0.05\n",
      "Epoch [1417/20000], Loss: 4206.15478515625, Entropy -1404.0576171875, Learning Rate: 0.05\n",
      "Epoch [1418/20000], Loss: 4311.03955078125, Entropy -1398.78564453125, Learning Rate: 0.05\n",
      "Epoch [1419/20000], Loss: 4266.19482421875, Entropy -1353.396484375, Learning Rate: 0.05\n",
      "Epoch [1420/20000], Loss: 4242.591796875, Entropy -1399.99267578125, Learning Rate: 0.05\n",
      "Epoch [1421/20000], Loss: 4246.55126953125, Entropy -1370.734375, Learning Rate: 0.05\n",
      "Epoch [1422/20000], Loss: 4233.21826171875, Entropy -1273.66552734375, Learning Rate: 0.05\n",
      "Epoch [1423/20000], Loss: 4240.708984375, Entropy -1429.89697265625, Learning Rate: 0.05\n",
      "Epoch [1424/20000], Loss: 4268.623046875, Entropy -1367.75537109375, Learning Rate: 0.05\n",
      "Epoch [1425/20000], Loss: 4256.1015625, Entropy -1437.1767578125, Learning Rate: 0.05\n",
      "Epoch [1426/20000], Loss: 4316.93310546875, Entropy -1315.22021484375, Learning Rate: 0.05\n",
      "Epoch [1427/20000], Loss: 4304.43994140625, Entropy -1383.89111328125, Learning Rate: 0.05\n",
      "Epoch [1428/20000], Loss: 4348.9736328125, Entropy -1529.767822265625, Learning Rate: 0.05\n",
      "Epoch [1429/20000], Loss: 4197.8828125, Entropy -1251.9326171875, Learning Rate: 0.05\n",
      "Epoch [1430/20000], Loss: 4343.6005859375, Entropy -1513.21923828125, Learning Rate: 0.05\n",
      "Epoch [1431/20000], Loss: 4261.0537109375, Entropy -1359.00146484375, Learning Rate: 0.05\n",
      "Epoch [1432/20000], Loss: 4296.34814453125, Entropy -1364.453125, Learning Rate: 0.05\n",
      "Epoch [1433/20000], Loss: 4207.33203125, Entropy -1459.7177734375, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1434/20000], Loss: 4303.3447265625, Entropy -1375.9833984375, Learning Rate: 0.05\n",
      "Epoch [1435/20000], Loss: 4235.39208984375, Entropy -1364.79638671875, Learning Rate: 0.05\n",
      "Epoch [1436/20000], Loss: 4214.22216796875, Entropy -1341.8740234375, Learning Rate: 0.05\n",
      "Epoch [1437/20000], Loss: 4249.58251953125, Entropy -1384.04248046875, Learning Rate: 0.05\n",
      "Epoch [1438/20000], Loss: 4259.4765625, Entropy -1387.54052734375, Learning Rate: 0.05\n",
      "Epoch [1439/20000], Loss: 4196.939453125, Entropy -1325.43212890625, Learning Rate: 0.05\n",
      "Epoch [1440/20000], Loss: 4134.822265625, Entropy -1309.61865234375, Learning Rate: 0.05\n",
      "Epoch [1441/20000], Loss: 4320.56201171875, Entropy -1422.8212890625, Learning Rate: 0.05\n",
      "Epoch [1442/20000], Loss: 4240.240234375, Entropy -1405.82958984375, Learning Rate: 0.05\n",
      "Epoch [1443/20000], Loss: 4213.01123046875, Entropy -1372.13720703125, Learning Rate: 0.05\n",
      "Epoch [1444/20000], Loss: 4223.60546875, Entropy -1417.39453125, Learning Rate: 0.05\n",
      "Epoch [1445/20000], Loss: 4306.1611328125, Entropy -1554.67919921875, Learning Rate: 0.05\n",
      "Epoch [1446/20000], Loss: 4227.8994140625, Entropy -1353.42041015625, Learning Rate: 0.05\n",
      "Epoch [1447/20000], Loss: 4215.61865234375, Entropy -1393.7177734375, Learning Rate: 0.05\n",
      "Epoch [1448/20000], Loss: 4281.76708984375, Entropy -1409.232421875, Learning Rate: 0.05\n",
      "Epoch [1449/20000], Loss: 4213.57275390625, Entropy -1426.189697265625, Learning Rate: 0.05\n",
      "Epoch [1450/20000], Loss: 4209.64794921875, Entropy -1363.23095703125, Learning Rate: 0.05\n",
      "Epoch [1451/20000], Loss: 4285.81640625, Entropy -1433.93212890625, Learning Rate: 0.05\n",
      "Epoch [1452/20000], Loss: 4305.033203125, Entropy -1512.33056640625, Learning Rate: 0.05\n",
      "Epoch [1453/20000], Loss: 4258.51025390625, Entropy -1356.77734375, Learning Rate: 0.05\n",
      "Epoch [1454/20000], Loss: 4234.0576171875, Entropy -1322.2607421875, Learning Rate: 0.05\n",
      "Epoch [1455/20000], Loss: 4356.79736328125, Entropy -1379.2978515625, Learning Rate: 0.05\n",
      "Epoch [1456/20000], Loss: 4230.66796875, Entropy -1421.51171875, Learning Rate: 0.05\n",
      "Epoch [1457/20000], Loss: 4239.017578125, Entropy -1404.546630859375, Learning Rate: 0.05\n",
      "Epoch [1458/20000], Loss: 4261.2861328125, Entropy -1424.912109375, Learning Rate: 0.05\n",
      "Epoch [1459/20000], Loss: 4229.45263671875, Entropy -1269.1259765625, Learning Rate: 0.05\n",
      "Epoch [1460/20000], Loss: 4267.00244140625, Entropy -1406.65966796875, Learning Rate: 0.05\n",
      "Epoch [1461/20000], Loss: 4278.822265625, Entropy -1462.13134765625, Learning Rate: 0.05\n",
      "Epoch [1462/20000], Loss: 4212.17626953125, Entropy -1427.698486328125, Learning Rate: 0.05\n",
      "Epoch [1463/20000], Loss: 4226.203125, Entropy -1382.87353515625, Learning Rate: 0.05\n",
      "Epoch [1464/20000], Loss: 4201.37890625, Entropy -1363.4775390625, Learning Rate: 0.05\n",
      "Epoch [1465/20000], Loss: 4237.34619140625, Entropy -1435.932373046875, Learning Rate: 0.05\n",
      "Epoch [1466/20000], Loss: 4241.17822265625, Entropy -1458.155029296875, Learning Rate: 0.05\n",
      "Epoch [1467/20000], Loss: 4171.89013671875, Entropy -1300.998046875, Learning Rate: 0.05\n",
      "Epoch [1468/20000], Loss: 4269.40673828125, Entropy -1396.228759765625, Learning Rate: 0.05\n",
      "Epoch [1469/20000], Loss: 4289.05322265625, Entropy -1347.50390625, Learning Rate: 0.05\n",
      "Epoch [1470/20000], Loss: 4279.6328125, Entropy -1399.104736328125, Learning Rate: 0.05\n",
      "Epoch [1471/20000], Loss: 4264.1015625, Entropy -1362.43017578125, Learning Rate: 0.05\n",
      "Epoch [1472/20000], Loss: 4263.72705078125, Entropy -1411.396484375, Learning Rate: 0.05\n",
      "Epoch [1473/20000], Loss: 4196.58447265625, Entropy -1354.955078125, Learning Rate: 0.05\n",
      "Epoch [1474/20000], Loss: 4226.0146484375, Entropy -1204.3818359375, Learning Rate: 0.05\n",
      "Epoch [1475/20000], Loss: 4219.212890625, Entropy -1239.32763671875, Learning Rate: 0.05\n",
      "Epoch [1476/20000], Loss: 4236.52783203125, Entropy -1398.5068359375, Learning Rate: 0.05\n",
      "Epoch [1477/20000], Loss: 4184.732421875, Entropy -1265.85693359375, Learning Rate: 0.05\n",
      "Epoch [1478/20000], Loss: 4222.46728515625, Entropy -1232.50390625, Learning Rate: 0.05\n",
      "Epoch [1479/20000], Loss: 4214.482421875, Entropy -1304.92529296875, Learning Rate: 0.05\n",
      "Epoch [1480/20000], Loss: 4235.8203125, Entropy -1438.8935546875, Learning Rate: 0.05\n",
      "Epoch [1481/20000], Loss: 4244.4990234375, Entropy -1394.004638671875, Learning Rate: 0.05\n",
      "Epoch [1482/20000], Loss: 4222.7646484375, Entropy -1311.978515625, Learning Rate: 0.05\n",
      "Epoch [1483/20000], Loss: 4282.06005859375, Entropy -1369.69580078125, Learning Rate: 0.05\n",
      "Epoch [1484/20000], Loss: 4239.5869140625, Entropy -1352.11767578125, Learning Rate: 0.05\n",
      "Epoch [1485/20000], Loss: 4271.08251953125, Entropy -1439.65478515625, Learning Rate: 0.05\n",
      "Epoch [1486/20000], Loss: 4273.79541015625, Entropy -1382.37060546875, Learning Rate: 0.05\n",
      "Epoch [1487/20000], Loss: 4275.7265625, Entropy -1456.35693359375, Learning Rate: 0.05\n",
      "Epoch [1488/20000], Loss: 4231.603515625, Entropy -1432.088134765625, Learning Rate: 0.05\n",
      "Epoch [1489/20000], Loss: 4188.763671875, Entropy -1383.4755859375, Learning Rate: 0.05\n",
      "Epoch [1490/20000], Loss: 4242.81103515625, Entropy -1313.3095703125, Learning Rate: 0.05\n",
      "Epoch [1491/20000], Loss: 4197.72705078125, Entropy -1318.45458984375, Learning Rate: 0.05\n",
      "Epoch [1492/20000], Loss: 4279.10498046875, Entropy -1381.04541015625, Learning Rate: 0.05\n",
      "Epoch [1493/20000], Loss: 4237.2685546875, Entropy -1381.61669921875, Learning Rate: 0.05\n",
      "Epoch [1494/20000], Loss: 4201.68115234375, Entropy -1259.9609375, Learning Rate: 0.05\n",
      "Epoch [1495/20000], Loss: 4208.29638671875, Entropy -1339.37744140625, Learning Rate: 0.05\n",
      "Epoch [1496/20000], Loss: 4330.908203125, Entropy -1429.64013671875, Learning Rate: 0.05\n",
      "Epoch [1497/20000], Loss: 4257.76220703125, Entropy -1477.133544921875, Learning Rate: 0.05\n",
      "Epoch [1498/20000], Loss: 4289.453125, Entropy -1362.93359375, Learning Rate: 0.05\n",
      "Epoch [1499/20000], Loss: 4196.60498046875, Entropy -1372.0087890625, Learning Rate: 0.05\n",
      "Epoch [1500/20000], Loss: 4167.1982421875, Entropy -1360.8857421875, Learning Rate: 0.05\n",
      "Epoch [1501/20000], Loss: 4145.0078125, Entropy -1230.44287109375, Learning Rate: 0.05\n",
      "Epoch [1502/20000], Loss: 4255.08154296875, Entropy -1448.137939453125, Learning Rate: 0.05\n",
      "Epoch [1503/20000], Loss: 4199.92041015625, Entropy -1362.56005859375, Learning Rate: 0.05\n",
      "Epoch [1504/20000], Loss: 4258.06103515625, Entropy -1407.289794921875, Learning Rate: 0.05\n",
      "Epoch [1505/20000], Loss: 4266.91259765625, Entropy -1369.01416015625, Learning Rate: 0.05\n",
      "Epoch [1506/20000], Loss: 4204.0703125, Entropy -1373.28125, Learning Rate: 0.05\n",
      "Epoch [1507/20000], Loss: 4266.36083984375, Entropy -1386.869140625, Learning Rate: 0.05\n",
      "Epoch [1508/20000], Loss: 4262.96728515625, Entropy -1330.99609375, Learning Rate: 0.05\n",
      "Epoch [1509/20000], Loss: 4192.01904296875, Entropy -1308.03857421875, Learning Rate: 0.05\n",
      "Epoch [1510/20000], Loss: 4221.25830078125, Entropy -1399.647705078125, Learning Rate: 0.05\n",
      "Epoch [1511/20000], Loss: 4273.34423828125, Entropy -1439.301025390625, Learning Rate: 0.05\n",
      "Epoch [1512/20000], Loss: 4353.478515625, Entropy -1462.219970703125, Learning Rate: 0.05\n",
      "Epoch [1513/20000], Loss: 4214.423828125, Entropy -1281.59521484375, Learning Rate: 0.05\n",
      "Epoch [1514/20000], Loss: 4311.5361328125, Entropy -1406.66943359375, Learning Rate: 0.05\n",
      "Epoch [1515/20000], Loss: 4192.1220703125, Entropy -1322.27587890625, Learning Rate: 0.05\n",
      "Epoch [1516/20000], Loss: 4250.5439453125, Entropy -1334.17822265625, Learning Rate: 0.05\n",
      "Epoch [1517/20000], Loss: 4244.34521484375, Entropy -1495.1796875, Learning Rate: 0.05\n",
      "Epoch [1518/20000], Loss: 4211.9951171875, Entropy -1309.3525390625, Learning Rate: 0.05\n",
      "Epoch [1519/20000], Loss: 4296.443359375, Entropy -1353.1455078125, Learning Rate: 0.05\n",
      "Epoch [1520/20000], Loss: 4247.5244140625, Entropy -1500.840576171875, Learning Rate: 0.05\n",
      "Epoch [1521/20000], Loss: 4278.294921875, Entropy -1396.911376953125, Learning Rate: 0.05\n",
      "Epoch [1522/20000], Loss: 4285.68212890625, Entropy -1332.2314453125, Learning Rate: 0.05\n",
      "Epoch [1523/20000], Loss: 4198.74755859375, Entropy -1442.8623046875, Learning Rate: 0.05\n",
      "Epoch [1524/20000], Loss: 4263.0634765625, Entropy -1467.928955078125, Learning Rate: 0.05\n",
      "Epoch [1525/20000], Loss: 4276.37451171875, Entropy -1459.90673828125, Learning Rate: 0.05\n",
      "Epoch [1526/20000], Loss: 4242.150390625, Entropy -1372.55615234375, Learning Rate: 0.05\n",
      "Epoch [1527/20000], Loss: 4202.158203125, Entropy -1295.25244140625, Learning Rate: 0.05\n",
      "Epoch [1528/20000], Loss: 4294.83349609375, Entropy -1325.23779296875, Learning Rate: 0.05\n",
      "Epoch [1529/20000], Loss: 4266.99609375, Entropy -1433.76708984375, Learning Rate: 0.05\n",
      "Epoch [1530/20000], Loss: 4174.42578125, Entropy -1318.7490234375, Learning Rate: 0.05\n",
      "Epoch [1531/20000], Loss: 4283.16064453125, Entropy -1366.14794921875, Learning Rate: 0.05\n",
      "Epoch [1532/20000], Loss: 4191.69921875, Entropy -1306.421875, Learning Rate: 0.05\n",
      "Epoch [1533/20000], Loss: 4218.677734375, Entropy -1408.726318359375, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1534/20000], Loss: 4360.5390625, Entropy -1430.31787109375, Learning Rate: 0.05\n",
      "Epoch [1535/20000], Loss: 4178.7412109375, Entropy -1339.84130859375, Learning Rate: 0.05\n",
      "Epoch [1536/20000], Loss: 4167.40966796875, Entropy -1300.18994140625, Learning Rate: 0.05\n",
      "Epoch [1537/20000], Loss: 4250.8935546875, Entropy -1383.08935546875, Learning Rate: 0.05\n",
      "Epoch [1538/20000], Loss: 4276.78564453125, Entropy -1413.791015625, Learning Rate: 0.05\n",
      "Epoch [1539/20000], Loss: 4222.810546875, Entropy -1285.884765625, Learning Rate: 0.05\n",
      "Epoch [1540/20000], Loss: 4409.61669921875, Entropy -1318.46533203125, Learning Rate: 0.05\n",
      "Epoch [1541/20000], Loss: 4321.26513671875, Entropy -1368.31884765625, Learning Rate: 0.05\n",
      "Epoch [1542/20000], Loss: 4279.00146484375, Entropy -1337.048828125, Learning Rate: 0.05\n",
      "Epoch [1543/20000], Loss: 4211.9814453125, Entropy -1378.2626953125, Learning Rate: 0.05\n",
      "Epoch [1544/20000], Loss: 4204.45703125, Entropy -1399.171875, Learning Rate: 0.05\n",
      "Epoch [1545/20000], Loss: 4212.5693359375, Entropy -1294.43310546875, Learning Rate: 0.05\n",
      "Epoch [1546/20000], Loss: 4208.07080078125, Entropy -1426.76171875, Learning Rate: 0.05\n",
      "Epoch [1547/20000], Loss: 4224.86669921875, Entropy -1329.51513671875, Learning Rate: 0.05\n",
      "Epoch [1548/20000], Loss: 4195.9072265625, Entropy -1355.68115234375, Learning Rate: 0.05\n",
      "Epoch [1549/20000], Loss: 4244.53564453125, Entropy -1473.539306640625, Learning Rate: 0.05\n",
      "Epoch [1550/20000], Loss: 4255.40966796875, Entropy -1255.78515625, Learning Rate: 0.05\n",
      "Epoch [1551/20000], Loss: 4277.7568359375, Entropy -1244.89599609375, Learning Rate: 0.05\n",
      "Epoch [1552/20000], Loss: 4211.46923828125, Entropy -1350.267578125, Learning Rate: 0.05\n",
      "Epoch [1553/20000], Loss: 4285.8779296875, Entropy -1468.12646484375, Learning Rate: 0.05\n",
      "Epoch [1554/20000], Loss: 4177.71044921875, Entropy -1336.83349609375, Learning Rate: 0.05\n",
      "Epoch [1555/20000], Loss: 4193.87841796875, Entropy -1438.811767578125, Learning Rate: 0.05\n",
      "Epoch [1556/20000], Loss: 4189.0595703125, Entropy -1374.259765625, Learning Rate: 0.05\n",
      "Epoch [1557/20000], Loss: 4235.67578125, Entropy -1421.85498046875, Learning Rate: 0.05\n",
      "Epoch [1558/20000], Loss: 4202.4775390625, Entropy -1386.27294921875, Learning Rate: 0.05\n",
      "Epoch [1559/20000], Loss: 4285.63818359375, Entropy -1364.33154296875, Learning Rate: 0.05\n",
      "Epoch [1560/20000], Loss: 4237.388671875, Entropy -1327.30517578125, Learning Rate: 0.05\n",
      "Epoch [1561/20000], Loss: 4182.123046875, Entropy -1272.39697265625, Learning Rate: 0.05\n",
      "Epoch [1562/20000], Loss: 4101.63916015625, Entropy -1283.3056640625, Learning Rate: 0.05\n",
      "Epoch [1563/20000], Loss: 4180.88232421875, Entropy -1406.808837890625, Learning Rate: 0.05\n",
      "Epoch [1564/20000], Loss: 4279.39892578125, Entropy -1445.0283203125, Learning Rate: 0.05\n",
      "Epoch [1565/20000], Loss: 4190.43359375, Entropy -1330.86767578125, Learning Rate: 0.05\n",
      "Epoch [1566/20000], Loss: 4196.30322265625, Entropy -1234.6552734375, Learning Rate: 0.05\n",
      "Epoch [1567/20000], Loss: 4283.96142578125, Entropy -1527.927001953125, Learning Rate: 0.05\n",
      "Epoch [1568/20000], Loss: 4263.2861328125, Entropy -1493.1298828125, Learning Rate: 0.05\n",
      "Epoch [1569/20000], Loss: 4229.7890625, Entropy -1342.8583984375, Learning Rate: 0.05\n",
      "Epoch [1570/20000], Loss: 4284.2216796875, Entropy -1395.379150390625, Learning Rate: 0.05\n",
      "Epoch [1571/20000], Loss: 4196.35302734375, Entropy -1237.11474609375, Learning Rate: 0.05\n",
      "Epoch [1572/20000], Loss: 4260.068359375, Entropy -1377.94970703125, Learning Rate: 0.05\n",
      "Epoch [1573/20000], Loss: 4289.53125, Entropy -1593.420166015625, Learning Rate: 0.05\n",
      "Epoch [1574/20000], Loss: 4214.88037109375, Entropy -1421.52001953125, Learning Rate: 0.05\n",
      "Epoch [1575/20000], Loss: 4175.39453125, Entropy -1227.6240234375, Learning Rate: 0.05\n",
      "Epoch [1576/20000], Loss: 4295.88330078125, Entropy -1450.181396484375, Learning Rate: 0.05\n",
      "Epoch [1577/20000], Loss: 4248.4375, Entropy -1375.93408203125, Learning Rate: 0.05\n",
      "Epoch [1578/20000], Loss: 4179.7958984375, Entropy -1270.50146484375, Learning Rate: 0.05\n",
      "Epoch [1579/20000], Loss: 4289.2890625, Entropy -1388.42431640625, Learning Rate: 0.05\n",
      "Epoch [1580/20000], Loss: 4265.16650390625, Entropy -1405.37060546875, Learning Rate: 0.05\n",
      "Epoch [1581/20000], Loss: 4273.4677734375, Entropy -1363.1435546875, Learning Rate: 0.05\n",
      "Epoch [1582/20000], Loss: 4182.8232421875, Entropy -1320.2763671875, Learning Rate: 0.05\n",
      "Epoch [1583/20000], Loss: 4253.85498046875, Entropy -1500.768798828125, Learning Rate: 0.05\n",
      "Epoch [1584/20000], Loss: 4316.427734375, Entropy -1453.97216796875, Learning Rate: 0.05\n",
      "Epoch [1585/20000], Loss: 4189.06005859375, Entropy -1322.5966796875, Learning Rate: 0.05\n",
      "Epoch [1586/20000], Loss: 4217.48046875, Entropy -1383.09765625, Learning Rate: 0.05\n",
      "Epoch [1587/20000], Loss: 4240.76416015625, Entropy -1351.6015625, Learning Rate: 0.05\n",
      "Epoch [1588/20000], Loss: 4198.2275390625, Entropy -1334.3349609375, Learning Rate: 0.05\n",
      "Epoch [1589/20000], Loss: 4202.2939453125, Entropy -1344.84619140625, Learning Rate: 0.05\n",
      "Epoch [1590/20000], Loss: 4232.8115234375, Entropy -1286.62646484375, Learning Rate: 0.05\n",
      "Epoch [1591/20000], Loss: 4260.78369140625, Entropy -1324.86669921875, Learning Rate: 0.05\n",
      "Epoch [1592/20000], Loss: 4228.10888671875, Entropy -1423.458984375, Learning Rate: 0.05\n",
      "Epoch [1593/20000], Loss: 4157.85400390625, Entropy -1270.99853515625, Learning Rate: 0.05\n",
      "Epoch [1594/20000], Loss: 4192.693359375, Entropy -1167.240234375, Learning Rate: 0.05\n",
      "Epoch [1595/20000], Loss: 4273.8974609375, Entropy -1417.08740234375, Learning Rate: 0.05\n",
      "Epoch [1596/20000], Loss: 4254.1357421875, Entropy -1421.602294921875, Learning Rate: 0.05\n",
      "Epoch [1597/20000], Loss: 4184.052734375, Entropy -1247.84521484375, Learning Rate: 0.05\n",
      "Epoch [1598/20000], Loss: 4296.6640625, Entropy -1313.0078125, Learning Rate: 0.05\n",
      "Epoch [1599/20000], Loss: 4253.173828125, Entropy -1290.68896484375, Learning Rate: 0.05\n",
      "Epoch [1600/20000], Loss: 4222.44140625, Entropy -1358.5009765625, Learning Rate: 0.05\n",
      "Epoch [1601/20000], Loss: 4235.2880859375, Entropy -1354.908203125, Learning Rate: 0.05\n",
      "Epoch [1602/20000], Loss: 4217.2998046875, Entropy -1347.31396484375, Learning Rate: 0.05\n",
      "Epoch [1603/20000], Loss: 4190.74853515625, Entropy -1338.37060546875, Learning Rate: 0.05\n",
      "Epoch [1604/20000], Loss: 4168.3896484375, Entropy -1313.52978515625, Learning Rate: 0.05\n",
      "Epoch [1605/20000], Loss: 4295.291015625, Entropy -1362.80615234375, Learning Rate: 0.05\n",
      "Epoch [1606/20000], Loss: 4206.2392578125, Entropy -1387.8017578125, Learning Rate: 0.05\n",
      "Epoch [1607/20000], Loss: 4219.7705078125, Entropy -1257.43505859375, Learning Rate: 0.05\n",
      "Epoch [1608/20000], Loss: 4194.517578125, Entropy -1404.064697265625, Learning Rate: 0.05\n",
      "Epoch [1609/20000], Loss: 4258.92236328125, Entropy -1392.89111328125, Learning Rate: 0.05\n",
      "Epoch [1610/20000], Loss: 4208.54833984375, Entropy -1355.18798828125, Learning Rate: 0.05\n",
      "Epoch [1611/20000], Loss: 4279.3671875, Entropy -1400.218017578125, Learning Rate: 0.05\n",
      "Epoch [1612/20000], Loss: 4208.8173828125, Entropy -1327.81884765625, Learning Rate: 0.05\n",
      "Epoch [1613/20000], Loss: 4222.24609375, Entropy -1345.86474609375, Learning Rate: 0.05\n",
      "Epoch [1614/20000], Loss: 4239.43701171875, Entropy -1434.18701171875, Learning Rate: 0.05\n",
      "Epoch [1615/20000], Loss: 4175.52099609375, Entropy -1252.6455078125, Learning Rate: 0.05\n",
      "Epoch [1616/20000], Loss: 4259.830078125, Entropy -1372.6064453125, Learning Rate: 0.05\n",
      "Epoch [1617/20000], Loss: 4183.525390625, Entropy -1207.86767578125, Learning Rate: 0.05\n",
      "Epoch [1618/20000], Loss: 4162.5908203125, Entropy -1371.490234375, Learning Rate: 0.05\n",
      "Epoch [1619/20000], Loss: 4235.7919921875, Entropy -1372.4091796875, Learning Rate: 0.05\n",
      "Epoch [1620/20000], Loss: 4198.859375, Entropy -1295.94970703125, Learning Rate: 0.05\n",
      "Epoch [1621/20000], Loss: 4219.19873046875, Entropy -1438.0859375, Learning Rate: 0.05\n",
      "Epoch [1622/20000], Loss: 4208.9951171875, Entropy -1395.648681640625, Learning Rate: 0.05\n",
      "Epoch [1623/20000], Loss: 4173.10986328125, Entropy -1406.02197265625, Learning Rate: 0.05\n",
      "Epoch [1624/20000], Loss: 4284.6787109375, Entropy -1228.22802734375, Learning Rate: 0.05\n",
      "Epoch [1625/20000], Loss: 4210.07177734375, Entropy -1294.154296875, Learning Rate: 0.05\n",
      "Epoch [1626/20000], Loss: 4258.02099609375, Entropy -1419.260986328125, Learning Rate: 0.05\n",
      "Epoch [1627/20000], Loss: 4188.42919921875, Entropy -1269.27978515625, Learning Rate: 0.05\n",
      "Epoch [1628/20000], Loss: 4219.87744140625, Entropy -1413.134765625, Learning Rate: 0.05\n",
      "Epoch [1629/20000], Loss: 4250.5791015625, Entropy -1410.962646484375, Learning Rate: 0.05\n",
      "Epoch [1630/20000], Loss: 4214.03076171875, Entropy -1366.17919921875, Learning Rate: 0.05\n",
      "Epoch [1631/20000], Loss: 4232.4052734375, Entropy -1444.61865234375, Learning Rate: 0.05\n",
      "Epoch [1632/20000], Loss: 4235.0205078125, Entropy -1338.52197265625, Learning Rate: 0.05\n",
      "Epoch [1633/20000], Loss: 4208.6611328125, Entropy -1338.5546875, Learning Rate: 0.05\n",
      "Epoch [1634/20000], Loss: 4149.42626953125, Entropy -1353.60107421875, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1635/20000], Loss: 4268.25537109375, Entropy -1354.70947265625, Learning Rate: 0.05\n",
      "Epoch [1636/20000], Loss: 4183.14501953125, Entropy -1306.05419921875, Learning Rate: 0.05\n",
      "Epoch [1637/20000], Loss: 4214.91259765625, Entropy -1411.262939453125, Learning Rate: 0.05\n",
      "Epoch [1638/20000], Loss: 4249.7705078125, Entropy -1424.610107421875, Learning Rate: 0.05\n",
      "Epoch [1639/20000], Loss: 4248.27880859375, Entropy -1426.399658203125, Learning Rate: 0.05\n",
      "Epoch [1640/20000], Loss: 4188.22265625, Entropy -1361.736328125, Learning Rate: 0.05\n",
      "Epoch [1641/20000], Loss: 4256.96533203125, Entropy -1484.55029296875, Learning Rate: 0.05\n",
      "Epoch [1642/20000], Loss: 4150.71826171875, Entropy -1263.025390625, Learning Rate: 0.05\n",
      "Epoch [1643/20000], Loss: 4171.8388671875, Entropy -1345.0576171875, Learning Rate: 0.05\n",
      "Epoch [1644/20000], Loss: 4208.38427734375, Entropy -1274.9931640625, Learning Rate: 0.05\n",
      "Epoch [1645/20000], Loss: 4219.10986328125, Entropy -1371.56005859375, Learning Rate: 0.05\n",
      "Epoch [1646/20000], Loss: 4229.1123046875, Entropy -1298.65966796875, Learning Rate: 0.05\n",
      "Epoch [1647/20000], Loss: 4240.76025390625, Entropy -1327.5146484375, Learning Rate: 0.05\n",
      "Epoch [1648/20000], Loss: 4162.9541015625, Entropy -1361.59521484375, Learning Rate: 0.05\n",
      "Epoch [1649/20000], Loss: 4213.30419921875, Entropy -1388.51220703125, Learning Rate: 0.05\n",
      "Epoch [1650/20000], Loss: 4197.890625, Entropy -1338.86865234375, Learning Rate: 0.05\n",
      "Epoch [1651/20000], Loss: 4222.736328125, Entropy -1511.162841796875, Learning Rate: 0.05\n",
      "Epoch [1652/20000], Loss: 4208.828125, Entropy -1369.71484375, Learning Rate: 0.05\n",
      "Epoch [1653/20000], Loss: 4214.59326171875, Entropy -1338.67333984375, Learning Rate: 0.05\n",
      "Epoch [1654/20000], Loss: 4168.80078125, Entropy -1274.7900390625, Learning Rate: 0.05\n",
      "Epoch [1655/20000], Loss: 4258.05517578125, Entropy -1388.37353515625, Learning Rate: 0.05\n",
      "Epoch [1656/20000], Loss: 4217.0322265625, Entropy -1460.930908203125, Learning Rate: 0.05\n",
      "Epoch [1657/20000], Loss: 4168.32275390625, Entropy -1302.638671875, Learning Rate: 0.05\n",
      "Epoch [1658/20000], Loss: 4230.71875, Entropy -1289.328125, Learning Rate: 0.05\n",
      "Epoch [1659/20000], Loss: 4280.20556640625, Entropy -1444.724609375, Learning Rate: 0.05\n",
      "Epoch [1660/20000], Loss: 4206.703125, Entropy -1308.0849609375, Learning Rate: 0.05\n",
      "Epoch [1661/20000], Loss: 4171.44091796875, Entropy -1261.02197265625, Learning Rate: 0.05\n",
      "Epoch [1662/20000], Loss: 4209.58349609375, Entropy -1290.5791015625, Learning Rate: 0.05\n",
      "Epoch [1663/20000], Loss: 4192.71875, Entropy -1425.439453125, Learning Rate: 0.05\n",
      "Epoch [1664/20000], Loss: 4184.77490234375, Entropy -1222.25244140625, Learning Rate: 0.05\n",
      "Epoch [1665/20000], Loss: 4151.3818359375, Entropy -1323.1171875, Learning Rate: 0.05\n",
      "Epoch [1666/20000], Loss: 4205.89990234375, Entropy -1385.55126953125, Learning Rate: 0.05\n",
      "Epoch [1667/20000], Loss: 4157.04833984375, Entropy -1279.10302734375, Learning Rate: 0.05\n",
      "Epoch [1668/20000], Loss: 4261.19970703125, Entropy -1437.3505859375, Learning Rate: 0.05\n",
      "Epoch [1669/20000], Loss: 4201.66845703125, Entropy -1242.74951171875, Learning Rate: 0.05\n",
      "Epoch [1670/20000], Loss: 4185.564453125, Entropy -1347.3974609375, Learning Rate: 0.05\n",
      "Epoch [1671/20000], Loss: 4225.70263671875, Entropy -1368.4609375, Learning Rate: 0.05\n",
      "Epoch [1672/20000], Loss: 4262.822265625, Entropy -1237.64892578125, Learning Rate: 0.05\n",
      "Epoch [1673/20000], Loss: 4214.455078125, Entropy -1295.64404296875, Learning Rate: 0.05\n",
      "Epoch [1674/20000], Loss: 4223.97021484375, Entropy -1392.942626953125, Learning Rate: 0.05\n",
      "Epoch [1675/20000], Loss: 4193.78125, Entropy -1415.95654296875, Learning Rate: 0.05\n",
      "Epoch [1676/20000], Loss: 4195.5234375, Entropy -1356.12451171875, Learning Rate: 0.05\n",
      "Epoch [1677/20000], Loss: 4264.35400390625, Entropy -1484.92041015625, Learning Rate: 0.05\n",
      "Epoch [1678/20000], Loss: 4225.56640625, Entropy -1364.02294921875, Learning Rate: 0.05\n",
      "Epoch [1679/20000], Loss: 4188.2451171875, Entropy -1363.8740234375, Learning Rate: 0.05\n",
      "Epoch [1680/20000], Loss: 4294.44921875, Entropy -1352.98193359375, Learning Rate: 0.05\n",
      "Epoch [1681/20000], Loss: 4223.73828125, Entropy -1417.083984375, Learning Rate: 0.05\n",
      "Epoch [1682/20000], Loss: 4295.73046875, Entropy -1463.36376953125, Learning Rate: 0.05\n",
      "Epoch [1683/20000], Loss: 4249.7216796875, Entropy -1378.92822265625, Learning Rate: 0.05\n",
      "Epoch [1684/20000], Loss: 4117.87451171875, Entropy -1204.1669921875, Learning Rate: 0.05\n",
      "Epoch [1685/20000], Loss: 4158.599609375, Entropy -1232.8701171875, Learning Rate: 0.05\n",
      "Epoch [1686/20000], Loss: 4209.88232421875, Entropy -1371.322265625, Learning Rate: 0.05\n",
      "Epoch [1687/20000], Loss: 4192.6552734375, Entropy -1338.9541015625, Learning Rate: 0.05\n",
      "Epoch [1688/20000], Loss: 4234.03955078125, Entropy -1397.283447265625, Learning Rate: 0.05\n",
      "Epoch [1689/20000], Loss: 4165.81884765625, Entropy -1323.564453125, Learning Rate: 0.05\n",
      "Epoch [1690/20000], Loss: 4152.21630859375, Entropy -1286.93505859375, Learning Rate: 0.05\n",
      "Epoch [1691/20000], Loss: 4199.73876953125, Entropy -1416.71630859375, Learning Rate: 0.05\n",
      "Epoch [1692/20000], Loss: 4154.84375, Entropy -1328.92333984375, Learning Rate: 0.05\n",
      "Epoch [1693/20000], Loss: 4194.556640625, Entropy -1451.53076171875, Learning Rate: 0.05\n",
      "Epoch [1694/20000], Loss: 4195.63720703125, Entropy -1367.865234375, Learning Rate: 0.05\n",
      "Epoch [1695/20000], Loss: 4291.39697265625, Entropy -1299.12646484375, Learning Rate: 0.05\n",
      "Epoch [1696/20000], Loss: 4191.2744140625, Entropy -1276.42138671875, Learning Rate: 0.05\n",
      "Epoch [1697/20000], Loss: 4220.56591796875, Entropy -1415.130859375, Learning Rate: 0.05\n",
      "Epoch [1698/20000], Loss: 4183.44482421875, Entropy -1368.09423828125, Learning Rate: 0.05\n",
      "Epoch [1699/20000], Loss: 4246.9375, Entropy -1393.12353515625, Learning Rate: 0.05\n",
      "Epoch [1700/20000], Loss: 4210.1240234375, Entropy -1387.240234375, Learning Rate: 0.05\n",
      "Epoch [1701/20000], Loss: 4236.7021484375, Entropy -1321.48583984375, Learning Rate: 0.05\n",
      "Epoch [1702/20000], Loss: 4201.66455078125, Entropy -1338.6806640625, Learning Rate: 0.05\n",
      "Epoch [1703/20000], Loss: 4205.2685546875, Entropy -1289.29248046875, Learning Rate: 0.05\n",
      "Epoch [1704/20000], Loss: 4166.69580078125, Entropy -1350.57275390625, Learning Rate: 0.05\n",
      "Epoch [1705/20000], Loss: 4193.43505859375, Entropy -1251.9482421875, Learning Rate: 0.05\n",
      "Epoch [1706/20000], Loss: 4207.376953125, Entropy -1301.36767578125, Learning Rate: 0.05\n",
      "Epoch [1707/20000], Loss: 4229.75048828125, Entropy -1330.21923828125, Learning Rate: 0.05\n",
      "Epoch [1708/20000], Loss: 4180.35693359375, Entropy -1305.32177734375, Learning Rate: 0.05\n",
      "Epoch [1709/20000], Loss: 4200.5693359375, Entropy -1367.3193359375, Learning Rate: 0.05\n",
      "Epoch [1710/20000], Loss: 4221.89453125, Entropy -1342.7294921875, Learning Rate: 0.05\n",
      "Epoch [1711/20000], Loss: 4185.23291015625, Entropy -1313.15087890625, Learning Rate: 0.05\n",
      "Epoch [1712/20000], Loss: 4257.86376953125, Entropy -1486.1640625, Learning Rate: 0.05\n",
      "Epoch [1713/20000], Loss: 4191.93603515625, Entropy -1308.4873046875, Learning Rate: 0.05\n",
      "Epoch [1714/20000], Loss: 4229.08740234375, Entropy -1342.81884765625, Learning Rate: 0.05\n",
      "Epoch [1715/20000], Loss: 4166.853515625, Entropy -1252.3779296875, Learning Rate: 0.05\n",
      "Epoch [1716/20000], Loss: 4255.08984375, Entropy -1427.90673828125, Learning Rate: 0.05\n",
      "Epoch [1717/20000], Loss: 4132.9853515625, Entropy -1222.25634765625, Learning Rate: 0.05\n",
      "Epoch [1718/20000], Loss: 4160.4755859375, Entropy -1287.59912109375, Learning Rate: 0.05\n",
      "Epoch [1719/20000], Loss: 4178.2333984375, Entropy -1392.98681640625, Learning Rate: 0.05\n",
      "Epoch [1720/20000], Loss: 4128.77099609375, Entropy -1265.96826171875, Learning Rate: 0.05\n",
      "Epoch [1721/20000], Loss: 4243.919921875, Entropy -1413.941162109375, Learning Rate: 0.05\n",
      "Epoch [1722/20000], Loss: 4200.07177734375, Entropy -1259.72900390625, Learning Rate: 0.05\n",
      "Epoch [1723/20000], Loss: 4171.9638671875, Entropy -1376.67822265625, Learning Rate: 0.05\n",
      "Epoch [1724/20000], Loss: 4190.654296875, Entropy -1376.16943359375, Learning Rate: 0.05\n",
      "Epoch [1725/20000], Loss: 4153.6279296875, Entropy -1308.60693359375, Learning Rate: 0.05\n",
      "Epoch [1726/20000], Loss: 4188.0615234375, Entropy -1346.25634765625, Learning Rate: 0.05\n",
      "Epoch [1727/20000], Loss: 4166.3671875, Entropy -1273.666015625, Learning Rate: 0.05\n",
      "Epoch [1728/20000], Loss: 4197.82861328125, Entropy -1466.61767578125, Learning Rate: 0.05\n",
      "Epoch [1729/20000], Loss: 4286.53125, Entropy -1494.337890625, Learning Rate: 0.05\n",
      "Epoch [1730/20000], Loss: 4219.14111328125, Entropy -1360.57861328125, Learning Rate: 0.05\n",
      "Epoch [1731/20000], Loss: 4219.59033203125, Entropy -1298.583984375, Learning Rate: 0.05\n",
      "Epoch [1732/20000], Loss: 4204.81005859375, Entropy -1348.1787109375, Learning Rate: 0.05\n",
      "Epoch [1733/20000], Loss: 4176.45166015625, Entropy -1398.543212890625, Learning Rate: 0.05\n",
      "Epoch [1734/20000], Loss: 4206.27001953125, Entropy -1384.45849609375, Learning Rate: 0.05\n",
      "Epoch [1735/20000], Loss: 4273.1142578125, Entropy -1358.68505859375, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1736/20000], Loss: 4171.40478515625, Entropy -1349.65966796875, Learning Rate: 0.05\n",
      "Epoch [1737/20000], Loss: 4267.45947265625, Entropy -1391.82470703125, Learning Rate: 0.05\n",
      "Epoch [1738/20000], Loss: 4227.97119140625, Entropy -1256.85693359375, Learning Rate: 0.05\n",
      "Epoch [1739/20000], Loss: 4264.2294921875, Entropy -1380.85009765625, Learning Rate: 0.05\n",
      "Epoch [1740/20000], Loss: 4310.27880859375, Entropy -1443.87939453125, Learning Rate: 0.05\n",
      "Epoch [1741/20000], Loss: 4270.3515625, Entropy -1405.25341796875, Learning Rate: 0.05\n",
      "Epoch [1742/20000], Loss: 4219.70458984375, Entropy -1410.471923828125, Learning Rate: 0.05\n",
      "Epoch [1743/20000], Loss: 4205.15185546875, Entropy -1339.4638671875, Learning Rate: 0.05\n",
      "Epoch [1744/20000], Loss: 4233.24951171875, Entropy -1428.420166015625, Learning Rate: 0.05\n",
      "Epoch [1745/20000], Loss: 4189.87890625, Entropy -1397.1162109375, Learning Rate: 0.05\n",
      "Epoch [1746/20000], Loss: 4173.18798828125, Entropy -1302.0771484375, Learning Rate: 0.05\n",
      "Epoch [1747/20000], Loss: 4185.14306640625, Entropy -1255.19384765625, Learning Rate: 0.05\n",
      "Epoch [1748/20000], Loss: 4219.81494140625, Entropy -1290.61572265625, Learning Rate: 0.05\n",
      "Epoch [1749/20000], Loss: 4180.1220703125, Entropy -1247.90380859375, Learning Rate: 0.05\n",
      "Epoch [1750/20000], Loss: 4203.00390625, Entropy -1309.65478515625, Learning Rate: 0.05\n",
      "Epoch [1751/20000], Loss: 4269.935546875, Entropy -1341.77099609375, Learning Rate: 0.05\n",
      "Epoch [1752/20000], Loss: 4205.4599609375, Entropy -1382.55908203125, Learning Rate: 0.05\n",
      "Epoch [1753/20000], Loss: 4222.7265625, Entropy -1349.33837890625, Learning Rate: 0.05\n",
      "Epoch [1754/20000], Loss: 4163.59765625, Entropy -1295.69189453125, Learning Rate: 0.05\n",
      "Epoch [1755/20000], Loss: 4122.72119140625, Entropy -1237.19970703125, Learning Rate: 0.05\n",
      "Epoch [1756/20000], Loss: 4184.16943359375, Entropy -1384.9306640625, Learning Rate: 0.05\n",
      "Epoch [1757/20000], Loss: 4200.9931640625, Entropy -1431.909912109375, Learning Rate: 0.05\n",
      "Epoch [1758/20000], Loss: 4180.37841796875, Entropy -1386.9423828125, Learning Rate: 0.05\n",
      "Epoch [1759/20000], Loss: 4184.185546875, Entropy -1411.055419921875, Learning Rate: 0.05\n",
      "Epoch [1760/20000], Loss: 4201.5595703125, Entropy -1309.63671875, Learning Rate: 0.05\n",
      "Epoch [1761/20000], Loss: 4191.46142578125, Entropy -1281.3466796875, Learning Rate: 0.05\n",
      "Epoch [1762/20000], Loss: 4168.88427734375, Entropy -1297.22998046875, Learning Rate: 0.05\n",
      "Epoch [1763/20000], Loss: 4201.0888671875, Entropy -1292.87109375, Learning Rate: 0.05\n",
      "Epoch [1764/20000], Loss: 4103.35400390625, Entropy -1266.78759765625, Learning Rate: 0.025\n",
      "Epoch [1765/20000], Loss: 4244.77490234375, Entropy -1321.86376953125, Learning Rate: 0.025\n",
      "Epoch [1766/20000], Loss: 4189.0361328125, Entropy -1300.70947265625, Learning Rate: 0.025\n",
      "Epoch [1767/20000], Loss: 4242.818359375, Entropy -1311.45361328125, Learning Rate: 0.025\n",
      "Epoch [1768/20000], Loss: 4133.52392578125, Entropy -1253.61669921875, Learning Rate: 0.025\n",
      "Epoch [1769/20000], Loss: 4163.3955078125, Entropy -1314.990234375, Learning Rate: 0.025\n",
      "Epoch [1770/20000], Loss: 4166.09375, Entropy -1223.71337890625, Learning Rate: 0.025\n",
      "Epoch [1771/20000], Loss: 4166.341796875, Entropy -1352.64990234375, Learning Rate: 0.025\n",
      "Epoch [1772/20000], Loss: 4167.740234375, Entropy -1428.88134765625, Learning Rate: 0.025\n",
      "Epoch [1773/20000], Loss: 4174.205078125, Entropy -1332.21875, Learning Rate: 0.025\n",
      "Epoch [1774/20000], Loss: 4157.556640625, Entropy -1377.57470703125, Learning Rate: 0.025\n",
      "Epoch [1775/20000], Loss: 4168.9453125, Entropy -1326.11865234375, Learning Rate: 0.025\n",
      "Epoch [1776/20000], Loss: 4183.1826171875, Entropy -1287.0576171875, Learning Rate: 0.025\n",
      "Epoch [1777/20000], Loss: 4203.30419921875, Entropy -1358.99169921875, Learning Rate: 0.025\n",
      "Epoch [1778/20000], Loss: 4153.58251953125, Entropy -1303.8037109375, Learning Rate: 0.025\n",
      "Epoch [1779/20000], Loss: 4187.6943359375, Entropy -1339.802734375, Learning Rate: 0.025\n",
      "Epoch [1780/20000], Loss: 4212.5244140625, Entropy -1401.2978515625, Learning Rate: 0.025\n",
      "Epoch [1781/20000], Loss: 4167.5986328125, Entropy -1237.6162109375, Learning Rate: 0.025\n",
      "Epoch [1782/20000], Loss: 4140.3369140625, Entropy -1258.49267578125, Learning Rate: 0.025\n",
      "Epoch [1783/20000], Loss: 4174.908203125, Entropy -1175.939453125, Learning Rate: 0.025\n",
      "Epoch [1784/20000], Loss: 4124.8974609375, Entropy -1299.4677734375, Learning Rate: 0.025\n",
      "Epoch [1785/20000], Loss: 4227.6201171875, Entropy -1383.349609375, Learning Rate: 0.025\n",
      "Epoch [1786/20000], Loss: 4229.16259765625, Entropy -1427.482421875, Learning Rate: 0.025\n",
      "Epoch [1787/20000], Loss: 4164.87890625, Entropy -1308.51025390625, Learning Rate: 0.025\n",
      "Epoch [1788/20000], Loss: 4135.84033203125, Entropy -1244.80908203125, Learning Rate: 0.025\n",
      "Epoch [1789/20000], Loss: 4131.515625, Entropy -1275.13623046875, Learning Rate: 0.025\n",
      "Epoch [1790/20000], Loss: 4191.4140625, Entropy -1332.8896484375, Learning Rate: 0.025\n",
      "Epoch [1791/20000], Loss: 4176.12548828125, Entropy -1222.35693359375, Learning Rate: 0.025\n",
      "Epoch [1792/20000], Loss: 4230.84814453125, Entropy -1393.15576171875, Learning Rate: 0.025\n",
      "Epoch [1793/20000], Loss: 4192.103515625, Entropy -1384.306640625, Learning Rate: 0.025\n",
      "Epoch [1794/20000], Loss: 4158.5849609375, Entropy -1354.9072265625, Learning Rate: 0.025\n",
      "Epoch [1795/20000], Loss: 4157.12841796875, Entropy -1337.23046875, Learning Rate: 0.025\n",
      "Epoch [1796/20000], Loss: 4191.2734375, Entropy -1326.416015625, Learning Rate: 0.025\n",
      "Epoch [1797/20000], Loss: 4171.4501953125, Entropy -1355.72412109375, Learning Rate: 0.025\n",
      "Epoch [1798/20000], Loss: 4193.55078125, Entropy -1304.478515625, Learning Rate: 0.025\n",
      "Epoch [1799/20000], Loss: 4240.50048828125, Entropy -1419.6103515625, Learning Rate: 0.025\n",
      "Epoch [1800/20000], Loss: 4177.904296875, Entropy -1398.8203125, Learning Rate: 0.025\n",
      "Epoch [1801/20000], Loss: 4236.82177734375, Entropy -1289.48583984375, Learning Rate: 0.025\n",
      "Epoch [1802/20000], Loss: 4233.09033203125, Entropy -1267.04052734375, Learning Rate: 0.025\n",
      "Epoch [1803/20000], Loss: 4216.3701171875, Entropy -1445.28076171875, Learning Rate: 0.025\n",
      "Epoch [1804/20000], Loss: 4226.2841796875, Entropy -1403.8798828125, Learning Rate: 0.025\n",
      "Epoch [1805/20000], Loss: 4148.865234375, Entropy -1299.3154296875, Learning Rate: 0.025\n",
      "Epoch [1806/20000], Loss: 4159.5185546875, Entropy -1322.80615234375, Learning Rate: 0.025\n",
      "Epoch [1807/20000], Loss: 4209.470703125, Entropy -1357.86572265625, Learning Rate: 0.025\n",
      "Epoch [1808/20000], Loss: 4152.64501953125, Entropy -1216.8984375, Learning Rate: 0.025\n",
      "Epoch [1809/20000], Loss: 4199.94775390625, Entropy -1297.248046875, Learning Rate: 0.025\n",
      "Epoch [1810/20000], Loss: 4174.8701171875, Entropy -1369.63330078125, Learning Rate: 0.025\n",
      "Epoch [1811/20000], Loss: 4173.76171875, Entropy -1353.87109375, Learning Rate: 0.025\n",
      "Epoch [1812/20000], Loss: 4206.30517578125, Entropy -1312.2607421875, Learning Rate: 0.025\n",
      "Epoch [1813/20000], Loss: 4196.05712890625, Entropy -1322.142578125, Learning Rate: 0.025\n",
      "Epoch [1814/20000], Loss: 4151.8115234375, Entropy -1296.97314453125, Learning Rate: 0.025\n",
      "Epoch [1815/20000], Loss: 4196.99609375, Entropy -1187.7587890625, Learning Rate: 0.025\n",
      "Epoch [1816/20000], Loss: 4263.0146484375, Entropy -1375.76513671875, Learning Rate: 0.025\n",
      "Epoch [1817/20000], Loss: 4163.87353515625, Entropy -1344.01806640625, Learning Rate: 0.025\n",
      "Epoch [1818/20000], Loss: 4180.06494140625, Entropy -1339.8427734375, Learning Rate: 0.025\n",
      "Epoch [1819/20000], Loss: 4248.44384765625, Entropy -1302.93359375, Learning Rate: 0.025\n",
      "Epoch [1820/20000], Loss: 4236.82666015625, Entropy -1249.5302734375, Learning Rate: 0.025\n",
      "Epoch [1821/20000], Loss: 4165.07275390625, Entropy -1326.3251953125, Learning Rate: 0.025\n",
      "Epoch [1822/20000], Loss: 4161.64208984375, Entropy -1332.109375, Learning Rate: 0.025\n",
      "Epoch [1823/20000], Loss: 4210.09716796875, Entropy -1340.28076171875, Learning Rate: 0.025\n",
      "Epoch [1824/20000], Loss: 4188.50048828125, Entropy -1298.17919921875, Learning Rate: 0.025\n",
      "Epoch [1825/20000], Loss: 4173.6591796875, Entropy -1383.8955078125, Learning Rate: 0.025\n",
      "Epoch [1826/20000], Loss: 4211.720703125, Entropy -1331.31689453125, Learning Rate: 0.025\n",
      "Epoch [1827/20000], Loss: 4153.06103515625, Entropy -1381.3203125, Learning Rate: 0.025\n",
      "Epoch [1828/20000], Loss: 4240.7724609375, Entropy -1214.37255859375, Learning Rate: 0.025\n",
      "Epoch [1829/20000], Loss: 4166.5537109375, Entropy -1298.43115234375, Learning Rate: 0.025\n",
      "Epoch [1830/20000], Loss: 4154.39501953125, Entropy -1304.0029296875, Learning Rate: 0.025\n",
      "Epoch [1831/20000], Loss: 4188.65283203125, Entropy -1337.8564453125, Learning Rate: 0.025\n",
      "Epoch [1832/20000], Loss: 4143.99853515625, Entropy -1359.2900390625, Learning Rate: 0.025\n",
      "Epoch [1833/20000], Loss: 4157.5419921875, Entropy -1260.64892578125, Learning Rate: 0.025\n",
      "Epoch [1834/20000], Loss: 4157.97705078125, Entropy -1351.68603515625, Learning Rate: 0.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1835/20000], Loss: 4145.4775390625, Entropy -1365.505859375, Learning Rate: 0.025\n",
      "Epoch [1836/20000], Loss: 4202.55712890625, Entropy -1308.69091796875, Learning Rate: 0.025\n",
      "Epoch [1837/20000], Loss: 4187.28076171875, Entropy -1266.97412109375, Learning Rate: 0.025\n",
      "Epoch [1838/20000], Loss: 4180.27587890625, Entropy -1327.78857421875, Learning Rate: 0.025\n",
      "Epoch [1839/20000], Loss: 4198.74755859375, Entropy -1384.92724609375, Learning Rate: 0.025\n",
      "Epoch [1840/20000], Loss: 4121.82177734375, Entropy -1294.15283203125, Learning Rate: 0.025\n",
      "Epoch [1841/20000], Loss: 4177.513671875, Entropy -1322.38818359375, Learning Rate: 0.025\n",
      "Epoch [1842/20000], Loss: 4176.50048828125, Entropy -1345.33740234375, Learning Rate: 0.025\n",
      "Epoch [1843/20000], Loss: 4253.80517578125, Entropy -1394.62890625, Learning Rate: 0.025\n",
      "Epoch [1844/20000], Loss: 4129.36279296875, Entropy -1207.2783203125, Learning Rate: 0.025\n",
      "Epoch [1845/20000], Loss: 4170.02197265625, Entropy -1219.974609375, Learning Rate: 0.025\n",
      "Epoch [1846/20000], Loss: 4220.42431640625, Entropy -1336.9375, Learning Rate: 0.025\n",
      "Epoch [1847/20000], Loss: 4167.744140625, Entropy -1342.98974609375, Learning Rate: 0.025\n",
      "Epoch [1848/20000], Loss: 4221.87109375, Entropy -1418.867919921875, Learning Rate: 0.025\n",
      "Epoch [1849/20000], Loss: 4195.734375, Entropy -1303.16064453125, Learning Rate: 0.025\n",
      "Epoch [1850/20000], Loss: 4176.2666015625, Entropy -1357.20556640625, Learning Rate: 0.025\n",
      "Epoch [1851/20000], Loss: 4155.6591796875, Entropy -1224.38720703125, Learning Rate: 0.025\n",
      "Epoch [1852/20000], Loss: 4214.69873046875, Entropy -1403.474853515625, Learning Rate: 0.025\n",
      "Epoch [1853/20000], Loss: 4183.4462890625, Entropy -1291.24072265625, Learning Rate: 0.025\n",
      "Epoch [1854/20000], Loss: 4217.2353515625, Entropy -1369.74755859375, Learning Rate: 0.025\n",
      "Epoch [1855/20000], Loss: 4193.0126953125, Entropy -1361.69873046875, Learning Rate: 0.025\n",
      "Epoch [1856/20000], Loss: 4072.308837890625, Entropy -1254.17431640625, Learning Rate: 0.025\n",
      "Epoch [1857/20000], Loss: 4131.8251953125, Entropy -1302.27685546875, Learning Rate: 0.025\n",
      "Epoch [1858/20000], Loss: 4229.10302734375, Entropy -1334.14013671875, Learning Rate: 0.025\n",
      "Epoch [1859/20000], Loss: 4171.97802734375, Entropy -1372.58203125, Learning Rate: 0.025\n",
      "Epoch [1860/20000], Loss: 4210.02294921875, Entropy -1397.1064453125, Learning Rate: 0.025\n",
      "Epoch [1861/20000], Loss: 4177.4580078125, Entropy -1235.65185546875, Learning Rate: 0.025\n",
      "Epoch [1862/20000], Loss: 4219.11328125, Entropy -1410.80810546875, Learning Rate: 0.025\n",
      "Epoch [1863/20000], Loss: 4195.1572265625, Entropy -1433.838623046875, Learning Rate: 0.025\n",
      "Epoch [1864/20000], Loss: 4167.93310546875, Entropy -1300.78271484375, Learning Rate: 0.025\n",
      "Epoch [1865/20000], Loss: 4164.66064453125, Entropy -1375.84521484375, Learning Rate: 0.025\n",
      "Epoch [1866/20000], Loss: 4106.4619140625, Entropy -1193.728515625, Learning Rate: 0.025\n",
      "Epoch [1867/20000], Loss: 4168.0498046875, Entropy -1310.55712890625, Learning Rate: 0.025\n",
      "Epoch [1868/20000], Loss: 4193.1279296875, Entropy -1229.81396484375, Learning Rate: 0.025\n",
      "Epoch [1869/20000], Loss: 4127.244140625, Entropy -1368.4619140625, Learning Rate: 0.025\n",
      "Epoch [1870/20000], Loss: 4148.39013671875, Entropy -1263.81982421875, Learning Rate: 0.025\n",
      "Epoch [1871/20000], Loss: 4150.42578125, Entropy -1279.6845703125, Learning Rate: 0.025\n",
      "Epoch [1872/20000], Loss: 4134.2041015625, Entropy -1301.36181640625, Learning Rate: 0.025\n",
      "Epoch [1873/20000], Loss: 4190.244140625, Entropy -1256.61669921875, Learning Rate: 0.025\n",
      "Epoch [1874/20000], Loss: 4211.759765625, Entropy -1332.90283203125, Learning Rate: 0.025\n",
      "Epoch [1875/20000], Loss: 4183.79052734375, Entropy -1323.869140625, Learning Rate: 0.025\n",
      "Epoch [1876/20000], Loss: 4095.298583984375, Entropy -1171.74658203125, Learning Rate: 0.025\n",
      "Epoch [1877/20000], Loss: 4136.27490234375, Entropy -1246.6396484375, Learning Rate: 0.025\n",
      "Epoch [1878/20000], Loss: 4187.42333984375, Entropy -1274.462890625, Learning Rate: 0.025\n",
      "Epoch [1879/20000], Loss: 4091.935791015625, Entropy -1242.1083984375, Learning Rate: 0.025\n",
      "Epoch [1880/20000], Loss: 4118.2236328125, Entropy -1241.35498046875, Learning Rate: 0.025\n",
      "Epoch [1881/20000], Loss: 4192.85986328125, Entropy -1204.32177734375, Learning Rate: 0.025\n",
      "Epoch [1882/20000], Loss: 4141.69091796875, Entropy -1185.04443359375, Learning Rate: 0.025\n",
      "Epoch [1883/20000], Loss: 4176.05029296875, Entropy -1372.0166015625, Learning Rate: 0.025\n",
      "Epoch [1884/20000], Loss: 4166.49267578125, Entropy -1340.46630859375, Learning Rate: 0.025\n",
      "Epoch [1885/20000], Loss: 4187.9951171875, Entropy -1307.77294921875, Learning Rate: 0.025\n",
      "Epoch [1886/20000], Loss: 4148.95947265625, Entropy -1394.26708984375, Learning Rate: 0.025\n",
      "Epoch [1887/20000], Loss: 4175.87939453125, Entropy -1314.16845703125, Learning Rate: 0.025\n",
      "Epoch [1888/20000], Loss: 4190.03076171875, Entropy -1323.42236328125, Learning Rate: 0.025\n",
      "Epoch [1889/20000], Loss: 4138.76220703125, Entropy -1226.64501953125, Learning Rate: 0.025\n",
      "Epoch [1890/20000], Loss: 4215.5498046875, Entropy -1427.609619140625, Learning Rate: 0.025\n",
      "Epoch [1891/20000], Loss: 4157.3154296875, Entropy -1270.58349609375, Learning Rate: 0.025\n",
      "Epoch [1892/20000], Loss: 4138.40380859375, Entropy -1344.7705078125, Learning Rate: 0.025\n",
      "Epoch [1893/20000], Loss: 4122.53271484375, Entropy -1183.6171875, Learning Rate: 0.025\n",
      "Epoch [1894/20000], Loss: 4103.0068359375, Entropy -1324.125, Learning Rate: 0.025\n",
      "Epoch [1895/20000], Loss: 4153.81494140625, Entropy -1352.93896484375, Learning Rate: 0.025\n",
      "Epoch [1896/20000], Loss: 4134.7919921875, Entropy -1364.87255859375, Learning Rate: 0.025\n",
      "Epoch [1897/20000], Loss: 4251.09619140625, Entropy -1498.34716796875, Learning Rate: 0.025\n",
      "Epoch [1898/20000], Loss: 4205.84765625, Entropy -1321.46435546875, Learning Rate: 0.025\n",
      "Epoch [1899/20000], Loss: 4125.55224609375, Entropy -1243.8662109375, Learning Rate: 0.025\n",
      "Epoch [1900/20000], Loss: 4186.77294921875, Entropy -1350.98583984375, Learning Rate: 0.025\n",
      "Epoch [1901/20000], Loss: 4177.33837890625, Entropy -1192.40869140625, Learning Rate: 0.025\n",
      "Epoch [1902/20000], Loss: 4127.912109375, Entropy -1260.73291015625, Learning Rate: 0.025\n",
      "Epoch [1903/20000], Loss: 4201.826171875, Entropy -1355.87890625, Learning Rate: 0.025\n",
      "Epoch [1904/20000], Loss: 4199.5869140625, Entropy -1391.72119140625, Learning Rate: 0.025\n",
      "Epoch [1905/20000], Loss: 4127.97119140625, Entropy -1237.07958984375, Learning Rate: 0.025\n",
      "Epoch [1906/20000], Loss: 4150.9384765625, Entropy -1354.77880859375, Learning Rate: 0.025\n",
      "Epoch [1907/20000], Loss: 4138.13525390625, Entropy -1289.04541015625, Learning Rate: 0.025\n",
      "Epoch [1908/20000], Loss: 4198.90576171875, Entropy -1379.6083984375, Learning Rate: 0.025\n",
      "Epoch [1909/20000], Loss: 4187.34814453125, Entropy -1339.50830078125, Learning Rate: 0.025\n",
      "Epoch [1910/20000], Loss: 4111.49462890625, Entropy -1270.36376953125, Learning Rate: 0.025\n",
      "Epoch [1911/20000], Loss: 4146.7802734375, Entropy -1315.1865234375, Learning Rate: 0.025\n",
      "Epoch [1912/20000], Loss: 4155.521484375, Entropy -1242.5693359375, Learning Rate: 0.025\n",
      "Epoch [1913/20000], Loss: 4123.580078125, Entropy -1308.58544921875, Learning Rate: 0.025\n",
      "Epoch [1914/20000], Loss: 4181.07080078125, Entropy -1334.88232421875, Learning Rate: 0.025\n",
      "Epoch [1915/20000], Loss: 4229.55810546875, Entropy -1343.05810546875, Learning Rate: 0.025\n",
      "Epoch [1916/20000], Loss: 4131.9140625, Entropy -1289.4326171875, Learning Rate: 0.025\n",
      "Epoch [1917/20000], Loss: 4150.2080078125, Entropy -1268.03759765625, Learning Rate: 0.025\n",
      "Epoch [1918/20000], Loss: 4140.06005859375, Entropy -1280.45556640625, Learning Rate: 0.025\n",
      "Epoch [1919/20000], Loss: 4184.96240234375, Entropy -1341.05517578125, Learning Rate: 0.025\n",
      "Epoch [1920/20000], Loss: 4239.96630859375, Entropy -1301.6455078125, Learning Rate: 0.025\n",
      "Epoch [1921/20000], Loss: 4155.6005859375, Entropy -1309.9658203125, Learning Rate: 0.025\n",
      "Epoch [1922/20000], Loss: 4178.220703125, Entropy -1404.940673828125, Learning Rate: 0.025\n",
      "Epoch [1923/20000], Loss: 4242.0947265625, Entropy -1376.666015625, Learning Rate: 0.025\n",
      "Epoch [1924/20000], Loss: 4153.666015625, Entropy -1247.4658203125, Learning Rate: 0.025\n",
      "Epoch [1925/20000], Loss: 4180.486328125, Entropy -1359.4296875, Learning Rate: 0.025\n",
      "Epoch [1926/20000], Loss: 4182.7373046875, Entropy -1247.08251953125, Learning Rate: 0.025\n",
      "Epoch [1927/20000], Loss: 4159.73583984375, Entropy -1328.80615234375, Learning Rate: 0.025\n",
      "Epoch [1928/20000], Loss: 4119.51806640625, Entropy -1286.8515625, Learning Rate: 0.025\n",
      "Epoch [1929/20000], Loss: 4139.2529296875, Entropy -1383.15771484375, Learning Rate: 0.025\n",
      "Epoch [1930/20000], Loss: 4087.219970703125, Entropy -1170.2197265625, Learning Rate: 0.025\n",
      "Epoch [1931/20000], Loss: 4139.6953125, Entropy -1299.4970703125, Learning Rate: 0.025\n",
      "Epoch [1932/20000], Loss: 4227.18896484375, Entropy -1317.70361328125, Learning Rate: 0.025\n",
      "Epoch [1933/20000], Loss: 4216.23388671875, Entropy -1284.95556640625, Learning Rate: 0.025\n",
      "Epoch [1934/20000], Loss: 4205.73681640625, Entropy -1301.77001953125, Learning Rate: 0.025\n",
      "Epoch [1935/20000], Loss: 4150.58984375, Entropy -1327.45263671875, Learning Rate: 0.025\n",
      "Epoch [1936/20000], Loss: 4242.53515625, Entropy -1365.7607421875, Learning Rate: 0.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1937/20000], Loss: 4107.06396484375, Entropy -1260.70458984375, Learning Rate: 0.025\n",
      "Epoch [1938/20000], Loss: 4114.66162109375, Entropy -1195.296875, Learning Rate: 0.025\n",
      "Epoch [1939/20000], Loss: 4153.38330078125, Entropy -1398.781005859375, Learning Rate: 0.025\n",
      "Epoch [1940/20000], Loss: 4167.85205078125, Entropy -1261.9521484375, Learning Rate: 0.025\n",
      "Epoch [1941/20000], Loss: 4206.88427734375, Entropy -1215.65673828125, Learning Rate: 0.025\n",
      "Epoch [1942/20000], Loss: 4143.91162109375, Entropy -1260.65869140625, Learning Rate: 0.025\n",
      "Epoch [1943/20000], Loss: 4117.3193359375, Entropy -1277.26318359375, Learning Rate: 0.025\n",
      "Epoch [1944/20000], Loss: 4089.144775390625, Entropy -1200.1708984375, Learning Rate: 0.025\n",
      "Epoch [1945/20000], Loss: 4232.552734375, Entropy -1286.16064453125, Learning Rate: 0.025\n",
      "Epoch [1946/20000], Loss: 4173.25634765625, Entropy -1301.9091796875, Learning Rate: 0.025\n",
      "Epoch [1947/20000], Loss: 4147.48193359375, Entropy -1367.5380859375, Learning Rate: 0.025\n",
      "Epoch [1948/20000], Loss: 4153.99169921875, Entropy -1386.2197265625, Learning Rate: 0.025\n",
      "Epoch [1949/20000], Loss: 4170.2412109375, Entropy -1286.29345703125, Learning Rate: 0.025\n",
      "Epoch [1950/20000], Loss: 4169.71923828125, Entropy -1352.92626953125, Learning Rate: 0.025\n",
      "Epoch [1951/20000], Loss: 4161.88525390625, Entropy -1267.9609375, Learning Rate: 0.025\n",
      "Epoch [1952/20000], Loss: 4199.43505859375, Entropy -1357.17041015625, Learning Rate: 0.025\n",
      "Epoch [1953/20000], Loss: 4105.45068359375, Entropy -1293.49951171875, Learning Rate: 0.025\n",
      "Epoch [1954/20000], Loss: 4181.74462890625, Entropy -1294.5849609375, Learning Rate: 0.025\n",
      "Epoch [1955/20000], Loss: 4177.94921875, Entropy -1341.56396484375, Learning Rate: 0.025\n",
      "Epoch [1956/20000], Loss: 4111.57421875, Entropy -1251.87646484375, Learning Rate: 0.025\n",
      "Epoch [1957/20000], Loss: 4138.3369140625, Entropy -1351.66162109375, Learning Rate: 0.025\n",
      "Epoch [1958/20000], Loss: 4123.4033203125, Entropy -1217.12646484375, Learning Rate: 0.025\n",
      "Epoch [1959/20000], Loss: 4164.00146484375, Entropy -1310.3076171875, Learning Rate: 0.025\n",
      "Epoch [1960/20000], Loss: 4102.73779296875, Entropy -1246.71240234375, Learning Rate: 0.025\n",
      "Epoch [1961/20000], Loss: 4166.9990234375, Entropy -1322.13232421875, Learning Rate: 0.025\n",
      "Epoch [1962/20000], Loss: 4169.03369140625, Entropy -1241.71240234375, Learning Rate: 0.025\n",
      "Epoch [1963/20000], Loss: 4212.3779296875, Entropy -1384.57958984375, Learning Rate: 0.025\n",
      "Epoch [1964/20000], Loss: 4163.50146484375, Entropy -1360.9521484375, Learning Rate: 0.025\n",
      "Epoch [1965/20000], Loss: 4125.70849609375, Entropy -1366.65771484375, Learning Rate: 0.025\n",
      "Epoch [1966/20000], Loss: 4195.68408203125, Entropy -1324.32470703125, Learning Rate: 0.025\n",
      "Epoch [1967/20000], Loss: 4137.94384765625, Entropy -1304.6904296875, Learning Rate: 0.025\n",
      "Epoch [1968/20000], Loss: 4094.517578125, Entropy -1217.400390625, Learning Rate: 0.025\n",
      "Epoch [1969/20000], Loss: 4243.095703125, Entropy -1320.791015625, Learning Rate: 0.025\n",
      "Epoch [1970/20000], Loss: 4162.8642578125, Entropy -1388.4765625, Learning Rate: 0.025\n",
      "Epoch [1971/20000], Loss: 4117.474609375, Entropy -1260.421875, Learning Rate: 0.025\n",
      "Epoch [1972/20000], Loss: 4177.181640625, Entropy -1353.822265625, Learning Rate: 0.025\n",
      "Epoch [1973/20000], Loss: 4175.19140625, Entropy -1439.88720703125, Learning Rate: 0.025\n",
      "Epoch [1974/20000], Loss: 4170.486328125, Entropy -1319.603515625, Learning Rate: 0.025\n",
      "Epoch [1975/20000], Loss: 4109.1875, Entropy -1309.4775390625, Learning Rate: 0.025\n",
      "Epoch [1976/20000], Loss: 4151.16357421875, Entropy -1282.97119140625, Learning Rate: 0.025\n",
      "Epoch [1977/20000], Loss: 4132.8916015625, Entropy -1312.62548828125, Learning Rate: 0.025\n",
      "Epoch [1978/20000], Loss: 4176.5029296875, Entropy -1253.09326171875, Learning Rate: 0.025\n",
      "Epoch [1979/20000], Loss: 4174.029296875, Entropy -1295.50146484375, Learning Rate: 0.025\n",
      "Epoch [1980/20000], Loss: 4249.7998046875, Entropy -1311.11572265625, Learning Rate: 0.025\n",
      "Epoch [1981/20000], Loss: 4229.6474609375, Entropy -1280.97705078125, Learning Rate: 0.025\n",
      "Epoch [1982/20000], Loss: 4163.8544921875, Entropy -1300.82080078125, Learning Rate: 0.025\n",
      "Epoch [1983/20000], Loss: 4195.22607421875, Entropy -1374.01708984375, Learning Rate: 0.025\n",
      "Epoch [1984/20000], Loss: 4177.5166015625, Entropy -1292.82763671875, Learning Rate: 0.025\n",
      "Epoch [1985/20000], Loss: 4114.982421875, Entropy -1246.0048828125, Learning Rate: 0.025\n",
      "Epoch [1986/20000], Loss: 4188.59033203125, Entropy -1390.91552734375, Learning Rate: 0.025\n",
      "Epoch [1987/20000], Loss: 4207.57421875, Entropy -1211.0478515625, Learning Rate: 0.025\n",
      "Epoch [1988/20000], Loss: 4207.76806640625, Entropy -1282.013671875, Learning Rate: 0.025\n",
      "Epoch [1989/20000], Loss: 4178.337890625, Entropy -1353.7666015625, Learning Rate: 0.025\n",
      "Epoch [1990/20000], Loss: 4188.5732421875, Entropy -1376.056640625, Learning Rate: 0.025\n",
      "Epoch [1991/20000], Loss: 4113.36083984375, Entropy -1256.42822265625, Learning Rate: 0.025\n",
      "Epoch [1992/20000], Loss: 4206.08984375, Entropy -1280.6552734375, Learning Rate: 0.025\n",
      "Epoch [1993/20000], Loss: 4150.630859375, Entropy -1290.20166015625, Learning Rate: 0.025\n",
      "Epoch [1994/20000], Loss: 4133.505859375, Entropy -1259.00244140625, Learning Rate: 0.025\n",
      "Epoch [1995/20000], Loss: 4170.80810546875, Entropy -1365.638671875, Learning Rate: 0.025\n",
      "Epoch [1996/20000], Loss: 4086.59228515625, Entropy -1258.85400390625, Learning Rate: 0.025\n",
      "Epoch [1997/20000], Loss: 4173.52197265625, Entropy -1370.79150390625, Learning Rate: 0.025\n",
      "Epoch [1998/20000], Loss: 4162.69970703125, Entropy -1358.94091796875, Learning Rate: 0.025\n",
      "Epoch [1999/20000], Loss: 4137.2880859375, Entropy -1290.94091796875, Learning Rate: 0.025\n",
      "Epoch [2000/20000], Loss: 4152.24609375, Entropy -1363.41015625, Learning Rate: 0.025\n",
      "Epoch [2001/20000], Loss: 4189.23681640625, Entropy -1338.32958984375, Learning Rate: 0.025\n",
      "Epoch [2002/20000], Loss: 4196.6904296875, Entropy -1334.67041015625, Learning Rate: 0.025\n",
      "Epoch [2003/20000], Loss: 4141.59228515625, Entropy -1308.515625, Learning Rate: 0.025\n",
      "Epoch [2004/20000], Loss: 4105.5810546875, Entropy -1246.90771484375, Learning Rate: 0.025\n",
      "Epoch [2005/20000], Loss: 4164.35693359375, Entropy -1372.48046875, Learning Rate: 0.025\n",
      "Epoch [2006/20000], Loss: 4171.328125, Entropy -1386.54833984375, Learning Rate: 0.025\n",
      "Epoch [2007/20000], Loss: 4166.60400390625, Entropy -1298.05029296875, Learning Rate: 0.025\n",
      "Epoch [2008/20000], Loss: 4192.36181640625, Entropy -1257.67333984375, Learning Rate: 0.025\n",
      "Epoch [2009/20000], Loss: 4246.5087890625, Entropy -1309.58447265625, Learning Rate: 0.025\n",
      "Epoch [2010/20000], Loss: 4223.30029296875, Entropy -1426.8037109375, Learning Rate: 0.025\n",
      "Epoch [2011/20000], Loss: 4188.259765625, Entropy -1380.19873046875, Learning Rate: 0.025\n",
      "Epoch [2012/20000], Loss: 4168.94189453125, Entropy -1171.52685546875, Learning Rate: 0.025\n",
      "Epoch [2013/20000], Loss: 4277.248046875, Entropy -1242.697265625, Learning Rate: 0.025\n",
      "Epoch [2014/20000], Loss: 4224.1318359375, Entropy -1264.10888671875, Learning Rate: 0.025\n",
      "Epoch [2015/20000], Loss: 4160.71728515625, Entropy -1233.07861328125, Learning Rate: 0.025\n",
      "Epoch [2016/20000], Loss: 4167.6474609375, Entropy -1284.59228515625, Learning Rate: 0.025\n",
      "Epoch [2017/20000], Loss: 4136.7734375, Entropy -1241.96435546875, Learning Rate: 0.025\n",
      "Epoch [2018/20000], Loss: 4147.11767578125, Entropy -1230.619140625, Learning Rate: 0.025\n",
      "Epoch [2019/20000], Loss: 4111.71240234375, Entropy -1266.17041015625, Learning Rate: 0.025\n",
      "Epoch [2020/20000], Loss: 4129.21044921875, Entropy -1274.33349609375, Learning Rate: 0.025\n",
      "Epoch [2021/20000], Loss: 4155.0478515625, Entropy -1265.37353515625, Learning Rate: 0.025\n",
      "Epoch [2022/20000], Loss: 4278.19921875, Entropy -1518.70068359375, Learning Rate: 0.025\n",
      "Epoch [2023/20000], Loss: 4149.2646484375, Entropy -1358.35009765625, Learning Rate: 0.025\n",
      "Epoch [2024/20000], Loss: 4156.59716796875, Entropy -1162.724609375, Learning Rate: 0.025\n",
      "Epoch [2025/20000], Loss: 4147.01513671875, Entropy -1334.580078125, Learning Rate: 0.025\n",
      "Epoch [2026/20000], Loss: 4144.80908203125, Entropy -1186.18408203125, Learning Rate: 0.025\n",
      "Epoch [2027/20000], Loss: 4152.61474609375, Entropy -1327.18896484375, Learning Rate: 0.025\n",
      "Epoch [2028/20000], Loss: 4122.09033203125, Entropy -1304.28125, Learning Rate: 0.025\n",
      "Epoch [2029/20000], Loss: 4140.55322265625, Entropy -1205.35205078125, Learning Rate: 0.025\n",
      "Epoch [2030/20000], Loss: 4124.623046875, Entropy -1273.10107421875, Learning Rate: 0.025\n",
      "Epoch [2031/20000], Loss: 4112.5263671875, Entropy -1210.79150390625, Learning Rate: 0.025\n",
      "Epoch [2032/20000], Loss: 4195.955078125, Entropy -1299.55419921875, Learning Rate: 0.025\n",
      "Epoch [2033/20000], Loss: 4176.2568359375, Entropy -1240.5634765625, Learning Rate: 0.025\n",
      "Epoch [2034/20000], Loss: 4172.77392578125, Entropy -1293.18798828125, Learning Rate: 0.025\n",
      "Epoch [2035/20000], Loss: 4219.55029296875, Entropy -1423.2373046875, Learning Rate: 0.025\n",
      "Epoch [2036/20000], Loss: 4149.75537109375, Entropy -1347.337890625, Learning Rate: 0.025\n",
      "Epoch [2037/20000], Loss: 4137.28125, Entropy -1304.63427734375, Learning Rate: 0.025\n",
      "Epoch [2038/20000], Loss: 4240.201171875, Entropy -1322.12060546875, Learning Rate: 0.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2039/20000], Loss: 4129.2763671875, Entropy -1235.13232421875, Learning Rate: 0.025\n",
      "Epoch [2040/20000], Loss: 4232.03857421875, Entropy -1289.966796875, Learning Rate: 0.025\n",
      "Epoch [2041/20000], Loss: 4162.8486328125, Entropy -1433.00732421875, Learning Rate: 0.025\n",
      "Epoch [2042/20000], Loss: 4155.1708984375, Entropy -1354.48291015625, Learning Rate: 0.025\n",
      "Epoch [2043/20000], Loss: 4179.19189453125, Entropy -1293.8681640625, Learning Rate: 0.025\n",
      "Epoch [2044/20000], Loss: 4177.431640625, Entropy -1417.571533203125, Learning Rate: 0.025\n",
      "Epoch [2045/20000], Loss: 4196.69775390625, Entropy -1377.49267578125, Learning Rate: 0.025\n",
      "Epoch [2046/20000], Loss: 4143.07861328125, Entropy -1303.4931640625, Learning Rate: 0.025\n",
      "Epoch [2047/20000], Loss: 4249.1435546875, Entropy -1310.3828125, Learning Rate: 0.025\n",
      "Epoch [2048/20000], Loss: 4116.88427734375, Entropy -1189.57861328125, Learning Rate: 0.025\n",
      "Epoch [2049/20000], Loss: 4171.1044921875, Entropy -1287.9833984375, Learning Rate: 0.025\n",
      "Epoch [2050/20000], Loss: 4154.65478515625, Entropy -1322.728515625, Learning Rate: 0.025\n",
      "Epoch [2051/20000], Loss: 4160.81982421875, Entropy -1244.89013671875, Learning Rate: 0.025\n",
      "Epoch [2052/20000], Loss: 4186.1552734375, Entropy -1326.3466796875, Learning Rate: 0.025\n",
      "Epoch [2053/20000], Loss: 4136.30078125, Entropy -1277.70751953125, Learning Rate: 0.025\n",
      "Epoch [2054/20000], Loss: 4197.22607421875, Entropy -1216.98681640625, Learning Rate: 0.025\n",
      "Epoch [2055/20000], Loss: 4177.09228515625, Entropy -1306.80712890625, Learning Rate: 0.025\n",
      "Epoch [2056/20000], Loss: 4120.9208984375, Entropy -1325.99609375, Learning Rate: 0.025\n",
      "Epoch [2057/20000], Loss: 4150.17822265625, Entropy -1418.885986328125, Learning Rate: 0.025\n",
      "Epoch [2058/20000], Loss: 4115.31494140625, Entropy -1330.6982421875, Learning Rate: 0.0125\n",
      "Epoch [2059/20000], Loss: 4183.7890625, Entropy -1198.66845703125, Learning Rate: 0.0125\n",
      "Epoch [2060/20000], Loss: 4113.529296875, Entropy -1298.7529296875, Learning Rate: 0.0125\n",
      "Epoch [2061/20000], Loss: 4124.4189453125, Entropy -1328.86962890625, Learning Rate: 0.0125\n",
      "Epoch [2062/20000], Loss: 4124.150390625, Entropy -1326.57568359375, Learning Rate: 0.0125\n",
      "Epoch [2063/20000], Loss: 4109.87548828125, Entropy -1201.5595703125, Learning Rate: 0.0125\n",
      "Epoch [2064/20000], Loss: 4162.2578125, Entropy -1302.22802734375, Learning Rate: 0.0125\n",
      "Epoch [2065/20000], Loss: 4138.32275390625, Entropy -1228.1943359375, Learning Rate: 0.0125\n",
      "Epoch [2066/20000], Loss: 4143.2978515625, Entropy -1399.97705078125, Learning Rate: 0.0125\n",
      "Epoch [2067/20000], Loss: 4124.16064453125, Entropy -1298.330078125, Learning Rate: 0.0125\n",
      "Epoch [2068/20000], Loss: 4250.12939453125, Entropy -1368.224609375, Learning Rate: 0.0125\n",
      "Epoch [2069/20000], Loss: 4226.11376953125, Entropy -1421.3076171875, Learning Rate: 0.0125\n",
      "Epoch [2070/20000], Loss: 4092.003662109375, Entropy -1310.81640625, Learning Rate: 0.0125\n",
      "Epoch [2071/20000], Loss: 4231.470703125, Entropy -1419.5712890625, Learning Rate: 0.0125\n",
      "Epoch [2072/20000], Loss: 4177.09814453125, Entropy -1316.7939453125, Learning Rate: 0.0125\n",
      "Epoch [2073/20000], Loss: 4175.607421875, Entropy -1314.130859375, Learning Rate: 0.0125\n",
      "Epoch [2074/20000], Loss: 4135.99560546875, Entropy -1325.6728515625, Learning Rate: 0.0125\n",
      "Epoch [2075/20000], Loss: 4153.31494140625, Entropy -1309.466796875, Learning Rate: 0.0125\n",
      "Epoch [2076/20000], Loss: 4086.7451171875, Entropy -1327.60595703125, Learning Rate: 0.0125\n",
      "Epoch [2077/20000], Loss: 4114.97021484375, Entropy -1259.658203125, Learning Rate: 0.0125\n",
      "Epoch [2078/20000], Loss: 4160.2119140625, Entropy -1278.85595703125, Learning Rate: 0.0125\n",
      "Epoch [2079/20000], Loss: 4152.46875, Entropy -1350.01171875, Learning Rate: 0.0125\n",
      "Epoch [2080/20000], Loss: 4181.560546875, Entropy -1246.36767578125, Learning Rate: 0.0125\n",
      "Epoch [2081/20000], Loss: 4189.2060546875, Entropy -1298.28173828125, Learning Rate: 0.0125\n",
      "Epoch [2082/20000], Loss: 4174.56396484375, Entropy -1231.7333984375, Learning Rate: 0.0125\n",
      "Epoch [2083/20000], Loss: 4115.29345703125, Entropy -1327.44287109375, Learning Rate: 0.0125\n",
      "Epoch [2084/20000], Loss: 4155.86962890625, Entropy -1290.01513671875, Learning Rate: 0.0125\n",
      "Epoch [2085/20000], Loss: 4200.78955078125, Entropy -1307.90380859375, Learning Rate: 0.0125\n",
      "Epoch [2086/20000], Loss: 4140.73095703125, Entropy -1303.36328125, Learning Rate: 0.0125\n",
      "Epoch [2087/20000], Loss: 4086.117919921875, Entropy -1216.30615234375, Learning Rate: 0.0125\n",
      "Epoch [2088/20000], Loss: 4124.52001953125, Entropy -1328.60595703125, Learning Rate: 0.0125\n",
      "Epoch [2089/20000], Loss: 4140.576171875, Entropy -1221.38623046875, Learning Rate: 0.0125\n",
      "Epoch [2090/20000], Loss: 4193.150390625, Entropy -1229.20556640625, Learning Rate: 0.0125\n",
      "Epoch [2091/20000], Loss: 4069.960205078125, Entropy -1167.91845703125, Learning Rate: 0.0125\n",
      "Epoch [2092/20000], Loss: 4178.080078125, Entropy -1338.8017578125, Learning Rate: 0.0125\n",
      "Epoch [2093/20000], Loss: 4232.240234375, Entropy -1196.33349609375, Learning Rate: 0.0125\n",
      "Epoch [2094/20000], Loss: 4072.462158203125, Entropy -1182.16943359375, Learning Rate: 0.0125\n",
      "Epoch [2095/20000], Loss: 4119.0185546875, Entropy -1283.4345703125, Learning Rate: 0.0125\n",
      "Epoch [2096/20000], Loss: 4204.60205078125, Entropy -1277.7978515625, Learning Rate: 0.0125\n",
      "Epoch [2097/20000], Loss: 4133.77392578125, Entropy -1274.921875, Learning Rate: 0.0125\n",
      "Epoch [2098/20000], Loss: 4117.10107421875, Entropy -1374.97705078125, Learning Rate: 0.0125\n",
      "Epoch [2099/20000], Loss: 4152.40966796875, Entropy -1276.93505859375, Learning Rate: 0.0125\n",
      "Epoch [2100/20000], Loss: 4184.1025390625, Entropy -1168.38720703125, Learning Rate: 0.0125\n",
      "Epoch [2101/20000], Loss: 4121.2939453125, Entropy -1282.28466796875, Learning Rate: 0.0125\n",
      "Epoch [2102/20000], Loss: 4150.521484375, Entropy -1277.03662109375, Learning Rate: 0.0125\n",
      "Epoch [2103/20000], Loss: 4137.30810546875, Entropy -1325.43896484375, Learning Rate: 0.0125\n",
      "Epoch [2104/20000], Loss: 4183.5859375, Entropy -1423.72802734375, Learning Rate: 0.0125\n",
      "Epoch [2105/20000], Loss: 4199.98828125, Entropy -1454.9716796875, Learning Rate: 0.0125\n",
      "Epoch [2106/20000], Loss: 4166.39501953125, Entropy -1371.310546875, Learning Rate: 0.0125\n",
      "Epoch [2107/20000], Loss: 4191.56640625, Entropy -1358.60107421875, Learning Rate: 0.0125\n",
      "Epoch [2108/20000], Loss: 4214.48046875, Entropy -1397.90283203125, Learning Rate: 0.0125\n",
      "Epoch [2109/20000], Loss: 4144.7177734375, Entropy -1350.9072265625, Learning Rate: 0.0125\n",
      "Epoch [2110/20000], Loss: 4133.330078125, Entropy -1265.89404296875, Learning Rate: 0.0125\n",
      "Epoch [2111/20000], Loss: 4162.4609375, Entropy -1306.37109375, Learning Rate: 0.0125\n",
      "Epoch [2112/20000], Loss: 4169.1552734375, Entropy -1226.64208984375, Learning Rate: 0.0125\n",
      "Epoch [2113/20000], Loss: 4193.66357421875, Entropy -1234.673828125, Learning Rate: 0.0125\n",
      "Epoch [2114/20000], Loss: 4155.99267578125, Entropy -1410.47314453125, Learning Rate: 0.0125\n",
      "Epoch [2115/20000], Loss: 4157.103515625, Entropy -1371.9892578125, Learning Rate: 0.0125\n",
      "Epoch [2116/20000], Loss: 4187.47314453125, Entropy -1425.8193359375, Learning Rate: 0.0125\n",
      "Epoch [2117/20000], Loss: 4183.9462890625, Entropy -1322.4697265625, Learning Rate: 0.0125\n",
      "Epoch [2118/20000], Loss: 4104.22216796875, Entropy -1299.35302734375, Learning Rate: 0.0125\n",
      "Epoch [2119/20000], Loss: 4144.50146484375, Entropy -1311.1015625, Learning Rate: 0.0125\n",
      "Epoch [2120/20000], Loss: 4209.52099609375, Entropy -1369.8720703125, Learning Rate: 0.0125\n",
      "Epoch [2121/20000], Loss: 4218.64892578125, Entropy -1375.67919921875, Learning Rate: 0.0125\n",
      "Epoch [2122/20000], Loss: 4122.47119140625, Entropy -1220.8837890625, Learning Rate: 0.0125\n",
      "Epoch [2123/20000], Loss: 4245.58935546875, Entropy -1433.9599609375, Learning Rate: 0.0125\n",
      "Epoch [2124/20000], Loss: 4142.3203125, Entropy -1286.16455078125, Learning Rate: 0.0125\n",
      "Epoch [2125/20000], Loss: 4172.99951171875, Entropy -1267.59619140625, Learning Rate: 0.0125\n",
      "Epoch [2126/20000], Loss: 4142.92431640625, Entropy -1314.84423828125, Learning Rate: 0.0125\n",
      "Epoch [2127/20000], Loss: 4182.380859375, Entropy -1288.8759765625, Learning Rate: 0.0125\n",
      "Epoch [2128/20000], Loss: 4195.1298828125, Entropy -1249.58837890625, Learning Rate: 0.0125\n",
      "Epoch [2129/20000], Loss: 4175.5595703125, Entropy -1359.98779296875, Learning Rate: 0.0125\n",
      "Epoch [2130/20000], Loss: 4156.77880859375, Entropy -1299.14111328125, Learning Rate: 0.0125\n",
      "Epoch [2131/20000], Loss: 4084.428955078125, Entropy -1205.14599609375, Learning Rate: 0.0125\n",
      "Epoch [2132/20000], Loss: 4146.1904296875, Entropy -1405.353759765625, Learning Rate: 0.0125\n",
      "Epoch [2133/20000], Loss: 4189.08251953125, Entropy -1281.12646484375, Learning Rate: 0.0125\n",
      "Epoch [2134/20000], Loss: 4112.14892578125, Entropy -1205.1474609375, Learning Rate: 0.0125\n",
      "Epoch [2135/20000], Loss: 4146.611328125, Entropy -1260.71728515625, Learning Rate: 0.0125\n",
      "Epoch [2136/20000], Loss: 4185.57275390625, Entropy -1249.79345703125, Learning Rate: 0.0125\n",
      "Epoch [2137/20000], Loss: 4154.68017578125, Entropy -1276.05029296875, Learning Rate: 0.0125\n",
      "Epoch [2138/20000], Loss: 4205.89208984375, Entropy -1325.5771484375, Learning Rate: 0.0125\n",
      "Epoch [2139/20000], Loss: 4143.9462890625, Entropy -1194.2421875, Learning Rate: 0.0125\n",
      "Epoch [2140/20000], Loss: 4146.8662109375, Entropy -1238.08642578125, Learning Rate: 0.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2141/20000], Loss: 4174.75, Entropy -1342.23974609375, Learning Rate: 0.0125\n",
      "Epoch [2142/20000], Loss: 4122.029296875, Entropy -1233.18798828125, Learning Rate: 0.0125\n",
      "Epoch [2143/20000], Loss: 4157.146484375, Entropy -1369.56005859375, Learning Rate: 0.0125\n",
      "Epoch [2144/20000], Loss: 4137.2841796875, Entropy -1296.623046875, Learning Rate: 0.0125\n",
      "Epoch [2145/20000], Loss: 4143.54150390625, Entropy -1217.2802734375, Learning Rate: 0.0125\n",
      "Epoch [2146/20000], Loss: 4205.13916015625, Entropy -1375.06640625, Learning Rate: 0.0125\n",
      "Epoch [2147/20000], Loss: 4154.7412109375, Entropy -1317.55419921875, Learning Rate: 0.0125\n",
      "Epoch [2148/20000], Loss: 4238.84033203125, Entropy -1294.896484375, Learning Rate: 0.0125\n",
      "Epoch [2149/20000], Loss: 4124.0673828125, Entropy -1314.6943359375, Learning Rate: 0.0125\n",
      "Epoch [2150/20000], Loss: 4145.64404296875, Entropy -1239.5361328125, Learning Rate: 0.0125\n",
      "Epoch [2151/20000], Loss: 4077.592041015625, Entropy -1302.70361328125, Learning Rate: 0.0125\n",
      "Epoch [2152/20000], Loss: 4186.54931640625, Entropy -1404.7626953125, Learning Rate: 0.0125\n",
      "Epoch [2153/20000], Loss: 4148.28955078125, Entropy -1282.666015625, Learning Rate: 0.0125\n",
      "Epoch [2154/20000], Loss: 4149.34912109375, Entropy -1241.31787109375, Learning Rate: 0.0125\n",
      "Epoch [2155/20000], Loss: 4106.2822265625, Entropy -1311.22314453125, Learning Rate: 0.0125\n",
      "Epoch [2156/20000], Loss: 4121.12548828125, Entropy -1271.705078125, Learning Rate: 0.0125\n",
      "Epoch [2157/20000], Loss: 4113.6767578125, Entropy -1314.33056640625, Learning Rate: 0.0125\n",
      "Epoch [2158/20000], Loss: 4087.884033203125, Entropy -1202.8857421875, Learning Rate: 0.0125\n",
      "Epoch [2159/20000], Loss: 4164.37255859375, Entropy -1300.39453125, Learning Rate: 0.0125\n",
      "Epoch [2160/20000], Loss: 4071.861083984375, Entropy -1260.970703125, Learning Rate: 0.0125\n",
      "Epoch [2161/20000], Loss: 4112.03759765625, Entropy -1296.447265625, Learning Rate: 0.0125\n",
      "Epoch [2162/20000], Loss: 4146.26416015625, Entropy -1426.964599609375, Learning Rate: 0.0125\n",
      "Epoch [2163/20000], Loss: 4187.26513671875, Entropy -1394.81884765625, Learning Rate: 0.0125\n",
      "Epoch [2164/20000], Loss: 4162.8505859375, Entropy -1279.4873046875, Learning Rate: 0.0125\n",
      "Epoch [2165/20000], Loss: 4177.8359375, Entropy -1381.943359375, Learning Rate: 0.0125\n",
      "Epoch [2166/20000], Loss: 4174.1669921875, Entropy -1341.18798828125, Learning Rate: 0.0125\n",
      "Epoch [2167/20000], Loss: 4120.52880859375, Entropy -1336.576171875, Learning Rate: 0.0125\n",
      "Epoch [2168/20000], Loss: 4171.6064453125, Entropy -1368.82666015625, Learning Rate: 0.0125\n",
      "Epoch [2169/20000], Loss: 4142.3359375, Entropy -1331.52490234375, Learning Rate: 0.0125\n",
      "Epoch [2170/20000], Loss: 4137.158203125, Entropy -1285.33349609375, Learning Rate: 0.0125\n",
      "Epoch [2171/20000], Loss: 4117.19775390625, Entropy -1234.06298828125, Learning Rate: 0.0125\n",
      "Epoch [2172/20000], Loss: 4159.5146484375, Entropy -1367.83837890625, Learning Rate: 0.0125\n",
      "Epoch [2173/20000], Loss: 4159.06103515625, Entropy -1259.92529296875, Learning Rate: 0.0125\n",
      "Epoch [2174/20000], Loss: 4248.6591796875, Entropy -1427.3095703125, Learning Rate: 0.0125\n",
      "Epoch [2175/20000], Loss: 4174.2900390625, Entropy -1326.2685546875, Learning Rate: 0.0125\n",
      "Epoch [2176/20000], Loss: 4163.3486328125, Entropy -1363.77392578125, Learning Rate: 0.0125\n",
      "Epoch [2177/20000], Loss: 4137.6611328125, Entropy -1305.43212890625, Learning Rate: 0.0125\n",
      "Epoch [2178/20000], Loss: 4140.54736328125, Entropy -1348.212890625, Learning Rate: 0.0125\n",
      "Epoch [2179/20000], Loss: 4107.6435546875, Entropy -1301.52587890625, Learning Rate: 0.0125\n",
      "Epoch [2180/20000], Loss: 4071.302734375, Entropy -1221.59423828125, Learning Rate: 0.0125\n",
      "Epoch [2181/20000], Loss: 4205.021484375, Entropy -1404.02978515625, Learning Rate: 0.0125\n",
      "Epoch [2182/20000], Loss: 4170.2666015625, Entropy -1328.88720703125, Learning Rate: 0.0125\n",
      "Epoch [2183/20000], Loss: 4138.28369140625, Entropy -1264.4482421875, Learning Rate: 0.0125\n",
      "Epoch [2184/20000], Loss: 4140.45166015625, Entropy -1332.52001953125, Learning Rate: 0.0125\n",
      "Epoch [2185/20000], Loss: 4163.5537109375, Entropy -1365.27587890625, Learning Rate: 0.0125\n",
      "Epoch [2186/20000], Loss: 4141.404296875, Entropy -1244.3427734375, Learning Rate: 0.0125\n",
      "Epoch [2187/20000], Loss: 4122.51416015625, Entropy -1390.033203125, Learning Rate: 0.0125\n",
      "Epoch [2188/20000], Loss: 4194.2705078125, Entropy -1257.77392578125, Learning Rate: 0.0125\n",
      "Epoch [2189/20000], Loss: 4111.02392578125, Entropy -1314.56005859375, Learning Rate: 0.0125\n",
      "Epoch [2190/20000], Loss: 4150.34130859375, Entropy -1286.50048828125, Learning Rate: 0.0125\n",
      "Epoch [2191/20000], Loss: 4223.1474609375, Entropy -1347.01025390625, Learning Rate: 0.0125\n",
      "Epoch [2192/20000], Loss: 4131.35205078125, Entropy -1271.787109375, Learning Rate: 0.0125\n",
      "Epoch [2193/20000], Loss: 4148.9580078125, Entropy -1339.19775390625, Learning Rate: 0.0125\n",
      "Epoch [2194/20000], Loss: 4146.27783203125, Entropy -1392.6435546875, Learning Rate: 0.0125\n",
      "Epoch [2195/20000], Loss: 4182.84130859375, Entropy -1377.21728515625, Learning Rate: 0.0125\n",
      "Epoch [2196/20000], Loss: 4107.9658203125, Entropy -1253.97509765625, Learning Rate: 0.0125\n",
      "Epoch [2197/20000], Loss: 4133.1064453125, Entropy -1391.900146484375, Learning Rate: 0.0125\n",
      "Epoch [2198/20000], Loss: 4177.611328125, Entropy -1358.791015625, Learning Rate: 0.0125\n",
      "Epoch [2199/20000], Loss: 4074.598388671875, Entropy -1290.9169921875, Learning Rate: 0.0125\n",
      "Epoch [2200/20000], Loss: 4158.099609375, Entropy -1239.166015625, Learning Rate: 0.0125\n",
      "Epoch [2201/20000], Loss: 4129.93896484375, Entropy -1313.673828125, Learning Rate: 0.0125\n",
      "Epoch [2202/20000], Loss: 4116.681640625, Entropy -1272.31494140625, Learning Rate: 0.0125\n",
      "Epoch [2203/20000], Loss: 4107.908203125, Entropy -1156.6123046875, Learning Rate: 0.0125\n",
      "Epoch [2204/20000], Loss: 4183.92919921875, Entropy -1233.18896484375, Learning Rate: 0.0125\n",
      "Epoch [2205/20000], Loss: 4128.45751953125, Entropy -1328.5576171875, Learning Rate: 0.0125\n",
      "Epoch [2206/20000], Loss: 4149.7353515625, Entropy -1377.9296875, Learning Rate: 0.0125\n",
      "Epoch [2207/20000], Loss: 4139.32080078125, Entropy -1313.12255859375, Learning Rate: 0.0125\n",
      "Epoch [2208/20000], Loss: 4193.39501953125, Entropy -1363.5693359375, Learning Rate: 0.0125\n",
      "Epoch [2209/20000], Loss: 4254.15283203125, Entropy -1397.836181640625, Learning Rate: 0.0125\n",
      "Epoch [2210/20000], Loss: 4246.0888671875, Entropy -1420.046142578125, Learning Rate: 0.0125\n",
      "Epoch [2211/20000], Loss: 4238.06005859375, Entropy -1468.15673828125, Learning Rate: 0.0125\n",
      "Epoch [2212/20000], Loss: 4141.5703125, Entropy -1370.380859375, Learning Rate: 0.0125\n",
      "Epoch [2213/20000], Loss: 4166.92529296875, Entropy -1355.68359375, Learning Rate: 0.0125\n",
      "Epoch [2214/20000], Loss: 4101.1318359375, Entropy -1324.86083984375, Learning Rate: 0.0125\n",
      "Epoch [2215/20000], Loss: 4186.62060546875, Entropy -1359.6640625, Learning Rate: 0.0125\n",
      "Epoch [2216/20000], Loss: 4119.57421875, Entropy -1247.396484375, Learning Rate: 0.0125\n",
      "Epoch [2217/20000], Loss: 4179.59130859375, Entropy -1366.63720703125, Learning Rate: 0.0125\n",
      "Epoch [2218/20000], Loss: 4204.9326171875, Entropy -1327.42236328125, Learning Rate: 0.0125\n",
      "Epoch [2219/20000], Loss: 4092.80419921875, Entropy -1279.29052734375, Learning Rate: 0.0125\n",
      "Epoch [2220/20000], Loss: 4124.62939453125, Entropy -1276.18798828125, Learning Rate: 0.0125\n",
      "Epoch [2221/20000], Loss: 4102.31884765625, Entropy -1279.7900390625, Learning Rate: 0.0125\n",
      "Epoch [2222/20000], Loss: 4168.462890625, Entropy -1251.34423828125, Learning Rate: 0.0125\n",
      "Epoch [2223/20000], Loss: 4117.68896484375, Entropy -1227.390625, Learning Rate: 0.0125\n",
      "Epoch [2224/20000], Loss: 4122.02099609375, Entropy -1274.9677734375, Learning Rate: 0.0125\n",
      "Epoch [2225/20000], Loss: 4116.06103515625, Entropy -1270.65625, Learning Rate: 0.0125\n",
      "Epoch [2226/20000], Loss: 4200.05810546875, Entropy -1385.94287109375, Learning Rate: 0.0125\n",
      "Epoch [2227/20000], Loss: 4091.212646484375, Entropy -1271.75732421875, Learning Rate: 0.0125\n",
      "Epoch [2228/20000], Loss: 4160.92822265625, Entropy -1308.4541015625, Learning Rate: 0.0125\n",
      "Epoch [2229/20000], Loss: 4120.060546875, Entropy -1266.552734375, Learning Rate: 0.0125\n",
      "Epoch [2230/20000], Loss: 4103.22412109375, Entropy -1360.587890625, Learning Rate: 0.0125\n",
      "Epoch [2231/20000], Loss: 4172.04833984375, Entropy -1360.50439453125, Learning Rate: 0.0125\n",
      "Epoch [2232/20000], Loss: 4135.8544921875, Entropy -1246.8984375, Learning Rate: 0.0125\n",
      "Epoch [2233/20000], Loss: 4163.76611328125, Entropy -1335.48388671875, Learning Rate: 0.0125\n",
      "Epoch [2234/20000], Loss: 4141.69970703125, Entropy -1310.0927734375, Learning Rate: 0.0125\n",
      "Epoch [2235/20000], Loss: 4173.16650390625, Entropy -1277.83251953125, Learning Rate: 0.0125\n",
      "Epoch [2236/20000], Loss: 4094.5478515625, Entropy -1284.52001953125, Learning Rate: 0.0125\n",
      "Epoch [2237/20000], Loss: 4086.66259765625, Entropy -1201.990234375, Learning Rate: 0.0125\n",
      "Epoch [2238/20000], Loss: 4257.61865234375, Entropy -1328.17138671875, Learning Rate: 0.0125\n",
      "Epoch [2239/20000], Loss: 4129.62109375, Entropy -1337.63037109375, Learning Rate: 0.0125\n",
      "Epoch [2240/20000], Loss: 4121.798828125, Entropy -1296.69921875, Learning Rate: 0.0125\n",
      "Epoch [2241/20000], Loss: 4173.33935546875, Entropy -1236.93212890625, Learning Rate: 0.0125\n",
      "Epoch [2242/20000], Loss: 4143.443359375, Entropy -1364.580078125, Learning Rate: 0.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2243/20000], Loss: 4152.0498046875, Entropy -1301.1904296875, Learning Rate: 0.0125\n",
      "Epoch [2244/20000], Loss: 4113.42431640625, Entropy -1367.83056640625, Learning Rate: 0.0125\n",
      "Epoch [2245/20000], Loss: 4126.5439453125, Entropy -1287.75048828125, Learning Rate: 0.0125\n",
      "Epoch [2246/20000], Loss: 4098.9296875, Entropy -1173.0927734375, Learning Rate: 0.0125\n",
      "Epoch [2247/20000], Loss: 4124.5263671875, Entropy -1244.45703125, Learning Rate: 0.0125\n",
      "Epoch [2248/20000], Loss: 4174.794921875, Entropy -1350.61083984375, Learning Rate: 0.0125\n",
      "Epoch [2249/20000], Loss: 4087.468017578125, Entropy -1263.32958984375, Learning Rate: 0.0125\n",
      "Epoch [2250/20000], Loss: 4101.65283203125, Entropy -1172.94873046875, Learning Rate: 0.0125\n",
      "Epoch [2251/20000], Loss: 4139.6474609375, Entropy -1343.09912109375, Learning Rate: 0.0125\n",
      "Epoch [2252/20000], Loss: 4076.27978515625, Entropy -1217.61181640625, Learning Rate: 0.0125\n",
      "Epoch [2253/20000], Loss: 4110.18115234375, Entropy -1238.81005859375, Learning Rate: 0.0125\n",
      "Epoch [2254/20000], Loss: 4210.06494140625, Entropy -1351.6865234375, Learning Rate: 0.0125\n",
      "Epoch [2255/20000], Loss: 4151.0966796875, Entropy -1142.25146484375, Learning Rate: 0.0125\n",
      "Epoch [2256/20000], Loss: 4158.341796875, Entropy -1214.916015625, Learning Rate: 0.0125\n",
      "Epoch [2257/20000], Loss: 4188.21240234375, Entropy -1216.79150390625, Learning Rate: 0.0125\n",
      "Epoch [2258/20000], Loss: 4144.48388671875, Entropy -1290.58837890625, Learning Rate: 0.0125\n",
      "Epoch [2259/20000], Loss: 4135.72314453125, Entropy -1388.1337890625, Learning Rate: 0.0125\n",
      "Epoch [2260/20000], Loss: 4130.55517578125, Entropy -1288.91162109375, Learning Rate: 0.0125\n",
      "Epoch [2261/20000], Loss: 4124.435546875, Entropy -1323.12060546875, Learning Rate: 0.0125\n",
      "Epoch [2262/20000], Loss: 4164.833984375, Entropy -1320.29248046875, Learning Rate: 0.0125\n",
      "Epoch [2263/20000], Loss: 4157.83642578125, Entropy -1221.0400390625, Learning Rate: 0.0125\n",
      "Epoch [2264/20000], Loss: 4125.6875, Entropy -1331.1142578125, Learning Rate: 0.0125\n",
      "Epoch [2265/20000], Loss: 4121.69677734375, Entropy -1387.39599609375, Learning Rate: 0.0125\n",
      "Epoch [2266/20000], Loss: 4161.052734375, Entropy -1349.50537109375, Learning Rate: 0.0125\n",
      "Epoch [2267/20000], Loss: 4092.152099609375, Entropy -1219.18115234375, Learning Rate: 0.0125\n",
      "Epoch [2268/20000], Loss: 4119.0439453125, Entropy -1213.07568359375, Learning Rate: 0.0125\n",
      "Epoch [2269/20000], Loss: 4168.541015625, Entropy -1235.0439453125, Learning Rate: 0.0125\n",
      "Epoch [2270/20000], Loss: 4142.8505859375, Entropy -1260.98681640625, Learning Rate: 0.0125\n",
      "Epoch [2271/20000], Loss: 4162.11083984375, Entropy -1373.9697265625, Learning Rate: 0.0125\n",
      "Epoch [2272/20000], Loss: 4102.5390625, Entropy -1289.720703125, Learning Rate: 0.0125\n",
      "Epoch [2273/20000], Loss: 4137.09814453125, Entropy -1224.48876953125, Learning Rate: 0.0125\n",
      "Epoch [2274/20000], Loss: 4148.1708984375, Entropy -1369.87060546875, Learning Rate: 0.0125\n",
      "Epoch [2275/20000], Loss: 4185.25537109375, Entropy -1399.69140625, Learning Rate: 0.0125\n",
      "Epoch [2276/20000], Loss: 4115.8583984375, Entropy -1160.28076171875, Learning Rate: 0.0125\n",
      "Epoch [2277/20000], Loss: 4170.3544921875, Entropy -1346.15771484375, Learning Rate: 0.0125\n",
      "Epoch [2278/20000], Loss: 4122.34423828125, Entropy -1313.43603515625, Learning Rate: 0.0125\n",
      "Epoch [2279/20000], Loss: 4104.6005859375, Entropy -1246.33154296875, Learning Rate: 0.0125\n",
      "Epoch [2280/20000], Loss: 4143.66015625, Entropy -1311.85498046875, Learning Rate: 0.0125\n",
      "Epoch [2281/20000], Loss: 4166.5400390625, Entropy -1260.94482421875, Learning Rate: 0.0125\n",
      "Epoch [2282/20000], Loss: 4184.29443359375, Entropy -1228.18115234375, Learning Rate: 0.0125\n",
      "Epoch [2283/20000], Loss: 4208.70703125, Entropy -1340.0361328125, Learning Rate: 0.0125\n",
      "Epoch [2284/20000], Loss: 4194.13330078125, Entropy -1292.7373046875, Learning Rate: 0.0125\n",
      "Epoch [2285/20000], Loss: 4101.96923828125, Entropy -1209.74755859375, Learning Rate: 0.0125\n",
      "Epoch [2286/20000], Loss: 4110.36328125, Entropy -1282.421875, Learning Rate: 0.0125\n",
      "Epoch [2287/20000], Loss: 4127.10595703125, Entropy -1269.59375, Learning Rate: 0.0125\n",
      "Epoch [2288/20000], Loss: 4164.5380859375, Entropy -1342.11181640625, Learning Rate: 0.0125\n",
      "Epoch [2289/20000], Loss: 4163.375, Entropy -1383.1298828125, Learning Rate: 0.0125\n",
      "Epoch [2290/20000], Loss: 4113.47900390625, Entropy -1231.154296875, Learning Rate: 0.0125\n",
      "Epoch [2291/20000], Loss: 4123.55419921875, Entropy -1307.326171875, Learning Rate: 0.0125\n",
      "Epoch [2292/20000], Loss: 4139.02294921875, Entropy -1389.22509765625, Learning Rate: 0.0125\n",
      "Epoch [2293/20000], Loss: 4138.6162109375, Entropy -1263.79150390625, Learning Rate: 0.00625\n",
      "Epoch [2294/20000], Loss: 4126.96142578125, Entropy -1307.21533203125, Learning Rate: 0.00625\n",
      "Epoch [2295/20000], Loss: 4095.58203125, Entropy -1238.9697265625, Learning Rate: 0.00625\n",
      "Epoch [2296/20000], Loss: 4100.81640625, Entropy -1173.9248046875, Learning Rate: 0.00625\n",
      "Epoch [2297/20000], Loss: 4159.119140625, Entropy -1236.0634765625, Learning Rate: 0.00625\n",
      "Epoch [2298/20000], Loss: 4073.654296875, Entropy -1310.34619140625, Learning Rate: 0.00625\n",
      "Epoch [2299/20000], Loss: 4112.19873046875, Entropy -1294.43115234375, Learning Rate: 0.00625\n",
      "Epoch [2300/20000], Loss: 4166.078125, Entropy -1377.017578125, Learning Rate: 0.00625\n",
      "Epoch [2301/20000], Loss: 4124.44580078125, Entropy -1177.00244140625, Learning Rate: 0.00625\n",
      "Epoch [2302/20000], Loss: 4131.92919921875, Entropy -1217.275390625, Learning Rate: 0.00625\n",
      "Epoch [2303/20000], Loss: 4120.4814453125, Entropy -1267.68310546875, Learning Rate: 0.00625\n",
      "Epoch [2304/20000], Loss: 4103.35205078125, Entropy -1318.3896484375, Learning Rate: 0.00625\n",
      "Epoch [2305/20000], Loss: 4111.615234375, Entropy -1293.0810546875, Learning Rate: 0.00625\n",
      "Epoch [2306/20000], Loss: 4118.60205078125, Entropy -1284.4013671875, Learning Rate: 0.00625\n",
      "Epoch [2307/20000], Loss: 4227.58203125, Entropy -1398.549072265625, Learning Rate: 0.00625\n",
      "Epoch [2308/20000], Loss: 4139.630859375, Entropy -1238.9541015625, Learning Rate: 0.00625\n",
      "Epoch [2309/20000], Loss: 4127.5537109375, Entropy -1337.28271484375, Learning Rate: 0.00625\n",
      "Epoch [2310/20000], Loss: 4183.38330078125, Entropy -1385.4345703125, Learning Rate: 0.00625\n",
      "Epoch [2311/20000], Loss: 4238.9931640625, Entropy -1377.421875, Learning Rate: 0.00625\n",
      "Epoch [2312/20000], Loss: 4130.255859375, Entropy -1345.55615234375, Learning Rate: 0.00625\n",
      "Epoch [2313/20000], Loss: 4123.21728515625, Entropy -1281.14892578125, Learning Rate: 0.00625\n",
      "Epoch [2314/20000], Loss: 4207.734375, Entropy -1379.0205078125, Learning Rate: 0.00625\n",
      "Epoch [2315/20000], Loss: 4107.31591796875, Entropy -1260.5927734375, Learning Rate: 0.00625\n",
      "Epoch [2316/20000], Loss: 4227.861328125, Entropy -1346.25634765625, Learning Rate: 0.00625\n",
      "Epoch [2317/20000], Loss: 4178.40869140625, Entropy -1297.6982421875, Learning Rate: 0.00625\n",
      "Epoch [2318/20000], Loss: 4106.09228515625, Entropy -1338.701171875, Learning Rate: 0.00625\n",
      "Epoch [2319/20000], Loss: 4167.4462890625, Entropy -1386.7724609375, Learning Rate: 0.00625\n",
      "Epoch [2320/20000], Loss: 4127.91748046875, Entropy -1328.31201171875, Learning Rate: 0.00625\n",
      "Epoch [2321/20000], Loss: 4137.87890625, Entropy -1293.44580078125, Learning Rate: 0.00625\n",
      "Epoch [2322/20000], Loss: 4099.50537109375, Entropy -1346.48681640625, Learning Rate: 0.00625\n",
      "Epoch [2323/20000], Loss: 4094.665771484375, Entropy -1266.34521484375, Learning Rate: 0.00625\n",
      "Epoch [2324/20000], Loss: 4184.3408203125, Entropy -1399.919921875, Learning Rate: 0.00625\n",
      "Epoch [2325/20000], Loss: 4201.39306640625, Entropy -1298.23291015625, Learning Rate: 0.00625\n",
      "Epoch [2326/20000], Loss: 4186.77783203125, Entropy -1259.6767578125, Learning Rate: 0.00625\n",
      "Epoch [2327/20000], Loss: 4161.66650390625, Entropy -1244.99560546875, Learning Rate: 0.00625\n",
      "Epoch [2328/20000], Loss: 4155.27197265625, Entropy -1323.8369140625, Learning Rate: 0.00625\n",
      "Epoch [2329/20000], Loss: 4130.02001953125, Entropy -1280.4482421875, Learning Rate: 0.00625\n",
      "Epoch [2330/20000], Loss: 4155.61865234375, Entropy -1258.03173828125, Learning Rate: 0.00625\n",
      "Epoch [2331/20000], Loss: 4144.61669921875, Entropy -1255.5078125, Learning Rate: 0.00625\n",
      "Epoch [2332/20000], Loss: 4118.201171875, Entropy -1272.6005859375, Learning Rate: 0.00625\n",
      "Epoch [2333/20000], Loss: 4222.013671875, Entropy -1373.85400390625, Learning Rate: 0.00625\n",
      "Epoch [2334/20000], Loss: 4131.89794921875, Entropy -1296.9189453125, Learning Rate: 0.00625\n",
      "Epoch [2335/20000], Loss: 4106.2705078125, Entropy -1279.07861328125, Learning Rate: 0.00625\n",
      "Epoch [2336/20000], Loss: 4140.76123046875, Entropy -1240.95166015625, Learning Rate: 0.00625\n",
      "Epoch [2337/20000], Loss: 4118.900390625, Entropy -1300.86181640625, Learning Rate: 0.00625\n",
      "Epoch [2338/20000], Loss: 4111.23828125, Entropy -1323.74755859375, Learning Rate: 0.00625\n",
      "Epoch [2339/20000], Loss: 4107.0302734375, Entropy -1265.08251953125, Learning Rate: 0.00625\n",
      "Epoch [2340/20000], Loss: 4177.3896484375, Entropy -1210.21875, Learning Rate: 0.00625\n",
      "Epoch [2341/20000], Loss: 4128.42724609375, Entropy -1197.76513671875, Learning Rate: 0.00625\n",
      "Epoch [2342/20000], Loss: 4146.82177734375, Entropy -1242.63671875, Learning Rate: 0.00625\n",
      "Epoch [2343/20000], Loss: 4160.25732421875, Entropy -1352.7001953125, Learning Rate: 0.00625\n",
      "Epoch [2344/20000], Loss: 4086.135009765625, Entropy -1147.32373046875, Learning Rate: 0.00625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2345/20000], Loss: 4218.37451171875, Entropy -1379.51708984375, Learning Rate: 0.00625\n",
      "Epoch [2346/20000], Loss: 4130.37109375, Entropy -1261.97412109375, Learning Rate: 0.00625\n",
      "Epoch [2347/20000], Loss: 4128.45166015625, Entropy -1276.4462890625, Learning Rate: 0.00625\n",
      "Epoch [2348/20000], Loss: 4111.68896484375, Entropy -1225.22802734375, Learning Rate: 0.00625\n",
      "Epoch [2349/20000], Loss: 4085.28759765625, Entropy -1229.46337890625, Learning Rate: 0.00625\n",
      "Epoch [2350/20000], Loss: 4247.39599609375, Entropy -1366.67724609375, Learning Rate: 0.00625\n",
      "Epoch [2351/20000], Loss: 4190.43310546875, Entropy -1288.880859375, Learning Rate: 0.00625\n",
      "Epoch [2352/20000], Loss: 4091.47314453125, Entropy -1333.62255859375, Learning Rate: 0.00625\n",
      "Epoch [2353/20000], Loss: 4118.978515625, Entropy -1296.388671875, Learning Rate: 0.00625\n",
      "Epoch [2354/20000], Loss: 4137.90673828125, Entropy -1276.70263671875, Learning Rate: 0.00625\n",
      "Epoch [2355/20000], Loss: 4180.80078125, Entropy -1314.7548828125, Learning Rate: 0.00625\n",
      "Epoch [2356/20000], Loss: 4181.2880859375, Entropy -1361.23046875, Learning Rate: 0.00625\n",
      "Epoch [2357/20000], Loss: 4146.56640625, Entropy -1301.88818359375, Learning Rate: 0.00625\n",
      "Epoch [2358/20000], Loss: 4156.75439453125, Entropy -1361.388671875, Learning Rate: 0.00625\n",
      "Epoch [2359/20000], Loss: 4135.1181640625, Entropy -1267.1748046875, Learning Rate: 0.00625\n",
      "Epoch [2360/20000], Loss: 4103.7822265625, Entropy -1320.8447265625, Learning Rate: 0.00625\n",
      "Epoch [2361/20000], Loss: 4154.3828125, Entropy -1317.75830078125, Learning Rate: 0.00625\n",
      "Epoch [2362/20000], Loss: 4132.54443359375, Entropy -1342.1904296875, Learning Rate: 0.00625\n",
      "Epoch [2363/20000], Loss: 4114.162109375, Entropy -1205.00537109375, Learning Rate: 0.00625\n",
      "Epoch [2364/20000], Loss: 4091.53662109375, Entropy -1301.29052734375, Learning Rate: 0.00625\n",
      "Epoch [2365/20000], Loss: 4214.4765625, Entropy -1365.8271484375, Learning Rate: 0.00625\n",
      "Epoch [2366/20000], Loss: 4086.6064453125, Entropy -1262.744140625, Learning Rate: 0.00625\n",
      "Epoch [2367/20000], Loss: 4111.5869140625, Entropy -1275.13037109375, Learning Rate: 0.00625\n",
      "Epoch [2368/20000], Loss: 4121.83056640625, Entropy -1337.75634765625, Learning Rate: 0.00625\n",
      "Epoch [2369/20000], Loss: 4146.67578125, Entropy -1238.32373046875, Learning Rate: 0.00625\n",
      "Epoch [2370/20000], Loss: 4131.6005859375, Entropy -1266.1982421875, Learning Rate: 0.00625\n",
      "Epoch [2371/20000], Loss: 4141.705078125, Entropy -1311.44384765625, Learning Rate: 0.00625\n",
      "Epoch [2372/20000], Loss: 4121.77685546875, Entropy -1308.49951171875, Learning Rate: 0.00625\n",
      "Epoch [2373/20000], Loss: 4147.1953125, Entropy -1289.43017578125, Learning Rate: 0.00625\n",
      "Epoch [2374/20000], Loss: 4126.4384765625, Entropy -1298.8857421875, Learning Rate: 0.00625\n",
      "Epoch [2375/20000], Loss: 4152.53076171875, Entropy -1352.13427734375, Learning Rate: 0.00625\n",
      "Epoch [2376/20000], Loss: 4028.635009765625, Entropy -1127.5234375, Learning Rate: 0.00625\n",
      "Epoch [2377/20000], Loss: 4184.2431640625, Entropy -1247.85107421875, Learning Rate: 0.00625\n",
      "Epoch [2378/20000], Loss: 4167.34814453125, Entropy -1271.54443359375, Learning Rate: 0.00625\n",
      "Epoch [2379/20000], Loss: 4184.8330078125, Entropy -1321.22314453125, Learning Rate: 0.00625\n",
      "Epoch [2380/20000], Loss: 4196.14697265625, Entropy -1463.289306640625, Learning Rate: 0.00625\n",
      "Epoch [2381/20000], Loss: 4184.94287109375, Entropy -1315.54052734375, Learning Rate: 0.00625\n",
      "Epoch [2382/20000], Loss: 4149.21875, Entropy -1329.25341796875, Learning Rate: 0.00625\n",
      "Epoch [2383/20000], Loss: 4124.50537109375, Entropy -1231.47998046875, Learning Rate: 0.00625\n",
      "Epoch [2384/20000], Loss: 4115.462890625, Entropy -1239.64892578125, Learning Rate: 0.00625\n",
      "Epoch [2385/20000], Loss: 4177.22509765625, Entropy -1441.46630859375, Learning Rate: 0.00625\n",
      "Epoch [2386/20000], Loss: 4048.215087890625, Entropy -1115.37255859375, Learning Rate: 0.00625\n",
      "Epoch [2387/20000], Loss: 4192.22607421875, Entropy -1383.0068359375, Learning Rate: 0.00625\n",
      "Epoch [2388/20000], Loss: 4101.123046875, Entropy -1242.82666015625, Learning Rate: 0.00625\n",
      "Epoch [2389/20000], Loss: 4200.693359375, Entropy -1259.92578125, Learning Rate: 0.00625\n",
      "Epoch [2390/20000], Loss: 4187.37158203125, Entropy -1221.427734375, Learning Rate: 0.00625\n",
      "Epoch [2391/20000], Loss: 4147.0400390625, Entropy -1323.1396484375, Learning Rate: 0.00625\n",
      "Epoch [2392/20000], Loss: 4126.4619140625, Entropy -1268.46484375, Learning Rate: 0.00625\n",
      "Epoch [2393/20000], Loss: 4096.7939453125, Entropy -1234.45654296875, Learning Rate: 0.00625\n",
      "Epoch [2394/20000], Loss: 4187.61181640625, Entropy -1413.4853515625, Learning Rate: 0.00625\n",
      "Epoch [2395/20000], Loss: 4116.65966796875, Entropy -1221.15771484375, Learning Rate: 0.00625\n",
      "Epoch [2396/20000], Loss: 4091.519775390625, Entropy -1362.74658203125, Learning Rate: 0.00625\n",
      "Epoch [2397/20000], Loss: 4205.88720703125, Entropy -1388.75048828125, Learning Rate: 0.00625\n",
      "Epoch [2398/20000], Loss: 4137.75146484375, Entropy -1374.63623046875, Learning Rate: 0.00625\n",
      "Epoch [2399/20000], Loss: 4223.8251953125, Entropy -1457.219970703125, Learning Rate: 0.00625\n",
      "Epoch [2400/20000], Loss: 4147.45751953125, Entropy -1278.38134765625, Learning Rate: 0.00625\n",
      "Epoch [2401/20000], Loss: 4158.6845703125, Entropy -1355.35302734375, Learning Rate: 0.00625\n",
      "Epoch [2402/20000], Loss: 4123.587890625, Entropy -1338.056640625, Learning Rate: 0.00625\n",
      "Epoch [2403/20000], Loss: 4079.593017578125, Entropy -1227.32080078125, Learning Rate: 0.00625\n",
      "Epoch [2404/20000], Loss: 4124.70556640625, Entropy -1308.017578125, Learning Rate: 0.00625\n",
      "Epoch [2405/20000], Loss: 4178.22705078125, Entropy -1270.83203125, Learning Rate: 0.00625\n",
      "Epoch [2406/20000], Loss: 4169.94970703125, Entropy -1337.24658203125, Learning Rate: 0.00625\n",
      "Epoch [2407/20000], Loss: 4178.76904296875, Entropy -1358.5205078125, Learning Rate: 0.00625\n",
      "Epoch [2408/20000], Loss: 4118.61328125, Entropy -1343.46240234375, Learning Rate: 0.00625\n",
      "Epoch [2409/20000], Loss: 4170.24365234375, Entropy -1287.806640625, Learning Rate: 0.00625\n",
      "Epoch [2410/20000], Loss: 4156.97802734375, Entropy -1352.154296875, Learning Rate: 0.00625\n",
      "Epoch [2411/20000], Loss: 4211.59765625, Entropy -1402.776611328125, Learning Rate: 0.00625\n",
      "Epoch [2412/20000], Loss: 4137.0751953125, Entropy -1375.27099609375, Learning Rate: 0.00625\n",
      "Epoch [2413/20000], Loss: 4127.5029296875, Entropy -1350.57275390625, Learning Rate: 0.00625\n",
      "Epoch [2414/20000], Loss: 4097.8037109375, Entropy -1253.77587890625, Learning Rate: 0.00625\n",
      "Epoch [2415/20000], Loss: 4112.20166015625, Entropy -1154.08837890625, Learning Rate: 0.00625\n",
      "Epoch [2416/20000], Loss: 4161.552734375, Entropy -1386.72216796875, Learning Rate: 0.00625\n",
      "Epoch [2417/20000], Loss: 4162.1396484375, Entropy -1338.17236328125, Learning Rate: 0.00625\n",
      "Epoch [2418/20000], Loss: 4136.4375, Entropy -1378.9638671875, Learning Rate: 0.00625\n",
      "Epoch [2419/20000], Loss: 4198.98388671875, Entropy -1332.513671875, Learning Rate: 0.00625\n",
      "Epoch [2420/20000], Loss: 4122.197265625, Entropy -1255.7900390625, Learning Rate: 0.00625\n",
      "Epoch [2421/20000], Loss: 4096.76171875, Entropy -1254.36767578125, Learning Rate: 0.00625\n",
      "Epoch [2422/20000], Loss: 4116.44287109375, Entropy -1359.49951171875, Learning Rate: 0.00625\n",
      "Epoch [2423/20000], Loss: 4159.2060546875, Entropy -1297.806640625, Learning Rate: 0.00625\n",
      "Epoch [2424/20000], Loss: 4127.46240234375, Entropy -1270.73681640625, Learning Rate: 0.00625\n",
      "Epoch [2425/20000], Loss: 4200.79296875, Entropy -1322.07373046875, Learning Rate: 0.00625\n",
      "Epoch [2426/20000], Loss: 4092.68505859375, Entropy -1292.3798828125, Learning Rate: 0.00625\n",
      "Epoch [2427/20000], Loss: 4245.6083984375, Entropy -1309.17333984375, Learning Rate: 0.00625\n",
      "Epoch [2428/20000], Loss: 4192.94775390625, Entropy -1408.6806640625, Learning Rate: 0.00625\n",
      "Epoch [2429/20000], Loss: 4131.63671875, Entropy -1305.76171875, Learning Rate: 0.00625\n",
      "Epoch [2430/20000], Loss: 4157.14892578125, Entropy -1175.5419921875, Learning Rate: 0.00625\n",
      "Epoch [2431/20000], Loss: 4135.13671875, Entropy -1258.7314453125, Learning Rate: 0.00625\n",
      "Epoch [2432/20000], Loss: 4188.77734375, Entropy -1316.25537109375, Learning Rate: 0.00625\n",
      "Epoch [2433/20000], Loss: 4236.44287109375, Entropy -1364.36474609375, Learning Rate: 0.00625\n",
      "Epoch [2434/20000], Loss: 4169.4169921875, Entropy -1359.1552734375, Learning Rate: 0.00625\n",
      "Epoch [2435/20000], Loss: 4139.20263671875, Entropy -1266.6103515625, Learning Rate: 0.00625\n",
      "Epoch [2436/20000], Loss: 4101.3798828125, Entropy -1240.275390625, Learning Rate: 0.00625\n",
      "Epoch [2437/20000], Loss: 4146.23095703125, Entropy -1322.64453125, Learning Rate: 0.00625\n",
      "Epoch [2438/20000], Loss: 4215.5615234375, Entropy -1261.15380859375, Learning Rate: 0.00625\n",
      "Epoch [2439/20000], Loss: 4122.47021484375, Entropy -1248.34375, Learning Rate: 0.00625\n",
      "Epoch [2440/20000], Loss: 4142.53125, Entropy -1251.55517578125, Learning Rate: 0.00625\n",
      "Epoch [2441/20000], Loss: 4117.4248046875, Entropy -1259.47119140625, Learning Rate: 0.00625\n",
      "Epoch [2442/20000], Loss: 4127.69482421875, Entropy -1347.8388671875, Learning Rate: 0.00625\n",
      "Epoch [2443/20000], Loss: 4142.15087890625, Entropy -1266.3935546875, Learning Rate: 0.00625\n",
      "Epoch [2444/20000], Loss: 4135.67919921875, Entropy -1235.21875, Learning Rate: 0.00625\n",
      "Epoch [2445/20000], Loss: 4135.1484375, Entropy -1357.94677734375, Learning Rate: 0.00625\n",
      "Epoch [2446/20000], Loss: 4101.13232421875, Entropy -1335.85205078125, Learning Rate: 0.00625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2447/20000], Loss: 4121.49560546875, Entropy -1272.15869140625, Learning Rate: 0.00625\n",
      "Epoch [2448/20000], Loss: 4170.11474609375, Entropy -1202.14453125, Learning Rate: 0.00625\n",
      "Epoch [2449/20000], Loss: 4147.9482421875, Entropy -1395.154296875, Learning Rate: 0.00625\n",
      "Epoch [2450/20000], Loss: 4116.6435546875, Entropy -1294.6123046875, Learning Rate: 0.00625\n",
      "Epoch [2451/20000], Loss: 4126.22119140625, Entropy -1150.78173828125, Learning Rate: 0.00625\n",
      "Epoch [2452/20000], Loss: 4238.798828125, Entropy -1301.66650390625, Learning Rate: 0.00625\n",
      "Epoch [2453/20000], Loss: 4151.1279296875, Entropy -1350.2275390625, Learning Rate: 0.00625\n",
      "Epoch [2454/20000], Loss: 4129.166015625, Entropy -1305.04541015625, Learning Rate: 0.00625\n",
      "Epoch [2455/20000], Loss: 4170.78271484375, Entropy -1364.21630859375, Learning Rate: 0.00625\n",
      "Epoch [2456/20000], Loss: 4146.033203125, Entropy -1335.58447265625, Learning Rate: 0.00625\n",
      "Epoch [2457/20000], Loss: 4142.96435546875, Entropy -1205.423828125, Learning Rate: 0.00625\n",
      "Epoch [2458/20000], Loss: 4188.84375, Entropy -1373.23486328125, Learning Rate: 0.00625\n",
      "Epoch [2459/20000], Loss: 4162.04150390625, Entropy -1295.37744140625, Learning Rate: 0.00625\n",
      "Epoch [2460/20000], Loss: 4099.53564453125, Entropy -1229.80126953125, Learning Rate: 0.00625\n",
      "Epoch [2461/20000], Loss: 4195.4267578125, Entropy -1265.095703125, Learning Rate: 0.00625\n",
      "Epoch [2462/20000], Loss: 4122.21435546875, Entropy -1305.51025390625, Learning Rate: 0.00625\n",
      "Epoch [2463/20000], Loss: 4113.2197265625, Entropy -1271.17529296875, Learning Rate: 0.00625\n",
      "Epoch [2464/20000], Loss: 4179.62890625, Entropy -1345.57763671875, Learning Rate: 0.00625\n",
      "Epoch [2465/20000], Loss: 4112.3154296875, Entropy -1337.8212890625, Learning Rate: 0.00625\n",
      "Epoch [2466/20000], Loss: 4071.026611328125, Entropy -1207.888671875, Learning Rate: 0.00625\n",
      "Epoch [2467/20000], Loss: 4133.9697265625, Entropy -1228.16259765625, Learning Rate: 0.00625\n",
      "Epoch [2468/20000], Loss: 4126.173828125, Entropy -1276.18798828125, Learning Rate: 0.00625\n",
      "Epoch [2469/20000], Loss: 4121.84326171875, Entropy -1324.5986328125, Learning Rate: 0.00625\n",
      "Epoch [2470/20000], Loss: 4133.70947265625, Entropy -1251.55224609375, Learning Rate: 0.00625\n",
      "Epoch [2471/20000], Loss: 4177.140625, Entropy -1292.41455078125, Learning Rate: 0.00625\n",
      "Epoch [2472/20000], Loss: 4133.27294921875, Entropy -1346.07763671875, Learning Rate: 0.00625\n",
      "Epoch [2473/20000], Loss: 4187.9326171875, Entropy -1448.37939453125, Learning Rate: 0.00625\n",
      "Epoch [2474/20000], Loss: 4144.57470703125, Entropy -1250.853515625, Learning Rate: 0.00625\n",
      "Epoch [2475/20000], Loss: 4192.67236328125, Entropy -1347.1826171875, Learning Rate: 0.00625\n",
      "Epoch [2476/20000], Loss: 4097.271484375, Entropy -1248.43505859375, Learning Rate: 0.00625\n",
      "Epoch [2477/20000], Loss: 4139.33203125, Entropy -1255.70703125, Learning Rate: 0.00625\n",
      "Epoch [2478/20000], Loss: 4133.0419921875, Entropy -1296.4296875, Learning Rate: 0.00625\n",
      "Epoch [2479/20000], Loss: 4141.134765625, Entropy -1332.92041015625, Learning Rate: 0.00625\n",
      "Epoch [2480/20000], Loss: 4098.03662109375, Entropy -1180.87890625, Learning Rate: 0.00625\n",
      "Epoch [2481/20000], Loss: 4166.298828125, Entropy -1237.48291015625, Learning Rate: 0.00625\n",
      "Epoch [2482/20000], Loss: 4170.150390625, Entropy -1360.59765625, Learning Rate: 0.00625\n",
      "Epoch [2483/20000], Loss: 4159.04638671875, Entropy -1291.4248046875, Learning Rate: 0.00625\n",
      "Epoch [2484/20000], Loss: 4180.81005859375, Entropy -1290.9033203125, Learning Rate: 0.00625\n",
      "Epoch [2485/20000], Loss: 4125.962890625, Entropy -1180.94482421875, Learning Rate: 0.00625\n",
      "Epoch [2486/20000], Loss: 4151.4404296875, Entropy -1254.6484375, Learning Rate: 0.00625\n",
      "Epoch [2487/20000], Loss: 4084.552490234375, Entropy -1266.91162109375, Learning Rate: 0.00625\n",
      "Epoch [2488/20000], Loss: 4143.22705078125, Entropy -1304.3544921875, Learning Rate: 0.00625\n",
      "Epoch [2489/20000], Loss: 4121.06103515625, Entropy -1310.662109375, Learning Rate: 0.00625\n",
      "Epoch [2490/20000], Loss: 4135.9013671875, Entropy -1358.87353515625, Learning Rate: 0.00625\n",
      "Epoch [2491/20000], Loss: 4183.05859375, Entropy -1411.1298828125, Learning Rate: 0.00625\n",
      "Epoch [2492/20000], Loss: 4163.74462890625, Entropy -1355.18408203125, Learning Rate: 0.00625\n",
      "Epoch [2493/20000], Loss: 4166.2109375, Entropy -1356.3134765625, Learning Rate: 0.00625\n",
      "Epoch [2494/20000], Loss: 4096.4775390625, Entropy -1266.82080078125, Learning Rate: 0.00625\n",
      "Epoch [2495/20000], Loss: 4115.498046875, Entropy -1197.97802734375, Learning Rate: 0.00625\n",
      "Epoch [2496/20000], Loss: 4109.46484375, Entropy -1374.3134765625, Learning Rate: 0.00625\n",
      "Epoch [2497/20000], Loss: 4162.720703125, Entropy -1211.16259765625, Learning Rate: 0.00625\n",
      "Epoch [2498/20000], Loss: 4106.8466796875, Entropy -1219.26025390625, Learning Rate: 0.00625\n",
      "Epoch [2499/20000], Loss: 4153.263671875, Entropy -1249.763671875, Learning Rate: 0.00625\n",
      "Epoch [2500/20000], Loss: 4161.39794921875, Entropy -1239.38818359375, Learning Rate: 0.00625\n",
      "Epoch [2501/20000], Loss: 4121.955078125, Entropy -1267.42333984375, Learning Rate: 0.00625\n",
      "Epoch [2502/20000], Loss: 4135.94677734375, Entropy -1188.72900390625, Learning Rate: 0.00625\n",
      "Epoch [2503/20000], Loss: 4129.47216796875, Entropy -1281.15478515625, Learning Rate: 0.00625\n",
      "Epoch [2504/20000], Loss: 4157.7138671875, Entropy -1240.7119140625, Learning Rate: 0.00625\n",
      "Epoch [2505/20000], Loss: 4152.36328125, Entropy -1257.85595703125, Learning Rate: 0.00625\n",
      "Epoch [2506/20000], Loss: 4127.55126953125, Entropy -1242.31298828125, Learning Rate: 0.00625\n",
      "Epoch [2507/20000], Loss: 4120.17431640625, Entropy -1254.43408203125, Learning Rate: 0.00625\n",
      "Epoch [2508/20000], Loss: 4155.09912109375, Entropy -1189.025390625, Learning Rate: 0.00625\n",
      "Epoch [2509/20000], Loss: 4209.6044921875, Entropy -1301.3076171875, Learning Rate: 0.00625\n",
      "Epoch [2510/20000], Loss: 4135.70458984375, Entropy -1333.744140625, Learning Rate: 0.00625\n",
      "Epoch [2511/20000], Loss: 4158.98388671875, Entropy -1333.64990234375, Learning Rate: 0.00625\n",
      "Epoch [2512/20000], Loss: 4098.5732421875, Entropy -1207.00927734375, Learning Rate: 0.00625\n",
      "Epoch [2513/20000], Loss: 4103.6015625, Entropy -1247.76318359375, Learning Rate: 0.00625\n",
      "Epoch [2514/20000], Loss: 4098.943359375, Entropy -1310.0869140625, Learning Rate: 0.00625\n",
      "Epoch [2515/20000], Loss: 4116.05322265625, Entropy -1262.32373046875, Learning Rate: 0.00625\n",
      "Epoch [2516/20000], Loss: 4248.62451171875, Entropy -1433.805419921875, Learning Rate: 0.00625\n",
      "Epoch [2517/20000], Loss: 4160.107421875, Entropy -1353.0146484375, Learning Rate: 0.00625\n",
      "Epoch [2518/20000], Loss: 4184.39111328125, Entropy -1366.40380859375, Learning Rate: 0.00625\n",
      "Epoch [2519/20000], Loss: 4108.37255859375, Entropy -1237.51611328125, Learning Rate: 0.00625\n",
      "Epoch [2520/20000], Loss: 4134.8642578125, Entropy -1385.9775390625, Learning Rate: 0.00625\n",
      "Epoch [2521/20000], Loss: 4132.46923828125, Entropy -1198.35693359375, Learning Rate: 0.00625\n",
      "Epoch [2522/20000], Loss: 4154.88671875, Entropy -1281.65771484375, Learning Rate: 0.00625\n",
      "Epoch [2523/20000], Loss: 4179.640625, Entropy -1329.66064453125, Learning Rate: 0.00625\n",
      "Epoch [2524/20000], Loss: 4128.75927734375, Entropy -1370.09033203125, Learning Rate: 0.00625\n",
      "Epoch [2525/20000], Loss: 4185.9375, Entropy -1161.8798828125, Learning Rate: 0.00625\n",
      "Epoch [2526/20000], Loss: 4124.283203125, Entropy -1333.09814453125, Learning Rate: 0.00625\n",
      "Epoch [2527/20000], Loss: 4168.35498046875, Entropy -1353.81591796875, Learning Rate: 0.00625\n",
      "Epoch [2528/20000], Loss: 4215.14501953125, Entropy -1373.34912109375, Learning Rate: 0.00625\n",
      "Epoch [2529/20000], Loss: 4059.10791015625, Entropy -1257.513671875, Learning Rate: 0.00625\n",
      "Epoch [2530/20000], Loss: 4168.072265625, Entropy -1366.54541015625, Learning Rate: 0.00625\n",
      "Epoch [2531/20000], Loss: 4131.67041015625, Entropy -1298.5166015625, Learning Rate: 0.00625\n",
      "Epoch [2532/20000], Loss: 4156.56103515625, Entropy -1401.538818359375, Learning Rate: 0.00625\n",
      "Epoch [2533/20000], Loss: 4085.735107421875, Entropy -1265.62060546875, Learning Rate: 0.00625\n",
      "Epoch [2534/20000], Loss: 4143.3291015625, Entropy -1351.83984375, Learning Rate: 0.00625\n",
      "Epoch [2535/20000], Loss: 4114.3330078125, Entropy -1295.568359375, Learning Rate: 0.00625\n",
      "Epoch [2536/20000], Loss: 4169.31494140625, Entropy -1333.38720703125, Learning Rate: 0.00625\n",
      "Epoch [2537/20000], Loss: 4174.65380859375, Entropy -1371.56201171875, Learning Rate: 0.00625\n",
      "Epoch [2538/20000], Loss: 4172.09033203125, Entropy -1417.626953125, Learning Rate: 0.00625\n",
      "Epoch [2539/20000], Loss: 4105.14892578125, Entropy -1263.7421875, Learning Rate: 0.00625\n",
      "Epoch [2540/20000], Loss: 4153.2509765625, Entropy -1319.3291015625, Learning Rate: 0.00625\n",
      "Epoch [2541/20000], Loss: 4158.8681640625, Entropy -1308.03173828125, Learning Rate: 0.00625\n",
      "Epoch [2542/20000], Loss: 4106.98974609375, Entropy -1251.76123046875, Learning Rate: 0.00625\n",
      "Epoch [2543/20000], Loss: 4125.2275390625, Entropy -1323.22900390625, Learning Rate: 0.00625\n",
      "Epoch [2544/20000], Loss: 4118.2431640625, Entropy -1208.78173828125, Learning Rate: 0.00625\n",
      "Epoch [2545/20000], Loss: 4205.51025390625, Entropy -1450.87646484375, Learning Rate: 0.00625\n",
      "Epoch [2546/20000], Loss: 4146.47998046875, Entropy -1261.27294921875, Learning Rate: 0.00625\n",
      "Epoch [2547/20000], Loss: 4154.82470703125, Entropy -1229.4892578125, Learning Rate: 0.00625\n",
      "Epoch [2548/20000], Loss: 4163.32568359375, Entropy -1253.1953125, Learning Rate: 0.00625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2549/20000], Loss: 4114.2998046875, Entropy -1324.83349609375, Learning Rate: 0.00625\n",
      "Epoch [2550/20000], Loss: 4169.1103515625, Entropy -1401.40234375, Learning Rate: 0.00625\n",
      "Epoch [2551/20000], Loss: 4138.25, Entropy -1322.19580078125, Learning Rate: 0.00625\n",
      "Epoch [2552/20000], Loss: 4132.3212890625, Entropy -1315.48974609375, Learning Rate: 0.00625\n",
      "Epoch [2553/20000], Loss: 4132.5361328125, Entropy -1326.71923828125, Learning Rate: 0.00625\n",
      "Epoch [2554/20000], Loss: 4127.83447265625, Entropy -1222.072265625, Learning Rate: 0.00625\n",
      "Epoch [2555/20000], Loss: 4121.6484375, Entropy -1233.64013671875, Learning Rate: 0.00625\n",
      "Epoch [2556/20000], Loss: 4147.74853515625, Entropy -1319.66748046875, Learning Rate: 0.00625\n",
      "Epoch [2557/20000], Loss: 4114.54541015625, Entropy -1265.22216796875, Learning Rate: 0.00625\n",
      "Epoch [2558/20000], Loss: 4076.65576171875, Entropy -1247.2626953125, Learning Rate: 0.00625\n",
      "Epoch [2559/20000], Loss: 4080.716064453125, Entropy -1210.2724609375, Learning Rate: 0.00625\n",
      "Epoch [2560/20000], Loss: 4132.255859375, Entropy -1264.1259765625, Learning Rate: 0.00625\n",
      "Epoch [2561/20000], Loss: 4060.390869140625, Entropy -1197.91650390625, Learning Rate: 0.00625\n",
      "Epoch [2562/20000], Loss: 4108.35009765625, Entropy -1215.451171875, Learning Rate: 0.00625\n",
      "Epoch [2563/20000], Loss: 4147.5634765625, Entropy -1338.13720703125, Learning Rate: 0.00625\n",
      "Epoch [2564/20000], Loss: 4145.99072265625, Entropy -1385.00244140625, Learning Rate: 0.00625\n",
      "Epoch [2565/20000], Loss: 4116.0458984375, Entropy -1335.7958984375, Learning Rate: 0.00625\n",
      "Epoch [2566/20000], Loss: 4081.96630859375, Entropy -1172.69189453125, Learning Rate: 0.00625\n",
      "Epoch [2567/20000], Loss: 4155.9326171875, Entropy -1366.9775390625, Learning Rate: 0.00625\n",
      "Epoch [2568/20000], Loss: 4096.6044921875, Entropy -1282.17919921875, Learning Rate: 0.00625\n",
      "Epoch [2569/20000], Loss: 4036.41357421875, Entropy -1225.49462890625, Learning Rate: 0.00625\n",
      "Epoch [2570/20000], Loss: 4146.4912109375, Entropy -1297.93212890625, Learning Rate: 0.00625\n",
      "Epoch [2571/20000], Loss: 4121.84765625, Entropy -1325.88037109375, Learning Rate: 0.00625\n",
      "Epoch [2572/20000], Loss: 4180.5068359375, Entropy -1310.4189453125, Learning Rate: 0.00625\n",
      "Epoch [2573/20000], Loss: 4164.64697265625, Entropy -1281.4619140625, Learning Rate: 0.00625\n",
      "Epoch [2574/20000], Loss: 4098.68359375, Entropy -1319.24853515625, Learning Rate: 0.00625\n",
      "Epoch [2575/20000], Loss: 4112.013671875, Entropy -1340.16357421875, Learning Rate: 0.00625\n",
      "Epoch [2576/20000], Loss: 4083.422607421875, Entropy -1260.16796875, Learning Rate: 0.00625\n",
      "Epoch [2577/20000], Loss: 4095.815185546875, Entropy -1282.25927734375, Learning Rate: 0.00625\n",
      "Epoch [2578/20000], Loss: 4115.80029296875, Entropy -1310.77099609375, Learning Rate: 0.003125\n",
      "Epoch [2579/20000], Loss: 4217.47314453125, Entropy -1272.876953125, Learning Rate: 0.003125\n",
      "Epoch [2580/20000], Loss: 4142.212890625, Entropy -1175.85107421875, Learning Rate: 0.003125\n",
      "Epoch [2581/20000], Loss: 4142.7568359375, Entropy -1224.48583984375, Learning Rate: 0.003125\n",
      "Epoch [2582/20000], Loss: 4123.853515625, Entropy -1235.30224609375, Learning Rate: 0.003125\n",
      "Epoch [2583/20000], Loss: 4115.49365234375, Entropy -1186.9423828125, Learning Rate: 0.003125\n",
      "Epoch [2584/20000], Loss: 4119.669921875, Entropy -1360.052734375, Learning Rate: 0.003125\n",
      "Epoch [2585/20000], Loss: 4171.67041015625, Entropy -1330.65576171875, Learning Rate: 0.003125\n",
      "Epoch [2586/20000], Loss: 4117.42724609375, Entropy -1323.40576171875, Learning Rate: 0.003125\n",
      "Epoch [2587/20000], Loss: 4163.912109375, Entropy -1196.61669921875, Learning Rate: 0.003125\n",
      "Epoch [2588/20000], Loss: 4185.54052734375, Entropy -1226.63134765625, Learning Rate: 0.003125\n",
      "Epoch [2589/20000], Loss: 4110.09619140625, Entropy -1306.0830078125, Learning Rate: 0.003125\n",
      "Epoch [2590/20000], Loss: 4076.90966796875, Entropy -1171.4873046875, Learning Rate: 0.003125\n",
      "Epoch [2591/20000], Loss: 4142.970703125, Entropy -1294.97119140625, Learning Rate: 0.003125\n",
      "Epoch [2592/20000], Loss: 4134.99267578125, Entropy -1332.51318359375, Learning Rate: 0.003125\n",
      "Epoch [2593/20000], Loss: 4103.0029296875, Entropy -1226.205078125, Learning Rate: 0.003125\n",
      "Epoch [2594/20000], Loss: 4127.765625, Entropy -1248.30908203125, Learning Rate: 0.003125\n",
      "Epoch [2595/20000], Loss: 4136.478515625, Entropy -1336.52197265625, Learning Rate: 0.003125\n",
      "Epoch [2596/20000], Loss: 4132.76220703125, Entropy -1226.43408203125, Learning Rate: 0.003125\n",
      "Epoch [2597/20000], Loss: 4123.19775390625, Entropy -1377.88427734375, Learning Rate: 0.003125\n",
      "Epoch [2598/20000], Loss: 4143.77685546875, Entropy -1316.36474609375, Learning Rate: 0.003125\n",
      "Epoch [2599/20000], Loss: 4199.33251953125, Entropy -1351.2001953125, Learning Rate: 0.003125\n",
      "Epoch [2600/20000], Loss: 4129.39697265625, Entropy -1273.49658203125, Learning Rate: 0.003125\n",
      "Epoch [2601/20000], Loss: 4118.02392578125, Entropy -1281.099609375, Learning Rate: 0.003125\n",
      "Epoch [2602/20000], Loss: 4073.4111328125, Entropy -1194.345703125, Learning Rate: 0.003125\n",
      "Epoch [2603/20000], Loss: 4144.78369140625, Entropy -1328.3466796875, Learning Rate: 0.003125\n",
      "Epoch [2604/20000], Loss: 4112.986328125, Entropy -1227.54052734375, Learning Rate: 0.003125\n",
      "Epoch [2605/20000], Loss: 4121.64990234375, Entropy -1285.0283203125, Learning Rate: 0.003125\n",
      "Epoch [2606/20000], Loss: 4116.435546875, Entropy -1253.078125, Learning Rate: 0.003125\n",
      "Epoch [2607/20000], Loss: 4101.78466796875, Entropy -1239.599609375, Learning Rate: 0.003125\n",
      "Epoch [2608/20000], Loss: 4168.88623046875, Entropy -1200.70263671875, Learning Rate: 0.003125\n",
      "Epoch [2609/20000], Loss: 4123.6513671875, Entropy -1165.10498046875, Learning Rate: 0.003125\n",
      "Epoch [2610/20000], Loss: 4106.42041015625, Entropy -1198.45556640625, Learning Rate: 0.003125\n",
      "Epoch [2611/20000], Loss: 4074.78271484375, Entropy -1204.91943359375, Learning Rate: 0.003125\n",
      "Epoch [2612/20000], Loss: 4228.3837890625, Entropy -1358.39501953125, Learning Rate: 0.003125\n",
      "Epoch [2613/20000], Loss: 4078.40869140625, Entropy -1248.50048828125, Learning Rate: 0.003125\n",
      "Epoch [2614/20000], Loss: 4086.650390625, Entropy -1292.16015625, Learning Rate: 0.003125\n",
      "Epoch [2615/20000], Loss: 4094.764404296875, Entropy -1199.31396484375, Learning Rate: 0.003125\n",
      "Epoch [2616/20000], Loss: 4084.92138671875, Entropy -1202.48193359375, Learning Rate: 0.003125\n",
      "Epoch [2617/20000], Loss: 4163.5830078125, Entropy -1315.91162109375, Learning Rate: 0.003125\n",
      "Epoch [2618/20000], Loss: 4116.302734375, Entropy -1186.15625, Learning Rate: 0.003125\n",
      "Epoch [2619/20000], Loss: 4157.8466796875, Entropy -1245.1171875, Learning Rate: 0.003125\n",
      "Epoch [2620/20000], Loss: 4118.470703125, Entropy -1217.75537109375, Learning Rate: 0.003125\n",
      "Epoch [2621/20000], Loss: 4182.3935546875, Entropy -1407.12109375, Learning Rate: 0.003125\n",
      "Epoch [2622/20000], Loss: 4114.88916015625, Entropy -1258.962890625, Learning Rate: 0.003125\n",
      "Epoch [2623/20000], Loss: 4151.71240234375, Entropy -1391.04638671875, Learning Rate: 0.003125\n",
      "Epoch [2624/20000], Loss: 4129.462890625, Entropy -1323.1630859375, Learning Rate: 0.003125\n",
      "Epoch [2625/20000], Loss: 4186.29345703125, Entropy -1334.75, Learning Rate: 0.003125\n",
      "Epoch [2626/20000], Loss: 4133.46826171875, Entropy -1136.26025390625, Learning Rate: 0.003125\n",
      "Epoch [2627/20000], Loss: 4111.23974609375, Entropy -1267.07861328125, Learning Rate: 0.003125\n",
      "Epoch [2628/20000], Loss: 4112.125, Entropy -1225.65673828125, Learning Rate: 0.003125\n",
      "Epoch [2629/20000], Loss: 4098.7802734375, Entropy -1337.0166015625, Learning Rate: 0.003125\n",
      "Epoch [2630/20000], Loss: 4113.36962890625, Entropy -1265.3447265625, Learning Rate: 0.003125\n",
      "Epoch [2631/20000], Loss: 4148.03662109375, Entropy -1312.69140625, Learning Rate: 0.003125\n",
      "Epoch [2632/20000], Loss: 4235.61767578125, Entropy -1356.8447265625, Learning Rate: 0.003125\n",
      "Epoch [2633/20000], Loss: 4108.90673828125, Entropy -1271.51025390625, Learning Rate: 0.003125\n",
      "Epoch [2634/20000], Loss: 4209.07958984375, Entropy -1316.83935546875, Learning Rate: 0.003125\n",
      "Epoch [2635/20000], Loss: 4116.22216796875, Entropy -1296.14111328125, Learning Rate: 0.003125\n",
      "Epoch [2636/20000], Loss: 4178.10302734375, Entropy -1333.94384765625, Learning Rate: 0.003125\n",
      "Epoch [2637/20000], Loss: 4116.52978515625, Entropy -1248.42041015625, Learning Rate: 0.003125\n",
      "Epoch [2638/20000], Loss: 4119.60400390625, Entropy -1351.783203125, Learning Rate: 0.003125\n",
      "Epoch [2639/20000], Loss: 4101.8388671875, Entropy -1281.52880859375, Learning Rate: 0.003125\n",
      "Epoch [2640/20000], Loss: 4143.5146484375, Entropy -1357.51904296875, Learning Rate: 0.003125\n",
      "Epoch [2641/20000], Loss: 4113.2841796875, Entropy -1258.4609375, Learning Rate: 0.003125\n",
      "Epoch [2642/20000], Loss: 4060.3388671875, Entropy -1202.787109375, Learning Rate: 0.003125\n",
      "Epoch [2643/20000], Loss: 4114.2626953125, Entropy -1177.216796875, Learning Rate: 0.003125\n",
      "Epoch [2644/20000], Loss: 4133.29248046875, Entropy -1240.69189453125, Learning Rate: 0.003125\n",
      "Epoch [2645/20000], Loss: 4109.330078125, Entropy -1301.5498046875, Learning Rate: 0.003125\n",
      "Epoch [2646/20000], Loss: 4130.318359375, Entropy -1227.279296875, Learning Rate: 0.003125\n",
      "Epoch [2647/20000], Loss: 4126.91796875, Entropy -1332.41748046875, Learning Rate: 0.003125\n",
      "Epoch [2648/20000], Loss: 4161.73388671875, Entropy -1376.53564453125, Learning Rate: 0.003125\n",
      "Epoch [2649/20000], Loss: 4093.24365234375, Entropy -1274.74169921875, Learning Rate: 0.003125\n",
      "Epoch [2650/20000], Loss: 4102.1904296875, Entropy -1196.62939453125, Learning Rate: 0.003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2651/20000], Loss: 4139.1318359375, Entropy -1349.9599609375, Learning Rate: 0.003125\n",
      "Epoch [2652/20000], Loss: 4090.745849609375, Entropy -1304.384765625, Learning Rate: 0.003125\n",
      "Epoch [2653/20000], Loss: 4130.7802734375, Entropy -1241.18310546875, Learning Rate: 0.003125\n",
      "Epoch [2654/20000], Loss: 4151.71728515625, Entropy -1348.18505859375, Learning Rate: 0.003125\n",
      "Epoch [2655/20000], Loss: 4149.380859375, Entropy -1319.61962890625, Learning Rate: 0.003125\n",
      "Epoch [2656/20000], Loss: 4123.32275390625, Entropy -1306.66552734375, Learning Rate: 0.003125\n",
      "Epoch [2657/20000], Loss: 4189.2509765625, Entropy -1424.228271484375, Learning Rate: 0.003125\n",
      "Epoch [2658/20000], Loss: 4207.3662109375, Entropy -1293.359375, Learning Rate: 0.003125\n",
      "Epoch [2659/20000], Loss: 4116.67578125, Entropy -1261.17333984375, Learning Rate: 0.003125\n",
      "Epoch [2660/20000], Loss: 4109.24951171875, Entropy -1259.62744140625, Learning Rate: 0.003125\n",
      "Epoch [2661/20000], Loss: 4137.65869140625, Entropy -1227.19873046875, Learning Rate: 0.003125\n",
      "Epoch [2662/20000], Loss: 4127.14111328125, Entropy -1296.02197265625, Learning Rate: 0.003125\n",
      "Epoch [2663/20000], Loss: 4146.181640625, Entropy -1241.1455078125, Learning Rate: 0.003125\n",
      "Epoch [2664/20000], Loss: 4151.38134765625, Entropy -1324.80419921875, Learning Rate: 0.003125\n",
      "Epoch [2665/20000], Loss: 4163.60791015625, Entropy -1265.0390625, Learning Rate: 0.003125\n",
      "Epoch [2666/20000], Loss: 4114.76220703125, Entropy -1317.53515625, Learning Rate: 0.003125\n",
      "Epoch [2667/20000], Loss: 4183.16845703125, Entropy -1257.83837890625, Learning Rate: 0.003125\n",
      "Epoch [2668/20000], Loss: 4115.13818359375, Entropy -1309.611328125, Learning Rate: 0.003125\n",
      "Epoch [2669/20000], Loss: 4183.5966796875, Entropy -1283.50244140625, Learning Rate: 0.003125\n",
      "Epoch [2670/20000], Loss: 4028.921630859375, Entropy -1149.30224609375, Learning Rate: 0.003125\n",
      "Epoch [2671/20000], Loss: 4130.0625, Entropy -1246.2685546875, Learning Rate: 0.003125\n",
      "Epoch [2672/20000], Loss: 4177.3076171875, Entropy -1317.37353515625, Learning Rate: 0.003125\n",
      "Epoch [2673/20000], Loss: 4107.93017578125, Entropy -1287.32275390625, Learning Rate: 0.003125\n",
      "Epoch [2674/20000], Loss: 4161.71533203125, Entropy -1201.7060546875, Learning Rate: 0.003125\n",
      "Epoch [2675/20000], Loss: 4178.77685546875, Entropy -1353.95068359375, Learning Rate: 0.003125\n",
      "Epoch [2676/20000], Loss: 4139.10302734375, Entropy -1376.0400390625, Learning Rate: 0.003125\n",
      "Epoch [2677/20000], Loss: 4135.54638671875, Entropy -1340.82275390625, Learning Rate: 0.003125\n",
      "Epoch [2678/20000], Loss: 4078.89892578125, Entropy -1211.607421875, Learning Rate: 0.003125\n",
      "Epoch [2679/20000], Loss: 4226.2021484375, Entropy -1391.00732421875, Learning Rate: 0.003125\n",
      "Epoch [2680/20000], Loss: 4098.8583984375, Entropy -1274.27783203125, Learning Rate: 0.003125\n",
      "Epoch [2681/20000], Loss: 4139.90380859375, Entropy -1278.783203125, Learning Rate: 0.003125\n",
      "Epoch [2682/20000], Loss: 4135.3505859375, Entropy -1229.95654296875, Learning Rate: 0.003125\n",
      "Epoch [2683/20000], Loss: 4119.35498046875, Entropy -1294.7607421875, Learning Rate: 0.003125\n",
      "Epoch [2684/20000], Loss: 4103.72265625, Entropy -1332.19287109375, Learning Rate: 0.003125\n",
      "Epoch [2685/20000], Loss: 4151.13134765625, Entropy -1376.27294921875, Learning Rate: 0.003125\n",
      "Epoch [2686/20000], Loss: 4083.52587890625, Entropy -1211.16650390625, Learning Rate: 0.003125\n",
      "Epoch [2687/20000], Loss: 4195.712890625, Entropy -1357.83837890625, Learning Rate: 0.003125\n",
      "Epoch [2688/20000], Loss: 4125.05615234375, Entropy -1209.81591796875, Learning Rate: 0.003125\n",
      "Epoch [2689/20000], Loss: 4148.24560546875, Entropy -1314.3359375, Learning Rate: 0.003125\n",
      "Epoch [2690/20000], Loss: 4161.68212890625, Entropy -1369.10205078125, Learning Rate: 0.003125\n",
      "Epoch [2691/20000], Loss: 4126.09375, Entropy -1212.642578125, Learning Rate: 0.003125\n",
      "Epoch [2692/20000], Loss: 4098.48876953125, Entropy -1259.8671875, Learning Rate: 0.003125\n",
      "Epoch [2693/20000], Loss: 4091.131591796875, Entropy -1261.13671875, Learning Rate: 0.003125\n",
      "Epoch [2694/20000], Loss: 4157.5576171875, Entropy -1322.5732421875, Learning Rate: 0.003125\n",
      "Epoch [2695/20000], Loss: 4143.88134765625, Entropy -1229.17919921875, Learning Rate: 0.003125\n",
      "Epoch [2696/20000], Loss: 4084.364013671875, Entropy -1174.96923828125, Learning Rate: 0.003125\n",
      "Epoch [2697/20000], Loss: 4184.08056640625, Entropy -1309.25, Learning Rate: 0.003125\n",
      "Epoch [2698/20000], Loss: 4175.23095703125, Entropy -1333.861328125, Learning Rate: 0.003125\n",
      "Epoch [2699/20000], Loss: 4171.01708984375, Entropy -1401.498779296875, Learning Rate: 0.003125\n",
      "Epoch [2700/20000], Loss: 4075.36767578125, Entropy -1173.2021484375, Learning Rate: 0.003125\n",
      "Epoch [2701/20000], Loss: 4134.7470703125, Entropy -1261.740234375, Learning Rate: 0.003125\n",
      "Epoch [2702/20000], Loss: 4147.2822265625, Entropy -1200.9853515625, Learning Rate: 0.003125\n",
      "Epoch [2703/20000], Loss: 4174.103515625, Entropy -1303.98583984375, Learning Rate: 0.003125\n",
      "Epoch [2704/20000], Loss: 4158.6552734375, Entropy -1376.171875, Learning Rate: 0.003125\n",
      "Epoch [2705/20000], Loss: 4158.5498046875, Entropy -1435.67333984375, Learning Rate: 0.003125\n",
      "Epoch [2706/20000], Loss: 4085.260498046875, Entropy -1283.25341796875, Learning Rate: 0.003125\n",
      "Epoch [2707/20000], Loss: 4132.9423828125, Entropy -1249.57080078125, Learning Rate: 0.003125\n",
      "Epoch [2708/20000], Loss: 4111.93603515625, Entropy -1222.72021484375, Learning Rate: 0.003125\n",
      "Epoch [2709/20000], Loss: 4176.6748046875, Entropy -1215.072265625, Learning Rate: 0.003125\n",
      "Epoch [2710/20000], Loss: 4157.34228515625, Entropy -1258.6259765625, Learning Rate: 0.003125\n",
      "Epoch [2711/20000], Loss: 4156.73388671875, Entropy -1317.015625, Learning Rate: 0.003125\n",
      "Epoch [2712/20000], Loss: 4090.87109375, Entropy -1216.7802734375, Learning Rate: 0.003125\n",
      "Epoch [2713/20000], Loss: 4097.16162109375, Entropy -1354.72216796875, Learning Rate: 0.003125\n",
      "Epoch [2714/20000], Loss: 4080.121337890625, Entropy -1231.931640625, Learning Rate: 0.003125\n",
      "Epoch [2715/20000], Loss: 4175.49951171875, Entropy -1171.71875, Learning Rate: 0.003125\n",
      "Epoch [2716/20000], Loss: 4121.01220703125, Entropy -1265.30615234375, Learning Rate: 0.003125\n",
      "Epoch [2717/20000], Loss: 4187.71875, Entropy -1361.7314453125, Learning Rate: 0.003125\n",
      "Epoch [2718/20000], Loss: 4179.236328125, Entropy -1391.962158203125, Learning Rate: 0.003125\n",
      "Epoch [2719/20000], Loss: 4167.7685546875, Entropy -1280.2099609375, Learning Rate: 0.003125\n",
      "Epoch [2720/20000], Loss: 4101.97412109375, Entropy -1361.8798828125, Learning Rate: 0.003125\n",
      "Epoch [2721/20000], Loss: 4142.39892578125, Entropy -1286.2939453125, Learning Rate: 0.003125\n",
      "Epoch [2722/20000], Loss: 4204.8505859375, Entropy -1284.2939453125, Learning Rate: 0.003125\n",
      "Epoch [2723/20000], Loss: 4082.22509765625, Entropy -1257.0068359375, Learning Rate: 0.003125\n",
      "Epoch [2724/20000], Loss: 4124.26513671875, Entropy -1348.64990234375, Learning Rate: 0.003125\n",
      "Epoch [2725/20000], Loss: 4195.216796875, Entropy -1309.55615234375, Learning Rate: 0.003125\n",
      "Epoch [2726/20000], Loss: 4136.80029296875, Entropy -1307.71630859375, Learning Rate: 0.003125\n",
      "Epoch [2727/20000], Loss: 4126.51904296875, Entropy -1233.0712890625, Learning Rate: 0.003125\n",
      "Epoch [2728/20000], Loss: 4156.9619140625, Entropy -1242.43505859375, Learning Rate: 0.003125\n",
      "Epoch [2729/20000], Loss: 4119.18798828125, Entropy -1329.494140625, Learning Rate: 0.003125\n",
      "Epoch [2730/20000], Loss: 4071.28759765625, Entropy -1260.99609375, Learning Rate: 0.003125\n",
      "Epoch [2731/20000], Loss: 4147.9033203125, Entropy -1220.021484375, Learning Rate: 0.003125\n",
      "Epoch [2732/20000], Loss: 4097.1982421875, Entropy -1209.83447265625, Learning Rate: 0.003125\n",
      "Epoch [2733/20000], Loss: 4144.78955078125, Entropy -1331.67236328125, Learning Rate: 0.003125\n",
      "Epoch [2734/20000], Loss: 4102.177734375, Entropy -1249.91845703125, Learning Rate: 0.003125\n",
      "Epoch [2735/20000], Loss: 4144.89306640625, Entropy -1116.37841796875, Learning Rate: 0.003125\n",
      "Epoch [2736/20000], Loss: 4151.91064453125, Entropy -1269.93310546875, Learning Rate: 0.003125\n",
      "Epoch [2737/20000], Loss: 4099.76171875, Entropy -1258.59326171875, Learning Rate: 0.003125\n",
      "Epoch [2738/20000], Loss: 4093.189453125, Entropy -1327.3681640625, Learning Rate: 0.003125\n",
      "Epoch [2739/20000], Loss: 4116.572265625, Entropy -1253.82861328125, Learning Rate: 0.003125\n",
      "Epoch [2740/20000], Loss: 4125.5078125, Entropy -1215.56298828125, Learning Rate: 0.003125\n",
      "Epoch [2741/20000], Loss: 4197.80859375, Entropy -1287.05322265625, Learning Rate: 0.003125\n",
      "Epoch [2742/20000], Loss: 4165.162109375, Entropy -1313.43017578125, Learning Rate: 0.003125\n",
      "Epoch [2743/20000], Loss: 4069.8837890625, Entropy -1238.43701171875, Learning Rate: 0.003125\n",
      "Epoch [2744/20000], Loss: 4115.451171875, Entropy -1255.14990234375, Learning Rate: 0.003125\n",
      "Epoch [2745/20000], Loss: 4167.322265625, Entropy -1327.056640625, Learning Rate: 0.003125\n",
      "Epoch [2746/20000], Loss: 4128.4619140625, Entropy -1371.904296875, Learning Rate: 0.003125\n",
      "Epoch [2747/20000], Loss: 4073.24462890625, Entropy -1280.37060546875, Learning Rate: 0.003125\n",
      "Epoch [2748/20000], Loss: 4089.1103515625, Entropy -1216.5078125, Learning Rate: 0.003125\n",
      "Epoch [2749/20000], Loss: 4158.52978515625, Entropy -1208.19287109375, Learning Rate: 0.003125\n",
      "Epoch [2750/20000], Loss: 4103.1357421875, Entropy -1287.3525390625, Learning Rate: 0.003125\n",
      "Epoch [2751/20000], Loss: 4086.342529296875, Entropy -1261.1904296875, Learning Rate: 0.003125\n",
      "Epoch [2752/20000], Loss: 4095.072265625, Entropy -1182.60009765625, Learning Rate: 0.003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2753/20000], Loss: 4196.0556640625, Entropy -1284.48486328125, Learning Rate: 0.003125\n",
      "Epoch [2754/20000], Loss: 4146.49072265625, Entropy -1325.111328125, Learning Rate: 0.003125\n",
      "Epoch [2755/20000], Loss: 4123.45849609375, Entropy -1298.4521484375, Learning Rate: 0.003125\n",
      "Epoch [2756/20000], Loss: 4125.27880859375, Entropy -1352.32861328125, Learning Rate: 0.003125\n",
      "Epoch [2757/20000], Loss: 4120.7607421875, Entropy -1199.85986328125, Learning Rate: 0.003125\n",
      "Epoch [2758/20000], Loss: 4132.8662109375, Entropy -1298.9228515625, Learning Rate: 0.003125\n",
      "Epoch [2759/20000], Loss: 4188.048828125, Entropy -1400.04541015625, Learning Rate: 0.003125\n",
      "Epoch [2760/20000], Loss: 4093.42333984375, Entropy -1171.74462890625, Learning Rate: 0.003125\n",
      "Epoch [2761/20000], Loss: 4112.83837890625, Entropy -1252.90380859375, Learning Rate: 0.003125\n",
      "Epoch [2762/20000], Loss: 4087.149169921875, Entropy -1263.82958984375, Learning Rate: 0.003125\n",
      "Epoch [2763/20000], Loss: 4144.021484375, Entropy -1290.10009765625, Learning Rate: 0.003125\n",
      "Epoch [2764/20000], Loss: 4125.169921875, Entropy -1349.56689453125, Learning Rate: 0.003125\n",
      "Epoch [2765/20000], Loss: 4086.816162109375, Entropy -1136.92626953125, Learning Rate: 0.003125\n",
      "Epoch [2766/20000], Loss: 4148.68310546875, Entropy -1378.85693359375, Learning Rate: 0.003125\n",
      "Epoch [2767/20000], Loss: 4112.8388671875, Entropy -1253.37060546875, Learning Rate: 0.003125\n",
      "Epoch [2768/20000], Loss: 4099.2744140625, Entropy -1311.607421875, Learning Rate: 0.003125\n",
      "Epoch [2769/20000], Loss: 4278.0205078125, Entropy -1251.69970703125, Learning Rate: 0.003125\n",
      "Epoch [2770/20000], Loss: 4131.37939453125, Entropy -1295.41552734375, Learning Rate: 0.003125\n",
      "Epoch [2771/20000], Loss: 4093.144775390625, Entropy -1307.44140625, Learning Rate: 0.003125\n",
      "Epoch [2772/20000], Loss: 4109.1728515625, Entropy -1239.560546875, Learning Rate: 0.003125\n",
      "Epoch [2773/20000], Loss: 4130.92041015625, Entropy -1248.35791015625, Learning Rate: 0.003125\n",
      "Epoch [2774/20000], Loss: 4090.558837890625, Entropy -1287.181640625, Learning Rate: 0.003125\n",
      "Epoch [2775/20000], Loss: 4076.608642578125, Entropy -1307.2841796875, Learning Rate: 0.003125\n",
      "Epoch [2776/20000], Loss: 4113.03857421875, Entropy -1285.556640625, Learning Rate: 0.003125\n",
      "Epoch [2777/20000], Loss: 4127.08642578125, Entropy -1349.64697265625, Learning Rate: 0.003125\n",
      "Epoch [2778/20000], Loss: 4107.9794921875, Entropy -1291.82861328125, Learning Rate: 0.003125\n",
      "Epoch [2779/20000], Loss: 4100.1435546875, Entropy -1219.8095703125, Learning Rate: 0.0015625\n",
      "Epoch [2780/20000], Loss: 4159.9267578125, Entropy -1258.2353515625, Learning Rate: 0.0015625\n",
      "Epoch [2781/20000], Loss: 4157.76025390625, Entropy -1350.962890625, Learning Rate: 0.0015625\n",
      "Epoch [2782/20000], Loss: 4177.7783203125, Entropy -1296.517578125, Learning Rate: 0.0015625\n",
      "Epoch [2783/20000], Loss: 4134.87744140625, Entropy -1364.03466796875, Learning Rate: 0.0015625\n",
      "Epoch [2784/20000], Loss: 4177.2373046875, Entropy -1434.814453125, Learning Rate: 0.0015625\n",
      "Epoch [2785/20000], Loss: 4207.00537109375, Entropy -1263.25732421875, Learning Rate: 0.0015625\n",
      "Epoch [2786/20000], Loss: 4133.38037109375, Entropy -1270.857421875, Learning Rate: 0.0015625\n",
      "Epoch [2787/20000], Loss: 4121.7509765625, Entropy -1314.00634765625, Learning Rate: 0.0015625\n",
      "Epoch [2788/20000], Loss: 4162.3076171875, Entropy -1387.79736328125, Learning Rate: 0.0015625\n",
      "Epoch [2789/20000], Loss: 4165.2314453125, Entropy -1222.59033203125, Learning Rate: 0.0015625\n",
      "Epoch [2790/20000], Loss: 4115.66796875, Entropy -1284.61083984375, Learning Rate: 0.0015625\n",
      "Epoch [2791/20000], Loss: 4092.7431640625, Entropy -1248.81884765625, Learning Rate: 0.0015625\n",
      "Epoch [2792/20000], Loss: 4061.5458984375, Entropy -1200.86376953125, Learning Rate: 0.0015625\n",
      "Epoch [2793/20000], Loss: 4122.8037109375, Entropy -1322.32421875, Learning Rate: 0.0015625\n",
      "Epoch [2794/20000], Loss: 4131.7978515625, Entropy -1298.0654296875, Learning Rate: 0.0015625\n",
      "Epoch [2795/20000], Loss: 4152.4345703125, Entropy -1183.6328125, Learning Rate: 0.0015625\n",
      "Epoch [2796/20000], Loss: 4127.1142578125, Entropy -1292.2607421875, Learning Rate: 0.0015625\n",
      "Epoch [2797/20000], Loss: 4156.41357421875, Entropy -1362.0234375, Learning Rate: 0.0015625\n",
      "Epoch [2798/20000], Loss: 4133.74560546875, Entropy -1333.33642578125, Learning Rate: 0.0015625\n",
      "Epoch [2799/20000], Loss: 4125.0810546875, Entropy -1242.9072265625, Learning Rate: 0.0015625\n",
      "Epoch [2800/20000], Loss: 4167.68359375, Entropy -1276.3935546875, Learning Rate: 0.0015625\n",
      "Epoch [2801/20000], Loss: 4143.13232421875, Entropy -1245.17431640625, Learning Rate: 0.0015625\n",
      "Epoch [2802/20000], Loss: 4153.46240234375, Entropy -1340.26611328125, Learning Rate: 0.0015625\n",
      "Epoch [2803/20000], Loss: 4113.29052734375, Entropy -1300.6787109375, Learning Rate: 0.0015625\n",
      "Epoch [2804/20000], Loss: 4096.208984375, Entropy -1227.02001953125, Learning Rate: 0.0015625\n",
      "Epoch [2805/20000], Loss: 4139.642578125, Entropy -1324.88818359375, Learning Rate: 0.0015625\n",
      "Epoch [2806/20000], Loss: 4095.11474609375, Entropy -1269.18603515625, Learning Rate: 0.0015625\n",
      "Epoch [2807/20000], Loss: 4168.390625, Entropy -1236.099609375, Learning Rate: 0.0015625\n",
      "Epoch [2808/20000], Loss: 4131.9775390625, Entropy -1281.0693359375, Learning Rate: 0.0015625\n",
      "Epoch [2809/20000], Loss: 4162.9365234375, Entropy -1344.2333984375, Learning Rate: 0.0015625\n",
      "Epoch [2810/20000], Loss: 4097.7939453125, Entropy -1318.43408203125, Learning Rate: 0.0015625\n",
      "Epoch [2811/20000], Loss: 4171.19482421875, Entropy -1297.33642578125, Learning Rate: 0.0015625\n",
      "Epoch [2812/20000], Loss: 4074.632568359375, Entropy -1234.80322265625, Learning Rate: 0.0015625\n",
      "Epoch [2813/20000], Loss: 4164.724609375, Entropy -1335.85205078125, Learning Rate: 0.0015625\n",
      "Epoch [2814/20000], Loss: 4105.0791015625, Entropy -1263.20703125, Learning Rate: 0.0015625\n",
      "Epoch [2815/20000], Loss: 4085.01708984375, Entropy -1227.89404296875, Learning Rate: 0.0015625\n",
      "Epoch [2816/20000], Loss: 4122.55859375, Entropy -1216.59375, Learning Rate: 0.0015625\n",
      "Epoch [2817/20000], Loss: 4095.0576171875, Entropy -1255.27880859375, Learning Rate: 0.0015625\n",
      "Epoch [2818/20000], Loss: 4079.8408203125, Entropy -1225.1826171875, Learning Rate: 0.0015625\n",
      "Epoch [2819/20000], Loss: 4084.8447265625, Entropy -1341.17236328125, Learning Rate: 0.0015625\n",
      "Epoch [2820/20000], Loss: 4076.1181640625, Entropy -1189.75341796875, Learning Rate: 0.0015625\n",
      "Epoch [2821/20000], Loss: 4083.88916015625, Entropy -1258.423828125, Learning Rate: 0.0015625\n",
      "Epoch [2822/20000], Loss: 4112.4248046875, Entropy -1299.2685546875, Learning Rate: 0.0015625\n",
      "Epoch [2823/20000], Loss: 4136.4365234375, Entropy -1290.52490234375, Learning Rate: 0.0015625\n",
      "Epoch [2824/20000], Loss: 4161.1318359375, Entropy -1322.0068359375, Learning Rate: 0.0015625\n",
      "Epoch [2825/20000], Loss: 4126.7783203125, Entropy -1371.09033203125, Learning Rate: 0.0015625\n",
      "Epoch [2826/20000], Loss: 4098.302734375, Entropy -1261.30419921875, Learning Rate: 0.0015625\n",
      "Epoch [2827/20000], Loss: 4155.19921875, Entropy -1291.3837890625, Learning Rate: 0.0015625\n",
      "Epoch [2828/20000], Loss: 4149.39453125, Entropy -1350.53076171875, Learning Rate: 0.0015625\n",
      "Epoch [2829/20000], Loss: 4096.4130859375, Entropy -1276.009765625, Learning Rate: 0.0015625\n",
      "Epoch [2830/20000], Loss: 4089.870361328125, Entropy -1289.22216796875, Learning Rate: 0.0015625\n",
      "Epoch [2831/20000], Loss: 4117.91259765625, Entropy -1254.91357421875, Learning Rate: 0.0015625\n",
      "Epoch [2832/20000], Loss: 4060.4560546875, Entropy -1192.81298828125, Learning Rate: 0.0015625\n",
      "Epoch [2833/20000], Loss: 4108.59423828125, Entropy -1215.74609375, Learning Rate: 0.0015625\n",
      "Epoch [2834/20000], Loss: 4060.640869140625, Entropy -1147.05712890625, Learning Rate: 0.0015625\n",
      "Epoch [2835/20000], Loss: 4170.361328125, Entropy -1332.9326171875, Learning Rate: 0.0015625\n",
      "Epoch [2836/20000], Loss: 4128.32080078125, Entropy -1273.75341796875, Learning Rate: 0.0015625\n",
      "Epoch [2837/20000], Loss: 4143.546875, Entropy -1211.52734375, Learning Rate: 0.0015625\n",
      "Epoch [2838/20000], Loss: 4087.117431640625, Entropy -1162.1728515625, Learning Rate: 0.0015625\n",
      "Epoch [2839/20000], Loss: 4117.94677734375, Entropy -1287.16357421875, Learning Rate: 0.0015625\n",
      "Epoch [2840/20000], Loss: 4099.7001953125, Entropy -1217.7548828125, Learning Rate: 0.0015625\n",
      "Epoch [2841/20000], Loss: 4142.8720703125, Entropy -1407.63916015625, Learning Rate: 0.0015625\n",
      "Epoch [2842/20000], Loss: 4131.58984375, Entropy -1261.552734375, Learning Rate: 0.0015625\n",
      "Epoch [2843/20000], Loss: 4114.24267578125, Entropy -1296.22802734375, Learning Rate: 0.0015625\n",
      "Epoch [2844/20000], Loss: 4130.26953125, Entropy -1326.63916015625, Learning Rate: 0.0015625\n",
      "Epoch [2845/20000], Loss: 4176.56103515625, Entropy -1257.3486328125, Learning Rate: 0.0015625\n",
      "Epoch [2846/20000], Loss: 4181.12158203125, Entropy -1241.79248046875, Learning Rate: 0.0015625\n",
      "Epoch [2847/20000], Loss: 4142.921875, Entropy -1271.70458984375, Learning Rate: 0.0015625\n",
      "Epoch [2848/20000], Loss: 4079.86474609375, Entropy -1249.80517578125, Learning Rate: 0.0015625\n",
      "Epoch [2849/20000], Loss: 4136.6767578125, Entropy -1317.2529296875, Learning Rate: 0.0015625\n",
      "Epoch [2850/20000], Loss: 4133.8408203125, Entropy -1274.4248046875, Learning Rate: 0.0015625\n",
      "Epoch [2851/20000], Loss: 4082.35546875, Entropy -1253.69482421875, Learning Rate: 0.0015625\n",
      "Epoch [2852/20000], Loss: 4149.76806640625, Entropy -1218.10400390625, Learning Rate: 0.0015625\n",
      "Epoch [2853/20000], Loss: 4137.1748046875, Entropy -1296.65087890625, Learning Rate: 0.0015625\n",
      "Epoch [2854/20000], Loss: 4148.546875, Entropy -1308.00634765625, Learning Rate: 0.0015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2855/20000], Loss: 4132.0361328125, Entropy -1242.08056640625, Learning Rate: 0.0015625\n",
      "Epoch [2856/20000], Loss: 4118.07568359375, Entropy -1330.77685546875, Learning Rate: 0.0015625\n",
      "Epoch [2857/20000], Loss: 4112.81005859375, Entropy -1313.458984375, Learning Rate: 0.0015625\n",
      "Epoch [2858/20000], Loss: 4034.923828125, Entropy -1236.55419921875, Learning Rate: 0.0015625\n",
      "Epoch [2859/20000], Loss: 4192.02392578125, Entropy -1312.04248046875, Learning Rate: 0.0015625\n",
      "Epoch [2860/20000], Loss: 4068.5107421875, Entropy -1229.28515625, Learning Rate: 0.0015625\n",
      "Epoch [2861/20000], Loss: 4165.80712890625, Entropy -1323.09423828125, Learning Rate: 0.0015625\n",
      "Epoch [2862/20000], Loss: 4189.2919921875, Entropy -1301.70068359375, Learning Rate: 0.0015625\n",
      "Epoch [2863/20000], Loss: 4113.2158203125, Entropy -1257.19677734375, Learning Rate: 0.0015625\n",
      "Epoch [2864/20000], Loss: 4103.7177734375, Entropy -1268.30029296875, Learning Rate: 0.0015625\n",
      "Epoch [2865/20000], Loss: 4120.23876953125, Entropy -1264.7939453125, Learning Rate: 0.0015625\n",
      "Epoch [2866/20000], Loss: 4097.21923828125, Entropy -1279.96435546875, Learning Rate: 0.0015625\n",
      "Epoch [2867/20000], Loss: 4083.831787109375, Entropy -1177.42138671875, Learning Rate: 0.0015625\n",
      "Epoch [2868/20000], Loss: 4176.46533203125, Entropy -1444.30029296875, Learning Rate: 0.0015625\n",
      "Epoch [2869/20000], Loss: 4115.97216796875, Entropy -1226.1376953125, Learning Rate: 0.0015625\n",
      "Epoch [2870/20000], Loss: 4106.57763671875, Entropy -1249.86376953125, Learning Rate: 0.0015625\n",
      "Epoch [2871/20000], Loss: 4195.4990234375, Entropy -1337.55712890625, Learning Rate: 0.0015625\n",
      "Epoch [2872/20000], Loss: 4071.21337890625, Entropy -1246.32763671875, Learning Rate: 0.0015625\n",
      "Epoch [2873/20000], Loss: 4123.89794921875, Entropy -1416.04541015625, Learning Rate: 0.0015625\n",
      "Epoch [2874/20000], Loss: 4157.3271484375, Entropy -1230.48681640625, Learning Rate: 0.0015625\n",
      "Epoch [2875/20000], Loss: 4119.50830078125, Entropy -1298.30615234375, Learning Rate: 0.0015625\n",
      "Epoch [2876/20000], Loss: 4150.59912109375, Entropy -1265.75634765625, Learning Rate: 0.0015625\n",
      "Epoch [2877/20000], Loss: 4147.4453125, Entropy -1261.99267578125, Learning Rate: 0.0015625\n",
      "Epoch [2878/20000], Loss: 4078.692138671875, Entropy -1297.82861328125, Learning Rate: 0.0015625\n",
      "Epoch [2879/20000], Loss: 4175.29052734375, Entropy -1259.32470703125, Learning Rate: 0.0015625\n",
      "Epoch [2880/20000], Loss: 4123.5849609375, Entropy -1323.12060546875, Learning Rate: 0.0015625\n",
      "Epoch [2881/20000], Loss: 4109.52490234375, Entropy -1182.1328125, Learning Rate: 0.0015625\n",
      "Epoch [2882/20000], Loss: 4160.17626953125, Entropy -1288.88037109375, Learning Rate: 0.0015625\n",
      "Epoch [2883/20000], Loss: 4142.89697265625, Entropy -1253.38623046875, Learning Rate: 0.0015625\n",
      "Epoch [2884/20000], Loss: 4086.731201171875, Entropy -1246.2392578125, Learning Rate: 0.0015625\n",
      "Epoch [2885/20000], Loss: 4139.62353515625, Entropy -1271.9677734375, Learning Rate: 0.0015625\n",
      "Epoch [2886/20000], Loss: 4137.15966796875, Entropy -1222.81640625, Learning Rate: 0.0015625\n",
      "Epoch [2887/20000], Loss: 4125.3857421875, Entropy -1319.91845703125, Learning Rate: 0.0015625\n",
      "Epoch [2888/20000], Loss: 4121.61181640625, Entropy -1260.45849609375, Learning Rate: 0.0015625\n",
      "Epoch [2889/20000], Loss: 4128.04443359375, Entropy -1334.2255859375, Learning Rate: 0.0015625\n",
      "Epoch [2890/20000], Loss: 4113.8408203125, Entropy -1295.32568359375, Learning Rate: 0.0015625\n",
      "Epoch [2891/20000], Loss: 4099.86474609375, Entropy -1147.39013671875, Learning Rate: 0.0015625\n",
      "Epoch [2892/20000], Loss: 4074.806640625, Entropy -1237.7607421875, Learning Rate: 0.0015625\n",
      "Epoch [2893/20000], Loss: 4130.02587890625, Entropy -1292.814453125, Learning Rate: 0.0015625\n",
      "Epoch [2894/20000], Loss: 4085.789306640625, Entropy -1190.40380859375, Learning Rate: 0.0015625\n",
      "Epoch [2895/20000], Loss: 4123.826171875, Entropy -1423.018310546875, Learning Rate: 0.0015625\n",
      "Epoch [2896/20000], Loss: 4121.82470703125, Entropy -1211.76611328125, Learning Rate: 0.0015625\n",
      "Epoch [2897/20000], Loss: 4077.4677734375, Entropy -1233.1044921875, Learning Rate: 0.0015625\n",
      "Epoch [2898/20000], Loss: 4088.3427734375, Entropy -1176.1142578125, Learning Rate: 0.0015625\n",
      "Epoch [2899/20000], Loss: 4086.02490234375, Entropy -1317.6015625, Learning Rate: 0.0015625\n",
      "Epoch [2900/20000], Loss: 4111.734375, Entropy -1333.54541015625, Learning Rate: 0.0015625\n",
      "Epoch [2901/20000], Loss: 4079.709228515625, Entropy -1201.39892578125, Learning Rate: 0.0015625\n",
      "Epoch [2902/20000], Loss: 4136.9990234375, Entropy -1300.03955078125, Learning Rate: 0.0015625\n",
      "Epoch [2903/20000], Loss: 4109.7900390625, Entropy -1258.3603515625, Learning Rate: 0.0015625\n",
      "Epoch [2904/20000], Loss: 4203.677734375, Entropy -1383.85400390625, Learning Rate: 0.0015625\n",
      "Epoch [2905/20000], Loss: 4099.787109375, Entropy -1340.1552734375, Learning Rate: 0.0015625\n",
      "Epoch [2906/20000], Loss: 4220.2470703125, Entropy -1314.15185546875, Learning Rate: 0.0015625\n",
      "Epoch [2907/20000], Loss: 4081.314697265625, Entropy -1211.11083984375, Learning Rate: 0.0015625\n",
      "Epoch [2908/20000], Loss: 4176.30859375, Entropy -1355.33740234375, Learning Rate: 0.0015625\n",
      "Epoch [2909/20000], Loss: 4122.021484375, Entropy -1311.51123046875, Learning Rate: 0.0015625\n",
      "Epoch [2910/20000], Loss: 4175.59716796875, Entropy -1240.54345703125, Learning Rate: 0.0015625\n",
      "Epoch [2911/20000], Loss: 4112.2392578125, Entropy -1232.314453125, Learning Rate: 0.0015625\n",
      "Epoch [2912/20000], Loss: 4102.82080078125, Entropy -1233.59375, Learning Rate: 0.0015625\n",
      "Epoch [2913/20000], Loss: 4125.30810546875, Entropy -1224.61181640625, Learning Rate: 0.0015625\n",
      "Epoch [2914/20000], Loss: 4146.3701171875, Entropy -1327.53369140625, Learning Rate: 0.0015625\n",
      "Epoch [2915/20000], Loss: 4130.88525390625, Entropy -1303.267578125, Learning Rate: 0.0015625\n",
      "Epoch [2916/20000], Loss: 4085.833984375, Entropy -1263.11669921875, Learning Rate: 0.0015625\n",
      "Epoch [2917/20000], Loss: 4127.95263671875, Entropy -1260.15283203125, Learning Rate: 0.0015625\n",
      "Epoch [2918/20000], Loss: 4153.041015625, Entropy -1251.48095703125, Learning Rate: 0.0015625\n",
      "Epoch [2919/20000], Loss: 4158.52880859375, Entropy -1234.08740234375, Learning Rate: 0.0015625\n",
      "Epoch [2920/20000], Loss: 4078.462646484375, Entropy -1260.552734375, Learning Rate: 0.0015625\n",
      "Epoch [2921/20000], Loss: 4094.228271484375, Entropy -1210.26953125, Learning Rate: 0.0015625\n",
      "Epoch [2922/20000], Loss: 4153.3203125, Entropy -1336.0224609375, Learning Rate: 0.0015625\n",
      "Epoch [2923/20000], Loss: 4149.337890625, Entropy -1248.1240234375, Learning Rate: 0.0015625\n",
      "Epoch [2924/20000], Loss: 4127.3505859375, Entropy -1270.05810546875, Learning Rate: 0.0015625\n",
      "Epoch [2925/20000], Loss: 4138.7626953125, Entropy -1237.7109375, Learning Rate: 0.0015625\n",
      "Epoch [2926/20000], Loss: 4102.00390625, Entropy -1252.79052734375, Learning Rate: 0.0015625\n",
      "Epoch [2927/20000], Loss: 4120.61328125, Entropy -1270.31494140625, Learning Rate: 0.0015625\n",
      "Epoch [2928/20000], Loss: 4164.841796875, Entropy -1258.77880859375, Learning Rate: 0.0015625\n",
      "Epoch [2929/20000], Loss: 4188.43701171875, Entropy -1377.0283203125, Learning Rate: 0.0015625\n",
      "Epoch [2930/20000], Loss: 4178.3740234375, Entropy -1372.91748046875, Learning Rate: 0.0015625\n",
      "Epoch [2931/20000], Loss: 4116.55517578125, Entropy -1252.2802734375, Learning Rate: 0.0015625\n",
      "Epoch [2932/20000], Loss: 4130.2216796875, Entropy -1266.79150390625, Learning Rate: 0.0015625\n",
      "Epoch [2933/20000], Loss: 4128.47998046875, Entropy -1200.2734375, Learning Rate: 0.0015625\n",
      "Epoch [2934/20000], Loss: 4125.73291015625, Entropy -1309.66162109375, Learning Rate: 0.0015625\n",
      "Epoch [2935/20000], Loss: 4097.78759765625, Entropy -1327.349609375, Learning Rate: 0.0015625\n",
      "Epoch [2936/20000], Loss: 4207.580078125, Entropy -1290.39404296875, Learning Rate: 0.0015625\n",
      "Epoch [2937/20000], Loss: 4097.3662109375, Entropy -1248.06201171875, Learning Rate: 0.0015625\n",
      "Epoch [2938/20000], Loss: 4124.83740234375, Entropy -1357.8515625, Learning Rate: 0.0015625\n",
      "Epoch [2939/20000], Loss: 4152.7216796875, Entropy -1321.55419921875, Learning Rate: 0.0015625\n",
      "Epoch [2940/20000], Loss: 4084.010986328125, Entropy -1309.65185546875, Learning Rate: 0.0015625\n",
      "Epoch [2941/20000], Loss: 4060.822509765625, Entropy -1231.9599609375, Learning Rate: 0.0015625\n",
      "Epoch [2942/20000], Loss: 4105.080078125, Entropy -1185.76318359375, Learning Rate: 0.0015625\n",
      "Epoch [2943/20000], Loss: 4167.646484375, Entropy -1355.50146484375, Learning Rate: 0.0015625\n",
      "Epoch [2944/20000], Loss: 4075.787353515625, Entropy -1245.814453125, Learning Rate: 0.0015625\n",
      "Epoch [2945/20000], Loss: 4145.357421875, Entropy -1238.70849609375, Learning Rate: 0.0015625\n",
      "Epoch [2946/20000], Loss: 4163.21533203125, Entropy -1282.67919921875, Learning Rate: 0.0015625\n",
      "Epoch [2947/20000], Loss: 4136.34716796875, Entropy -1175.35546875, Learning Rate: 0.0015625\n",
      "Epoch [2948/20000], Loss: 4117.70556640625, Entropy -1264.8232421875, Learning Rate: 0.0015625\n",
      "Epoch [2949/20000], Loss: 4092.3134765625, Entropy -1178.85107421875, Learning Rate: 0.0015625\n",
      "Epoch [2950/20000], Loss: 4176.0537109375, Entropy -1406.28369140625, Learning Rate: 0.0015625\n",
      "Epoch [2951/20000], Loss: 4115.5, Entropy -1342.95849609375, Learning Rate: 0.0015625\n",
      "Epoch [2952/20000], Loss: 4143.6494140625, Entropy -1350.6240234375, Learning Rate: 0.0015625\n",
      "Epoch [2953/20000], Loss: 4092.08349609375, Entropy -1141.86767578125, Learning Rate: 0.0015625\n",
      "Epoch [2954/20000], Loss: 4072.37353515625, Entropy -1195.4072265625, Learning Rate: 0.0015625\n",
      "Epoch [2955/20000], Loss: 4102.26318359375, Entropy -1296.853515625, Learning Rate: 0.0015625\n",
      "Epoch [2956/20000], Loss: 4162.1298828125, Entropy -1329.16015625, Learning Rate: 0.0015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2957/20000], Loss: 4122.17333984375, Entropy -1270.77197265625, Learning Rate: 0.0015625\n",
      "Epoch [2958/20000], Loss: 4149.984375, Entropy -1307.97705078125, Learning Rate: 0.0015625\n",
      "Epoch [2959/20000], Loss: 4084.848388671875, Entropy -1259.65087890625, Learning Rate: 0.0015625\n",
      "Epoch [2960/20000], Loss: 4143.939453125, Entropy -1271.279296875, Learning Rate: 0.0015625\n",
      "Epoch [2961/20000], Loss: 4155.68505859375, Entropy -1368.70166015625, Learning Rate: 0.0015625\n",
      "Epoch [2962/20000], Loss: 4237.13427734375, Entropy -1419.832275390625, Learning Rate: 0.0015625\n",
      "Epoch [2963/20000], Loss: 4087.73388671875, Entropy -1334.84033203125, Learning Rate: 0.0015625\n",
      "Epoch [2964/20000], Loss: 4100.787109375, Entropy -1299.57275390625, Learning Rate: 0.0015625\n",
      "Epoch [2965/20000], Loss: 4104.65966796875, Entropy -1302.39306640625, Learning Rate: 0.0015625\n",
      "Epoch [2966/20000], Loss: 4184.962890625, Entropy -1313.05419921875, Learning Rate: 0.0015625\n",
      "Epoch [2967/20000], Loss: 4093.756103515625, Entropy -1318.01611328125, Learning Rate: 0.0015625\n",
      "Epoch [2968/20000], Loss: 4161.50537109375, Entropy -1277.4912109375, Learning Rate: 0.0015625\n",
      "Epoch [2969/20000], Loss: 4039.208740234375, Entropy -1133.01416015625, Learning Rate: 0.0015625\n",
      "Epoch [2970/20000], Loss: 4201.46142578125, Entropy -1168.24365234375, Learning Rate: 0.0015625\n",
      "Epoch [2971/20000], Loss: 4100.37646484375, Entropy -1218.33837890625, Learning Rate: 0.0015625\n",
      "Epoch [2972/20000], Loss: 4098.27587890625, Entropy -1279.46875, Learning Rate: 0.0015625\n",
      "Epoch [2973/20000], Loss: 4131.25, Entropy -1284.2548828125, Learning Rate: 0.0015625\n",
      "Epoch [2974/20000], Loss: 4205.52880859375, Entropy -1318.89599609375, Learning Rate: 0.0015625\n",
      "Epoch [2975/20000], Loss: 4129.681640625, Entropy -1294.58251953125, Learning Rate: 0.0015625\n",
      "Epoch [2976/20000], Loss: 4096.29931640625, Entropy -1203.4462890625, Learning Rate: 0.0015625\n",
      "Epoch [2977/20000], Loss: 4130.46435546875, Entropy -1261.31005859375, Learning Rate: 0.0015625\n",
      "Epoch [2978/20000], Loss: 4140.76806640625, Entropy -1262.4794921875, Learning Rate: 0.0015625\n",
      "Epoch [2979/20000], Loss: 4139.58251953125, Entropy -1367.2412109375, Learning Rate: 0.0015625\n",
      "Epoch [2980/20000], Loss: 4126.279296875, Entropy -1217.7431640625, Learning Rate: 0.00078125\n",
      "Epoch [2981/20000], Loss: 4102.86181640625, Entropy -1265.326171875, Learning Rate: 0.00078125\n",
      "Epoch [2982/20000], Loss: 4186.64990234375, Entropy -1340.123046875, Learning Rate: 0.00078125\n",
      "Epoch [2983/20000], Loss: 4170.3818359375, Entropy -1405.9130859375, Learning Rate: 0.00078125\n",
      "Epoch [2984/20000], Loss: 4136.1416015625, Entropy -1314.20654296875, Learning Rate: 0.00078125\n",
      "Epoch [2985/20000], Loss: 4113.427734375, Entropy -1293.26806640625, Learning Rate: 0.00078125\n",
      "Epoch [2986/20000], Loss: 4144.75244140625, Entropy -1311.59228515625, Learning Rate: 0.00078125\n",
      "Epoch [2987/20000], Loss: 4112.54931640625, Entropy -1200.482421875, Learning Rate: 0.00078125\n",
      "Epoch [2988/20000], Loss: 4116.0537109375, Entropy -1217.21875, Learning Rate: 0.00078125\n",
      "Epoch [2989/20000], Loss: 4124.92724609375, Entropy -1291.74853515625, Learning Rate: 0.00078125\n",
      "Epoch [2990/20000], Loss: 4118.1416015625, Entropy -1222.93701171875, Learning Rate: 0.00078125\n",
      "Epoch [2991/20000], Loss: 4111.36962890625, Entropy -1206.57080078125, Learning Rate: 0.00078125\n",
      "Epoch [2992/20000], Loss: 4158.12255859375, Entropy -1381.640625, Learning Rate: 0.00078125\n",
      "Epoch [2993/20000], Loss: 4226.34375, Entropy -1308.84228515625, Learning Rate: 0.00078125\n",
      "Epoch [2994/20000], Loss: 4183.6474609375, Entropy -1300.36279296875, Learning Rate: 0.00078125\n",
      "Epoch [2995/20000], Loss: 4085.086181640625, Entropy -1192.0478515625, Learning Rate: 0.00078125\n",
      "Epoch [2996/20000], Loss: 4134.59326171875, Entropy -1252.20263671875, Learning Rate: 0.00078125\n",
      "Epoch [2997/20000], Loss: 4143.09228515625, Entropy -1292.4736328125, Learning Rate: 0.00078125\n",
      "Epoch [2998/20000], Loss: 4080.178955078125, Entropy -1290.27685546875, Learning Rate: 0.00078125\n",
      "Epoch [2999/20000], Loss: 4095.6123046875, Entropy -1218.326171875, Learning Rate: 0.00078125\n",
      "Epoch [3000/20000], Loss: 4172.97607421875, Entropy -1191.47509765625, Learning Rate: 0.00078125\n",
      "Epoch [3001/20000], Loss: 4181.63916015625, Entropy -1274.00634765625, Learning Rate: 0.00078125\n",
      "Epoch [3002/20000], Loss: 4108.5048828125, Entropy -1236.66162109375, Learning Rate: 0.00078125\n",
      "Epoch [3003/20000], Loss: 4193.546875, Entropy -1275.39404296875, Learning Rate: 0.00078125\n",
      "Epoch [3004/20000], Loss: 4108.7001953125, Entropy -1269.50830078125, Learning Rate: 0.00078125\n",
      "Epoch [3005/20000], Loss: 4125.21142578125, Entropy -1268.734375, Learning Rate: 0.00078125\n",
      "Epoch [3006/20000], Loss: 4110.31396484375, Entropy -1344.59228515625, Learning Rate: 0.00078125\n",
      "Epoch [3007/20000], Loss: 4139.29296875, Entropy -1312.95458984375, Learning Rate: 0.00078125\n",
      "Epoch [3008/20000], Loss: 4103.25732421875, Entropy -1274.92333984375, Learning Rate: 0.00078125\n",
      "Epoch [3009/20000], Loss: 4140.56689453125, Entropy -1379.98583984375, Learning Rate: 0.00078125\n",
      "Epoch [3010/20000], Loss: 4096.6650390625, Entropy -1243.64013671875, Learning Rate: 0.00078125\n",
      "Epoch [3011/20000], Loss: 4052.45751953125, Entropy -1070.26220703125, Learning Rate: 0.00078125\n",
      "Epoch [3012/20000], Loss: 4199.1298828125, Entropy -1350.34912109375, Learning Rate: 0.00078125\n",
      "Epoch [3013/20000], Loss: 4060.775146484375, Entropy -1228.18701171875, Learning Rate: 0.00078125\n",
      "Epoch [3014/20000], Loss: 4140.68701171875, Entropy -1302.27587890625, Learning Rate: 0.00078125\n",
      "Epoch [3015/20000], Loss: 4141.54638671875, Entropy -1298.82861328125, Learning Rate: 0.00078125\n",
      "Epoch [3016/20000], Loss: 4135.49658203125, Entropy -1323.671875, Learning Rate: 0.00078125\n",
      "Epoch [3017/20000], Loss: 4108.7548828125, Entropy -1294.63427734375, Learning Rate: 0.00078125\n",
      "Epoch [3018/20000], Loss: 4125.64208984375, Entropy -1323.29345703125, Learning Rate: 0.00078125\n",
      "Epoch [3019/20000], Loss: 4090.408935546875, Entropy -1294.6513671875, Learning Rate: 0.00078125\n",
      "Epoch [3020/20000], Loss: 4128.65673828125, Entropy -1304.01025390625, Learning Rate: 0.00078125\n",
      "Epoch [3021/20000], Loss: 4176.3623046875, Entropy -1396.909912109375, Learning Rate: 0.00078125\n",
      "Epoch [3022/20000], Loss: 4112.72314453125, Entropy -1333.32666015625, Learning Rate: 0.00078125\n",
      "Epoch [3023/20000], Loss: 4194.2099609375, Entropy -1372.05224609375, Learning Rate: 0.00078125\n",
      "Epoch [3024/20000], Loss: 4142.72265625, Entropy -1353.27587890625, Learning Rate: 0.00078125\n",
      "Epoch [3025/20000], Loss: 4115.73388671875, Entropy -1275.16259765625, Learning Rate: 0.00078125\n",
      "Epoch [3026/20000], Loss: 4100.18994140625, Entropy -1283.4345703125, Learning Rate: 0.00078125\n",
      "Epoch [3027/20000], Loss: 4212.0126953125, Entropy -1334.81103515625, Learning Rate: 0.00078125\n",
      "Epoch [3028/20000], Loss: 4103.3115234375, Entropy -1322.22607421875, Learning Rate: 0.00078125\n",
      "Epoch [3029/20000], Loss: 4167.69580078125, Entropy -1360.80517578125, Learning Rate: 0.00078125\n",
      "Epoch [3030/20000], Loss: 4197.74951171875, Entropy -1357.23291015625, Learning Rate: 0.00078125\n",
      "Epoch [3031/20000], Loss: 4113.9658203125, Entropy -1180.53564453125, Learning Rate: 0.00078125\n",
      "Epoch [3032/20000], Loss: 4095.704345703125, Entropy -1320.77783203125, Learning Rate: 0.00078125\n",
      "Epoch [3033/20000], Loss: 4141.30029296875, Entropy -1337.6015625, Learning Rate: 0.00078125\n",
      "Epoch [3034/20000], Loss: 4130.14892578125, Entropy -1235.43310546875, Learning Rate: 0.00078125\n",
      "Epoch [3035/20000], Loss: 4136.248046875, Entropy -1277.95654296875, Learning Rate: 0.00078125\n",
      "Epoch [3036/20000], Loss: 4104.5546875, Entropy -1261.06591796875, Learning Rate: 0.00078125\n",
      "Epoch [3037/20000], Loss: 4281.22900390625, Entropy -1331.6669921875, Learning Rate: 0.00078125\n",
      "Epoch [3038/20000], Loss: 4067.798583984375, Entropy -1184.94873046875, Learning Rate: 0.00078125\n",
      "Epoch [3039/20000], Loss: 4074.52099609375, Entropy -1264.18115234375, Learning Rate: 0.00078125\n",
      "Epoch [3040/20000], Loss: 4122.18359375, Entropy -1319.109375, Learning Rate: 0.00078125\n",
      "Epoch [3041/20000], Loss: 4149.08740234375, Entropy -1380.49365234375, Learning Rate: 0.00078125\n",
      "Epoch [3042/20000], Loss: 4191.1787109375, Entropy -1367.115234375, Learning Rate: 0.00078125\n",
      "Epoch [3043/20000], Loss: 4142.16015625, Entropy -1304.0283203125, Learning Rate: 0.00078125\n",
      "Epoch [3044/20000], Loss: 4121.044921875, Entropy -1347.4970703125, Learning Rate: 0.00078125\n",
      "Epoch [3045/20000], Loss: 4122.615234375, Entropy -1267.19970703125, Learning Rate: 0.00078125\n",
      "Epoch [3046/20000], Loss: 4171.1181640625, Entropy -1337.40283203125, Learning Rate: 0.00078125\n",
      "Epoch [3047/20000], Loss: 4135.3017578125, Entropy -1376.61279296875, Learning Rate: 0.00078125\n",
      "Epoch [3048/20000], Loss: 4115.23681640625, Entropy -1281.2958984375, Learning Rate: 0.00078125\n",
      "Epoch [3049/20000], Loss: 4145.4033203125, Entropy -1376.14990234375, Learning Rate: 0.00078125\n",
      "Epoch [3050/20000], Loss: 4071.54345703125, Entropy -1231.08935546875, Learning Rate: 0.00078125\n",
      "Epoch [3051/20000], Loss: 4097.34033203125, Entropy -1262.076171875, Learning Rate: 0.00078125\n",
      "Epoch [3052/20000], Loss: 4100.9296875, Entropy -1239.533203125, Learning Rate: 0.00078125\n",
      "Epoch [3053/20000], Loss: 4122.64892578125, Entropy -1232.99560546875, Learning Rate: 0.00078125\n",
      "Epoch [3054/20000], Loss: 4155.30908203125, Entropy -1290.26611328125, Learning Rate: 0.00078125\n",
      "Epoch [3055/20000], Loss: 4069.142333984375, Entropy -1211.89697265625, Learning Rate: 0.00078125\n",
      "Epoch [3056/20000], Loss: 4131.20458984375, Entropy -1293.234375, Learning Rate: 0.00078125\n",
      "Epoch [3057/20000], Loss: 4110.3330078125, Entropy -1361.18017578125, Learning Rate: 0.00078125\n",
      "Epoch [3058/20000], Loss: 4127.435546875, Entropy -1387.70849609375, Learning Rate: 0.00078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3059/20000], Loss: 4115.6640625, Entropy -1254.74072265625, Learning Rate: 0.00078125\n",
      "Epoch [3060/20000], Loss: 4102.30224609375, Entropy -1221.65966796875, Learning Rate: 0.00078125\n",
      "Epoch [3061/20000], Loss: 4119.42236328125, Entropy -1320.50830078125, Learning Rate: 0.00078125\n",
      "Epoch [3062/20000], Loss: 4121.6396484375, Entropy -1235.6953125, Learning Rate: 0.00078125\n",
      "Epoch [3063/20000], Loss: 4089.31103515625, Entropy -1258.27294921875, Learning Rate: 0.00078125\n",
      "Epoch [3064/20000], Loss: 4089.916015625, Entropy -1238.52294921875, Learning Rate: 0.00078125\n",
      "Epoch [3065/20000], Loss: 4183.21240234375, Entropy -1225.1298828125, Learning Rate: 0.00078125\n",
      "Epoch [3066/20000], Loss: 4113.078125, Entropy -1375.07470703125, Learning Rate: 0.00078125\n",
      "Epoch [3067/20000], Loss: 4071.9482421875, Entropy -1141.35693359375, Learning Rate: 0.00078125\n",
      "Epoch [3068/20000], Loss: 4085.1865234375, Entropy -1177.46630859375, Learning Rate: 0.00078125\n",
      "Epoch [3069/20000], Loss: 4092.139404296875, Entropy -1298.732421875, Learning Rate: 0.00078125\n",
      "Epoch [3070/20000], Loss: 4170.72802734375, Entropy -1291.30859375, Learning Rate: 0.00078125\n",
      "Epoch [3071/20000], Loss: 4076.8046875, Entropy -1178.98974609375, Learning Rate: 0.00078125\n",
      "Epoch [3072/20000], Loss: 4052.605712890625, Entropy -1270.21484375, Learning Rate: 0.00078125\n",
      "Epoch [3073/20000], Loss: 4084.2275390625, Entropy -1310.9150390625, Learning Rate: 0.00078125\n",
      "Epoch [3074/20000], Loss: 4151.93359375, Entropy -1317.88818359375, Learning Rate: 0.00078125\n",
      "Epoch [3075/20000], Loss: 4136.65576171875, Entropy -1203.24462890625, Learning Rate: 0.00078125\n",
      "Epoch [3076/20000], Loss: 4072.1162109375, Entropy -1207.13037109375, Learning Rate: 0.00078125\n",
      "Epoch [3077/20000], Loss: 4092.0546875, Entropy -1283.92333984375, Learning Rate: 0.00078125\n",
      "Epoch [3078/20000], Loss: 4076.2802734375, Entropy -1228.5029296875, Learning Rate: 0.00078125\n",
      "Epoch [3079/20000], Loss: 4120.09521484375, Entropy -1289.30322265625, Learning Rate: 0.00078125\n",
      "Epoch [3080/20000], Loss: 4071.535888671875, Entropy -1279.2080078125, Learning Rate: 0.00078125\n",
      "Epoch [3081/20000], Loss: 4094.97900390625, Entropy -1290.69384765625, Learning Rate: 0.00078125\n",
      "Epoch [3082/20000], Loss: 4108.4091796875, Entropy -1318.07958984375, Learning Rate: 0.00078125\n",
      "Epoch [3083/20000], Loss: 4159.18896484375, Entropy -1283.9365234375, Learning Rate: 0.00078125\n",
      "Epoch [3084/20000], Loss: 4091.758056640625, Entropy -1299.8798828125, Learning Rate: 0.00078125\n",
      "Epoch [3085/20000], Loss: 4141.17626953125, Entropy -1297.6376953125, Learning Rate: 0.00078125\n",
      "Epoch [3086/20000], Loss: 4127.27392578125, Entropy -1282.47998046875, Learning Rate: 0.00078125\n",
      "Epoch [3087/20000], Loss: 4122.810546875, Entropy -1204.4521484375, Learning Rate: 0.00078125\n",
      "Epoch [3088/20000], Loss: 4122.18603515625, Entropy -1261.24462890625, Learning Rate: 0.00078125\n",
      "Epoch [3089/20000], Loss: 4163.7021484375, Entropy -1416.450927734375, Learning Rate: 0.00078125\n",
      "Epoch [3090/20000], Loss: 4221.39599609375, Entropy -1479.453125, Learning Rate: 0.00078125\n",
      "Epoch [3091/20000], Loss: 4130.94677734375, Entropy -1249.97216796875, Learning Rate: 0.00078125\n",
      "Epoch [3092/20000], Loss: 4126.8076171875, Entropy -1289.92236328125, Learning Rate: 0.00078125\n",
      "Epoch [3093/20000], Loss: 4104.1396484375, Entropy -1128.43115234375, Learning Rate: 0.00078125\n",
      "Epoch [3094/20000], Loss: 4135.7236328125, Entropy -1269.15673828125, Learning Rate: 0.00078125\n",
      "Epoch [3095/20000], Loss: 4199.7451171875, Entropy -1309.58544921875, Learning Rate: 0.00078125\n",
      "Epoch [3096/20000], Loss: 4240.0986328125, Entropy -1454.4716796875, Learning Rate: 0.00078125\n",
      "Epoch [3097/20000], Loss: 4102.70556640625, Entropy -1267.58642578125, Learning Rate: 0.00078125\n",
      "Epoch [3098/20000], Loss: 4106.35986328125, Entropy -1279.478515625, Learning Rate: 0.00078125\n",
      "Epoch [3099/20000], Loss: 4102.5771484375, Entropy -1302.748046875, Learning Rate: 0.00078125\n",
      "Epoch [3100/20000], Loss: 4090.00244140625, Entropy -1281.52978515625, Learning Rate: 0.00078125\n",
      "Epoch [3101/20000], Loss: 4139.744140625, Entropy -1255.76171875, Learning Rate: 0.00078125\n",
      "Epoch [3102/20000], Loss: 4180.89208984375, Entropy -1431.35205078125, Learning Rate: 0.00078125\n",
      "Epoch [3103/20000], Loss: 4070.684326171875, Entropy -1188.26611328125, Learning Rate: 0.00078125\n",
      "Epoch [3104/20000], Loss: 4095.728515625, Entropy -1306.2255859375, Learning Rate: 0.00078125\n",
      "Epoch [3105/20000], Loss: 4155.724609375, Entropy -1373.55419921875, Learning Rate: 0.00078125\n",
      "Epoch [3106/20000], Loss: 4109.9072265625, Entropy -1297.66650390625, Learning Rate: 0.00078125\n",
      "Epoch [3107/20000], Loss: 4135.96630859375, Entropy -1298.1708984375, Learning Rate: 0.00078125\n",
      "Epoch [3108/20000], Loss: 4192.63916015625, Entropy -1297.81640625, Learning Rate: 0.00078125\n",
      "Epoch [3109/20000], Loss: 4130.61572265625, Entropy -1360.822265625, Learning Rate: 0.00078125\n",
      "Epoch [3110/20000], Loss: 4102.46044921875, Entropy -1220.88720703125, Learning Rate: 0.00078125\n",
      "Epoch [3111/20000], Loss: 4111.46044921875, Entropy -1220.29833984375, Learning Rate: 0.00078125\n",
      "Epoch [3112/20000], Loss: 4142.73876953125, Entropy -1345.32568359375, Learning Rate: 0.00078125\n",
      "Epoch [3113/20000], Loss: 4173.6767578125, Entropy -1337.24072265625, Learning Rate: 0.00078125\n",
      "Epoch [3114/20000], Loss: 4189.1650390625, Entropy -1360.76220703125, Learning Rate: 0.00078125\n",
      "Epoch [3115/20000], Loss: 4154.15673828125, Entropy -1294.68603515625, Learning Rate: 0.00078125\n",
      "Epoch [3116/20000], Loss: 4152.48583984375, Entropy -1275.2578125, Learning Rate: 0.00078125\n",
      "Epoch [3117/20000], Loss: 4070.926025390625, Entropy -1229.09423828125, Learning Rate: 0.00078125\n",
      "Epoch [3118/20000], Loss: 4151.02490234375, Entropy -1336.36865234375, Learning Rate: 0.00078125\n",
      "Epoch [3119/20000], Loss: 4168.14501953125, Entropy -1296.5234375, Learning Rate: 0.00078125\n",
      "Epoch [3120/20000], Loss: 4135.3671875, Entropy -1233.47216796875, Learning Rate: 0.00078125\n",
      "Epoch [3121/20000], Loss: 4156.52294921875, Entropy -1331.22607421875, Learning Rate: 0.00078125\n",
      "Epoch [3122/20000], Loss: 4122.73779296875, Entropy -1397.278564453125, Learning Rate: 0.00078125\n",
      "Epoch [3123/20000], Loss: 4101.07373046875, Entropy -1241.35009765625, Learning Rate: 0.00078125\n",
      "Epoch [3124/20000], Loss: 4147.02783203125, Entropy -1308.46337890625, Learning Rate: 0.00078125\n",
      "Epoch [3125/20000], Loss: 4071.9970703125, Entropy -1161.47900390625, Learning Rate: 0.00078125\n",
      "Epoch [3126/20000], Loss: 4152.4521484375, Entropy -1419.26708984375, Learning Rate: 0.00078125\n",
      "Epoch [3127/20000], Loss: 4164.93212890625, Entropy -1223.7958984375, Learning Rate: 0.00078125\n",
      "Epoch [3128/20000], Loss: 4124.84912109375, Entropy -1265.4833984375, Learning Rate: 0.00078125\n",
      "Epoch [3129/20000], Loss: 4094.378173828125, Entropy -1213.69775390625, Learning Rate: 0.00078125\n",
      "Epoch [3130/20000], Loss: 4185.5068359375, Entropy -1316.32080078125, Learning Rate: 0.00078125\n",
      "Epoch [3131/20000], Loss: 4112.47705078125, Entropy -1308.83935546875, Learning Rate: 0.00078125\n",
      "Epoch [3132/20000], Loss: 4110.2470703125, Entropy -1278.07177734375, Learning Rate: 0.00078125\n",
      "Epoch [3133/20000], Loss: 4153.9931640625, Entropy -1282.61279296875, Learning Rate: 0.00078125\n",
      "Epoch [3134/20000], Loss: 4233.09521484375, Entropy -1363.93798828125, Learning Rate: 0.00078125\n",
      "Epoch [3135/20000], Loss: 4082.36962890625, Entropy -1204.876953125, Learning Rate: 0.00078125\n",
      "Epoch [3136/20000], Loss: 4192.09814453125, Entropy -1268.57080078125, Learning Rate: 0.00078125\n",
      "Epoch [3137/20000], Loss: 4149.6669921875, Entropy -1400.1943359375, Learning Rate: 0.00078125\n",
      "Epoch [3138/20000], Loss: 4233.55908203125, Entropy -1366.74658203125, Learning Rate: 0.00078125\n",
      "Epoch [3139/20000], Loss: 4180.68896484375, Entropy -1443.146728515625, Learning Rate: 0.00078125\n",
      "Epoch [3140/20000], Loss: 4134.40869140625, Entropy -1308.87744140625, Learning Rate: 0.00078125\n",
      "Epoch [3141/20000], Loss: 4220.1865234375, Entropy -1349.67236328125, Learning Rate: 0.00078125\n",
      "Epoch [3142/20000], Loss: 4074.531494140625, Entropy -1151.9453125, Learning Rate: 0.00078125\n",
      "Epoch [3143/20000], Loss: 4147.40966796875, Entropy -1347.46337890625, Learning Rate: 0.00078125\n",
      "Epoch [3144/20000], Loss: 4096.40234375, Entropy -1306.33056640625, Learning Rate: 0.00078125\n",
      "Epoch [3145/20000], Loss: 4083.03076171875, Entropy -1230.38720703125, Learning Rate: 0.00078125\n",
      "Epoch [3146/20000], Loss: 4204.2265625, Entropy -1303.359375, Learning Rate: 0.00078125\n",
      "Epoch [3147/20000], Loss: 4119.93359375, Entropy -1280.07666015625, Learning Rate: 0.00078125\n",
      "Epoch [3148/20000], Loss: 4137.0478515625, Entropy -1283.4609375, Learning Rate: 0.00078125\n",
      "Epoch [3149/20000], Loss: 4229.96728515625, Entropy -1305.72607421875, Learning Rate: 0.00078125\n",
      "Epoch [3150/20000], Loss: 4111.06884765625, Entropy -1272.33251953125, Learning Rate: 0.00078125\n",
      "Epoch [3151/20000], Loss: 4101.08740234375, Entropy -1225.11962890625, Learning Rate: 0.00078125\n",
      "Epoch [3152/20000], Loss: 4133.6943359375, Entropy -1306.1708984375, Learning Rate: 0.00078125\n",
      "Epoch [3153/20000], Loss: 4171.369140625, Entropy -1216.75634765625, Learning Rate: 0.00078125\n",
      "Epoch [3154/20000], Loss: 4088.41064453125, Entropy -1256.65576171875, Learning Rate: 0.00078125\n",
      "Epoch [3155/20000], Loss: 4153.1220703125, Entropy -1267.2626953125, Learning Rate: 0.00078125\n",
      "Epoch [3156/20000], Loss: 4165.4326171875, Entropy -1440.031494140625, Learning Rate: 0.00078125\n",
      "Epoch [3157/20000], Loss: 4086.439208984375, Entropy -1231.85791015625, Learning Rate: 0.00078125\n",
      "Epoch [3158/20000], Loss: 4102.15966796875, Entropy -1272.77197265625, Learning Rate: 0.00078125\n",
      "Epoch [3159/20000], Loss: 4150.9521484375, Entropy -1387.99169921875, Learning Rate: 0.00078125\n",
      "Epoch [3160/20000], Loss: 4054.900146484375, Entropy -1161.47705078125, Learning Rate: 0.00078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3161/20000], Loss: 4030.943115234375, Entropy -1181.76806640625, Learning Rate: 0.00078125\n",
      "Epoch [3162/20000], Loss: 4136.29345703125, Entropy -1179.0087890625, Learning Rate: 0.00078125\n",
      "Epoch [3163/20000], Loss: 4159.5087890625, Entropy -1295.61181640625, Learning Rate: 0.00078125\n",
      "Epoch [3164/20000], Loss: 4103.9873046875, Entropy -1274.91943359375, Learning Rate: 0.00078125\n",
      "Epoch [3165/20000], Loss: 4165.24951171875, Entropy -1316.1103515625, Learning Rate: 0.00078125\n",
      "Epoch [3166/20000], Loss: 4137.291015625, Entropy -1356.38916015625, Learning Rate: 0.00078125\n",
      "Epoch [3167/20000], Loss: 4115.53125, Entropy -1278.02392578125, Learning Rate: 0.00078125\n",
      "Epoch [3168/20000], Loss: 4157.22119140625, Entropy -1261.404296875, Learning Rate: 0.00078125\n",
      "Epoch [3169/20000], Loss: 4173.8125, Entropy -1168.61767578125, Learning Rate: 0.00078125\n",
      "Epoch [3170/20000], Loss: 4091.794677734375, Entropy -1107.015625, Learning Rate: 0.00078125\n",
      "Epoch [3171/20000], Loss: 4136.4736328125, Entropy -1195.01611328125, Learning Rate: 0.00078125\n",
      "Epoch [3172/20000], Loss: 4171.693359375, Entropy -1226.42138671875, Learning Rate: 0.00078125\n",
      "Epoch [3173/20000], Loss: 4121.712890625, Entropy -1314.62646484375, Learning Rate: 0.00078125\n",
      "Epoch [3174/20000], Loss: 4068.830078125, Entropy -1260.380859375, Learning Rate: 0.00078125\n",
      "Epoch [3175/20000], Loss: 4125.517578125, Entropy -1244.4912109375, Learning Rate: 0.00078125\n",
      "Epoch [3176/20000], Loss: 4165.09912109375, Entropy -1259.46630859375, Learning Rate: 0.00078125\n",
      "Epoch [3177/20000], Loss: 4102.0771484375, Entropy -1360.783203125, Learning Rate: 0.00078125\n",
      "Epoch [3178/20000], Loss: 4109.5712890625, Entropy -1343.51611328125, Learning Rate: 0.00078125\n",
      "Epoch [3179/20000], Loss: 4132.5634765625, Entropy -1318.611328125, Learning Rate: 0.00078125\n",
      "Epoch [3180/20000], Loss: 4063.786376953125, Entropy -1265.48486328125, Learning Rate: 0.00078125\n",
      "Epoch [3181/20000], Loss: 4086.515869140625, Entropy -1196.2763671875, Learning Rate: 0.000390625\n",
      "Epoch [3182/20000], Loss: 4127.5751953125, Entropy -1264.23046875, Learning Rate: 0.000390625\n",
      "Epoch [3183/20000], Loss: 4120.88037109375, Entropy -1380.6123046875, Learning Rate: 0.000390625\n",
      "Epoch [3184/20000], Loss: 4132.6416015625, Entropy -1324.18896484375, Learning Rate: 0.000390625\n",
      "Epoch [3185/20000], Loss: 4192.03759765625, Entropy -1192.95263671875, Learning Rate: 0.000390625\n",
      "Epoch [3186/20000], Loss: 4174.19091796875, Entropy -1374.97314453125, Learning Rate: 0.000390625\n",
      "Epoch [3187/20000], Loss: 4067.92822265625, Entropy -1233.51611328125, Learning Rate: 0.000390625\n",
      "Epoch [3188/20000], Loss: 4159.69189453125, Entropy -1427.532470703125, Learning Rate: 0.000390625\n",
      "Epoch [3189/20000], Loss: 4088.84521484375, Entropy -1223.517578125, Learning Rate: 0.000390625\n",
      "Epoch [3190/20000], Loss: 4135.82470703125, Entropy -1314.82421875, Learning Rate: 0.000390625\n",
      "Epoch [3191/20000], Loss: 4088.614501953125, Entropy -1215.04052734375, Learning Rate: 0.000390625\n",
      "Epoch [3192/20000], Loss: 4137.90478515625, Entropy -1260.91943359375, Learning Rate: 0.000390625\n",
      "Epoch [3193/20000], Loss: 4067.013671875, Entropy -1245.2568359375, Learning Rate: 0.000390625\n",
      "Epoch [3194/20000], Loss: 4091.7080078125, Entropy -1323.81005859375, Learning Rate: 0.000390625\n",
      "Epoch [3195/20000], Loss: 4100.8154296875, Entropy -1326.5283203125, Learning Rate: 0.000390625\n",
      "Epoch [3196/20000], Loss: 4121.6435546875, Entropy -1273.27880859375, Learning Rate: 0.000390625\n",
      "Epoch [3197/20000], Loss: 4083.966796875, Entropy -1249.62451171875, Learning Rate: 0.000390625\n",
      "Epoch [3198/20000], Loss: 4104.3427734375, Entropy -1220.43505859375, Learning Rate: 0.000390625\n",
      "Epoch [3199/20000], Loss: 4100.31884765625, Entropy -1215.46875, Learning Rate: 0.000390625\n",
      "Epoch [3200/20000], Loss: 4083.697998046875, Entropy -1237.314453125, Learning Rate: 0.000390625\n",
      "Epoch [3201/20000], Loss: 4086.265380859375, Entropy -1232.51611328125, Learning Rate: 0.000390625\n",
      "Epoch [3202/20000], Loss: 4121.43359375, Entropy -1216.03564453125, Learning Rate: 0.000390625\n",
      "Epoch [3203/20000], Loss: 4155.470703125, Entropy -1350.1044921875, Learning Rate: 0.000390625\n",
      "Epoch [3204/20000], Loss: 4148.88720703125, Entropy -1295.9208984375, Learning Rate: 0.000390625\n",
      "Epoch [3205/20000], Loss: 4121.90869140625, Entropy -1354.15283203125, Learning Rate: 0.000390625\n",
      "Epoch [3206/20000], Loss: 4204.42333984375, Entropy -1400.25048828125, Learning Rate: 0.000390625\n",
      "Epoch [3207/20000], Loss: 4159.48583984375, Entropy -1319.1796875, Learning Rate: 0.000390625\n",
      "Epoch [3208/20000], Loss: 4113.1240234375, Entropy -1336.181640625, Learning Rate: 0.000390625\n",
      "Epoch [3209/20000], Loss: 4224.73779296875, Entropy -1311.33203125, Learning Rate: 0.000390625\n",
      "Epoch [3210/20000], Loss: 4120.35498046875, Entropy -1341.88623046875, Learning Rate: 0.000390625\n",
      "Epoch [3211/20000], Loss: 4157.7392578125, Entropy -1293.5126953125, Learning Rate: 0.000390625\n",
      "Epoch [3212/20000], Loss: 4071.94970703125, Entropy -1218.9130859375, Learning Rate: 0.000390625\n",
      "Epoch [3213/20000], Loss: 4121.0224609375, Entropy -1336.03759765625, Learning Rate: 0.000390625\n",
      "Epoch [3214/20000], Loss: 4060.69189453125, Entropy -1186.03759765625, Learning Rate: 0.000390625\n",
      "Epoch [3215/20000], Loss: 4086.4150390625, Entropy -1229.06787109375, Learning Rate: 0.000390625\n",
      "Epoch [3216/20000], Loss: 4141.78955078125, Entropy -1255.34716796875, Learning Rate: 0.000390625\n",
      "Epoch [3217/20000], Loss: 4072.078125, Entropy -1169.46630859375, Learning Rate: 0.000390625\n",
      "Epoch [3218/20000], Loss: 4065.4833984375, Entropy -1272.46435546875, Learning Rate: 0.000390625\n",
      "Epoch [3219/20000], Loss: 4131.93701171875, Entropy -1313.431640625, Learning Rate: 0.000390625\n",
      "Epoch [3220/20000], Loss: 4105.78515625, Entropy -1245.00927734375, Learning Rate: 0.000390625\n",
      "Epoch [3221/20000], Loss: 4129.38525390625, Entropy -1276.66357421875, Learning Rate: 0.000390625\n",
      "Epoch [3222/20000], Loss: 4148.52001953125, Entropy -1376.47314453125, Learning Rate: 0.000390625\n",
      "Epoch [3223/20000], Loss: 4110.19677734375, Entropy -1287.767578125, Learning Rate: 0.000390625\n",
      "Epoch [3224/20000], Loss: 4090.3388671875, Entropy -1264.26611328125, Learning Rate: 0.000390625\n",
      "Epoch [3225/20000], Loss: 4136.63134765625, Entropy -1248.69970703125, Learning Rate: 0.000390625\n",
      "Epoch [3226/20000], Loss: 4134.20361328125, Entropy -1290.97412109375, Learning Rate: 0.000390625\n",
      "Epoch [3227/20000], Loss: 4136.3515625, Entropy -1324.31982421875, Learning Rate: 0.000390625\n",
      "Epoch [3228/20000], Loss: 4154.15380859375, Entropy -1348.9443359375, Learning Rate: 0.000390625\n",
      "Epoch [3229/20000], Loss: 4132.5537109375, Entropy -1314.2373046875, Learning Rate: 0.000390625\n",
      "Epoch [3230/20000], Loss: 4135.09716796875, Entropy -1321.4462890625, Learning Rate: 0.000390625\n",
      "Epoch [3231/20000], Loss: 4172.146484375, Entropy -1290.3515625, Learning Rate: 0.000390625\n",
      "Epoch [3232/20000], Loss: 4134.06396484375, Entropy -1335.90234375, Learning Rate: 0.000390625\n",
      "Epoch [3233/20000], Loss: 4120.439453125, Entropy -1221.2978515625, Learning Rate: 0.000390625\n",
      "Epoch [3234/20000], Loss: 4100.615234375, Entropy -1247.47900390625, Learning Rate: 0.000390625\n",
      "Epoch [3235/20000], Loss: 4145.216796875, Entropy -1321.83984375, Learning Rate: 0.000390625\n",
      "Epoch [3236/20000], Loss: 4139.7431640625, Entropy -1262.71142578125, Learning Rate: 0.000390625\n",
      "Epoch [3237/20000], Loss: 4122.24365234375, Entropy -1326.47216796875, Learning Rate: 0.000390625\n",
      "Epoch [3238/20000], Loss: 4100.60595703125, Entropy -1228.6953125, Learning Rate: 0.000390625\n",
      "Epoch [3239/20000], Loss: 4126.1318359375, Entropy -1328.9130859375, Learning Rate: 0.000390625\n",
      "Epoch [3240/20000], Loss: 4146.79931640625, Entropy -1344.97607421875, Learning Rate: 0.000390625\n",
      "Epoch [3241/20000], Loss: 4099.111328125, Entropy -1197.7890625, Learning Rate: 0.000390625\n",
      "Epoch [3242/20000], Loss: 4152.2001953125, Entropy -1310.61474609375, Learning Rate: 0.000390625\n",
      "Epoch [3243/20000], Loss: 4129.095703125, Entropy -1310.49560546875, Learning Rate: 0.000390625\n",
      "Epoch [3244/20000], Loss: 4083.48974609375, Entropy -1245.85693359375, Learning Rate: 0.000390625\n",
      "Epoch [3245/20000], Loss: 4090.765625, Entropy -1264.01904296875, Learning Rate: 0.000390625\n",
      "Epoch [3246/20000], Loss: 4198.58154296875, Entropy -1309.47216796875, Learning Rate: 0.000390625\n",
      "Epoch [3247/20000], Loss: 4091.17626953125, Entropy -1275.888671875, Learning Rate: 0.000390625\n",
      "Epoch [3248/20000], Loss: 4131.62109375, Entropy -1237.07080078125, Learning Rate: 0.000390625\n",
      "Epoch [3249/20000], Loss: 4105.27685546875, Entropy -1300.50439453125, Learning Rate: 0.000390625\n",
      "Epoch [3250/20000], Loss: 4162.49560546875, Entropy -1283.1083984375, Learning Rate: 0.000390625\n",
      "Epoch [3251/20000], Loss: 4129.63818359375, Entropy -1321.24169921875, Learning Rate: 0.000390625\n",
      "Epoch [3252/20000], Loss: 4147.5908203125, Entropy -1320.48779296875, Learning Rate: 0.000390625\n",
      "Epoch [3253/20000], Loss: 4058.365478515625, Entropy -1172.544921875, Learning Rate: 0.000390625\n",
      "Epoch [3254/20000], Loss: 4211.1630859375, Entropy -1248.2998046875, Learning Rate: 0.000390625\n",
      "Epoch [3255/20000], Loss: 4118.0732421875, Entropy -1133.96728515625, Learning Rate: 0.000390625\n",
      "Epoch [3256/20000], Loss: 4102.072265625, Entropy -1269.4208984375, Learning Rate: 0.000390625\n",
      "Epoch [3257/20000], Loss: 4119.806640625, Entropy -1306.47998046875, Learning Rate: 0.000390625\n",
      "Epoch [3258/20000], Loss: 4107.96044921875, Entropy -1196.58837890625, Learning Rate: 0.000390625\n",
      "Epoch [3259/20000], Loss: 4185.1337890625, Entropy -1224.6845703125, Learning Rate: 0.000390625\n",
      "Epoch [3260/20000], Loss: 4130.96142578125, Entropy -1299.125, Learning Rate: 0.000390625\n",
      "Epoch [3261/20000], Loss: 4085.54052734375, Entropy -1273.28662109375, Learning Rate: 0.000390625\n",
      "Epoch [3262/20000], Loss: 4111.369140625, Entropy -1294.81689453125, Learning Rate: 0.000390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3263/20000], Loss: 4173.65283203125, Entropy -1394.310791015625, Learning Rate: 0.000390625\n",
      "Epoch [3264/20000], Loss: 4091.84814453125, Entropy -1317.7646484375, Learning Rate: 0.000390625\n",
      "Epoch [3265/20000], Loss: 4193.783203125, Entropy -1329.1376953125, Learning Rate: 0.000390625\n",
      "Epoch [3266/20000], Loss: 4149.15869140625, Entropy -1344.27392578125, Learning Rate: 0.000390625\n",
      "Epoch [3267/20000], Loss: 4044.108154296875, Entropy -1255.83642578125, Learning Rate: 0.000390625\n",
      "Epoch [3268/20000], Loss: 4118.3701171875, Entropy -1328.37060546875, Learning Rate: 0.000390625\n",
      "Epoch [3269/20000], Loss: 4186.34716796875, Entropy -1310.60009765625, Learning Rate: 0.000390625\n",
      "Epoch [3270/20000], Loss: 4127.7568359375, Entropy -1293.99169921875, Learning Rate: 0.000390625\n",
      "Epoch [3271/20000], Loss: 4073.308349609375, Entropy -1136.18701171875, Learning Rate: 0.000390625\n",
      "Epoch [3272/20000], Loss: 4093.936279296875, Entropy -1281.8798828125, Learning Rate: 0.000390625\n",
      "Epoch [3273/20000], Loss: 4096.14453125, Entropy -1351.03271484375, Learning Rate: 0.000390625\n",
      "Epoch [3274/20000], Loss: 4123.306640625, Entropy -1088.6572265625, Learning Rate: 0.000390625\n",
      "Epoch [3275/20000], Loss: 4144.9853515625, Entropy -1280.39111328125, Learning Rate: 0.000390625\n",
      "Epoch [3276/20000], Loss: 4136.76806640625, Entropy -1179.53857421875, Learning Rate: 0.000390625\n",
      "Epoch [3277/20000], Loss: 4132.64111328125, Entropy -1230.85498046875, Learning Rate: 0.000390625\n",
      "Epoch [3278/20000], Loss: 4117.494140625, Entropy -1301.92236328125, Learning Rate: 0.000390625\n",
      "Epoch [3279/20000], Loss: 4117.85888671875, Entropy -1177.65185546875, Learning Rate: 0.000390625\n",
      "Epoch [3280/20000], Loss: 4097.3994140625, Entropy -1247.91748046875, Learning Rate: 0.000390625\n",
      "Epoch [3281/20000], Loss: 4163.02490234375, Entropy -1312.79443359375, Learning Rate: 0.000390625\n",
      "Epoch [3282/20000], Loss: 4189.57861328125, Entropy -1348.31591796875, Learning Rate: 0.000390625\n",
      "Epoch [3283/20000], Loss: 4123.0419921875, Entropy -1321.77490234375, Learning Rate: 0.000390625\n",
      "Epoch [3284/20000], Loss: 4177.30859375, Entropy -1348.74169921875, Learning Rate: 0.000390625\n",
      "Epoch [3285/20000], Loss: 4108.46630859375, Entropy -1271.67822265625, Learning Rate: 0.000390625\n",
      "Epoch [3286/20000], Loss: 4182.5625, Entropy -1282.54052734375, Learning Rate: 0.000390625\n",
      "Epoch [3287/20000], Loss: 4086.03564453125, Entropy -1241.24462890625, Learning Rate: 0.000390625\n",
      "Epoch [3288/20000], Loss: 4210.1513671875, Entropy -1197.560546875, Learning Rate: 0.000390625\n",
      "Epoch [3289/20000], Loss: 4087.9482421875, Entropy -1163.90771484375, Learning Rate: 0.000390625\n",
      "Epoch [3290/20000], Loss: 4094.35107421875, Entropy -1274.93896484375, Learning Rate: 0.000390625\n",
      "Epoch [3291/20000], Loss: 4056.0537109375, Entropy -1257.60546875, Learning Rate: 0.000390625\n",
      "Epoch [3292/20000], Loss: 4176.92626953125, Entropy -1351.3271484375, Learning Rate: 0.000390625\n",
      "Epoch [3293/20000], Loss: 4148.50537109375, Entropy -1340.734375, Learning Rate: 0.000390625\n",
      "Epoch [3294/20000], Loss: 4173.8046875, Entropy -1217.73779296875, Learning Rate: 0.000390625\n",
      "Epoch [3295/20000], Loss: 4083.356689453125, Entropy -1204.42919921875, Learning Rate: 0.000390625\n",
      "Epoch [3296/20000], Loss: 4079.215087890625, Entropy -1252.57421875, Learning Rate: 0.000390625\n",
      "Epoch [3297/20000], Loss: 4132.00537109375, Entropy -1343.92333984375, Learning Rate: 0.000390625\n",
      "Epoch [3298/20000], Loss: 4099.2099609375, Entropy -1304.1533203125, Learning Rate: 0.000390625\n",
      "Epoch [3299/20000], Loss: 4096.3408203125, Entropy -1144.68701171875, Learning Rate: 0.000390625\n",
      "Epoch [3300/20000], Loss: 4070.1865234375, Entropy -1214.4755859375, Learning Rate: 0.000390625\n",
      "Epoch [3301/20000], Loss: 4101.35009765625, Entropy -1248.45458984375, Learning Rate: 0.000390625\n",
      "Epoch [3302/20000], Loss: 4121.39111328125, Entropy -1287.34716796875, Learning Rate: 0.000390625\n",
      "Epoch [3303/20000], Loss: 4120.8203125, Entropy -1249.45361328125, Learning Rate: 0.000390625\n",
      "Epoch [3304/20000], Loss: 4077.373046875, Entropy -1183.837890625, Learning Rate: 0.000390625\n",
      "Epoch [3305/20000], Loss: 4144.58740234375, Entropy -1343.47021484375, Learning Rate: 0.000390625\n",
      "Epoch [3306/20000], Loss: 4144.46533203125, Entropy -1177.66650390625, Learning Rate: 0.000390625\n",
      "Epoch [3307/20000], Loss: 4112.11962890625, Entropy -1312.5673828125, Learning Rate: 0.000390625\n",
      "Epoch [3308/20000], Loss: 4063.485595703125, Entropy -1225.9365234375, Learning Rate: 0.000390625\n",
      "Epoch [3309/20000], Loss: 4142.8095703125, Entropy -1353.841796875, Learning Rate: 0.000390625\n",
      "Epoch [3310/20000], Loss: 4140.26318359375, Entropy -1250.3134765625, Learning Rate: 0.000390625\n",
      "Epoch [3311/20000], Loss: 4162.86328125, Entropy -1370.822265625, Learning Rate: 0.000390625\n",
      "Epoch [3312/20000], Loss: 4083.1083984375, Entropy -1253.02880859375, Learning Rate: 0.000390625\n",
      "Epoch [3313/20000], Loss: 4108.68603515625, Entropy -1203.16748046875, Learning Rate: 0.000390625\n",
      "Epoch [3314/20000], Loss: 4104.7158203125, Entropy -1322.15576171875, Learning Rate: 0.000390625\n",
      "Epoch [3315/20000], Loss: 4113.1484375, Entropy -1312.34375, Learning Rate: 0.000390625\n",
      "Epoch [3316/20000], Loss: 4070.35791015625, Entropy -1210.57080078125, Learning Rate: 0.000390625\n",
      "Epoch [3317/20000], Loss: 4098.3408203125, Entropy -1268.5087890625, Learning Rate: 0.000390625\n",
      "Epoch [3318/20000], Loss: 4152.02978515625, Entropy -1209.259765625, Learning Rate: 0.000390625\n",
      "Epoch [3319/20000], Loss: 4133.61669921875, Entropy -1286.31396484375, Learning Rate: 0.000390625\n",
      "Epoch [3320/20000], Loss: 4110.07568359375, Entropy -1207.787109375, Learning Rate: 0.000390625\n",
      "Epoch [3321/20000], Loss: 4166.2373046875, Entropy -1387.72021484375, Learning Rate: 0.000390625\n",
      "Epoch [3322/20000], Loss: 4091.283203125, Entropy -1280.21240234375, Learning Rate: 0.000390625\n",
      "Epoch [3323/20000], Loss: 4089.029052734375, Entropy -1211.45751953125, Learning Rate: 0.000390625\n",
      "Epoch [3324/20000], Loss: 4154.91259765625, Entropy -1300.61669921875, Learning Rate: 0.000390625\n",
      "Epoch [3325/20000], Loss: 4148.078125, Entropy -1284.51513671875, Learning Rate: 0.000390625\n",
      "Epoch [3326/20000], Loss: 4079.120361328125, Entropy -1309.88671875, Learning Rate: 0.000390625\n",
      "Epoch [3327/20000], Loss: 4126.01953125, Entropy -1285.1611328125, Learning Rate: 0.000390625\n",
      "Epoch [3328/20000], Loss: 4165.47998046875, Entropy -1250.93212890625, Learning Rate: 0.000390625\n",
      "Epoch [3329/20000], Loss: 4056.9921875, Entropy -1193.54736328125, Learning Rate: 0.000390625\n",
      "Epoch [3330/20000], Loss: 4128.826171875, Entropy -1341.63427734375, Learning Rate: 0.000390625\n",
      "Epoch [3331/20000], Loss: 4116.119140625, Entropy -1250.9765625, Learning Rate: 0.000390625\n",
      "Epoch [3332/20000], Loss: 4180.8154296875, Entropy -1293.6396484375, Learning Rate: 0.000390625\n",
      "Epoch [3333/20000], Loss: 4087.34228515625, Entropy -1168.10498046875, Learning Rate: 0.000390625\n",
      "Epoch [3334/20000], Loss: 4136.87158203125, Entropy -1305.41162109375, Learning Rate: 0.000390625\n",
      "Epoch [3335/20000], Loss: 4105.70556640625, Entropy -1303.525390625, Learning Rate: 0.000390625\n",
      "Epoch [3336/20000], Loss: 4084.030029296875, Entropy -1175.83251953125, Learning Rate: 0.000390625\n",
      "Epoch [3337/20000], Loss: 4185.052734375, Entropy -1274.46044921875, Learning Rate: 0.000390625\n",
      "Epoch [3338/20000], Loss: 4154.658203125, Entropy -1260.16796875, Learning Rate: 0.000390625\n",
      "Epoch [3339/20000], Loss: 4148.55126953125, Entropy -1299.12646484375, Learning Rate: 0.000390625\n",
      "Epoch [3340/20000], Loss: 4117.08251953125, Entropy -1265.20751953125, Learning Rate: 0.000390625\n",
      "Epoch [3341/20000], Loss: 4090.060546875, Entropy -1162.36181640625, Learning Rate: 0.000390625\n",
      "Epoch [3342/20000], Loss: 4087.90380859375, Entropy -1302.3330078125, Learning Rate: 0.000390625\n",
      "Epoch [3343/20000], Loss: 4133.826171875, Entropy -1200.919921875, Learning Rate: 0.000390625\n",
      "Epoch [3344/20000], Loss: 4124.951171875, Entropy -1321.23681640625, Learning Rate: 0.000390625\n",
      "Epoch [3345/20000], Loss: 4160.24560546875, Entropy -1308.56396484375, Learning Rate: 0.000390625\n",
      "Epoch [3346/20000], Loss: 4053.802734375, Entropy -1230.01513671875, Learning Rate: 0.000390625\n",
      "Epoch [3347/20000], Loss: 4095.658447265625, Entropy -1197.84814453125, Learning Rate: 0.000390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3348/20000], Loss: 4094.306884765625, Entropy -1289.18603515625, Learning Rate: 0.000390625\n",
      "Epoch [3349/20000], Loss: 4109.22412109375, Entropy -1298.2763671875, Learning Rate: 0.000390625\n",
      "Epoch [3350/20000], Loss: 4126.775390625, Entropy -1261.18212890625, Learning Rate: 0.000390625\n",
      "Epoch [3351/20000], Loss: 4082.9580078125, Entropy -1311.26123046875, Learning Rate: 0.000390625\n",
      "Epoch [3352/20000], Loss: 4139.7978515625, Entropy -1295.61572265625, Learning Rate: 0.000390625\n",
      "Epoch [3353/20000], Loss: 4088.662353515625, Entropy -1118.64697265625, Learning Rate: 0.000390625\n",
      "Epoch [3354/20000], Loss: 4104.20166015625, Entropy -1252.74072265625, Learning Rate: 0.000390625\n",
      "Epoch [3355/20000], Loss: 4097.71630859375, Entropy -1205.96728515625, Learning Rate: 0.000390625\n",
      "Epoch [3356/20000], Loss: 4115.3837890625, Entropy -1258.4140625, Learning Rate: 0.000390625\n",
      "Epoch [3357/20000], Loss: 4150.75244140625, Entropy -1273.798828125, Learning Rate: 0.000390625\n",
      "Epoch [3358/20000], Loss: 4086.85791015625, Entropy -1220.5751953125, Learning Rate: 0.000390625\n",
      "Epoch [3359/20000], Loss: 4151.376953125, Entropy -1195.95849609375, Learning Rate: 0.000390625\n",
      "Epoch [3360/20000], Loss: 4176.28369140625, Entropy -1361.27734375, Learning Rate: 0.000390625\n",
      "Epoch [3361/20000], Loss: 4103.93994140625, Entropy -1270.361328125, Learning Rate: 0.000390625\n",
      "Epoch [3362/20000], Loss: 4118.43994140625, Entropy -1393.423583984375, Learning Rate: 0.000390625\n",
      "Epoch [3363/20000], Loss: 4103.447265625, Entropy -1272.55517578125, Learning Rate: 0.000390625\n",
      "Epoch [3364/20000], Loss: 4113.31884765625, Entropy -1312.59375, Learning Rate: 0.000390625\n",
      "Epoch [3365/20000], Loss: 4162.52587890625, Entropy -1344.97900390625, Learning Rate: 0.000390625\n",
      "Epoch [3366/20000], Loss: 4139.34423828125, Entropy -1329.72998046875, Learning Rate: 0.000390625\n",
      "Epoch [3367/20000], Loss: 4122.8623046875, Entropy -1280.01513671875, Learning Rate: 0.000390625\n",
      "Epoch [3368/20000], Loss: 4136.46533203125, Entropy -1334.62890625, Learning Rate: 0.000390625\n",
      "Epoch [3369/20000], Loss: 4122.0986328125, Entropy -1284.21435546875, Learning Rate: 0.000390625\n",
      "Epoch [3370/20000], Loss: 4098.9072265625, Entropy -1267.7890625, Learning Rate: 0.000390625\n",
      "Epoch [3371/20000], Loss: 4137.5498046875, Entropy -1344.806640625, Learning Rate: 0.000390625\n",
      "Epoch [3372/20000], Loss: 4096.42333984375, Entropy -1359.05615234375, Learning Rate: 0.000390625\n",
      "Epoch [3373/20000], Loss: 4105.71875, Entropy -1290.6201171875, Learning Rate: 0.000390625\n",
      "Epoch [3374/20000], Loss: 4125.349609375, Entropy -1191.0, Learning Rate: 0.000390625\n",
      "Epoch [3375/20000], Loss: 4098.56689453125, Entropy -1247.89599609375, Learning Rate: 0.000390625\n",
      "Epoch [3376/20000], Loss: 4110.37353515625, Entropy -1228.19580078125, Learning Rate: 0.000390625\n",
      "Epoch [3377/20000], Loss: 4136.90234375, Entropy -1209.89453125, Learning Rate: 0.000390625\n",
      "Epoch [3378/20000], Loss: 4113.86962890625, Entropy -1183.23291015625, Learning Rate: 0.000390625\n",
      "Epoch [3379/20000], Loss: 4090.82275390625, Entropy -1154.779296875, Learning Rate: 0.000390625\n",
      "Epoch [3380/20000], Loss: 4113.6474609375, Entropy -1278.46728515625, Learning Rate: 0.000390625\n",
      "Epoch [3381/20000], Loss: 4187.09326171875, Entropy -1324.26513671875, Learning Rate: 0.000390625\n",
      "Epoch [3382/20000], Loss: 4202.68212890625, Entropy -1335.25537109375, Learning Rate: 0.0001953125\n",
      "Epoch [3383/20000], Loss: 4096.78125, Entropy -1281.47705078125, Learning Rate: 0.0001953125\n",
      "Epoch [3384/20000], Loss: 4152.744140625, Entropy -1334.333984375, Learning Rate: 0.0001953125\n",
      "Epoch [3385/20000], Loss: 4111.8955078125, Entropy -1267.13330078125, Learning Rate: 0.0001953125\n",
      "Epoch [3386/20000], Loss: 4121.38671875, Entropy -1323.251953125, Learning Rate: 0.0001953125\n",
      "Epoch [3387/20000], Loss: 4085.666748046875, Entropy -1231.85205078125, Learning Rate: 0.0001953125\n",
      "Epoch [3388/20000], Loss: 4162.33203125, Entropy -1283.650390625, Learning Rate: 0.0001953125\n",
      "Epoch [3389/20000], Loss: 4084.785888671875, Entropy -1286.9521484375, Learning Rate: 0.0001953125\n",
      "Epoch [3390/20000], Loss: 4102.29541015625, Entropy -1307.2265625, Learning Rate: 0.0001953125\n",
      "Epoch [3391/20000], Loss: 4101.18994140625, Entropy -1295.78076171875, Learning Rate: 0.0001953125\n",
      "Epoch [3392/20000], Loss: 4143.8955078125, Entropy -1292.95556640625, Learning Rate: 0.0001953125\n",
      "Epoch [3393/20000], Loss: 4171.73681640625, Entropy -1166.14404296875, Learning Rate: 0.0001953125\n",
      "Epoch [3394/20000], Loss: 4108.21923828125, Entropy -1341.36083984375, Learning Rate: 0.0001953125\n",
      "Epoch [3395/20000], Loss: 4124.5830078125, Entropy -1203.03173828125, Learning Rate: 0.0001953125\n",
      "Epoch [3396/20000], Loss: 4119.95556640625, Entropy -1298.712890625, Learning Rate: 0.0001953125\n",
      "Epoch [3397/20000], Loss: 4133.48291015625, Entropy -1351.96826171875, Learning Rate: 0.0001953125\n",
      "Epoch [3398/20000], Loss: 4145.45458984375, Entropy -1198.35302734375, Learning Rate: 0.0001953125\n",
      "Epoch [3399/20000], Loss: 4108.91064453125, Entropy -1309.50732421875, Learning Rate: 0.0001953125\n",
      "Epoch [3400/20000], Loss: 4094.93505859375, Entropy -1274.27197265625, Learning Rate: 0.0001953125\n",
      "Epoch [3401/20000], Loss: 4086.41796875, Entropy -1287.1015625, Learning Rate: 0.0001953125\n",
      "Epoch [3402/20000], Loss: 4315.4140625, Entropy -1473.3173828125, Learning Rate: 0.0001953125\n",
      "Epoch [3403/20000], Loss: 4150.56298828125, Entropy -1347.7626953125, Learning Rate: 0.0001953125\n",
      "Epoch [3404/20000], Loss: 4092.69970703125, Entropy -1198.83447265625, Learning Rate: 0.0001953125\n",
      "Epoch [3405/20000], Loss: 4087.61279296875, Entropy -1255.1318359375, Learning Rate: 0.0001953125\n",
      "Epoch [3406/20000], Loss: 4054.60302734375, Entropy -1093.349609375, Learning Rate: 0.0001953125\n",
      "Epoch [3407/20000], Loss: 4018.305419921875, Entropy -1089.17138671875, Learning Rate: 0.0001953125\n",
      "Epoch [3408/20000], Loss: 4073.40576171875, Entropy -1223.52685546875, Learning Rate: 0.0001953125\n",
      "Epoch [3409/20000], Loss: 4080.58154296875, Entropy -1300.97705078125, Learning Rate: 0.0001953125\n",
      "Epoch [3410/20000], Loss: 4100.7470703125, Entropy -1321.96875, Learning Rate: 0.0001953125\n",
      "Epoch [3411/20000], Loss: 4126.67578125, Entropy -1344.86279296875, Learning Rate: 0.0001953125\n",
      "Epoch [3412/20000], Loss: 4135.373046875, Entropy -1314.35498046875, Learning Rate: 0.0001953125\n",
      "Epoch [3413/20000], Loss: 4168.6455078125, Entropy -1252.8515625, Learning Rate: 0.0001953125\n",
      "Epoch [3414/20000], Loss: 4158.6240234375, Entropy -1208.95849609375, Learning Rate: 0.0001953125\n",
      "Epoch [3415/20000], Loss: 4057.4677734375, Entropy -1251.43359375, Learning Rate: 0.0001953125\n",
      "Epoch [3416/20000], Loss: 4176.79345703125, Entropy -1344.97705078125, Learning Rate: 0.0001953125\n",
      "Epoch [3417/20000], Loss: 4153.234375, Entropy -1344.1748046875, Learning Rate: 0.0001953125\n",
      "Epoch [3418/20000], Loss: 4086.2939453125, Entropy -1231.55615234375, Learning Rate: 0.0001953125\n",
      "Epoch [3419/20000], Loss: 4219.36181640625, Entropy -1304.61962890625, Learning Rate: 0.0001953125\n",
      "Epoch [3420/20000], Loss: 4102.82470703125, Entropy -1275.76123046875, Learning Rate: 0.0001953125\n",
      "Epoch [3421/20000], Loss: 4103.0712890625, Entropy -1254.2236328125, Learning Rate: 0.0001953125\n",
      "Epoch [3422/20000], Loss: 4132.04248046875, Entropy -1153.2294921875, Learning Rate: 0.0001953125\n",
      "Epoch [3423/20000], Loss: 4132.90087890625, Entropy -1351.37109375, Learning Rate: 0.0001953125\n",
      "Epoch [3424/20000], Loss: 4091.71728515625, Entropy -1364.271484375, Learning Rate: 0.0001953125\n",
      "Epoch [3425/20000], Loss: 4094.13916015625, Entropy -1281.74560546875, Learning Rate: 0.0001953125\n",
      "Epoch [3426/20000], Loss: 4077.46142578125, Entropy -1228.681640625, Learning Rate: 0.0001953125\n",
      "Epoch [3427/20000], Loss: 4104.18212890625, Entropy -1256.88134765625, Learning Rate: 0.0001953125\n",
      "Epoch [3428/20000], Loss: 4155.3515625, Entropy -1296.35595703125, Learning Rate: 0.0001953125\n",
      "Epoch [3429/20000], Loss: 4151.36376953125, Entropy -1359.72998046875, Learning Rate: 0.0001953125\n",
      "Epoch [3430/20000], Loss: 4108.7783203125, Entropy -1271.3525390625, Learning Rate: 0.0001953125\n",
      "Epoch [3431/20000], Loss: 4122.3076171875, Entropy -1328.7333984375, Learning Rate: 0.0001953125\n",
      "Epoch [3432/20000], Loss: 4142.7900390625, Entropy -1318.42431640625, Learning Rate: 0.0001953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3433/20000], Loss: 4091.19287109375, Entropy -1310.37744140625, Learning Rate: 0.0001953125\n",
      "Epoch [3434/20000], Loss: 4100.7236328125, Entropy -1212.83349609375, Learning Rate: 0.0001953125\n",
      "Epoch [3435/20000], Loss: 4088.4541015625, Entropy -1282.87255859375, Learning Rate: 0.0001953125\n",
      "Epoch [3436/20000], Loss: 4089.675537109375, Entropy -1272.74560546875, Learning Rate: 0.0001953125\n",
      "Epoch [3437/20000], Loss: 4123.12841796875, Entropy -1285.10400390625, Learning Rate: 0.0001953125\n",
      "Epoch [3438/20000], Loss: 4137.65234375, Entropy -1384.51708984375, Learning Rate: 0.0001953125\n",
      "Epoch [3439/20000], Loss: 4140.24462890625, Entropy -1288.205078125, Learning Rate: 0.0001953125\n",
      "Epoch [3440/20000], Loss: 4098.580078125, Entropy -1217.89990234375, Learning Rate: 0.0001953125\n",
      "Epoch [3441/20000], Loss: 4144.78173828125, Entropy -1305.2705078125, Learning Rate: 0.0001953125\n",
      "Epoch [3442/20000], Loss: 4097.03662109375, Entropy -1237.27099609375, Learning Rate: 0.0001953125\n",
      "Epoch [3443/20000], Loss: 4162.91455078125, Entropy -1317.392578125, Learning Rate: 0.0001953125\n",
      "Epoch [3444/20000], Loss: 4110.328125, Entropy -1225.1015625, Learning Rate: 0.0001953125\n",
      "Epoch [3445/20000], Loss: 4074.99609375, Entropy -1258.00927734375, Learning Rate: 0.0001953125\n",
      "Epoch [3446/20000], Loss: 4126.3388671875, Entropy -1285.8984375, Learning Rate: 0.0001953125\n",
      "Epoch [3447/20000], Loss: 4049.496337890625, Entropy -1282.28466796875, Learning Rate: 0.0001953125\n",
      "Epoch [3448/20000], Loss: 4149.9287109375, Entropy -1255.33935546875, Learning Rate: 0.0001953125\n",
      "Epoch [3449/20000], Loss: 4150.263671875, Entropy -1383.3857421875, Learning Rate: 0.0001953125\n",
      "Epoch [3450/20000], Loss: 4151.60498046875, Entropy -1248.77587890625, Learning Rate: 0.0001953125\n",
      "Epoch [3451/20000], Loss: 4096.400390625, Entropy -1271.693359375, Learning Rate: 0.0001953125\n",
      "Epoch [3452/20000], Loss: 4062.739990234375, Entropy -1157.51416015625, Learning Rate: 0.0001953125\n",
      "Epoch [3453/20000], Loss: 4079.36328125, Entropy -1234.4716796875, Learning Rate: 0.0001953125\n",
      "Epoch [3454/20000], Loss: 4103.6513671875, Entropy -1321.75830078125, Learning Rate: 0.0001953125\n",
      "Epoch [3455/20000], Loss: 4121.42626953125, Entropy -1219.31298828125, Learning Rate: 0.0001953125\n",
      "Epoch [3456/20000], Loss: 4100.2490234375, Entropy -1239.84521484375, Learning Rate: 0.0001953125\n",
      "Epoch [3457/20000], Loss: 4117.46728515625, Entropy -1281.9453125, Learning Rate: 0.0001953125\n",
      "Epoch [3458/20000], Loss: 4102.60888671875, Entropy -1138.70947265625, Learning Rate: 0.0001953125\n",
      "Epoch [3459/20000], Loss: 4073.99365234375, Entropy -1287.423828125, Learning Rate: 0.0001953125\n",
      "Epoch [3460/20000], Loss: 4178.28564453125, Entropy -1276.306640625, Learning Rate: 0.0001953125\n",
      "Epoch [3461/20000], Loss: 4087.247314453125, Entropy -1252.44677734375, Learning Rate: 0.0001953125\n",
      "Epoch [3462/20000], Loss: 4196.55810546875, Entropy -1360.009765625, Learning Rate: 0.0001953125\n",
      "Epoch [3463/20000], Loss: 4175.6591796875, Entropy -1252.48388671875, Learning Rate: 0.0001953125\n",
      "Epoch [3464/20000], Loss: 4096.6103515625, Entropy -1259.2421875, Learning Rate: 0.0001953125\n",
      "Epoch [3465/20000], Loss: 4059.26025390625, Entropy -1225.93603515625, Learning Rate: 0.0001953125\n",
      "Epoch [3466/20000], Loss: 4140.935546875, Entropy -1278.123046875, Learning Rate: 0.0001953125\n",
      "Epoch [3467/20000], Loss: 4116.0107421875, Entropy -1276.33154296875, Learning Rate: 0.0001953125\n",
      "Epoch [3468/20000], Loss: 4076.95849609375, Entropy -1172.90576171875, Learning Rate: 0.0001953125\n",
      "Epoch [3469/20000], Loss: 4100.630859375, Entropy -1281.56591796875, Learning Rate: 0.0001953125\n",
      "Epoch [3470/20000], Loss: 4118.2978515625, Entropy -1346.7021484375, Learning Rate: 0.0001953125\n",
      "Epoch [3471/20000], Loss: 4097.96435546875, Entropy -1249.431640625, Learning Rate: 0.0001953125\n",
      "Epoch [3472/20000], Loss: 4119.46337890625, Entropy -1274.85498046875, Learning Rate: 0.0001953125\n",
      "Epoch [3473/20000], Loss: 4150.96044921875, Entropy -1237.0888671875, Learning Rate: 0.0001953125\n",
      "Epoch [3474/20000], Loss: 4086.42138671875, Entropy -1266.11474609375, Learning Rate: 0.0001953125\n",
      "Epoch [3475/20000], Loss: 4108.25, Entropy -1236.9755859375, Learning Rate: 0.0001953125\n",
      "Epoch [3476/20000], Loss: 4166.83251953125, Entropy -1393.78076171875, Learning Rate: 0.0001953125\n",
      "Epoch [3477/20000], Loss: 4117.5341796875, Entropy -1321.23388671875, Learning Rate: 0.0001953125\n",
      "Epoch [3478/20000], Loss: 4124.17919921875, Entropy -1267.298828125, Learning Rate: 0.0001953125\n",
      "Epoch [3479/20000], Loss: 4109.34130859375, Entropy -1314.18994140625, Learning Rate: 0.0001953125\n",
      "Epoch [3480/20000], Loss: 4096.73828125, Entropy -1300.43994140625, Learning Rate: 0.0001953125\n",
      "Epoch [3481/20000], Loss: 4083.15625, Entropy -1239.1728515625, Learning Rate: 0.0001953125\n",
      "Epoch [3482/20000], Loss: 4200.71533203125, Entropy -1297.29052734375, Learning Rate: 0.0001953125\n",
      "Epoch [3483/20000], Loss: 4125.33203125, Entropy -1188.08935546875, Learning Rate: 0.0001953125\n",
      "Epoch [3484/20000], Loss: 4122.39599609375, Entropy -1336.72119140625, Learning Rate: 0.0001953125\n",
      "Epoch [3485/20000], Loss: 4178.60205078125, Entropy -1309.66552734375, Learning Rate: 0.0001953125\n",
      "Epoch [3486/20000], Loss: 4088.8212890625, Entropy -1235.88330078125, Learning Rate: 0.0001953125\n",
      "Epoch [3487/20000], Loss: 4115.24658203125, Entropy -1208.7646484375, Learning Rate: 0.0001953125\n",
      "Epoch [3488/20000], Loss: 4090.373291015625, Entropy -1178.16748046875, Learning Rate: 0.0001953125\n",
      "Epoch [3489/20000], Loss: 4136.95556640625, Entropy -1339.04931640625, Learning Rate: 0.0001953125\n",
      "Epoch [3490/20000], Loss: 4103.978515625, Entropy -1221.86474609375, Learning Rate: 0.0001953125\n",
      "Epoch [3491/20000], Loss: 4061.759033203125, Entropy -1224.5673828125, Learning Rate: 0.0001953125\n",
      "Epoch [3492/20000], Loss: 4147.1044921875, Entropy -1286.9287109375, Learning Rate: 0.0001953125\n",
      "Epoch [3493/20000], Loss: 4113.44775390625, Entropy -1330.90185546875, Learning Rate: 0.0001953125\n",
      "Epoch [3494/20000], Loss: 4156.8759765625, Entropy -1279.51513671875, Learning Rate: 0.0001953125\n",
      "Epoch [3495/20000], Loss: 4091.0966796875, Entropy -1298.2314453125, Learning Rate: 0.0001953125\n",
      "Epoch [3496/20000], Loss: 4112.76220703125, Entropy -1239.990234375, Learning Rate: 0.0001953125\n",
      "Epoch [3497/20000], Loss: 4156.0908203125, Entropy -1327.75732421875, Learning Rate: 0.0001953125\n",
      "Epoch [3498/20000], Loss: 4084.976318359375, Entropy -1223.33251953125, Learning Rate: 0.0001953125\n",
      "Epoch [3499/20000], Loss: 4187.6044921875, Entropy -1219.7119140625, Learning Rate: 0.0001953125\n",
      "Epoch [3500/20000], Loss: 4135.37353515625, Entropy -1308.27783203125, Learning Rate: 0.0001953125\n",
      "Epoch [3501/20000], Loss: 4061.353759765625, Entropy -1332.19580078125, Learning Rate: 0.0001953125\n",
      "Epoch [3502/20000], Loss: 4099.41845703125, Entropy -1246.63916015625, Learning Rate: 0.0001953125\n",
      "Epoch [3503/20000], Loss: 4109.96630859375, Entropy -1264.26123046875, Learning Rate: 0.0001953125\n",
      "Epoch [3504/20000], Loss: 4041.697509765625, Entropy -1211.21533203125, Learning Rate: 0.0001953125\n",
      "Epoch [3505/20000], Loss: 4117.72265625, Entropy -1207.2021484375, Learning Rate: 0.0001953125\n",
      "Epoch [3506/20000], Loss: 4138.6552734375, Entropy -1311.67529296875, Learning Rate: 0.0001953125\n",
      "Epoch [3507/20000], Loss: 4092.757080078125, Entropy -1249.57177734375, Learning Rate: 0.0001953125\n",
      "Epoch [3508/20000], Loss: 4108.53125, Entropy -1291.98095703125, Learning Rate: 0.0001953125\n",
      "Epoch [3509/20000], Loss: 4139.39306640625, Entropy -1299.29345703125, Learning Rate: 0.0001953125\n",
      "Epoch [3510/20000], Loss: 4063.05517578125, Entropy -1240.71923828125, Learning Rate: 0.0001953125\n",
      "Epoch [3511/20000], Loss: 4112.0869140625, Entropy -1294.99462890625, Learning Rate: 0.0001953125\n",
      "Epoch [3512/20000], Loss: 4123.34423828125, Entropy -1313.17431640625, Learning Rate: 0.0001953125\n",
      "Epoch [3513/20000], Loss: 4115.373046875, Entropy -1305.267578125, Learning Rate: 0.0001953125\n",
      "Epoch [3514/20000], Loss: 4104.62890625, Entropy -1189.65087890625, Learning Rate: 0.0001953125\n",
      "Epoch [3515/20000], Loss: 4096.0546875, Entropy -1312.587890625, Learning Rate: 0.0001953125\n",
      "Epoch [3516/20000], Loss: 4076.403076171875, Entropy -1231.93798828125, Learning Rate: 0.0001953125\n",
      "Epoch [3517/20000], Loss: 4177.59521484375, Entropy -1400.560546875, Learning Rate: 0.0001953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3518/20000], Loss: 4122.30615234375, Entropy -1358.56884765625, Learning Rate: 0.0001953125\n",
      "Epoch [3519/20000], Loss: 4072.994140625, Entropy -1304.00146484375, Learning Rate: 0.0001953125\n",
      "Epoch [3520/20000], Loss: 4175.6904296875, Entropy -1289.48583984375, Learning Rate: 0.0001953125\n",
      "Epoch [3521/20000], Loss: 4127.34375, Entropy -1185.21630859375, Learning Rate: 0.0001953125\n",
      "Epoch [3522/20000], Loss: 4106.150390625, Entropy -1280.6123046875, Learning Rate: 0.0001953125\n",
      "Epoch [3523/20000], Loss: 4135.537109375, Entropy -1271.47509765625, Learning Rate: 0.0001953125\n",
      "Epoch [3524/20000], Loss: 4084.2294921875, Entropy -1289.05419921875, Learning Rate: 0.0001953125\n",
      "Epoch [3525/20000], Loss: 4174.03173828125, Entropy -1359.44091796875, Learning Rate: 0.0001953125\n",
      "Epoch [3526/20000], Loss: 4198.4482421875, Entropy -1454.34619140625, Learning Rate: 0.0001953125\n",
      "Epoch [3527/20000], Loss: 4159.8203125, Entropy -1370.4736328125, Learning Rate: 0.0001953125\n",
      "Epoch [3528/20000], Loss: 4142.50830078125, Entropy -1269.94482421875, Learning Rate: 0.0001953125\n",
      "Epoch [3529/20000], Loss: 4064.7734375, Entropy -1213.44873046875, Learning Rate: 0.0001953125\n",
      "Epoch [3530/20000], Loss: 4056.338134765625, Entropy -1247.56005859375, Learning Rate: 0.0001953125\n",
      "Epoch [3531/20000], Loss: 4080.3193359375, Entropy -1279.65380859375, Learning Rate: 0.0001953125\n",
      "Epoch [3532/20000], Loss: 4146.919921875, Entropy -1413.4169921875, Learning Rate: 0.0001953125\n",
      "Epoch [3533/20000], Loss: 4124.33154296875, Entropy -1302.81103515625, Learning Rate: 0.0001953125\n",
      "Epoch [3534/20000], Loss: 4107.8876953125, Entropy -1261.44775390625, Learning Rate: 0.0001953125\n",
      "Epoch [3535/20000], Loss: 4137.02587890625, Entropy -1210.939453125, Learning Rate: 0.0001953125\n",
      "Epoch [3536/20000], Loss: 4184.9365234375, Entropy -1316.56298828125, Learning Rate: 0.0001953125\n",
      "Epoch [3537/20000], Loss: 4084.9599609375, Entropy -1297.52978515625, Learning Rate: 0.0001953125\n",
      "Epoch [3538/20000], Loss: 4128.66845703125, Entropy -1290.46630859375, Learning Rate: 0.0001953125\n",
      "Epoch [3539/20000], Loss: 4149.06689453125, Entropy -1312.0888671875, Learning Rate: 0.0001953125\n",
      "Epoch [3540/20000], Loss: 4154.87548828125, Entropy -1461.60498046875, Learning Rate: 0.0001953125\n",
      "Epoch [3541/20000], Loss: 4109.66943359375, Entropy -1181.40283203125, Learning Rate: 0.0001953125\n",
      "Epoch [3542/20000], Loss: 4136.2626953125, Entropy -1238.12890625, Learning Rate: 0.0001953125\n",
      "Epoch [3543/20000], Loss: 4145.35302734375, Entropy -1242.77197265625, Learning Rate: 0.0001953125\n",
      "Epoch [3544/20000], Loss: 4105.29443359375, Entropy -1288.78662109375, Learning Rate: 0.0001953125\n",
      "Epoch [3545/20000], Loss: 4071.8427734375, Entropy -1261.76123046875, Learning Rate: 0.0001953125\n",
      "Epoch [3546/20000], Loss: 4105.97216796875, Entropy -1242.18212890625, Learning Rate: 0.0001953125\n",
      "Epoch [3547/20000], Loss: 4106.740234375, Entropy -1293.58349609375, Learning Rate: 0.0001953125\n",
      "Epoch [3548/20000], Loss: 4163.9345703125, Entropy -1364.82177734375, Learning Rate: 0.0001953125\n",
      "Epoch [3549/20000], Loss: 4148.30029296875, Entropy -1404.00537109375, Learning Rate: 0.0001953125\n",
      "Epoch [3550/20000], Loss: 4058.70703125, Entropy -1210.98095703125, Learning Rate: 0.0001953125\n",
      "Epoch [3551/20000], Loss: 4098.0673828125, Entropy -1195.63330078125, Learning Rate: 0.0001953125\n",
      "Epoch [3552/20000], Loss: 4116.2880859375, Entropy -1196.0341796875, Learning Rate: 0.0001953125\n",
      "Epoch [3553/20000], Loss: 4111.01171875, Entropy -1220.13916015625, Learning Rate: 0.0001953125\n",
      "Epoch [3554/20000], Loss: 4125.353515625, Entropy -1372.65673828125, Learning Rate: 0.0001953125\n",
      "Epoch [3555/20000], Loss: 4114.31494140625, Entropy -1280.19921875, Learning Rate: 0.0001953125\n",
      "Epoch [3556/20000], Loss: 4068.798095703125, Entropy -1247.23291015625, Learning Rate: 0.0001953125\n",
      "Epoch [3557/20000], Loss: 4148.474609375, Entropy -1346.13916015625, Learning Rate: 0.0001953125\n",
      "Epoch [3558/20000], Loss: 4108.40283203125, Entropy -1290.568359375, Learning Rate: 0.0001953125\n",
      "Epoch [3559/20000], Loss: 4103.1083984375, Entropy -1243.09423828125, Learning Rate: 0.0001953125\n",
      "Epoch [3560/20000], Loss: 4082.5712890625, Entropy -1256.6376953125, Learning Rate: 0.0001953125\n",
      "Epoch [3561/20000], Loss: 4152.095703125, Entropy -1386.01123046875, Learning Rate: 0.0001953125\n",
      "Epoch [3562/20000], Loss: 4097.6728515625, Entropy -1215.9521484375, Learning Rate: 0.0001953125\n",
      "Epoch [3563/20000], Loss: 4102.11328125, Entropy -1282.83837890625, Learning Rate: 0.0001953125\n",
      "Epoch [3564/20000], Loss: 4113.3388671875, Entropy -1248.00537109375, Learning Rate: 0.0001953125\n",
      "Epoch [3565/20000], Loss: 4080.646240234375, Entropy -1223.3798828125, Learning Rate: 0.0001953125\n",
      "Epoch [3566/20000], Loss: 4081.90478515625, Entropy -1146.892578125, Learning Rate: 0.0001953125\n",
      "Epoch [3567/20000], Loss: 4031.657470703125, Entropy -1155.8056640625, Learning Rate: 0.0001953125\n",
      "Epoch [3568/20000], Loss: 4049.517333984375, Entropy -1273.12939453125, Learning Rate: 0.0001953125\n",
      "Epoch [3569/20000], Loss: 4100.73046875, Entropy -1243.94677734375, Learning Rate: 0.0001953125\n",
      "Epoch [3570/20000], Loss: 4118.62255859375, Entropy -1296.43115234375, Learning Rate: 0.0001953125\n",
      "Epoch [3571/20000], Loss: 4101.23828125, Entropy -1201.7744140625, Learning Rate: 0.0001953125\n",
      "Epoch [3572/20000], Loss: 4127.74560546875, Entropy -1270.98193359375, Learning Rate: 0.0001953125\n",
      "Epoch [3573/20000], Loss: 4117.943359375, Entropy -1272.0849609375, Learning Rate: 0.0001953125\n",
      "Epoch [3574/20000], Loss: 4059.141845703125, Entropy -1198.45361328125, Learning Rate: 0.0001953125\n",
      "Epoch [3575/20000], Loss: 4116.6494140625, Entropy -1305.72265625, Learning Rate: 0.0001953125\n",
      "Epoch [3576/20000], Loss: 4120.37646484375, Entropy -1359.14990234375, Learning Rate: 0.0001953125\n",
      "Epoch [3577/20000], Loss: 4141.14404296875, Entropy -1319.994140625, Learning Rate: 0.0001953125\n",
      "Epoch [3578/20000], Loss: 4149.0791015625, Entropy -1360.98388671875, Learning Rate: 0.0001953125\n",
      "Epoch [3579/20000], Loss: 4121.93798828125, Entropy -1331.13134765625, Learning Rate: 0.0001953125\n",
      "Epoch [3580/20000], Loss: 4128.9599609375, Entropy -1332.43798828125, Learning Rate: 0.0001953125\n",
      "Epoch [3581/20000], Loss: 4156.51953125, Entropy -1261.65185546875, Learning Rate: 0.0001953125\n",
      "Epoch [3582/20000], Loss: 4112.30078125, Entropy -1242.68310546875, Learning Rate: 0.0001953125\n",
      "Epoch [3583/20000], Loss: 4063.247802734375, Entropy -1237.53125, Learning Rate: 0.0001953125\n",
      "Epoch [3584/20000], Loss: 4145.1630859375, Entropy -1297.8466796875, Learning Rate: 0.0001953125\n",
      "Epoch [3585/20000], Loss: 4121.27197265625, Entropy -1258.50927734375, Learning Rate: 0.0001953125\n",
      "Epoch [3586/20000], Loss: 4077.4912109375, Entropy -1281.0390625, Learning Rate: 0.0001953125\n",
      "Epoch [3587/20000], Loss: 4167.21533203125, Entropy -1274.4677734375, Learning Rate: 0.0001953125\n",
      "Epoch [3588/20000], Loss: 4064.3603515625, Entropy -1219.63818359375, Learning Rate: 0.0001953125\n",
      "Epoch [3589/20000], Loss: 4151.580078125, Entropy -1265.6875, Learning Rate: 0.0001953125\n",
      "Epoch [3590/20000], Loss: 4085.862060546875, Entropy -1215.17626953125, Learning Rate: 0.0001953125\n",
      "Epoch [3591/20000], Loss: 4145.95166015625, Entropy -1374.0029296875, Learning Rate: 0.0001953125\n",
      "Epoch [3592/20000], Loss: 4112.72900390625, Entropy -1304.16748046875, Learning Rate: 0.0001953125\n",
      "Epoch [3593/20000], Loss: 4130.58154296875, Entropy -1273.24267578125, Learning Rate: 0.0001953125\n",
      "Epoch [3594/20000], Loss: 4070.822509765625, Entropy -1264.92236328125, Learning Rate: 0.0001953125\n",
      "Epoch [3595/20000], Loss: 4186.98291015625, Entropy -1323.322265625, Learning Rate: 0.0001953125\n",
      "Epoch [3596/20000], Loss: 4203.5615234375, Entropy -1354.89794921875, Learning Rate: 0.0001953125\n",
      "Epoch [3597/20000], Loss: 4062.445556640625, Entropy -1272.16943359375, Learning Rate: 0.0001953125\n",
      "Epoch [3598/20000], Loss: 4069.992919921875, Entropy -1295.375, Learning Rate: 0.0001953125\n",
      "Epoch [3599/20000], Loss: 4093.172119140625, Entropy -1164.1376953125, Learning Rate: 0.0001953125\n",
      "Epoch [3600/20000], Loss: 4047.51513671875, Entropy -1222.49560546875, Learning Rate: 0.0001953125\n",
      "Epoch [3601/20000], Loss: 4171.39306640625, Entropy -1261.42236328125, Learning Rate: 0.0001953125\n",
      "Epoch [3602/20000], Loss: 4169.400390625, Entropy -1370.31982421875, Learning Rate: 0.0001953125\n",
      "Epoch [3603/20000], Loss: 4147.1298828125, Entropy -1122.068359375, Learning Rate: 0.0001953125\n",
      "Epoch [3604/20000], Loss: 4131.74609375, Entropy -1204.22802734375, Learning Rate: 0.0001953125\n",
      "Epoch [3605/20000], Loss: 4111.4990234375, Entropy -1214.71875, Learning Rate: 0.0001953125\n",
      "Epoch [3606/20000], Loss: 4106.8193359375, Entropy -1257.8330078125, Learning Rate: 0.0001953125\n",
      "Epoch [3607/20000], Loss: 4152.1201171875, Entropy -1274.48095703125, Learning Rate: 0.0001953125\n",
      "Epoch [3608/20000], Loss: 4144.70751953125, Entropy -1340.45947265625, Learning Rate: 0.0001953125\n",
      "Epoch [3609/20000], Loss: 4091.34912109375, Entropy -1315.52490234375, Learning Rate: 9.765625e-05\n",
      "Epoch [3610/20000], Loss: 4085.961669921875, Entropy -1244.20556640625, Learning Rate: 9.765625e-05\n",
      "Epoch [3611/20000], Loss: 4026.86572265625, Entropy -1161.34912109375, Learning Rate: 9.765625e-05\n",
      "Epoch [3612/20000], Loss: 4093.330078125, Entropy -1299.58203125, Learning Rate: 9.765625e-05\n",
      "Epoch [3613/20000], Loss: 4218.91259765625, Entropy -1233.9580078125, Learning Rate: 9.765625e-05\n",
      "Epoch [3614/20000], Loss: 4150.73486328125, Entropy -1129.82666015625, Learning Rate: 9.765625e-05\n",
      "Epoch [3615/20000], Loss: 4181.30517578125, Entropy -1451.01611328125, Learning Rate: 9.765625e-05\n",
      "Epoch [3616/20000], Loss: 4100.47216796875, Entropy -1340.98193359375, Learning Rate: 9.765625e-05\n",
      "Epoch [3617/20000], Loss: 4050.551513671875, Entropy -1241.9873046875, Learning Rate: 9.765625e-05\n",
      "Epoch [3618/20000], Loss: 4126.43359375, Entropy -1295.26806640625, Learning Rate: 9.765625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3619/20000], Loss: 4094.220703125, Entropy -1236.37548828125, Learning Rate: 9.765625e-05\n",
      "Epoch [3620/20000], Loss: 4119.57666015625, Entropy -1262.76513671875, Learning Rate: 9.765625e-05\n",
      "Epoch [3621/20000], Loss: 4128.95166015625, Entropy -1284.43408203125, Learning Rate: 9.765625e-05\n",
      "Epoch [3622/20000], Loss: 4070.636962890625, Entropy -1239.87939453125, Learning Rate: 9.765625e-05\n",
      "Epoch [3623/20000], Loss: 4101.40380859375, Entropy -1226.6162109375, Learning Rate: 9.765625e-05\n",
      "Epoch [3624/20000], Loss: 4144.81396484375, Entropy -1313.60791015625, Learning Rate: 9.765625e-05\n",
      "Epoch [3625/20000], Loss: 4117.681640625, Entropy -1256.96435546875, Learning Rate: 9.765625e-05\n",
      "Epoch [3626/20000], Loss: 4122.78564453125, Entropy -1349.26953125, Learning Rate: 9.765625e-05\n",
      "Epoch [3627/20000], Loss: 4154.23291015625, Entropy -1353.59130859375, Learning Rate: 9.765625e-05\n",
      "Epoch [3628/20000], Loss: 4138.0322265625, Entropy -1182.6943359375, Learning Rate: 9.765625e-05\n",
      "Epoch [3629/20000], Loss: 4138.4326171875, Entropy -1152.27685546875, Learning Rate: 9.765625e-05\n",
      "Epoch [3630/20000], Loss: 4199.9365234375, Entropy -1371.23095703125, Learning Rate: 9.765625e-05\n",
      "Epoch [3631/20000], Loss: 4089.376953125, Entropy -1256.93017578125, Learning Rate: 9.765625e-05\n",
      "Epoch [3632/20000], Loss: 4104.2265625, Entropy -1162.380859375, Learning Rate: 9.765625e-05\n",
      "Epoch [3633/20000], Loss: 4108.8603515625, Entropy -1267.75732421875, Learning Rate: 9.765625e-05\n",
      "Epoch [3634/20000], Loss: 4141.21337890625, Entropy -1224.14599609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3635/20000], Loss: 4096.56103515625, Entropy -1301.576171875, Learning Rate: 9.765625e-05\n",
      "Epoch [3636/20000], Loss: 4109.71728515625, Entropy -1252.2666015625, Learning Rate: 9.765625e-05\n",
      "Epoch [3637/20000], Loss: 4100.5244140625, Entropy -1220.4384765625, Learning Rate: 9.765625e-05\n",
      "Epoch [3638/20000], Loss: 4110.48291015625, Entropy -1362.83740234375, Learning Rate: 9.765625e-05\n",
      "Epoch [3639/20000], Loss: 4086.3232421875, Entropy -1151.5771484375, Learning Rate: 9.765625e-05\n",
      "Epoch [3640/20000], Loss: 4103.4091796875, Entropy -1291.78369140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3641/20000], Loss: 4081.771728515625, Entropy -1277.5771484375, Learning Rate: 9.765625e-05\n",
      "Epoch [3642/20000], Loss: 4125.802734375, Entropy -1299.23486328125, Learning Rate: 9.765625e-05\n",
      "Epoch [3643/20000], Loss: 4114.55126953125, Entropy -1220.46044921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3644/20000], Loss: 4147.8671875, Entropy -1349.26904296875, Learning Rate: 9.765625e-05\n",
      "Epoch [3645/20000], Loss: 4098.29541015625, Entropy -1240.25439453125, Learning Rate: 9.765625e-05\n",
      "Epoch [3646/20000], Loss: 4174.2275390625, Entropy -1320.70703125, Learning Rate: 9.765625e-05\n",
      "Epoch [3647/20000], Loss: 4096.77685546875, Entropy -1276.16845703125, Learning Rate: 9.765625e-05\n",
      "Epoch [3648/20000], Loss: 4062.385498046875, Entropy -1250.99609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3649/20000], Loss: 4083.24072265625, Entropy -1234.1650390625, Learning Rate: 9.765625e-05\n",
      "Epoch [3650/20000], Loss: 4110.3662109375, Entropy -1298.98974609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3651/20000], Loss: 4116.75244140625, Entropy -1336.994140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3652/20000], Loss: 4111.2587890625, Entropy -1300.13427734375, Learning Rate: 9.765625e-05\n",
      "Epoch [3653/20000], Loss: 4146.28564453125, Entropy -1315.18701171875, Learning Rate: 9.765625e-05\n",
      "Epoch [3654/20000], Loss: 4125.12744140625, Entropy -1335.32666015625, Learning Rate: 9.765625e-05\n",
      "Epoch [3655/20000], Loss: 4135.3037109375, Entropy -1265.0185546875, Learning Rate: 9.765625e-05\n",
      "Epoch [3656/20000], Loss: 4088.071533203125, Entropy -1307.44189453125, Learning Rate: 9.765625e-05\n",
      "Epoch [3657/20000], Loss: 4097.51025390625, Entropy -1243.4931640625, Learning Rate: 9.765625e-05\n",
      "Epoch [3658/20000], Loss: 4082.481689453125, Entropy -1246.513671875, Learning Rate: 9.765625e-05\n",
      "Epoch [3659/20000], Loss: 4075.351318359375, Entropy -1266.57568359375, Learning Rate: 9.765625e-05\n",
      "Epoch [3660/20000], Loss: 4150.31396484375, Entropy -1400.162841796875, Learning Rate: 9.765625e-05\n",
      "Epoch [3661/20000], Loss: 4266.20947265625, Entropy -1310.37109375, Learning Rate: 9.765625e-05\n",
      "Epoch [3662/20000], Loss: 4145.33056640625, Entropy -1202.03369140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3663/20000], Loss: 4093.7685546875, Entropy -1262.0751953125, Learning Rate: 9.765625e-05\n",
      "Epoch [3664/20000], Loss: 4180.23291015625, Entropy -1221.900390625, Learning Rate: 9.765625e-05\n",
      "Epoch [3665/20000], Loss: 4134.58203125, Entropy -1242.56689453125, Learning Rate: 9.765625e-05\n",
      "Epoch [3666/20000], Loss: 4178.00732421875, Entropy -1309.78466796875, Learning Rate: 9.765625e-05\n",
      "Epoch [3667/20000], Loss: 4142.62109375, Entropy -1314.83544921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3668/20000], Loss: 4130.62646484375, Entropy -1259.76220703125, Learning Rate: 9.765625e-05\n",
      "Epoch [3669/20000], Loss: 4097.576171875, Entropy -1226.72607421875, Learning Rate: 9.765625e-05\n",
      "Epoch [3670/20000], Loss: 4088.429931640625, Entropy -1205.4951171875, Learning Rate: 9.765625e-05\n",
      "Epoch [3671/20000], Loss: 4123.998046875, Entropy -1307.28759765625, Learning Rate: 9.765625e-05\n",
      "Epoch [3672/20000], Loss: 4094.41845703125, Entropy -1296.203125, Learning Rate: 9.765625e-05\n",
      "Epoch [3673/20000], Loss: 4155.576171875, Entropy -1205.765625, Learning Rate: 9.765625e-05\n",
      "Epoch [3674/20000], Loss: 4117.06982421875, Entropy -1290.52978515625, Learning Rate: 9.765625e-05\n",
      "Epoch [3675/20000], Loss: 4118.51611328125, Entropy -1260.96728515625, Learning Rate: 9.765625e-05\n",
      "Epoch [3676/20000], Loss: 4093.76171875, Entropy -1266.37548828125, Learning Rate: 9.765625e-05\n",
      "Epoch [3677/20000], Loss: 4129.873046875, Entropy -1195.7646484375, Learning Rate: 9.765625e-05\n",
      "Epoch [3678/20000], Loss: 4161.29541015625, Entropy -1283.478515625, Learning Rate: 9.765625e-05\n",
      "Epoch [3679/20000], Loss: 4120.19921875, Entropy -1246.29443359375, Learning Rate: 9.765625e-05\n",
      "Epoch [3680/20000], Loss: 4189.5302734375, Entropy -1315.6591796875, Learning Rate: 9.765625e-05\n",
      "Epoch [3681/20000], Loss: 4120.341796875, Entropy -1284.52001953125, Learning Rate: 9.765625e-05\n",
      "Epoch [3682/20000], Loss: 4151.822265625, Entropy -1153.74169921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3683/20000], Loss: 4082.369140625, Entropy -1267.0439453125, Learning Rate: 9.765625e-05\n",
      "Epoch [3684/20000], Loss: 4122.68408203125, Entropy -1289.9365234375, Learning Rate: 9.765625e-05\n",
      "Epoch [3685/20000], Loss: 4170.392578125, Entropy -1302.07666015625, Learning Rate: 9.765625e-05\n",
      "Epoch [3686/20000], Loss: 4093.4453125, Entropy -1146.1591796875, Learning Rate: 9.765625e-05\n",
      "Epoch [3687/20000], Loss: 4145.4248046875, Entropy -1291.69287109375, Learning Rate: 9.765625e-05\n",
      "Epoch [3688/20000], Loss: 4118.58837890625, Entropy -1277.91796875, Learning Rate: 9.765625e-05\n",
      "Epoch [3689/20000], Loss: 4133.11669921875, Entropy -1288.578125, Learning Rate: 9.765625e-05\n",
      "Epoch [3690/20000], Loss: 4179.8828125, Entropy -1347.68603515625, Learning Rate: 9.765625e-05\n",
      "Epoch [3691/20000], Loss: 4231.1455078125, Entropy -1329.9853515625, Learning Rate: 9.765625e-05\n",
      "Epoch [3692/20000], Loss: 4145.29443359375, Entropy -1223.69140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3693/20000], Loss: 4145.12060546875, Entropy -1304.482421875, Learning Rate: 9.765625e-05\n",
      "Epoch [3694/20000], Loss: 4093.282958984375, Entropy -1199.36474609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3695/20000], Loss: 4158.84521484375, Entropy -1359.38330078125, Learning Rate: 9.765625e-05\n",
      "Epoch [3696/20000], Loss: 4048.378173828125, Entropy -1280.08447265625, Learning Rate: 9.765625e-05\n",
      "Epoch [3697/20000], Loss: 4170.091796875, Entropy -1346.33544921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3698/20000], Loss: 4120.76513671875, Entropy -1336.59912109375, Learning Rate: 9.765625e-05\n",
      "Epoch [3699/20000], Loss: 4103.00341796875, Entropy -1221.87744140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3700/20000], Loss: 4112.9453125, Entropy -1284.26904296875, Learning Rate: 9.765625e-05\n",
      "Epoch [3701/20000], Loss: 4135.875, Entropy -1311.849609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3702/20000], Loss: 4123.88916015625, Entropy -1295.37060546875, Learning Rate: 9.765625e-05\n",
      "Epoch [3703/20000], Loss: 4065.70263671875, Entropy -1219.82080078125, Learning Rate: 9.765625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3704/20000], Loss: 4148.62109375, Entropy -1253.3583984375, Learning Rate: 9.765625e-05\n",
      "Epoch [3705/20000], Loss: 4102.22216796875, Entropy -1294.40185546875, Learning Rate: 9.765625e-05\n",
      "Epoch [3706/20000], Loss: 4144.19580078125, Entropy -1327.12451171875, Learning Rate: 9.765625e-05\n",
      "Epoch [3707/20000], Loss: 4139.58544921875, Entropy -1324.10009765625, Learning Rate: 9.765625e-05\n",
      "Epoch [3708/20000], Loss: 4110.841796875, Entropy -1323.63525390625, Learning Rate: 9.765625e-05\n",
      "Epoch [3709/20000], Loss: 4111.56201171875, Entropy -1289.95849609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3710/20000], Loss: 4147.96142578125, Entropy -1350.80419921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3711/20000], Loss: 4172.9169921875, Entropy -1403.227294921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3712/20000], Loss: 4178.3984375, Entropy -1396.115478515625, Learning Rate: 9.765625e-05\n",
      "Epoch [3713/20000], Loss: 4136.1123046875, Entropy -1191.9326171875, Learning Rate: 9.765625e-05\n",
      "Epoch [3714/20000], Loss: 4109.00439453125, Entropy -1288.39404296875, Learning Rate: 9.765625e-05\n",
      "Epoch [3715/20000], Loss: 4085.613525390625, Entropy -1249.19677734375, Learning Rate: 9.765625e-05\n",
      "Epoch [3716/20000], Loss: 4235.00537109375, Entropy -1301.94580078125, Learning Rate: 9.765625e-05\n",
      "Epoch [3717/20000], Loss: 4184.26513671875, Entropy -1397.119140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3718/20000], Loss: 4071.46826171875, Entropy -1157.904296875, Learning Rate: 9.765625e-05\n",
      "Epoch [3719/20000], Loss: 4121.72216796875, Entropy -1277.875, Learning Rate: 9.765625e-05\n",
      "Epoch [3720/20000], Loss: 4090.9794921875, Entropy -1282.69677734375, Learning Rate: 9.765625e-05\n",
      "Epoch [3721/20000], Loss: 4101.22998046875, Entropy -1299.169921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3722/20000], Loss: 4174.9248046875, Entropy -1360.75244140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3723/20000], Loss: 4106.28271484375, Entropy -1321.9140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3724/20000], Loss: 4150.35498046875, Entropy -1290.95458984375, Learning Rate: 9.765625e-05\n",
      "Epoch [3725/20000], Loss: 4109.10888671875, Entropy -1294.75390625, Learning Rate: 9.765625e-05\n",
      "Epoch [3726/20000], Loss: 4050.251708984375, Entropy -1207.9111328125, Learning Rate: 9.765625e-05\n",
      "Epoch [3727/20000], Loss: 4152.6572265625, Entropy -1369.67822265625, Learning Rate: 9.765625e-05\n",
      "Epoch [3728/20000], Loss: 4066.871337890625, Entropy -1185.29638671875, Learning Rate: 9.765625e-05\n",
      "Epoch [3729/20000], Loss: 4066.8232421875, Entropy -1187.9970703125, Learning Rate: 9.765625e-05\n",
      "Epoch [3730/20000], Loss: 4124.6943359375, Entropy -1292.5751953125, Learning Rate: 9.765625e-05\n",
      "Epoch [3731/20000], Loss: 4135.3232421875, Entropy -1251.47412109375, Learning Rate: 9.765625e-05\n",
      "Epoch [3732/20000], Loss: 4097.41796875, Entropy -1269.06396484375, Learning Rate: 9.765625e-05\n",
      "Epoch [3733/20000], Loss: 4134.9052734375, Entropy -1182.32373046875, Learning Rate: 9.765625e-05\n",
      "Epoch [3734/20000], Loss: 4147.35009765625, Entropy -1402.67724609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3735/20000], Loss: 4085.706787109375, Entropy -1303.56396484375, Learning Rate: 9.765625e-05\n",
      "Epoch [3736/20000], Loss: 4119.61962890625, Entropy -1289.09912109375, Learning Rate: 9.765625e-05\n",
      "Epoch [3737/20000], Loss: 4116.47509765625, Entropy -1304.8046875, Learning Rate: 9.765625e-05\n",
      "Epoch [3738/20000], Loss: 4116.26611328125, Entropy -1323.97509765625, Learning Rate: 9.765625e-05\n",
      "Epoch [3739/20000], Loss: 4072.3271484375, Entropy -1282.4033203125, Learning Rate: 9.765625e-05\n",
      "Epoch [3740/20000], Loss: 4151.09423828125, Entropy -1286.12841796875, Learning Rate: 9.765625e-05\n",
      "Epoch [3741/20000], Loss: 4112.23095703125, Entropy -1265.615234375, Learning Rate: 9.765625e-05\n",
      "Epoch [3742/20000], Loss: 4208.8203125, Entropy -1440.027587890625, Learning Rate: 9.765625e-05\n",
      "Epoch [3743/20000], Loss: 4146.78369140625, Entropy -1315.6708984375, Learning Rate: 9.765625e-05\n",
      "Epoch [3744/20000], Loss: 4141.19140625, Entropy -1352.89697265625, Learning Rate: 9.765625e-05\n",
      "Epoch [3745/20000], Loss: 4221.126953125, Entropy -1254.16552734375, Learning Rate: 9.765625e-05\n",
      "Epoch [3746/20000], Loss: 4159.01123046875, Entropy -1213.17431640625, Learning Rate: 9.765625e-05\n",
      "Epoch [3747/20000], Loss: 4125.67919921875, Entropy -1314.8193359375, Learning Rate: 9.765625e-05\n",
      "Epoch [3748/20000], Loss: 4070.043212890625, Entropy -1158.87890625, Learning Rate: 9.765625e-05\n",
      "Epoch [3749/20000], Loss: 4142.474609375, Entropy -1343.9013671875, Learning Rate: 9.765625e-05\n",
      "Epoch [3750/20000], Loss: 4171.701171875, Entropy -1423.1650390625, Learning Rate: 9.765625e-05\n",
      "Epoch [3751/20000], Loss: 4135.79345703125, Entropy -1237.98974609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3752/20000], Loss: 4077.741455078125, Entropy -1293.65869140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3753/20000], Loss: 4118.56689453125, Entropy -1245.1875, Learning Rate: 9.765625e-05\n",
      "Epoch [3754/20000], Loss: 4132.5029296875, Entropy -1333.18408203125, Learning Rate: 9.765625e-05\n",
      "Epoch [3755/20000], Loss: 4123.38818359375, Entropy -1276.99609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3756/20000], Loss: 4136.20703125, Entropy -1284.76123046875, Learning Rate: 9.765625e-05\n",
      "Epoch [3757/20000], Loss: 4135.0546875, Entropy -1318.03076171875, Learning Rate: 9.765625e-05\n",
      "Epoch [3758/20000], Loss: 4064.05908203125, Entropy -1257.25146484375, Learning Rate: 9.765625e-05\n",
      "Epoch [3759/20000], Loss: 4141.380859375, Entropy -1290.154296875, Learning Rate: 9.765625e-05\n",
      "Epoch [3760/20000], Loss: 4068.065185546875, Entropy -1229.6865234375, Learning Rate: 9.765625e-05\n",
      "Epoch [3761/20000], Loss: 4158.5576171875, Entropy -1298.57568359375, Learning Rate: 9.765625e-05\n",
      "Epoch [3762/20000], Loss: 4031.20458984375, Entropy -1224.47216796875, Learning Rate: 9.765625e-05\n",
      "Epoch [3763/20000], Loss: 4176.66796875, Entropy -1427.87548828125, Learning Rate: 9.765625e-05\n",
      "Epoch [3764/20000], Loss: 4146.7353515625, Entropy -1224.169921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3765/20000], Loss: 4180.533203125, Entropy -1223.42431640625, Learning Rate: 9.765625e-05\n",
      "Epoch [3766/20000], Loss: 4119.0478515625, Entropy -1267.89892578125, Learning Rate: 9.765625e-05\n",
      "Epoch [3767/20000], Loss: 4127.310546875, Entropy -1390.82958984375, Learning Rate: 9.765625e-05\n",
      "Epoch [3768/20000], Loss: 4111.36572265625, Entropy -1265.32080078125, Learning Rate: 9.765625e-05\n",
      "Epoch [3769/20000], Loss: 4126.71240234375, Entropy -1278.22021484375, Learning Rate: 9.765625e-05\n",
      "Epoch [3770/20000], Loss: 4082.7900390625, Entropy -1349.35107421875, Learning Rate: 9.765625e-05\n",
      "Epoch [3771/20000], Loss: 4082.501220703125, Entropy -1257.44140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3772/20000], Loss: 4108.81298828125, Entropy -1230.25927734375, Learning Rate: 9.765625e-05\n",
      "Epoch [3773/20000], Loss: 4083.967529296875, Entropy -1264.44482421875, Learning Rate: 9.765625e-05\n",
      "Epoch [3774/20000], Loss: 4102.12939453125, Entropy -1219.79248046875, Learning Rate: 9.765625e-05\n",
      "Epoch [3775/20000], Loss: 4162.365234375, Entropy -1259.58837890625, Learning Rate: 9.765625e-05\n",
      "Epoch [3776/20000], Loss: 4106.41259765625, Entropy -1275.6787109375, Learning Rate: 9.765625e-05\n",
      "Epoch [3777/20000], Loss: 4139.53662109375, Entropy -1339.43408203125, Learning Rate: 9.765625e-05\n",
      "Epoch [3778/20000], Loss: 4207.94287109375, Entropy -1387.61083984375, Learning Rate: 9.765625e-05\n",
      "Epoch [3779/20000], Loss: 4136.77880859375, Entropy -1260.70068359375, Learning Rate: 9.765625e-05\n",
      "Epoch [3780/20000], Loss: 4078.120361328125, Entropy -1235.0625, Learning Rate: 9.765625e-05\n",
      "Epoch [3781/20000], Loss: 4132.06689453125, Entropy -1291.72900390625, Learning Rate: 9.765625e-05\n",
      "Epoch [3782/20000], Loss: 4082.9345703125, Entropy -1258.92333984375, Learning Rate: 9.765625e-05\n",
      "Epoch [3783/20000], Loss: 4130.86865234375, Entropy -1198.79931640625, Learning Rate: 9.765625e-05\n",
      "Epoch [3784/20000], Loss: 4086.546142578125, Entropy -1235.580078125, Learning Rate: 9.765625e-05\n",
      "Epoch [3785/20000], Loss: 4141.111328125, Entropy -1270.21533203125, Learning Rate: 9.765625e-05\n",
      "Epoch [3786/20000], Loss: 4190.4775390625, Entropy -1260.8544921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3787/20000], Loss: 4080.166015625, Entropy -1203.6064453125, Learning Rate: 9.765625e-05\n",
      "Epoch [3788/20000], Loss: 4159.16015625, Entropy -1291.33251953125, Learning Rate: 9.765625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3789/20000], Loss: 4103.2666015625, Entropy -1183.57958984375, Learning Rate: 9.765625e-05\n",
      "Epoch [3790/20000], Loss: 4147.47998046875, Entropy -1220.61181640625, Learning Rate: 9.765625e-05\n",
      "Epoch [3791/20000], Loss: 4130.359375, Entropy -1251.361328125, Learning Rate: 9.765625e-05\n",
      "Epoch [3792/20000], Loss: 4144.2099609375, Entropy -1392.361572265625, Learning Rate: 9.765625e-05\n",
      "Epoch [3793/20000], Loss: 4064.44970703125, Entropy -1223.53466796875, Learning Rate: 9.765625e-05\n",
      "Epoch [3794/20000], Loss: 4086.0966796875, Entropy -1177.99267578125, Learning Rate: 9.765625e-05\n",
      "Epoch [3795/20000], Loss: 4087.0390625, Entropy -1238.974609375, Learning Rate: 9.765625e-05\n",
      "Epoch [3796/20000], Loss: 4116.81005859375, Entropy -1258.6376953125, Learning Rate: 9.765625e-05\n",
      "Epoch [3797/20000], Loss: 4108.2109375, Entropy -1309.46826171875, Learning Rate: 9.765625e-05\n",
      "Epoch [3798/20000], Loss: 4095.073974609375, Entropy -1350.44140625, Learning Rate: 9.765625e-05\n",
      "Epoch [3799/20000], Loss: 4102.34619140625, Entropy -1182.232421875, Learning Rate: 9.765625e-05\n",
      "Epoch [3800/20000], Loss: 4125.03955078125, Entropy -1322.853515625, Learning Rate: 9.765625e-05\n",
      "Epoch [3801/20000], Loss: 4129.48583984375, Entropy -1249.68017578125, Learning Rate: 9.765625e-05\n",
      "Epoch [3802/20000], Loss: 4113.525390625, Entropy -1239.9873046875, Learning Rate: 9.765625e-05\n",
      "Epoch [3803/20000], Loss: 4168.9931640625, Entropy -1290.89990234375, Learning Rate: 9.765625e-05\n",
      "Epoch [3804/20000], Loss: 4140.42724609375, Entropy -1391.48486328125, Learning Rate: 9.765625e-05\n",
      "Epoch [3805/20000], Loss: 4090.8798828125, Entropy -1232.18603515625, Learning Rate: 9.765625e-05\n",
      "Epoch [3806/20000], Loss: 4074.76611328125, Entropy -1199.7265625, Learning Rate: 9.765625e-05\n",
      "Epoch [3807/20000], Loss: 4082.5947265625, Entropy -1255.55419921875, Learning Rate: 9.765625e-05\n",
      "Epoch [3808/20000], Loss: 4129.8818359375, Entropy -1187.46923828125, Learning Rate: 9.765625e-05\n",
      "Epoch [3809/20000], Loss: 4151.40869140625, Entropy -1305.009765625, Learning Rate: 9.765625e-05\n",
      "Epoch [3810/20000], Loss: 4226.28125, Entropy -1361.4638671875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3811/20000], Loss: 4128.13427734375, Entropy -1234.76513671875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3812/20000], Loss: 4064.6669921875, Entropy -1195.46435546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3813/20000], Loss: 4119.60693359375, Entropy -1267.82568359375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3814/20000], Loss: 4105.37158203125, Entropy -1262.48486328125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3815/20000], Loss: 4107.626953125, Entropy -1235.52490234375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3816/20000], Loss: 4167.48681640625, Entropy -1280.8779296875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3817/20000], Loss: 4138.021484375, Entropy -1315.04443359375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3818/20000], Loss: 4071.860595703125, Entropy -1260.5234375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3819/20000], Loss: 4070.205810546875, Entropy -1284.44287109375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3820/20000], Loss: 4205.7685546875, Entropy -1365.314453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3821/20000], Loss: 4214.24560546875, Entropy -1281.1416015625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3822/20000], Loss: 4090.32177734375, Entropy -1183.79345703125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3823/20000], Loss: 4132.34033203125, Entropy -1309.66845703125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3824/20000], Loss: 4080.7841796875, Entropy -1235.46435546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3825/20000], Loss: 4172.705078125, Entropy -1184.37744140625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3826/20000], Loss: 4148.3212890625, Entropy -1312.302734375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3827/20000], Loss: 4065.866455078125, Entropy -1207.7666015625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3828/20000], Loss: 4045.705322265625, Entropy -1236.62158203125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3829/20000], Loss: 4159.9755859375, Entropy -1253.69189453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3830/20000], Loss: 4109.91015625, Entropy -1304.01513671875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3831/20000], Loss: 4146.044921875, Entropy -1358.5634765625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3832/20000], Loss: 4181.05615234375, Entropy -1288.359375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3833/20000], Loss: 4132.50830078125, Entropy -1286.98779296875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3834/20000], Loss: 4117.8037109375, Entropy -1283.248046875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3835/20000], Loss: 4097.341796875, Entropy -1252.8466796875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3836/20000], Loss: 4183.4462890625, Entropy -1164.66552734375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3837/20000], Loss: 4204.8349609375, Entropy -1302.138671875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3838/20000], Loss: 4162.9140625, Entropy -1306.78564453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3839/20000], Loss: 4258.04248046875, Entropy -1270.89990234375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3840/20000], Loss: 4134.52392578125, Entropy -1208.23193359375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3841/20000], Loss: 4113.2060546875, Entropy -1256.58056640625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3842/20000], Loss: 4098.02392578125, Entropy -1262.6943359375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3843/20000], Loss: 4168.9345703125, Entropy -1340.7783203125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3844/20000], Loss: 4064.1279296875, Entropy -1298.2587890625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3845/20000], Loss: 4090.19091796875, Entropy -1295.43359375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3846/20000], Loss: 4046.6630859375, Entropy -1207.7958984375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3847/20000], Loss: 4133.4228515625, Entropy -1283.380859375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3848/20000], Loss: 4170.80615234375, Entropy -1423.84423828125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3849/20000], Loss: 4110.56689453125, Entropy -1270.677734375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3850/20000], Loss: 4100.681640625, Entropy -1275.69189453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3851/20000], Loss: 4090.238037109375, Entropy -1290.560546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3852/20000], Loss: 4136.150390625, Entropy -1232.69189453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3853/20000], Loss: 4087.72021484375, Entropy -1218.2177734375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3854/20000], Loss: 4160.7236328125, Entropy -1297.28271484375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3855/20000], Loss: 4087.5703125, Entropy -1179.7060546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3856/20000], Loss: 4112.3134765625, Entropy -1341.73681640625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3857/20000], Loss: 4086.035888671875, Entropy -1272.16064453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3858/20000], Loss: 4131.423828125, Entropy -1368.32177734375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3859/20000], Loss: 4070.97021484375, Entropy -1233.02490234375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3860/20000], Loss: 4112.99755859375, Entropy -1221.55712890625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3861/20000], Loss: 4104.9541015625, Entropy -1270.99072265625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3862/20000], Loss: 4142.462890625, Entropy -1335.048828125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3863/20000], Loss: 4153.1572265625, Entropy -1335.90478515625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3864/20000], Loss: 4089.1611328125, Entropy -1318.6337890625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3865/20000], Loss: 4066.28515625, Entropy -1269.40185546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3866/20000], Loss: 4229.171875, Entropy -1402.78515625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3867/20000], Loss: 4140.390625, Entropy -1230.48974609375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3868/20000], Loss: 4134.34521484375, Entropy -1263.02490234375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3869/20000], Loss: 4152.63134765625, Entropy -1342.00439453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3870/20000], Loss: 4102.84130859375, Entropy -1301.75146484375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3871/20000], Loss: 4082.538818359375, Entropy -1149.90380859375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3872/20000], Loss: 4114.86181640625, Entropy -1197.45166015625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3873/20000], Loss: 4187.193359375, Entropy -1318.03076171875, Learning Rate: 4.8828125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3874/20000], Loss: 4078.39794921875, Entropy -1227.51806640625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3875/20000], Loss: 4140.2509765625, Entropy -1336.1025390625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3876/20000], Loss: 4108.9130859375, Entropy -1307.072265625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3877/20000], Loss: 4148.09912109375, Entropy -1320.7275390625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3878/20000], Loss: 4180.783203125, Entropy -1211.8837890625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3879/20000], Loss: 4220.333984375, Entropy -1183.994140625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3880/20000], Loss: 4165.13623046875, Entropy -1378.041015625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3881/20000], Loss: 4108.73095703125, Entropy -1288.45947265625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3882/20000], Loss: 4157.734375, Entropy -1342.27392578125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3883/20000], Loss: 4156.88916015625, Entropy -1185.66650390625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3884/20000], Loss: 4143.42431640625, Entropy -1309.47509765625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3885/20000], Loss: 4091.2197265625, Entropy -1300.56982421875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3886/20000], Loss: 4110.04150390625, Entropy -1311.5849609375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3887/20000], Loss: 4079.74609375, Entropy -1219.3837890625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3888/20000], Loss: 4105.82958984375, Entropy -1223.61962890625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3889/20000], Loss: 4049.303955078125, Entropy -1232.87744140625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3890/20000], Loss: 4146.07861328125, Entropy -1347.0576171875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3891/20000], Loss: 4151.27685546875, Entropy -1380.3603515625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3892/20000], Loss: 4127.83837890625, Entropy -1310.033203125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3893/20000], Loss: 4075.333984375, Entropy -1293.453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3894/20000], Loss: 4138.05126953125, Entropy -1359.88623046875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3895/20000], Loss: 4077.32275390625, Entropy -1150.75927734375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3896/20000], Loss: 4114.68310546875, Entropy -1322.72509765625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3897/20000], Loss: 4129.1240234375, Entropy -1214.94189453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3898/20000], Loss: 4117.25732421875, Entropy -1280.84130859375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3899/20000], Loss: 4089.909423828125, Entropy -1308.02978515625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3900/20000], Loss: 4155.2236328125, Entropy -1310.23974609375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3901/20000], Loss: 4150.85498046875, Entropy -1284.7744140625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3902/20000], Loss: 4099.17919921875, Entropy -1249.66845703125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3903/20000], Loss: 4079.591796875, Entropy -1194.2333984375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3904/20000], Loss: 4048.112548828125, Entropy -1242.56494140625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3905/20000], Loss: 4102.212890625, Entropy -1352.60302734375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3906/20000], Loss: 4077.259033203125, Entropy -1202.0517578125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3907/20000], Loss: 4167.064453125, Entropy -1295.59716796875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3908/20000], Loss: 4106.86865234375, Entropy -1232.75537109375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3909/20000], Loss: 4142.2568359375, Entropy -1326.30078125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3910/20000], Loss: 4119.359375, Entropy -1258.28857421875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3911/20000], Loss: 4136.62744140625, Entropy -1277.533203125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3912/20000], Loss: 4158.0029296875, Entropy -1263.33740234375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3913/20000], Loss: 4050.116943359375, Entropy -1256.015625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3914/20000], Loss: 4064.795166015625, Entropy -1204.47216796875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3915/20000], Loss: 4111.3984375, Entropy -1321.88037109375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3916/20000], Loss: 4138.37353515625, Entropy -1204.38037109375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3917/20000], Loss: 4092.177734375, Entropy -1185.76025390625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3918/20000], Loss: 4069.869140625, Entropy -1156.52392578125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3919/20000], Loss: 4129.525390625, Entropy -1360.53662109375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3920/20000], Loss: 4107.41845703125, Entropy -1308.63720703125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3921/20000], Loss: 4185.54931640625, Entropy -1333.42041015625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3922/20000], Loss: 4096.07958984375, Entropy -1209.2626953125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3923/20000], Loss: 4197.78955078125, Entropy -1237.86279296875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3924/20000], Loss: 4158.64990234375, Entropy -1198.76708984375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3925/20000], Loss: 4120.56787109375, Entropy -1365.72509765625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3926/20000], Loss: 4045.56640625, Entropy -1275.0185546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3927/20000], Loss: 4187.87255859375, Entropy -1176.09716796875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3928/20000], Loss: 4120.9501953125, Entropy -1325.87646484375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3929/20000], Loss: 4144.38671875, Entropy -1279.46044921875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3930/20000], Loss: 4121.14404296875, Entropy -1194.6435546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3931/20000], Loss: 4167.16162109375, Entropy -1204.81396484375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3932/20000], Loss: 4099.90869140625, Entropy -1303.7080078125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3933/20000], Loss: 4118.66259765625, Entropy -1319.67626953125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3934/20000], Loss: 4086.53271484375, Entropy -1083.96875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3935/20000], Loss: 4156.87646484375, Entropy -1290.654296875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3936/20000], Loss: 4106.92822265625, Entropy -1255.62158203125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3937/20000], Loss: 4069.673095703125, Entropy -1253.42626953125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3938/20000], Loss: 4069.556884765625, Entropy -1262.18505859375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3939/20000], Loss: 4081.04248046875, Entropy -1235.2783203125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3940/20000], Loss: 4112.5546875, Entropy -1221.83984375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3941/20000], Loss: 4056.609130859375, Entropy -1201.97509765625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3942/20000], Loss: 4093.81689453125, Entropy -1273.89990234375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3943/20000], Loss: 4111.6640625, Entropy -1273.38720703125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3944/20000], Loss: 4103.98779296875, Entropy -1266.15380859375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3945/20000], Loss: 4172.09423828125, Entropy -1441.2587890625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3946/20000], Loss: 4096.1728515625, Entropy -1180.615234375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3947/20000], Loss: 4095.720458984375, Entropy -1249.271484375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3948/20000], Loss: 4128.2568359375, Entropy -1282.59912109375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3949/20000], Loss: 4114.8046875, Entropy -1285.1064453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3950/20000], Loss: 4106.41357421875, Entropy -1269.85888671875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3951/20000], Loss: 4143.23779296875, Entropy -1282.69580078125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3952/20000], Loss: 4035.095947265625, Entropy -1169.44482421875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3953/20000], Loss: 4152.072265625, Entropy -1324.26318359375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3954/20000], Loss: 4157.25390625, Entropy -1360.84716796875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3955/20000], Loss: 4117.9970703125, Entropy -1342.18505859375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3956/20000], Loss: 4153.681640625, Entropy -1196.26025390625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3957/20000], Loss: 4104.5693359375, Entropy -1216.234375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3958/20000], Loss: 4086.08544921875, Entropy -1258.90869140625, Learning Rate: 4.8828125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3959/20000], Loss: 4067.131103515625, Entropy -1270.07958984375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3960/20000], Loss: 4129.228515625, Entropy -1294.67529296875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3961/20000], Loss: 4103.9287109375, Entropy -1233.83447265625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3962/20000], Loss: 4149.02490234375, Entropy -1360.05517578125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3963/20000], Loss: 4125.8701171875, Entropy -1317.689453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3964/20000], Loss: 4121.0244140625, Entropy -1258.0908203125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3965/20000], Loss: 4183.4052734375, Entropy -1213.27294921875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3966/20000], Loss: 4103.55810546875, Entropy -1184.59619140625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3967/20000], Loss: 4085.104736328125, Entropy -1304.04296875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3968/20000], Loss: 4152.83984375, Entropy -1299.18017578125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3969/20000], Loss: 4150.7900390625, Entropy -1238.80322265625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3970/20000], Loss: 4117.93798828125, Entropy -1257.81103515625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3971/20000], Loss: 4135.1826171875, Entropy -1262.57275390625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3972/20000], Loss: 4129.52880859375, Entropy -1286.62353515625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3973/20000], Loss: 4116.6884765625, Entropy -1313.12841796875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3974/20000], Loss: 4115.451171875, Entropy -1254.9404296875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3975/20000], Loss: 4088.1015625, Entropy -1303.86279296875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3976/20000], Loss: 4177.15576171875, Entropy -1340.771484375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3977/20000], Loss: 4146.9814453125, Entropy -1207.81640625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3978/20000], Loss: 4101.6533203125, Entropy -1314.38623046875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3979/20000], Loss: 4044.169189453125, Entropy -1208.78564453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3980/20000], Loss: 4118.45068359375, Entropy -1280.185546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3981/20000], Loss: 4096.67822265625, Entropy -1309.61083984375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3982/20000], Loss: 4162.998046875, Entropy -1336.21533203125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3983/20000], Loss: 4151.80712890625, Entropy -1346.50439453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3984/20000], Loss: 4113.00390625, Entropy -1202.3330078125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3985/20000], Loss: 4194.42333984375, Entropy -1330.4560546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3986/20000], Loss: 4178.66259765625, Entropy -1361.35888671875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3987/20000], Loss: 4141.4638671875, Entropy -1176.95361328125, Learning Rate: 4.8828125e-05\n",
      "Epoch [3988/20000], Loss: 4186.9169921875, Entropy -1245.55859375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3989/20000], Loss: 4179.4833984375, Entropy -1315.82666015625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3990/20000], Loss: 4078.056884765625, Entropy -1212.77294921875, Learning Rate: 4.8828125e-05\n",
      "Epoch [3991/20000], Loss: 4136.345703125, Entropy -1307.744140625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3992/20000], Loss: 4103.46142578125, Entropy -1267.41943359375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3993/20000], Loss: 4095.8818359375, Entropy -1189.13134765625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3994/20000], Loss: 4057.36572265625, Entropy -1210.5947265625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3995/20000], Loss: 4115.06787109375, Entropy -1311.0634765625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3996/20000], Loss: 4095.953125, Entropy -1233.177734375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3997/20000], Loss: 4140.09130859375, Entropy -1342.58984375, Learning Rate: 4.8828125e-05\n",
      "Epoch [3998/20000], Loss: 4095.4150390625, Entropy -1245.59619140625, Learning Rate: 4.8828125e-05\n",
      "Epoch [3999/20000], Loss: 4175.6865234375, Entropy -1336.26513671875, Learning Rate: 4.8828125e-05\n",
      "Epoch [4000/20000], Loss: 4128.03955078125, Entropy -1276.166015625, Learning Rate: 4.8828125e-05\n",
      "Epoch [4001/20000], Loss: 4121.794921875, Entropy -1364.716796875, Learning Rate: 4.8828125e-05\n",
      "Epoch [4002/20000], Loss: 4128.19775390625, Entropy -1331.62939453125, Learning Rate: 4.8828125e-05\n",
      "Epoch [4003/20000], Loss: 4158.80224609375, Entropy -1257.48193359375, Learning Rate: 4.8828125e-05\n",
      "Epoch [4004/20000], Loss: 4165.4912109375, Entropy -1272.46435546875, Learning Rate: 4.8828125e-05\n",
      "Epoch [4005/20000], Loss: 4111.333984375, Entropy -1269.95263671875, Learning Rate: 4.8828125e-05\n",
      "Epoch [4006/20000], Loss: 4094.326416015625, Entropy -1326.302734375, Learning Rate: 4.8828125e-05\n",
      "Epoch [4007/20000], Loss: 4088.16455078125, Entropy -1214.45556640625, Learning Rate: 4.8828125e-05\n",
      "Epoch [4008/20000], Loss: 4067.84228515625, Entropy -1196.50537109375, Learning Rate: 4.8828125e-05\n",
      "Epoch [4009/20000], Loss: 4184.39599609375, Entropy -1402.18212890625, Learning Rate: 4.8828125e-05\n",
      "Epoch [4010/20000], Loss: 4181.27099609375, Entropy -1333.41650390625, Learning Rate: 4.8828125e-05\n",
      "Epoch [4011/20000], Loss: 4093.845703125, Entropy -1332.87646484375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4012/20000], Loss: 4089.0166015625, Entropy -1285.64697265625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4013/20000], Loss: 4095.25, Entropy -1218.08251953125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4014/20000], Loss: 4140.2470703125, Entropy -1299.02978515625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4015/20000], Loss: 4102.11767578125, Entropy -1315.623046875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4016/20000], Loss: 4140.966796875, Entropy -1405.216552734375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4017/20000], Loss: 4048.582763671875, Entropy -1195.2939453125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4018/20000], Loss: 4158.20556640625, Entropy -1376.255859375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4019/20000], Loss: 4128.90478515625, Entropy -1325.4189453125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4020/20000], Loss: 4152.5712890625, Entropy -1264.70751953125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4021/20000], Loss: 4109.15087890625, Entropy -1381.50634765625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4022/20000], Loss: 4136.54833984375, Entropy -1263.20703125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4023/20000], Loss: 4156.0888671875, Entropy -1339.17919921875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4024/20000], Loss: 4115.05859375, Entropy -1225.34912109375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4025/20000], Loss: 4186.43603515625, Entropy -1161.08935546875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4026/20000], Loss: 4110.59130859375, Entropy -1326.7470703125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4027/20000], Loss: 4160.88427734375, Entropy -1214.23486328125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4028/20000], Loss: 4129.72216796875, Entropy -1186.22705078125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4029/20000], Loss: 4198.0634765625, Entropy -1281.47412109375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4030/20000], Loss: 4145.6318359375, Entropy -1354.13720703125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4031/20000], Loss: 4127.8125, Entropy -1298.6083984375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4032/20000], Loss: 4093.212158203125, Entropy -1161.44189453125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4033/20000], Loss: 4116.037109375, Entropy -1250.5859375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4034/20000], Loss: 4115.1435546875, Entropy -1329.0703125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4035/20000], Loss: 4156.783203125, Entropy -1409.66748046875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4036/20000], Loss: 4136.06689453125, Entropy -1286.0322265625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4037/20000], Loss: 4128.5263671875, Entropy -1198.3720703125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4038/20000], Loss: 4168.365234375, Entropy -1293.32080078125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4039/20000], Loss: 4092.197265625, Entropy -1173.71435546875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4040/20000], Loss: 4054.35791015625, Entropy -1218.1640625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4041/20000], Loss: 4250.11572265625, Entropy -1218.49658203125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4042/20000], Loss: 4163.5810546875, Entropy -1340.23388671875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4043/20000], Loss: 4123.17333984375, Entropy -1302.2900390625, Learning Rate: 2.44140625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4044/20000], Loss: 4088.69970703125, Entropy -1256.1708984375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4045/20000], Loss: 4122.16650390625, Entropy -1270.82421875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4046/20000], Loss: 4136.19482421875, Entropy -1267.6259765625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4047/20000], Loss: 4111.6103515625, Entropy -1214.44482421875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4048/20000], Loss: 4161.1630859375, Entropy -1400.40966796875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4049/20000], Loss: 4061.40283203125, Entropy -1225.82373046875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4050/20000], Loss: 4048.879638671875, Entropy -1183.90966796875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4051/20000], Loss: 4062.73388671875, Entropy -1234.90234375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4052/20000], Loss: 4161.6884765625, Entropy -1307.07861328125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4053/20000], Loss: 4088.94140625, Entropy -1274.23828125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4054/20000], Loss: 4103.88916015625, Entropy -1286.98779296875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4055/20000], Loss: 4107.46240234375, Entropy -1341.97412109375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4056/20000], Loss: 4120.08984375, Entropy -1368.36181640625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4057/20000], Loss: 4073.474365234375, Entropy -1213.74267578125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4058/20000], Loss: 4102.56396484375, Entropy -1238.9599609375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4059/20000], Loss: 4102.3828125, Entropy -1250.19287109375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4060/20000], Loss: 4126.96435546875, Entropy -1294.6513671875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4061/20000], Loss: 4112.63037109375, Entropy -1272.61865234375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4062/20000], Loss: 4099.2470703125, Entropy -1235.41748046875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4063/20000], Loss: 4161.1064453125, Entropy -1322.39892578125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4064/20000], Loss: 4083.170654296875, Entropy -1364.54736328125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4065/20000], Loss: 4110.8076171875, Entropy -1215.74365234375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4066/20000], Loss: 4114.162109375, Entropy -1278.302734375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4067/20000], Loss: 4115.01953125, Entropy -1307.69775390625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4068/20000], Loss: 4102.5625, Entropy -1202.93994140625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4069/20000], Loss: 4128.9638671875, Entropy -1332.39892578125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4070/20000], Loss: 4090.43212890625, Entropy -1263.67041015625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4071/20000], Loss: 4133.8447265625, Entropy -1314.26806640625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4072/20000], Loss: 4129.26220703125, Entropy -1288.25927734375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4073/20000], Loss: 4110.36279296875, Entropy -1208.23974609375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4074/20000], Loss: 4096.09765625, Entropy -1190.82568359375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4075/20000], Loss: 4066.88427734375, Entropy -1298.86376953125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4076/20000], Loss: 4121.87744140625, Entropy -1280.09619140625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4077/20000], Loss: 4089.417724609375, Entropy -1153.93359375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4078/20000], Loss: 4098.97802734375, Entropy -1240.5458984375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4079/20000], Loss: 4128.421875, Entropy -1351.171875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4080/20000], Loss: 4146.83203125, Entropy -1255.3369140625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4081/20000], Loss: 4129.23193359375, Entropy -1324.849609375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4082/20000], Loss: 4147.3486328125, Entropy -1316.83935546875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4083/20000], Loss: 4106.4462890625, Entropy -1259.20947265625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4084/20000], Loss: 4071.71484375, Entropy -1198.0322265625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4085/20000], Loss: 4138.15771484375, Entropy -1318.79833984375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4086/20000], Loss: 4142.16748046875, Entropy -1347.54638671875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4087/20000], Loss: 4117.13427734375, Entropy -1303.97607421875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4088/20000], Loss: 4096.4580078125, Entropy -1331.4833984375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4089/20000], Loss: 4154.90380859375, Entropy -1357.55615234375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4090/20000], Loss: 4085.857177734375, Entropy -1264.583984375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4091/20000], Loss: 4096.04638671875, Entropy -1224.19482421875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4092/20000], Loss: 4114.50390625, Entropy -1139.05615234375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4093/20000], Loss: 4139.7666015625, Entropy -1194.50537109375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4094/20000], Loss: 4095.654296875, Entropy -1319.3740234375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4095/20000], Loss: 4150.07275390625, Entropy -1260.10888671875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4096/20000], Loss: 4104.0078125, Entropy -1205.14599609375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4097/20000], Loss: 4149.63916015625, Entropy -1305.71728515625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4098/20000], Loss: 4153.7177734375, Entropy -1324.6162109375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4099/20000], Loss: 4118.6337890625, Entropy -1193.41748046875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4100/20000], Loss: 4147.12109375, Entropy -1262.21142578125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4101/20000], Loss: 4122.578125, Entropy -1283.595703125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4102/20000], Loss: 4084.09228515625, Entropy -1310.77490234375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4103/20000], Loss: 4157.12939453125, Entropy -1181.94384765625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4104/20000], Loss: 4187.92431640625, Entropy -1365.24560546875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4105/20000], Loss: 4121.662109375, Entropy -1243.9130859375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4106/20000], Loss: 4130.4267578125, Entropy -1247.83642578125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4107/20000], Loss: 4097.427734375, Entropy -1284.57373046875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4108/20000], Loss: 4084.169921875, Entropy -1246.3427734375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4109/20000], Loss: 4149.69580078125, Entropy -1292.93408203125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4110/20000], Loss: 4103.3037109375, Entropy -1262.716796875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4111/20000], Loss: 4081.01904296875, Entropy -1254.9560546875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4112/20000], Loss: 4140.42529296875, Entropy -1311.7236328125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4113/20000], Loss: 4073.058837890625, Entropy -1269.42041015625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4114/20000], Loss: 4082.44677734375, Entropy -1238.97314453125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4115/20000], Loss: 4086.513671875, Entropy -1156.94970703125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4116/20000], Loss: 4121.01953125, Entropy -1292.09814453125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4117/20000], Loss: 4131.2255859375, Entropy -1259.43408203125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4118/20000], Loss: 4105.759765625, Entropy -1317.00146484375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4119/20000], Loss: 4115.83935546875, Entropy -1285.4775390625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4120/20000], Loss: 4113.62646484375, Entropy -1262.5771484375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4121/20000], Loss: 4115.73193359375, Entropy -1284.5107421875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4122/20000], Loss: 4068.70068359375, Entropy -1188.564453125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4123/20000], Loss: 4144.07373046875, Entropy -1198.06298828125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4124/20000], Loss: 4210.1591796875, Entropy -1312.9931640625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4125/20000], Loss: 4109.57275390625, Entropy -1209.35009765625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4126/20000], Loss: 4165.6240234375, Entropy -1279.43994140625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4127/20000], Loss: 4174.67626953125, Entropy -1215.10302734375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4128/20000], Loss: 4143.54052734375, Entropy -1277.52294921875, Learning Rate: 2.44140625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4129/20000], Loss: 4079.14306640625, Entropy -1219.85546875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4130/20000], Loss: 4165.400390625, Entropy -1244.1396484375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4131/20000], Loss: 4082.876220703125, Entropy -1142.95068359375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4132/20000], Loss: 4133.26123046875, Entropy -1383.76708984375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4133/20000], Loss: 4079.803466796875, Entropy -1166.53515625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4134/20000], Loss: 4099.3955078125, Entropy -1267.6318359375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4135/20000], Loss: 4143.20263671875, Entropy -1199.05126953125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4136/20000], Loss: 4100.39697265625, Entropy -1195.60302734375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4137/20000], Loss: 4112.39453125, Entropy -1287.34716796875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4138/20000], Loss: 4077.9521484375, Entropy -1236.4931640625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4139/20000], Loss: 4139.587890625, Entropy -1264.2373046875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4140/20000], Loss: 4131.572265625, Entropy -1314.83642578125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4141/20000], Loss: 4120.52392578125, Entropy -1245.6455078125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4142/20000], Loss: 4056.46142578125, Entropy -1245.1435546875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4143/20000], Loss: 4099.57080078125, Entropy -1318.22265625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4144/20000], Loss: 4146.85498046875, Entropy -1182.93701171875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4145/20000], Loss: 4109.7060546875, Entropy -1234.73193359375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4146/20000], Loss: 4132.4892578125, Entropy -1227.67529296875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4147/20000], Loss: 4108.2490234375, Entropy -1310.31201171875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4148/20000], Loss: 4155.53662109375, Entropy -1353.96044921875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4149/20000], Loss: 4156.3330078125, Entropy -1361.142578125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4150/20000], Loss: 4138.35595703125, Entropy -1300.755859375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4151/20000], Loss: 4102.9619140625, Entropy -1287.30419921875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4152/20000], Loss: 4159.23681640625, Entropy -1276.46484375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4153/20000], Loss: 4132.08642578125, Entropy -1218.46630859375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4154/20000], Loss: 4126.14013671875, Entropy -1364.74462890625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4155/20000], Loss: 4155.41796875, Entropy -1305.72802734375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4156/20000], Loss: 4131.09765625, Entropy -1282.0205078125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4157/20000], Loss: 4094.986083984375, Entropy -1236.966796875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4158/20000], Loss: 4108.3701171875, Entropy -1180.96533203125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4159/20000], Loss: 4127.60009765625, Entropy -1297.1279296875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4160/20000], Loss: 4144.90625, Entropy -1270.17626953125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4161/20000], Loss: 4147.16259765625, Entropy -1321.787109375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4162/20000], Loss: 4082.6787109375, Entropy -1292.70654296875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4163/20000], Loss: 4121.98095703125, Entropy -1244.1044921875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4164/20000], Loss: 4123.19921875, Entropy -1244.73388671875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4165/20000], Loss: 4161.54248046875, Entropy -1270.166015625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4166/20000], Loss: 4097.40283203125, Entropy -1129.49951171875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4167/20000], Loss: 4149.43798828125, Entropy -1355.27099609375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4168/20000], Loss: 4124.34375, Entropy -1288.3056640625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4169/20000], Loss: 4160.56005859375, Entropy -1316.3955078125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4170/20000], Loss: 4226.3076171875, Entropy -1298.4033203125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4171/20000], Loss: 4154.95556640625, Entropy -1261.27099609375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4172/20000], Loss: 4087.793701171875, Entropy -1295.830078125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4173/20000], Loss: 4100.81787109375, Entropy -1222.80224609375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4174/20000], Loss: 4073.468505859375, Entropy -1237.1435546875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4175/20000], Loss: 4115.318359375, Entropy -1294.345703125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4176/20000], Loss: 4126.75048828125, Entropy -1294.375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4177/20000], Loss: 4089.765625, Entropy -1230.72900390625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4178/20000], Loss: 4142.955078125, Entropy -1236.7138671875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4179/20000], Loss: 4150.17578125, Entropy -1237.841796875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4180/20000], Loss: 4051.086669921875, Entropy -1129.59521484375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4181/20000], Loss: 4193.275390625, Entropy -1335.19482421875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4182/20000], Loss: 4088.014892578125, Entropy -1303.6416015625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4183/20000], Loss: 4098.931640625, Entropy -1237.9501953125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4184/20000], Loss: 4122.8349609375, Entropy -1343.6669921875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4185/20000], Loss: 4150.34814453125, Entropy -1298.41650390625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4186/20000], Loss: 4121.59521484375, Entropy -1286.53173828125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4187/20000], Loss: 4114.39892578125, Entropy -1297.451171875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4188/20000], Loss: 4109.21435546875, Entropy -1212.46923828125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4189/20000], Loss: 4140.78564453125, Entropy -1334.54150390625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4190/20000], Loss: 4131.38623046875, Entropy -1224.31494140625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4191/20000], Loss: 4167.60693359375, Entropy -1264.7333984375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4192/20000], Loss: 4098.794921875, Entropy -1239.56103515625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4193/20000], Loss: 4160.31640625, Entropy -1303.81005859375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4194/20000], Loss: 4110.513671875, Entropy -1286.02734375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4195/20000], Loss: 4144.72119140625, Entropy -1266.56396484375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4196/20000], Loss: 4154.44921875, Entropy -1317.72119140625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4197/20000], Loss: 4143.416015625, Entropy -1363.5546875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4198/20000], Loss: 4102.279296875, Entropy -1280.1552734375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4199/20000], Loss: 4146.7900390625, Entropy -1393.2822265625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4200/20000], Loss: 4118.00341796875, Entropy -1295.84814453125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4201/20000], Loss: 4304.63623046875, Entropy -1277.96923828125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4202/20000], Loss: 4070.11279296875, Entropy -1270.14599609375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4203/20000], Loss: 4077.011474609375, Entropy -1331.41796875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4204/20000], Loss: 4165.59912109375, Entropy -1369.34912109375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4205/20000], Loss: 4148.158203125, Entropy -1388.8271484375, Learning Rate: 2.44140625e-05\n",
      "Epoch [4206/20000], Loss: 4135.64990234375, Entropy -1355.04638671875, Learning Rate: 2.44140625e-05\n",
      "Epoch [4207/20000], Loss: 4083.9013671875, Entropy -1231.28369140625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4208/20000], Loss: 4149.61962890625, Entropy -1369.43408203125, Learning Rate: 2.44140625e-05\n",
      "Epoch [4209/20000], Loss: 4102.97412109375, Entropy -1297.26416015625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4210/20000], Loss: 4176.9462890625, Entropy -1244.45947265625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4211/20000], Loss: 4116.7958984375, Entropy -1244.33447265625, Learning Rate: 2.44140625e-05\n",
      "Epoch [4212/20000], Loss: 4135.205078125, Entropy -1256.11962890625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4213/20000], Loss: 4206.8564453125, Entropy -1310.94970703125, Learning Rate: 1.220703125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4214/20000], Loss: 4119.98046875, Entropy -1315.546875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4215/20000], Loss: 4097.51416015625, Entropy -1283.34814453125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4216/20000], Loss: 4111.77294921875, Entropy -1269.57568359375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4217/20000], Loss: 4141.18017578125, Entropy -1406.89404296875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4218/20000], Loss: 4124.40185546875, Entropy -1230.23388671875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4219/20000], Loss: 4128.12255859375, Entropy -1195.4541015625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4220/20000], Loss: 4186.51171875, Entropy -1264.9521484375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4221/20000], Loss: 4110.57666015625, Entropy -1313.1240234375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4222/20000], Loss: 4089.45166015625, Entropy -1298.470703125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4223/20000], Loss: 4063.56884765625, Entropy -1212.87158203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4224/20000], Loss: 4151.5654296875, Entropy -1176.2705078125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4225/20000], Loss: 4076.718017578125, Entropy -1291.20654296875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4226/20000], Loss: 4163.244140625, Entropy -1238.48681640625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4227/20000], Loss: 4147.9921875, Entropy -1295.93115234375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4228/20000], Loss: 4072.311767578125, Entropy -1200.29150390625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4229/20000], Loss: 4102.49853515625, Entropy -1242.47265625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4230/20000], Loss: 4140.248046875, Entropy -1340.4482421875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4231/20000], Loss: 4187.49560546875, Entropy -1340.87890625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4232/20000], Loss: 4087.68701171875, Entropy -1236.19873046875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4233/20000], Loss: 4110.35205078125, Entropy -1214.71044921875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4234/20000], Loss: 4143.2509765625, Entropy -1294.63623046875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4235/20000], Loss: 4062.859130859375, Entropy -1183.30126953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4236/20000], Loss: 4145.53564453125, Entropy -1250.67431640625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4237/20000], Loss: 4124.16015625, Entropy -1226.46630859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4238/20000], Loss: 4129.23486328125, Entropy -1256.85595703125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4239/20000], Loss: 4113.1796875, Entropy -1296.2587890625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4240/20000], Loss: 4197.1328125, Entropy -1422.554443359375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4241/20000], Loss: 4112.8193359375, Entropy -1282.09130859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4242/20000], Loss: 4119.76513671875, Entropy -1340.7197265625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4243/20000], Loss: 4074.7275390625, Entropy -1212.9736328125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4244/20000], Loss: 4144.85205078125, Entropy -1281.25146484375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4245/20000], Loss: 4099.88818359375, Entropy -1194.640625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4246/20000], Loss: 4177.4560546875, Entropy -1201.3466796875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4247/20000], Loss: 4106.7431640625, Entropy -1294.28564453125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4248/20000], Loss: 4124.65869140625, Entropy -1333.37109375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4249/20000], Loss: 4131.3818359375, Entropy -1399.19677734375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4250/20000], Loss: 4086.902587890625, Entropy -1214.38037109375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4251/20000], Loss: 4106.15380859375, Entropy -1183.71630859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4252/20000], Loss: 4109.9443359375, Entropy -1303.46484375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4253/20000], Loss: 4174.7236328125, Entropy -1180.79833984375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4254/20000], Loss: 4132.05322265625, Entropy -1251.67041015625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4255/20000], Loss: 4111.93017578125, Entropy -1238.67529296875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4256/20000], Loss: 4112.376953125, Entropy -1374.6708984375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4257/20000], Loss: 4067.907470703125, Entropy -1314.46630859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4258/20000], Loss: 4165.14697265625, Entropy -1193.5791015625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4259/20000], Loss: 4130.349609375, Entropy -1244.3369140625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4260/20000], Loss: 4107.98876953125, Entropy -1241.56640625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4261/20000], Loss: 4073.5869140625, Entropy -1311.39013671875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4262/20000], Loss: 4114.4560546875, Entropy -1349.41796875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4263/20000], Loss: 4163.05419921875, Entropy -1392.58251953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4264/20000], Loss: 4073.391845703125, Entropy -1211.28955078125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4265/20000], Loss: 4101.31005859375, Entropy -1190.44921875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4266/20000], Loss: 4128.09912109375, Entropy -1241.86669921875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4267/20000], Loss: 4168.708984375, Entropy -1161.34130859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4268/20000], Loss: 4218.22705078125, Entropy -1320.99072265625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4269/20000], Loss: 4147.34228515625, Entropy -1385.4638671875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4270/20000], Loss: 4094.64306640625, Entropy -1220.86083984375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4271/20000], Loss: 4096.9169921875, Entropy -1242.18408203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4272/20000], Loss: 4143.05810546875, Entropy -1316.69091796875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4273/20000], Loss: 4111.98046875, Entropy -1361.09521484375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4274/20000], Loss: 4150.23388671875, Entropy -1329.17236328125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4275/20000], Loss: 4122.60400390625, Entropy -1261.12158203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4276/20000], Loss: 4132.71435546875, Entropy -1354.28466796875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4277/20000], Loss: 4114.81787109375, Entropy -1245.63037109375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4278/20000], Loss: 4086.70361328125, Entropy -1251.69677734375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4279/20000], Loss: 4080.4794921875, Entropy -1269.99853515625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4280/20000], Loss: 4075.960693359375, Entropy -1216.29931640625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4281/20000], Loss: 4130.751953125, Entropy -1345.08447265625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4282/20000], Loss: 4185.13623046875, Entropy -1393.98583984375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4283/20000], Loss: 4167.57421875, Entropy -1288.43017578125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4284/20000], Loss: 4165.87890625, Entropy -1234.20751953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4285/20000], Loss: 4136.46728515625, Entropy -1263.123046875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4286/20000], Loss: 4090.79443359375, Entropy -1296.02197265625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4287/20000], Loss: 4115.1123046875, Entropy -1204.33935546875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4288/20000], Loss: 4049.257568359375, Entropy -1164.3759765625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4289/20000], Loss: 4119.40234375, Entropy -1300.63037109375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4290/20000], Loss: 4081.057373046875, Entropy -1190.13525390625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4291/20000], Loss: 4143.9423828125, Entropy -1226.15185546875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4292/20000], Loss: 4077.9970703125, Entropy -1203.5126953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4293/20000], Loss: 4082.983642578125, Entropy -1268.13623046875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4294/20000], Loss: 4100.7314453125, Entropy -1183.82666015625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4295/20000], Loss: 4049.31689453125, Entropy -1246.12255859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4296/20000], Loss: 4121.43359375, Entropy -1236.41259765625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4297/20000], Loss: 4095.591552734375, Entropy -1228.53466796875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4298/20000], Loss: 4103.2080078125, Entropy -1254.31103515625, Learning Rate: 1.220703125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4299/20000], Loss: 4101.9091796875, Entropy -1359.486328125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4300/20000], Loss: 4144.29638671875, Entropy -1293.3076171875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4301/20000], Loss: 4147.46875, Entropy -1232.7939453125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4302/20000], Loss: 4197.14111328125, Entropy -1283.34228515625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4303/20000], Loss: 4092.798828125, Entropy -1293.67041015625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4304/20000], Loss: 4154.6103515625, Entropy -1304.32373046875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4305/20000], Loss: 4143.88427734375, Entropy -1189.6767578125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4306/20000], Loss: 4196.62109375, Entropy -1198.66162109375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4307/20000], Loss: 4111.66455078125, Entropy -1107.3994140625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4308/20000], Loss: 4120.90869140625, Entropy -1287.82861328125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4309/20000], Loss: 4100.130859375, Entropy -1333.31396484375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4310/20000], Loss: 4081.91650390625, Entropy -1215.9296875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4311/20000], Loss: 4162.66748046875, Entropy -1294.1767578125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4312/20000], Loss: 4144.97265625, Entropy -1245.5908203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4313/20000], Loss: 4113.79833984375, Entropy -1242.08740234375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4314/20000], Loss: 4155.611328125, Entropy -1195.67138671875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4315/20000], Loss: 4163.7197265625, Entropy -1360.46435546875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4316/20000], Loss: 4143.36962890625, Entropy -1307.767578125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4317/20000], Loss: 4135.466796875, Entropy -1233.82177734375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4318/20000], Loss: 4071.60107421875, Entropy -1265.89599609375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4319/20000], Loss: 4108.2431640625, Entropy -1296.36376953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4320/20000], Loss: 4065.605224609375, Entropy -1217.783203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4321/20000], Loss: 4130.4443359375, Entropy -1262.5048828125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4322/20000], Loss: 4130.29248046875, Entropy -1263.50244140625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4323/20000], Loss: 4120.63232421875, Entropy -1255.2822265625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4324/20000], Loss: 4137.62109375, Entropy -1340.146484375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4325/20000], Loss: 4131.91845703125, Entropy -1332.00537109375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4326/20000], Loss: 4104.8681640625, Entropy -1203.12841796875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4327/20000], Loss: 4161.14697265625, Entropy -1277.939453125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4328/20000], Loss: 4069.547607421875, Entropy -1266.62646484375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4329/20000], Loss: 4146.47265625, Entropy -1214.22412109375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4330/20000], Loss: 4139.57373046875, Entropy -1376.36279296875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4331/20000], Loss: 4125.23681640625, Entropy -1245.43212890625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4332/20000], Loss: 4160.61669921875, Entropy -1289.349609375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4333/20000], Loss: 4090.991455078125, Entropy -1272.1435546875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4334/20000], Loss: 4149.25732421875, Entropy -1233.56494140625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4335/20000], Loss: 4064.295654296875, Entropy -1204.90771484375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4336/20000], Loss: 4089.135009765625, Entropy -1091.0517578125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4337/20000], Loss: 4060.542236328125, Entropy -1185.1298828125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4338/20000], Loss: 4136.8447265625, Entropy -1376.70654296875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4339/20000], Loss: 4118.52783203125, Entropy -1252.15869140625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4340/20000], Loss: 4084.845458984375, Entropy -1160.2490234375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4341/20000], Loss: 4133.82080078125, Entropy -1244.9921875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4342/20000], Loss: 4234.98046875, Entropy -1336.5400390625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4343/20000], Loss: 4138.0126953125, Entropy -1283.7783203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4344/20000], Loss: 4147.4482421875, Entropy -1163.37255859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4345/20000], Loss: 4082.6982421875, Entropy -1203.77880859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4346/20000], Loss: 4112.416015625, Entropy -1290.78466796875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4347/20000], Loss: 4089.8515625, Entropy -1281.56005859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4348/20000], Loss: 4079.585693359375, Entropy -1272.3408203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4349/20000], Loss: 4141.26171875, Entropy -1298.82861328125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4350/20000], Loss: 4199.21044921875, Entropy -1387.4873046875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4351/20000], Loss: 4096.89013671875, Entropy -1252.84228515625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4352/20000], Loss: 4079.689208984375, Entropy -1292.15869140625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4353/20000], Loss: 4144.00390625, Entropy -1228.89306640625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4354/20000], Loss: 4117.65234375, Entropy -1243.94677734375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4355/20000], Loss: 4175.39892578125, Entropy -1399.787353515625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4356/20000], Loss: 4120.62646484375, Entropy -1317.55126953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4357/20000], Loss: 4070.22607421875, Entropy -1175.025390625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4358/20000], Loss: 4175.474609375, Entropy -1357.47021484375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4359/20000], Loss: 4200.986328125, Entropy -1284.513671875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4360/20000], Loss: 4052.12060546875, Entropy -1244.0205078125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4361/20000], Loss: 4131.31298828125, Entropy -1250.92529296875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4362/20000], Loss: 4203.8173828125, Entropy -1437.901123046875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4363/20000], Loss: 4206.45068359375, Entropy -1186.28076171875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4364/20000], Loss: 4210.99267578125, Entropy -1230.23583984375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4365/20000], Loss: 4098.69482421875, Entropy -1324.48876953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4366/20000], Loss: 4086.84521484375, Entropy -1143.09814453125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4367/20000], Loss: 4111.72509765625, Entropy -1259.326171875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4368/20000], Loss: 4086.7666015625, Entropy -1247.61767578125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4369/20000], Loss: 4110.28515625, Entropy -1314.71337890625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4370/20000], Loss: 4116.77392578125, Entropy -1373.63818359375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4371/20000], Loss: 4116.39453125, Entropy -1292.7001953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4372/20000], Loss: 4049.997314453125, Entropy -1136.2421875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4373/20000], Loss: 4093.835205078125, Entropy -1264.6328125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4374/20000], Loss: 4079.077392578125, Entropy -1251.25927734375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4375/20000], Loss: 4098.1455078125, Entropy -1293.99365234375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4376/20000], Loss: 4090.039306640625, Entropy -1284.814453125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4377/20000], Loss: 4091.342529296875, Entropy -1269.02197265625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4378/20000], Loss: 4142.27978515625, Entropy -1300.46533203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4379/20000], Loss: 4077.1865234375, Entropy -1159.0712890625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4380/20000], Loss: 4126.4365234375, Entropy -1243.75830078125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4381/20000], Loss: 4130.5615234375, Entropy -1289.8232421875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4382/20000], Loss: 4144.19677734375, Entropy -1272.4892578125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4383/20000], Loss: 4101.88818359375, Entropy -1339.443359375, Learning Rate: 1.220703125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4384/20000], Loss: 4120.9599609375, Entropy -1241.62255859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4385/20000], Loss: 4127.7333984375, Entropy -1261.333984375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4386/20000], Loss: 4071.502685546875, Entropy -1217.3779296875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4387/20000], Loss: 4106.9140625, Entropy -1241.72998046875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4388/20000], Loss: 4140.44384765625, Entropy -1319.525390625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4389/20000], Loss: 4102.880859375, Entropy -1256.30859375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4390/20000], Loss: 4145.3251953125, Entropy -1316.693359375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4391/20000], Loss: 4120.8984375, Entropy -1334.07568359375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4392/20000], Loss: 4093.52978515625, Entropy -1293.8349609375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4393/20000], Loss: 4139.2841796875, Entropy -1244.55908203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4394/20000], Loss: 4120.21533203125, Entropy -1337.63134765625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4395/20000], Loss: 4112.40966796875, Entropy -1292.103515625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4396/20000], Loss: 4078.119384765625, Entropy -1276.16845703125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4397/20000], Loss: 4090.26513671875, Entropy -1152.548828125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4398/20000], Loss: 4120.46630859375, Entropy -1337.58203125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4399/20000], Loss: 4111.7041015625, Entropy -1255.79345703125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4400/20000], Loss: 4124.0283203125, Entropy -1281.41796875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4401/20000], Loss: 4101.306640625, Entropy -1214.7236328125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4402/20000], Loss: 4146.01025390625, Entropy -1387.84716796875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4403/20000], Loss: 4102.86279296875, Entropy -1290.10107421875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4404/20000], Loss: 4090.008544921875, Entropy -1261.13916015625, Learning Rate: 1.220703125e-05\n",
      "Epoch [4405/20000], Loss: 4119.3740234375, Entropy -1279.22998046875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4406/20000], Loss: 4144.7216796875, Entropy -1331.61474609375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4407/20000], Loss: 4121.68603515625, Entropy -1286.42626953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4408/20000], Loss: 4178.1513671875, Entropy -1370.42626953125, Learning Rate: 1.220703125e-05\n",
      "Epoch [4409/20000], Loss: 4057.998046875, Entropy -1171.58154296875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4410/20000], Loss: 4134.94140625, Entropy -1328.94482421875, Learning Rate: 1.220703125e-05\n",
      "Epoch [4411/20000], Loss: 4150.64111328125, Entropy -1312.7412109375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4412/20000], Loss: 4133.1494140625, Entropy -1410.488037109375, Learning Rate: 1.220703125e-05\n",
      "Epoch [4413/20000], Loss: 4111.57470703125, Entropy -1240.76416015625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4414/20000], Loss: 4057.864990234375, Entropy -1201.728515625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4415/20000], Loss: 4117.7470703125, Entropy -1240.37158203125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4416/20000], Loss: 4134.2490234375, Entropy -1252.19482421875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4417/20000], Loss: 4137.93212890625, Entropy -1288.47314453125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4418/20000], Loss: 4096.267578125, Entropy -1359.86328125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4419/20000], Loss: 4060.214599609375, Entropy -1277.27294921875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4420/20000], Loss: 4038.662353515625, Entropy -1172.75, Learning Rate: 6.103515625e-06\n",
      "Epoch [4421/20000], Loss: 4110.41064453125, Entropy -1362.79833984375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4422/20000], Loss: 4122.5791015625, Entropy -1259.4228515625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4423/20000], Loss: 4118.75830078125, Entropy -1398.408203125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4424/20000], Loss: 4100.30419921875, Entropy -1162.37841796875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4425/20000], Loss: 4080.17333984375, Entropy -1313.00341796875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4426/20000], Loss: 4101.66796875, Entropy -1288.00048828125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4427/20000], Loss: 4049.920654296875, Entropy -1152.31103515625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4428/20000], Loss: 4094.730712890625, Entropy -1290.37548828125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4429/20000], Loss: 4112.970703125, Entropy -1228.484375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4430/20000], Loss: 4112.75732421875, Entropy -1231.97314453125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4431/20000], Loss: 4162.869140625, Entropy -1289.9462890625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4432/20000], Loss: 4058.83056640625, Entropy -1202.10546875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4433/20000], Loss: 4095.7041015625, Entropy -1284.37841796875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4434/20000], Loss: 4135.2861328125, Entropy -1328.94189453125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4435/20000], Loss: 4126.314453125, Entropy -1304.3701171875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4436/20000], Loss: 4038.34716796875, Entropy -1185.27001953125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4437/20000], Loss: 4109.3876953125, Entropy -1277.728515625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4438/20000], Loss: 4116.69482421875, Entropy -1288.09716796875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4439/20000], Loss: 4098.2333984375, Entropy -1269.39794921875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4440/20000], Loss: 4147.6962890625, Entropy -1265.71826171875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4441/20000], Loss: 4173.2431640625, Entropy -1348.32568359375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4442/20000], Loss: 4145.17041015625, Entropy -1342.875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4443/20000], Loss: 4126.70166015625, Entropy -1323.333984375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4444/20000], Loss: 4117.69287109375, Entropy -1294.23291015625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4445/20000], Loss: 4124.0693359375, Entropy -1231.466796875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4446/20000], Loss: 4090.79443359375, Entropy -1294.8115234375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4447/20000], Loss: 4124.0703125, Entropy -1363.26611328125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4448/20000], Loss: 4169.91748046875, Entropy -1400.40283203125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4449/20000], Loss: 4057.9541015625, Entropy -1260.00537109375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4450/20000], Loss: 4107.97705078125, Entropy -1322.4453125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4451/20000], Loss: 4167.79345703125, Entropy -1348.33544921875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4452/20000], Loss: 4184.99853515625, Entropy -1438.16845703125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4453/20000], Loss: 4168.68994140625, Entropy -1272.55078125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4454/20000], Loss: 4084.22900390625, Entropy -1187.19873046875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4455/20000], Loss: 4173.1201171875, Entropy -1229.8193359375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4456/20000], Loss: 4068.618896484375, Entropy -1235.56005859375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4457/20000], Loss: 4211.20556640625, Entropy -1302.00830078125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4458/20000], Loss: 4308.966796875, Entropy -1355.80810546875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4459/20000], Loss: 4084.82861328125, Entropy -1184.73876953125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4460/20000], Loss: 4132.8779296875, Entropy -1347.45068359375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4461/20000], Loss: 4127.8408203125, Entropy -1302.0908203125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4462/20000], Loss: 4115.18212890625, Entropy -1349.7333984375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4463/20000], Loss: 4114.5888671875, Entropy -1099.82958984375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4464/20000], Loss: 4064.647216796875, Entropy -1160.765625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4465/20000], Loss: 4102.1845703125, Entropy -1178.73046875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4466/20000], Loss: 4060.984375, Entropy -1279.69189453125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4467/20000], Loss: 4168.98779296875, Entropy -1294.90869140625, Learning Rate: 6.103515625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4468/20000], Loss: 4179.46875, Entropy -1297.39892578125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4469/20000], Loss: 4080.1103515625, Entropy -1261.82666015625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4470/20000], Loss: 4161.88671875, Entropy -1306.21728515625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4471/20000], Loss: 4134.5009765625, Entropy -1269.912109375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4472/20000], Loss: 4188.70947265625, Entropy -1276.1416015625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4473/20000], Loss: 4155.0244140625, Entropy -1373.92236328125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4474/20000], Loss: 4120.138671875, Entropy -1206.265625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4475/20000], Loss: 4051.88720703125, Entropy -1284.0263671875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4476/20000], Loss: 4096.42236328125, Entropy -1235.09130859375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4477/20000], Loss: 4190.40673828125, Entropy -1369.267578125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4478/20000], Loss: 4124.97314453125, Entropy -1299.083984375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4479/20000], Loss: 4160.75390625, Entropy -1205.3125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4480/20000], Loss: 4073.427490234375, Entropy -1270.69970703125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4481/20000], Loss: 4088.6328125, Entropy -1250.2744140625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4482/20000], Loss: 4077.50537109375, Entropy -1217.3134765625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4483/20000], Loss: 4112.65185546875, Entropy -1227.26806640625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4484/20000], Loss: 4107.79248046875, Entropy -1381.24267578125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4485/20000], Loss: 4134.74853515625, Entropy -1360.20703125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4486/20000], Loss: 4117.92919921875, Entropy -1127.138671875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4487/20000], Loss: 4116.8857421875, Entropy -1320.97998046875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4488/20000], Loss: 4109.3203125, Entropy -1245.74853515625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4489/20000], Loss: 4055.20068359375, Entropy -1294.17626953125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4490/20000], Loss: 4150.06494140625, Entropy -1263.63134765625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4491/20000], Loss: 4139.07568359375, Entropy -1267.28662109375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4492/20000], Loss: 4183.447265625, Entropy -1261.49560546875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4493/20000], Loss: 4081.483154296875, Entropy -1169.16015625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4494/20000], Loss: 4151.00244140625, Entropy -1256.70849609375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4495/20000], Loss: 4110.216796875, Entropy -1301.19970703125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4496/20000], Loss: 4100.8388671875, Entropy -1287.48583984375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4497/20000], Loss: 4112.044921875, Entropy -1221.47802734375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4498/20000], Loss: 4091.414794921875, Entropy -1231.83544921875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4499/20000], Loss: 4108.5400390625, Entropy -1315.89990234375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4500/20000], Loss: 4148.1513671875, Entropy -1374.12451171875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4501/20000], Loss: 4226.97412109375, Entropy -1356.05322265625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4502/20000], Loss: 4147.5380859375, Entropy -1288.49462890625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4503/20000], Loss: 4097.31982421875, Entropy -1269.3896484375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4504/20000], Loss: 4103.392578125, Entropy -1258.17724609375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4505/20000], Loss: 4143.66015625, Entropy -1161.09521484375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4506/20000], Loss: 4146.76904296875, Entropy -1248.81396484375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4507/20000], Loss: 4144.3955078125, Entropy -1290.94873046875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4508/20000], Loss: 4167.66455078125, Entropy -1246.58740234375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4509/20000], Loss: 4048.395263671875, Entropy -1262.02197265625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4510/20000], Loss: 4145.06201171875, Entropy -1294.08251953125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4511/20000], Loss: 4049.31982421875, Entropy -1210.61572265625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4512/20000], Loss: 4150.6845703125, Entropy -1213.88671875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4513/20000], Loss: 4198.2119140625, Entropy -1461.96484375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4514/20000], Loss: 4076.45703125, Entropy -1192.587890625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4515/20000], Loss: 4125.41650390625, Entropy -1395.10498046875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4516/20000], Loss: 4171.1123046875, Entropy -1139.15185546875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4517/20000], Loss: 4151.9404296875, Entropy -1267.3720703125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4518/20000], Loss: 4152.27392578125, Entropy -1234.361328125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4519/20000], Loss: 4207.77783203125, Entropy -1353.38232421875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4520/20000], Loss: 4158.12890625, Entropy -1287.61572265625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4521/20000], Loss: 4139.3349609375, Entropy -1249.46435546875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4522/20000], Loss: 4105.7392578125, Entropy -1296.3662109375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4523/20000], Loss: 4128.89892578125, Entropy -1323.3857421875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4524/20000], Loss: 4071.298828125, Entropy -1244.955078125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4525/20000], Loss: 4112.953125, Entropy -1298.15576171875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4526/20000], Loss: 4090.095703125, Entropy -1342.10302734375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4527/20000], Loss: 4088.023681640625, Entropy -1259.27734375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4528/20000], Loss: 4096.20703125, Entropy -1261.1572265625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4529/20000], Loss: 4121.60693359375, Entropy -1185.330078125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4530/20000], Loss: 4109.67333984375, Entropy -1225.5341796875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4531/20000], Loss: 4171.10498046875, Entropy -1227.16259765625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4532/20000], Loss: 4068.66162109375, Entropy -1227.74951171875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4533/20000], Loss: 4127.9462890625, Entropy -1372.07470703125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4534/20000], Loss: 4129.53515625, Entropy -1254.1298828125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4535/20000], Loss: 4267.6005859375, Entropy -1194.85595703125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4536/20000], Loss: 4114.90380859375, Entropy -1222.30029296875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4537/20000], Loss: 4142.62646484375, Entropy -1240.41064453125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4538/20000], Loss: 4106.7353515625, Entropy -1206.98828125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4539/20000], Loss: 4091.17626953125, Entropy -1299.203125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4540/20000], Loss: 4082.464599609375, Entropy -1226.79052734375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4541/20000], Loss: 4162.6689453125, Entropy -1344.60498046875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4542/20000], Loss: 4190.02783203125, Entropy -1268.76953125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4543/20000], Loss: 4147.0556640625, Entropy -1324.50048828125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4544/20000], Loss: 4101.337890625, Entropy -1204.48583984375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4545/20000], Loss: 4244.52587890625, Entropy -1411.5185546875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4546/20000], Loss: 4089.22607421875, Entropy -1202.982421875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4547/20000], Loss: 4106.80029296875, Entropy -1263.4716796875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4548/20000], Loss: 4076.20166015625, Entropy -1264.8125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4549/20000], Loss: 4100.74609375, Entropy -1286.79638671875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4550/20000], Loss: 4103.533203125, Entropy -1371.3818359375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4551/20000], Loss: 4119.51171875, Entropy -1284.57421875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4552/20000], Loss: 4155.826171875, Entropy -1249.80419921875, Learning Rate: 6.103515625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4553/20000], Loss: 4095.080322265625, Entropy -1235.35498046875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4554/20000], Loss: 4101.06982421875, Entropy -1291.177734375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4555/20000], Loss: 4160.0361328125, Entropy -1286.671875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4556/20000], Loss: 4088.515869140625, Entropy -1185.83544921875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4557/20000], Loss: 4145.63037109375, Entropy -1371.0009765625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4558/20000], Loss: 4166.083984375, Entropy -1330.5263671875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4559/20000], Loss: 4056.31201171875, Entropy -1289.40576171875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4560/20000], Loss: 4141.5087890625, Entropy -1315.09423828125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4561/20000], Loss: 4162.482421875, Entropy -1213.75830078125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4562/20000], Loss: 4081.544189453125, Entropy -1155.57861328125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4563/20000], Loss: 4111.43505859375, Entropy -1302.8818359375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4564/20000], Loss: 4072.156494140625, Entropy -1259.05126953125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4565/20000], Loss: 4142.30810546875, Entropy -1300.36669921875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4566/20000], Loss: 4107.8505859375, Entropy -1241.45263671875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4567/20000], Loss: 4181.083984375, Entropy -1376.318359375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4568/20000], Loss: 4129.88427734375, Entropy -1318.7158203125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4569/20000], Loss: 4064.7001953125, Entropy -1261.22802734375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4570/20000], Loss: 4130.20263671875, Entropy -1197.77734375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4571/20000], Loss: 4109.8662109375, Entropy -1214.9560546875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4572/20000], Loss: 4107.80126953125, Entropy -1260.869140625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4573/20000], Loss: 4126.810546875, Entropy -1294.9609375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4574/20000], Loss: 4134.08154296875, Entropy -1362.7138671875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4575/20000], Loss: 4093.541259765625, Entropy -1236.65087890625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4576/20000], Loss: 4111.23779296875, Entropy -1264.54150390625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4577/20000], Loss: 4218.9794921875, Entropy -1425.7978515625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4578/20000], Loss: 4075.492919921875, Entropy -1263.39501953125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4579/20000], Loss: 4156.9443359375, Entropy -1258.37890625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4580/20000], Loss: 4152.53564453125, Entropy -1229.9853515625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4581/20000], Loss: 4115.17724609375, Entropy -1278.9345703125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4582/20000], Loss: 4089.889892578125, Entropy -1147.49365234375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4583/20000], Loss: 4182.2275390625, Entropy -1367.8173828125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4584/20000], Loss: 4121.59716796875, Entropy -1338.81298828125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4585/20000], Loss: 4121.7509765625, Entropy -1355.06005859375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4586/20000], Loss: 4098.83203125, Entropy -1257.27587890625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4587/20000], Loss: 4186.7939453125, Entropy -1506.7177734375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4588/20000], Loss: 4092.97216796875, Entropy -1207.31787109375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4589/20000], Loss: 4112.3564453125, Entropy -1326.05419921875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4590/20000], Loss: 4137.39013671875, Entropy -1237.1240234375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4591/20000], Loss: 4074.949462890625, Entropy -1202.216796875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4592/20000], Loss: 4116.505859375, Entropy -1294.22216796875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4593/20000], Loss: 4165.8232421875, Entropy -1288.87109375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4594/20000], Loss: 4094.26708984375, Entropy -1135.7666015625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4595/20000], Loss: 4061.8974609375, Entropy -1242.77880859375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4596/20000], Loss: 4105.96337890625, Entropy -1357.1455078125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4597/20000], Loss: 4088.694091796875, Entropy -1287.98779296875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4598/20000], Loss: 4057.609375, Entropy -1246.71142578125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4599/20000], Loss: 4075.80908203125, Entropy -1245.17529296875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4600/20000], Loss: 4116.5390625, Entropy -1223.44677734375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4601/20000], Loss: 4111.8251953125, Entropy -1189.13134765625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4602/20000], Loss: 4192.54736328125, Entropy -1284.19482421875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4603/20000], Loss: 4190.578125, Entropy -1193.759765625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4604/20000], Loss: 4079.552001953125, Entropy -1274.48974609375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4605/20000], Loss: 4074.427734375, Entropy -1295.38623046875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4606/20000], Loss: 4085.765625, Entropy -1203.77197265625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4607/20000], Loss: 4058.9736328125, Entropy -1270.79443359375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4608/20000], Loss: 4176.37939453125, Entropy -1186.6484375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4609/20000], Loss: 4112.56298828125, Entropy -1294.18115234375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4610/20000], Loss: 4140.63671875, Entropy -1263.310546875, Learning Rate: 6.103515625e-06\n",
      "Epoch [4611/20000], Loss: 4147.599609375, Entropy -1275.048828125, Learning Rate: 6.103515625e-06\n",
      "Epoch [4612/20000], Loss: 4083.890625, Entropy -1320.7568359375, Learning Rate: 6.103515625e-06\n",
      "Epoch [4613/20000], Loss: 4166.232421875, Entropy -1278.81103515625, Learning Rate: 6.103515625e-06\n",
      "Epoch [4614/20000], Loss: 4132.8916015625, Entropy -1345.8544921875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4615/20000], Loss: 4124.798828125, Entropy -1267.94287109375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4616/20000], Loss: 4040.597412109375, Entropy -1185.09130859375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4617/20000], Loss: 4244.1591796875, Entropy -1350.724609375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4618/20000], Loss: 4164.60595703125, Entropy -1359.46630859375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4619/20000], Loss: 4140.91650390625, Entropy -1258.1328125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4620/20000], Loss: 4146.52392578125, Entropy -1233.373046875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4621/20000], Loss: 4118.8076171875, Entropy -1287.4140625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4622/20000], Loss: 4141.34814453125, Entropy -1293.884765625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4623/20000], Loss: 4150.2392578125, Entropy -1288.9111328125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4624/20000], Loss: 4150.822265625, Entropy -1363.82373046875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4625/20000], Loss: 4130.4130859375, Entropy -1333.0908203125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4626/20000], Loss: 4073.780517578125, Entropy -1287.31982421875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4627/20000], Loss: 4095.089111328125, Entropy -1312.78955078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4628/20000], Loss: 4106.65576171875, Entropy -1232.39697265625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4629/20000], Loss: 4113.92529296875, Entropy -1240.3837890625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4630/20000], Loss: 4151.78564453125, Entropy -1326.806640625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4631/20000], Loss: 4118.37548828125, Entropy -1268.4501953125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4632/20000], Loss: 4101.60595703125, Entropy -1216.79248046875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4633/20000], Loss: 4134.75, Entropy -1321.2255859375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4634/20000], Loss: 4106.82373046875, Entropy -1330.34912109375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4635/20000], Loss: 4137.05078125, Entropy -1265.1552734375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4636/20000], Loss: 4085.36328125, Entropy -1266.376953125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4637/20000], Loss: 4147.71826171875, Entropy -1301.09130859375, Learning Rate: 3.0517578125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4638/20000], Loss: 4125.2001953125, Entropy -1317.2255859375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4639/20000], Loss: 4113.0830078125, Entropy -1310.96728515625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4640/20000], Loss: 4095.39306640625, Entropy -1264.83935546875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4641/20000], Loss: 4082.74609375, Entropy -1328.7919921875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4642/20000], Loss: 4086.0205078125, Entropy -1243.52685546875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4643/20000], Loss: 4104.66455078125, Entropy -1274.31494140625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4644/20000], Loss: 4129.8125, Entropy -1261.87841796875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4645/20000], Loss: 4102.251953125, Entropy -1317.84423828125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4646/20000], Loss: 4115.07958984375, Entropy -1230.34130859375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4647/20000], Loss: 4090.1865234375, Entropy -1194.42578125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4648/20000], Loss: 4155.48876953125, Entropy -1293.17138671875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4649/20000], Loss: 4084.822509765625, Entropy -1144.9609375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4650/20000], Loss: 4120.015625, Entropy -1252.998046875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4651/20000], Loss: 4084.59130859375, Entropy -1296.1279296875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4652/20000], Loss: 4087.734619140625, Entropy -1283.80419921875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4653/20000], Loss: 4140.37255859375, Entropy -1279.90966796875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4654/20000], Loss: 4092.8076171875, Entropy -1255.947265625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4655/20000], Loss: 4120.482421875, Entropy -1260.37109375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4656/20000], Loss: 4127.478515625, Entropy -1293.31298828125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4657/20000], Loss: 4146.12109375, Entropy -1362.61474609375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4658/20000], Loss: 4094.094970703125, Entropy -1235.55859375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4659/20000], Loss: 4116.53564453125, Entropy -1349.3642578125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4660/20000], Loss: 4116.4296875, Entropy -1239.54638671875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4661/20000], Loss: 4122.666015625, Entropy -1293.87890625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4662/20000], Loss: 4122.501953125, Entropy -1300.3857421875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4663/20000], Loss: 4135.40625, Entropy -1276.19677734375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4664/20000], Loss: 4097.89111328125, Entropy -1320.68115234375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4665/20000], Loss: 4187.14453125, Entropy -1344.10693359375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4666/20000], Loss: 4111.5693359375, Entropy -1249.24951171875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4667/20000], Loss: 4107.482421875, Entropy -1294.89306640625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4668/20000], Loss: 4145.00927734375, Entropy -1384.63720703125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4669/20000], Loss: 4097.31396484375, Entropy -1320.50244140625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4670/20000], Loss: 4165.25732421875, Entropy -1342.3544921875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4671/20000], Loss: 4109.22314453125, Entropy -1256.50341796875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4672/20000], Loss: 4102.32568359375, Entropy -1195.31298828125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4673/20000], Loss: 4141.2451171875, Entropy -1272.37060546875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4674/20000], Loss: 4124.263671875, Entropy -1204.32275390625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4675/20000], Loss: 4032.08251953125, Entropy -1165.10986328125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4676/20000], Loss: 4132.0390625, Entropy -1327.3056640625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4677/20000], Loss: 4137.626953125, Entropy -1407.384521484375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4678/20000], Loss: 4093.18603515625, Entropy -1181.974609375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4679/20000], Loss: 4117.35498046875, Entropy -1325.12353515625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4680/20000], Loss: 4129.107421875, Entropy -1317.49169921875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4681/20000], Loss: 4121.326171875, Entropy -1277.80712890625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4682/20000], Loss: 4118.0390625, Entropy -1206.22900390625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4683/20000], Loss: 4137.78369140625, Entropy -1224.5791015625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4684/20000], Loss: 4132.208984375, Entropy -1245.55810546875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4685/20000], Loss: 4133.87451171875, Entropy -1364.7138671875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4686/20000], Loss: 4098.52880859375, Entropy -1181.3310546875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4687/20000], Loss: 4142.52734375, Entropy -1296.69580078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4688/20000], Loss: 4114.669921875, Entropy -1196.8193359375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4689/20000], Loss: 4132.2548828125, Entropy -1244.13330078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4690/20000], Loss: 4108.28466796875, Entropy -1236.240234375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4691/20000], Loss: 4158.5859375, Entropy -1371.03369140625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4692/20000], Loss: 4106.21044921875, Entropy -1256.3330078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4693/20000], Loss: 4131.7197265625, Entropy -1251.85400390625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4694/20000], Loss: 4086.1181640625, Entropy -1234.07568359375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4695/20000], Loss: 4103.564453125, Entropy -1334.767578125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4696/20000], Loss: 4323.22802734375, Entropy -1353.2841796875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4697/20000], Loss: 4141.111328125, Entropy -1236.8525390625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4698/20000], Loss: 4126.833984375, Entropy -1300.33642578125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4699/20000], Loss: 4061.4833984375, Entropy -1211.50048828125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4700/20000], Loss: 4114.65478515625, Entropy -1228.84521484375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4701/20000], Loss: 4185.34228515625, Entropy -1221.83935546875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4702/20000], Loss: 4160.65185546875, Entropy -1324.72509765625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4703/20000], Loss: 4163.12548828125, Entropy -1278.20458984375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4704/20000], Loss: 4089.5400390625, Entropy -1298.25341796875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4705/20000], Loss: 4140.87158203125, Entropy -1283.0595703125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4706/20000], Loss: 4059.309814453125, Entropy -1227.388671875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4707/20000], Loss: 4151.6669921875, Entropy -1355.16845703125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4708/20000], Loss: 4137.60498046875, Entropy -1320.70458984375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4709/20000], Loss: 4081.45947265625, Entropy -1215.576171875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4710/20000], Loss: 4068.76513671875, Entropy -1254.552734375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4711/20000], Loss: 4116.06494140625, Entropy -1319.47509765625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4712/20000], Loss: 4123.490234375, Entropy -1314.349609375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4713/20000], Loss: 4146.24853515625, Entropy -1278.75048828125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4714/20000], Loss: 4087.822998046875, Entropy -1281.56884765625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4715/20000], Loss: 4216.3095703125, Entropy -1354.60205078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4716/20000], Loss: 4095.662841796875, Entropy -1303.25146484375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4717/20000], Loss: 4163.0439453125, Entropy -1443.87890625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4718/20000], Loss: 4144.9462890625, Entropy -1337.18212890625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4719/20000], Loss: 4124.7646484375, Entropy -1224.66748046875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4720/20000], Loss: 4126.02685546875, Entropy -1373.54345703125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4721/20000], Loss: 4184.90771484375, Entropy -1288.06494140625, Learning Rate: 3.0517578125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4722/20000], Loss: 4124.453125, Entropy -1366.60888671875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4723/20000], Loss: 4072.43408203125, Entropy -1296.93896484375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4724/20000], Loss: 4124.1044921875, Entropy -1283.19580078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4725/20000], Loss: 4179.44873046875, Entropy -1211.07421875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4726/20000], Loss: 4105.34765625, Entropy -1254.3310546875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4727/20000], Loss: 4146.88671875, Entropy -1310.27099609375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4728/20000], Loss: 4105.5615234375, Entropy -1224.7548828125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4729/20000], Loss: 4132.25390625, Entropy -1326.5478515625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4730/20000], Loss: 4165.8525390625, Entropy -1382.24951171875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4731/20000], Loss: 4087.32275390625, Entropy -1170.01904296875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4732/20000], Loss: 4088.302001953125, Entropy -1191.818359375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4733/20000], Loss: 4097.0888671875, Entropy -1146.20751953125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4734/20000], Loss: 4085.8154296875, Entropy -1228.1416015625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4735/20000], Loss: 4085.51025390625, Entropy -1221.9052734375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4736/20000], Loss: 4074.68115234375, Entropy -1257.08837890625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4737/20000], Loss: 4096.8896484375, Entropy -1316.19921875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4738/20000], Loss: 4115.51806640625, Entropy -1262.57177734375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4739/20000], Loss: 4133.74609375, Entropy -1298.89697265625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4740/20000], Loss: 4135.47607421875, Entropy -1254.0029296875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4741/20000], Loss: 4122.3544921875, Entropy -1347.818359375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4742/20000], Loss: 4218.5810546875, Entropy -1447.564453125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4743/20000], Loss: 4170.03125, Entropy -1236.18701171875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4744/20000], Loss: 4115.2275390625, Entropy -1350.56884765625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4745/20000], Loss: 4156.73779296875, Entropy -1363.88232421875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4746/20000], Loss: 4113.40576171875, Entropy -1341.96923828125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4747/20000], Loss: 4127.1875, Entropy -1317.1796875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4748/20000], Loss: 4160.7431640625, Entropy -1257.99072265625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4749/20000], Loss: 4147.85888671875, Entropy -1353.3994140625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4750/20000], Loss: 4058.6240234375, Entropy -1242.8193359375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4751/20000], Loss: 4124.81884765625, Entropy -1277.01171875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4752/20000], Loss: 4130.228515625, Entropy -1233.45263671875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4753/20000], Loss: 4103.11376953125, Entropy -1308.04833984375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4754/20000], Loss: 4091.662353515625, Entropy -1155.79150390625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4755/20000], Loss: 4110.03369140625, Entropy -1316.2880859375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4756/20000], Loss: 4105.50927734375, Entropy -1113.42333984375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4757/20000], Loss: 4093.0673828125, Entropy -1151.70458984375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4758/20000], Loss: 4126.96435546875, Entropy -1355.28173828125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4759/20000], Loss: 4143.103515625, Entropy -1267.85791015625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4760/20000], Loss: 4070.87841796875, Entropy -1227.1904296875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4761/20000], Loss: 4181.54345703125, Entropy -1309.31396484375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4762/20000], Loss: 4091.517822265625, Entropy -1289.19580078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4763/20000], Loss: 4132.8017578125, Entropy -1288.28955078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4764/20000], Loss: 4103.62255859375, Entropy -1263.77294921875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4765/20000], Loss: 4096.1240234375, Entropy -1331.31298828125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4766/20000], Loss: 4141.5458984375, Entropy -1233.50927734375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4767/20000], Loss: 4100.99072265625, Entropy -1297.1240234375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4768/20000], Loss: 4110.91796875, Entropy -1191.68505859375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4769/20000], Loss: 4135.11767578125, Entropy -1280.03662109375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4770/20000], Loss: 4127.9404296875, Entropy -1303.36279296875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4771/20000], Loss: 4162.8173828125, Entropy -1275.81591796875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4772/20000], Loss: 4120.2451171875, Entropy -1280.50341796875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4773/20000], Loss: 4109.4501953125, Entropy -1219.95849609375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4774/20000], Loss: 4055.20703125, Entropy -1228.6640625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4775/20000], Loss: 4162.787109375, Entropy -1359.20166015625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4776/20000], Loss: 4105.58349609375, Entropy -1256.16748046875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4777/20000], Loss: 4075.5732421875, Entropy -1238.98193359375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4778/20000], Loss: 4080.23095703125, Entropy -1240.1962890625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4779/20000], Loss: 4145.13720703125, Entropy -1314.57177734375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4780/20000], Loss: 4170.7861328125, Entropy -1253.720703125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4781/20000], Loss: 4115.9482421875, Entropy -1296.123046875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4782/20000], Loss: 4120.5576171875, Entropy -1263.94287109375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4783/20000], Loss: 4110.71484375, Entropy -1275.88134765625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4784/20000], Loss: 4093.37255859375, Entropy -1259.0400390625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4785/20000], Loss: 4108.13818359375, Entropy -1210.92236328125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4786/20000], Loss: 4155.70556640625, Entropy -1330.84619140625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4787/20000], Loss: 4145.91357421875, Entropy -1376.3203125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4788/20000], Loss: 4102.69921875, Entropy -1253.88525390625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4789/20000], Loss: 4080.787109375, Entropy -1292.95068359375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4790/20000], Loss: 4117.08642578125, Entropy -1324.73779296875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4791/20000], Loss: 4117.30224609375, Entropy -1239.583984375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4792/20000], Loss: 4046.62939453125, Entropy -1243.14013671875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4793/20000], Loss: 4074.744873046875, Entropy -1296.72119140625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4794/20000], Loss: 4108.16796875, Entropy -1264.30078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4795/20000], Loss: 4174.3154296875, Entropy -1286.42041015625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4796/20000], Loss: 4136.30419921875, Entropy -1267.9619140625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4797/20000], Loss: 4097.81689453125, Entropy -1191.583984375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4798/20000], Loss: 4156.59375, Entropy -1356.95263671875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4799/20000], Loss: 4118.416015625, Entropy -1151.3623046875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4800/20000], Loss: 4115.3193359375, Entropy -1355.14111328125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4801/20000], Loss: 4084.76123046875, Entropy -1251.7822265625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4802/20000], Loss: 4038.90771484375, Entropy -1228.36279296875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4803/20000], Loss: 4068.8291015625, Entropy -1231.5087890625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4804/20000], Loss: 4069.4384765625, Entropy -1242.77099609375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4805/20000], Loss: 4139.36474609375, Entropy -1295.9609375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4806/20000], Loss: 4122.8447265625, Entropy -1260.64306640625, Learning Rate: 3.0517578125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4807/20000], Loss: 4137.87548828125, Entropy -1300.36572265625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4808/20000], Loss: 4081.791259765625, Entropy -1195.78857421875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4809/20000], Loss: 4075.46533203125, Entropy -1204.30029296875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4810/20000], Loss: 4064.22900390625, Entropy -1218.19677734375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4811/20000], Loss: 4143.50830078125, Entropy -1297.4951171875, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4812/20000], Loss: 4098.2529296875, Entropy -1294.38330078125, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4813/20000], Loss: 4094.52294921875, Entropy -1236.7744140625, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4814/20000], Loss: 4177.2451171875, Entropy -1227.16162109375, Learning Rate: 3.0517578125e-06\n",
      "Epoch [4815/20000], Loss: 4062.730224609375, Entropy -1165.96923828125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4816/20000], Loss: 4084.88916015625, Entropy -1173.888671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4817/20000], Loss: 4176.51416015625, Entropy -1308.18701171875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4818/20000], Loss: 4139.09375, Entropy -1236.2646484375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4819/20000], Loss: 4133.64208984375, Entropy -1326.78369140625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4820/20000], Loss: 4083.3583984375, Entropy -1237.77734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4821/20000], Loss: 4126.01708984375, Entropy -1213.69775390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4822/20000], Loss: 4182.75732421875, Entropy -1356.58251953125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4823/20000], Loss: 4260.09912109375, Entropy -1542.234619140625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4824/20000], Loss: 4119.23828125, Entropy -1327.54736328125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4825/20000], Loss: 4071.897216796875, Entropy -1227.13232421875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4826/20000], Loss: 4134.48876953125, Entropy -1334.45703125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4827/20000], Loss: 4136.12548828125, Entropy -1179.365234375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4828/20000], Loss: 4182.55029296875, Entropy -1325.384765625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4829/20000], Loss: 4103.87255859375, Entropy -1281.48388671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4830/20000], Loss: 4160.90771484375, Entropy -1264.96630859375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4831/20000], Loss: 4170.2333984375, Entropy -1265.5517578125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4832/20000], Loss: 4162.046875, Entropy -1418.075439453125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4833/20000], Loss: 4129.705078125, Entropy -1316.02685546875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4834/20000], Loss: 4146.552734375, Entropy -1273.69775390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4835/20000], Loss: 4139.53369140625, Entropy -1395.973388671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4836/20000], Loss: 4131.4765625, Entropy -1256.546875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4837/20000], Loss: 4128.654296875, Entropy -1239.35986328125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4838/20000], Loss: 4132.0302734375, Entropy -1240.56298828125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4839/20000], Loss: 4098.25244140625, Entropy -1258.51220703125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4840/20000], Loss: 4097.09912109375, Entropy -1299.1572265625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4841/20000], Loss: 4116.0966796875, Entropy -1257.66650390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4842/20000], Loss: 4095.10400390625, Entropy -1253.46044921875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4843/20000], Loss: 4029.435546875, Entropy -1233.2763671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4844/20000], Loss: 4151.14111328125, Entropy -1302.0458984375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4845/20000], Loss: 4098.58203125, Entropy -1329.208984375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4846/20000], Loss: 4114.03076171875, Entropy -1322.12744140625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4847/20000], Loss: 4066.258544921875, Entropy -1231.2841796875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4848/20000], Loss: 4129.5458984375, Entropy -1274.58642578125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4849/20000], Loss: 4121.1572265625, Entropy -1299.33154296875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4850/20000], Loss: 4123.9228515625, Entropy -1295.25244140625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4851/20000], Loss: 4077.777099609375, Entropy -1180.41845703125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4852/20000], Loss: 4037.86767578125, Entropy -1201.67919921875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4853/20000], Loss: 4064.74658203125, Entropy -1201.716796875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4854/20000], Loss: 4075.419921875, Entropy -1112.8466796875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4855/20000], Loss: 4140.4931640625, Entropy -1334.60791015625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4856/20000], Loss: 4091.11669921875, Entropy -1284.43359375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4857/20000], Loss: 4095.91748046875, Entropy -1258.2236328125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4858/20000], Loss: 4104.45361328125, Entropy -1301.205078125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4859/20000], Loss: 4141.611328125, Entropy -1369.76611328125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4860/20000], Loss: 4138.08056640625, Entropy -1300.64453125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4861/20000], Loss: 4135.3916015625, Entropy -1352.39453125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4862/20000], Loss: 4114.29345703125, Entropy -1218.66357421875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4863/20000], Loss: 4118.5, Entropy -1327.51025390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4864/20000], Loss: 4083.473388671875, Entropy -1172.78857421875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4865/20000], Loss: 4166.6376953125, Entropy -1271.39794921875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4866/20000], Loss: 4116.8828125, Entropy -1250.90478515625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4867/20000], Loss: 4164.12158203125, Entropy -1289.97021484375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4868/20000], Loss: 4108.95654296875, Entropy -1330.96923828125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4869/20000], Loss: 4114.015625, Entropy -1233.478515625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4870/20000], Loss: 4084.470458984375, Entropy -1280.9375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4871/20000], Loss: 4102.0986328125, Entropy -1234.33837890625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4872/20000], Loss: 4124.89111328125, Entropy -1282.46923828125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4873/20000], Loss: 4121.1416015625, Entropy -1311.10693359375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4874/20000], Loss: 4065.927734375, Entropy -1235.947265625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4875/20000], Loss: 4115.88671875, Entropy -1238.33642578125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4876/20000], Loss: 4159.98486328125, Entropy -1288.71728515625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4877/20000], Loss: 4149.9990234375, Entropy -1248.46630859375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4878/20000], Loss: 4106.42529296875, Entropy -1316.35302734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4879/20000], Loss: 4102.46484375, Entropy -1312.27099609375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4880/20000], Loss: 4156.17822265625, Entropy -1302.7431640625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4881/20000], Loss: 4127.2578125, Entropy -1178.248046875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4882/20000], Loss: 4119.39501953125, Entropy -1346.08447265625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4883/20000], Loss: 4078.0537109375, Entropy -1227.7958984375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4884/20000], Loss: 4114.8251953125, Entropy -1358.31591796875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4885/20000], Loss: 4113.365234375, Entropy -1351.09130859375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4886/20000], Loss: 4104.09814453125, Entropy -1198.29833984375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4887/20000], Loss: 4068.736328125, Entropy -1216.58349609375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4888/20000], Loss: 4187.41650390625, Entropy -1342.470703125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4889/20000], Loss: 4076.44970703125, Entropy -1348.8544921875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4890/20000], Loss: 4176.58203125, Entropy -1267.87744140625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4891/20000], Loss: 4100.79541015625, Entropy -1253.66650390625, Learning Rate: 1.52587890625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4892/20000], Loss: 4099.63671875, Entropy -1298.21044921875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4893/20000], Loss: 4126.81884765625, Entropy -1282.6337890625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4894/20000], Loss: 4200.99169921875, Entropy -1403.0576171875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4895/20000], Loss: 4177.642578125, Entropy -1319.302734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4896/20000], Loss: 4092.30419921875, Entropy -1222.0771484375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4897/20000], Loss: 4126.45068359375, Entropy -1241.1240234375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4898/20000], Loss: 4171.81640625, Entropy -1338.51318359375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4899/20000], Loss: 4088.670166015625, Entropy -1214.0966796875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4900/20000], Loss: 4106.89404296875, Entropy -1330.54931640625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4901/20000], Loss: 4133.59423828125, Entropy -1293.27880859375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4902/20000], Loss: 4186.12646484375, Entropy -1275.95068359375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4903/20000], Loss: 4233.44677734375, Entropy -1326.9755859375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4904/20000], Loss: 4126.37646484375, Entropy -1284.2568359375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4905/20000], Loss: 4094.794677734375, Entropy -1300.0927734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4906/20000], Loss: 4103.9404296875, Entropy -1343.19921875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4907/20000], Loss: 4126.49755859375, Entropy -1288.52294921875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4908/20000], Loss: 4081.6220703125, Entropy -1207.7314453125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4909/20000], Loss: 4109.56884765625, Entropy -1315.9423828125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4910/20000], Loss: 4145.142578125, Entropy -1383.64013671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4911/20000], Loss: 4189.2099609375, Entropy -1159.02978515625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4912/20000], Loss: 4145.3212890625, Entropy -1345.048828125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4913/20000], Loss: 4231.1630859375, Entropy -1226.7275390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4914/20000], Loss: 4166.80126953125, Entropy -1276.53662109375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4915/20000], Loss: 4086.099609375, Entropy -1265.24609375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4916/20000], Loss: 4113.662109375, Entropy -1297.91845703125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4917/20000], Loss: 4084.267333984375, Entropy -1271.185546875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4918/20000], Loss: 4091.0859375, Entropy -1135.66064453125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4919/20000], Loss: 4123.58642578125, Entropy -1322.60888671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4920/20000], Loss: 4050.270263671875, Entropy -1212.357421875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4921/20000], Loss: 4081.655029296875, Entropy -1216.31005859375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4922/20000], Loss: 4175.349609375, Entropy -1416.49609375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4923/20000], Loss: 4141.54833984375, Entropy -1235.73681640625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4924/20000], Loss: 4079.45556640625, Entropy -1266.84716796875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4925/20000], Loss: 4094.101806640625, Entropy -1222.03466796875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4926/20000], Loss: 4149.240234375, Entropy -1348.177734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4927/20000], Loss: 4109.15087890625, Entropy -1244.26904296875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4928/20000], Loss: 4089.080078125, Entropy -1229.60595703125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4929/20000], Loss: 4183.2626953125, Entropy -1303.17724609375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4930/20000], Loss: 4126.52197265625, Entropy -1398.45947265625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4931/20000], Loss: 4133.42919921875, Entropy -1299.33447265625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4932/20000], Loss: 4164.32373046875, Entropy -1308.4248046875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4933/20000], Loss: 4125.34228515625, Entropy -1247.54638671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4934/20000], Loss: 4116.44482421875, Entropy -1301.72509765625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4935/20000], Loss: 4160.859375, Entropy -1300.60009765625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4936/20000], Loss: 4115.2529296875, Entropy -1258.10107421875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4937/20000], Loss: 4117.21630859375, Entropy -1386.484375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4938/20000], Loss: 4084.682373046875, Entropy -1214.828125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4939/20000], Loss: 4192.01220703125, Entropy -1329.685546875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4940/20000], Loss: 4089.66357421875, Entropy -1212.041015625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4941/20000], Loss: 4121.38134765625, Entropy -1285.26806640625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4942/20000], Loss: 4136.8564453125, Entropy -1155.552734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4943/20000], Loss: 4064.797119140625, Entropy -1288.06591796875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4944/20000], Loss: 4092.691162109375, Entropy -1288.91650390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4945/20000], Loss: 4063.722900390625, Entropy -1226.21240234375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4946/20000], Loss: 4102.4189453125, Entropy -1326.201171875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4947/20000], Loss: 4077.006591796875, Entropy -1264.2080078125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4948/20000], Loss: 4187.322265625, Entropy -1278.490234375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4949/20000], Loss: 4144.23095703125, Entropy -1315.24609375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4950/20000], Loss: 4090.77490234375, Entropy -1206.88134765625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4951/20000], Loss: 4174.5341796875, Entropy -1235.4052734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4952/20000], Loss: 4130.27587890625, Entropy -1350.7958984375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4953/20000], Loss: 4095.171142578125, Entropy -1256.0615234375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4954/20000], Loss: 4109.625, Entropy -1379.318359375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4955/20000], Loss: 4135.13232421875, Entropy -1215.86279296875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4956/20000], Loss: 4108.44873046875, Entropy -1315.96728515625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4957/20000], Loss: 4135.17919921875, Entropy -1311.75390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4958/20000], Loss: 4145.58984375, Entropy -1284.189453125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4959/20000], Loss: 4093.189208984375, Entropy -1229.70458984375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4960/20000], Loss: 4189.5224609375, Entropy -1304.72900390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4961/20000], Loss: 4154.787109375, Entropy -1243.52978515625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4962/20000], Loss: 4154.9423828125, Entropy -1326.85546875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4963/20000], Loss: 4097.337890625, Entropy -1216.9375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4964/20000], Loss: 4039.6572265625, Entropy -1211.3271484375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4965/20000], Loss: 4108.501953125, Entropy -1279.6826171875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4966/20000], Loss: 4090.68115234375, Entropy -1249.97021484375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4967/20000], Loss: 4151.3515625, Entropy -1403.413818359375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4968/20000], Loss: 4184.77783203125, Entropy -1384.8037109375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4969/20000], Loss: 4138.31884765625, Entropy -1375.04052734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4970/20000], Loss: 4143.0439453125, Entropy -1345.13427734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4971/20000], Loss: 4149.6181640625, Entropy -1350.25537109375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4972/20000], Loss: 4162.32080078125, Entropy -1232.8564453125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4973/20000], Loss: 4188.111328125, Entropy -1188.5537109375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4974/20000], Loss: 4137.8330078125, Entropy -1201.31494140625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4975/20000], Loss: 4134.04052734375, Entropy -1328.32177734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4976/20000], Loss: 4142.4697265625, Entropy -1259.27392578125, Learning Rate: 1.52587890625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4977/20000], Loss: 4179.078125, Entropy -1305.85107421875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4978/20000], Loss: 4078.635986328125, Entropy -1288.94287109375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4979/20000], Loss: 4121.12060546875, Entropy -1262.4462890625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4980/20000], Loss: 4078.728271484375, Entropy -1249.03515625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4981/20000], Loss: 4111.54541015625, Entropy -1310.20654296875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4982/20000], Loss: 4131.615234375, Entropy -1219.48291015625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4983/20000], Loss: 4121.60400390625, Entropy -1306.88916015625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4984/20000], Loss: 4139.54638671875, Entropy -1220.69287109375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4985/20000], Loss: 4114.51708984375, Entropy -1281.53662109375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4986/20000], Loss: 4097.10400390625, Entropy -1230.30517578125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4987/20000], Loss: 4205.54931640625, Entropy -1266.26611328125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4988/20000], Loss: 4140.31494140625, Entropy -1248.568359375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4989/20000], Loss: 4117.96044921875, Entropy -1237.56787109375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4990/20000], Loss: 4119.3740234375, Entropy -1200.04541015625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4991/20000], Loss: 4081.561279296875, Entropy -1251.09130859375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4992/20000], Loss: 4224.45751953125, Entropy -1243.9345703125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4993/20000], Loss: 4118.19775390625, Entropy -1205.59423828125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4994/20000], Loss: 4124.0244140625, Entropy -1215.6025390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4995/20000], Loss: 4105.2763671875, Entropy -1323.22802734375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4996/20000], Loss: 4080.349853515625, Entropy -1232.9013671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4997/20000], Loss: 4102.67626953125, Entropy -1262.76513671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4998/20000], Loss: 4183.9404296875, Entropy -1435.990234375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [4999/20000], Loss: 4140.56298828125, Entropy -1362.025390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5000/20000], Loss: 4133.78662109375, Entropy -1157.45458984375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5001/20000], Loss: 4209.0029296875, Entropy -1456.2529296875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5002/20000], Loss: 4128.31982421875, Entropy -1284.76513671875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5003/20000], Loss: 4123.71875, Entropy -1358.13818359375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5004/20000], Loss: 4086.2626953125, Entropy -1179.09521484375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5005/20000], Loss: 4164.041015625, Entropy -1300.66650390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5006/20000], Loss: 4117.02197265625, Entropy -1247.37353515625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5007/20000], Loss: 4159.83251953125, Entropy -1340.01708984375, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5008/20000], Loss: 4115.099609375, Entropy -1269.6328125, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5009/20000], Loss: 4170.78076171875, Entropy -1338.90966796875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5010/20000], Loss: 4094.360107421875, Entropy -1252.47900390625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5011/20000], Loss: 4146.3193359375, Entropy -1340.4931640625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5012/20000], Loss: 4142.6875, Entropy -1333.197265625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5013/20000], Loss: 4072.730224609375, Entropy -1213.11572265625, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5014/20000], Loss: 4124.4111328125, Entropy -1288.8310546875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5015/20000], Loss: 4168.6455078125, Entropy -1240.16748046875, Learning Rate: 1.52587890625e-06\n",
      "Epoch [5016/20000], Loss: 4144.599609375, Entropy -1322.32763671875, Learning Rate: 7.62939453125e-07\n",
      "3407 [tensor(4018.3054, device='cuda:0'), tensor(-1089.1714, device='cuda:0'), tensor(-2568.6096, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "\n",
    "k_MC=1000#size_sample\n",
    "\n",
    "#sample, = ax.scatter([],[],color='red',alpha=0.07)\n",
    "#fig.canvas.draw()\n",
    "    \n",
    "    \n",
    "'''\n",
    "def show(GeN,n,alpha=0.07):\n",
    "    Z=GeN(n).detach().clone().cpu()\n",
    "    plt.pcolormesh(grid_x.numpy(),grid_y.numpy(),p.exp().numpy())\n",
    "    plt.scatter(Z[:,0],Z[:,1],color='red',alpha=alpha) \n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "''' \n",
    "\n",
    "def show(GeN,n):\n",
    "    #Z=GeN(200).detach()\n",
    "    #fig=setup.makePlot(Z,device)\n",
    "    #plt.show()\n",
    "    return\n",
    "    \n",
    "#lr =.03 for lat_dim 5\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    optimizer = GeNPredVI(loglikelihood, logprior, projection, k_MC,\n",
    "\t\t                                    0, 100, 1000, 50, 50,\n",
    "\t\t                                    20000, .05, .000001, 200, .5,\n",
    "\t\t                                    device, True, temp_dir, save_best=True)\n",
    "    best_epoch, scores=optimizer.run(GeN,show)\n",
    "print(best_epoch,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2c7a407910>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "y=optimizer.score_entropy\n",
    "time=torch.arange(0,len(y),1.)\n",
    "plt.title('KL divergence')\n",
    "plt.plot(time, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2c7a41f6d0>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "y=torch.as_tensor(optimizer.score_elbo)\n",
    "time=torch.arange(0,len(y),1.)\n",
    "plt.title('ELBO')\n",
    "plt.plot(time, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2c7a381dd0>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "y=optimizer.score_lr\n",
    "time=torch.arange(0,len(y),1.)\n",
    "plt.plot(time, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "makePlot() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-3d44bbfdba43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-3d44bbfdba43>\u001b[0m in \u001b[0;36mshow\u001b[0;34m(GeN, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mZ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGeN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakePlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predictive Variatiational Inference, GeNVI lat_dim='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: makePlot() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def show(GeN,n):\n",
    "    Z=GeN(n).detach()\n",
    "    fig=setup.makePlot(Z,device)\n",
    "    plt.title('Predictive Variatiational Inference, GeNVI lat_dim='+str(lat_dim))\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "\n",
    "show(GeN,1000)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nLPP_train, nLPP_validation, nLPP_test, RSE_train, RSE_validation, RSE_test=setup.evaluate_metrics(GeN(1000).detach(),'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nLPP_valid: (tensor(1.0238), tensor(0.1000))\n",
      "SE_valid: (tensor(0.1158), tensor(0.1629))\n",
      "nLPP_test: (tensor(1.1159), tensor(0.5838))\n",
      "SE_test: (tensor(0.3392), tensor(1.3768))\n"
     ]
    }
   ],
   "source": [
    "print('nLPP_valid: '+str(nLPP_validation))\n",
    "print('SE_valid: '+str(RSE_validation))\n",
    "print('nLPP_test: '+str(nLPP_test))\n",
    "print('SE_test: '+str(RSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un choix de points $x_0,...,x_{n-1}$, on d√©finit:\n",
    "$$\n",
    "d(\\theta,\\theta')=\\frac{1}{n}\\sum_{i<n}\\vert f_\\theta(x_i)-f_{\\theta'}(x_i)\\vert\n",
    "$$\n",
    "ou\n",
    "$$\n",
    "d_2(\\theta,\\theta')=\\biggl(\\frac{1}{n}\\sum_{i<n}\\vert f_\\theta(x_i)-f_{\\theta'}(x_i)\\vert^2\\biggr)^{\\frac{1}{2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(f\\in A)=P(\\{\\theta \\mid f_\\theta\\in A\\})$\n",
    "\n",
    "$\\theta \\mapsto f_\\theta$ (is it continuous?)\n",
    "\n",
    "relation entre $d(\\theta,\\theta')$ et $d(f_\\theta,f_\\theta')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
