{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = 0.1\n",
    "(x_data, y_data) = torch.load('foong_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_norm(x, mu, std):\n",
    "    \"\"\"Compute the log pdf of x,\n",
    "    under a normal distribution with mean mu and standard deviation std.\"\"\"\n",
    "    return -0.5 * torch.log(2*np.pi*std**2) -(0.5 * (1/(std**2))* (x-mu)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho_to_sigma(rho):\n",
    "    sigma = torch.log(1 + torch.exp(rho))\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_to_rho(sigma):\n",
    "    rho = torch.log(torch.exp(sigma) - 1)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticLinear(nn.Module):\n",
    "    __constants__ = ['bias', 'in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True)#, q_weight_mu = None, q_weight_rho = None, q_bias_mu = None, q_bias_rho = None):\n",
    "        super(ProbabilisticLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.q_weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.q_weight_rho = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.q_bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.q_bias_rho = nn.Parameter(torch.Tensor(out_features))\n",
    "        \n",
    "        self.weight_epsilon = torch.Tensor(out_features, in_features)\n",
    "        self.bias_epsilon = torch.Tensor(out_features, in_features)\n",
    "        \n",
    "        self.weight_sample = torch.Tensor(out_features, in_features)\n",
    "        self.bias_sample = torch.Tensor(out_features, in_features)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "        mu = torch.tensor(0.0)\n",
    "        rho = sigma_to_rho(torch.tensor(1.0))\n",
    "        \n",
    "        self.prior_weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.prior_weight_rho = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.prior_bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.prior_bias_rho = nn.Parameter(torch.Tensor(out_features))\n",
    "        \n",
    "        self.prior_weight_mu.requires_grad = False\n",
    "        self.prior_weight_rho.requires_grad = False\n",
    "        self.prior_bias_mu.requires_grad = False\n",
    "        self.prior_bias_rho.requires_grad = False\n",
    "        \n",
    "        nn.init.constant_(self.prior_weight_mu, mu)\n",
    "        nn.init.constant_(self.prior_weight_rho, rho)\n",
    "        nn.init.constant_(self.prior_bias_mu, mu)\n",
    "        nn.init.constant_(self.prior_bias_rho, rho)\n",
    "        \n",
    "    def generate_rand(self):\n",
    "        self.weight_epsilon = torch.randn(size=self.q_weight_mu.size())\n",
    "        self.bias_epsilon = torch.randn(size=self.q_bias_mu.size())\n",
    "        return (self.weight_epsilon, self.bias_epsilon)\n",
    "    \n",
    "    def reparameterization(self):\n",
    "        sigma_weight = rho_to_sigma(self.q_weight_rho)\n",
    "        self.weight_sample = self.weight_epsilon.mul(sigma_weight).add(self.q_weight_mu)\n",
    "        \n",
    "        sigma_bias = rho_to_sigma(self.q_bias_rho)\n",
    "        self.bias_sample = self.bias_epsilon.mul(sigma_bias).add(self.q_bias_mu)\n",
    "        return (self.weight_sample, self.bias_sample)\n",
    "\n",
    "    def q_log_pdf(self):\n",
    "        sigma_weight = rho_to_sigma(self.q_weight_rho)\n",
    "        nw = torch.distributions.Normal(self.q_weight_mu, sigma_weight)\n",
    "        \n",
    "        sigma_bias = rho_to_sigma(self.q_bias_rho)\n",
    "        nb = torch.distributions.Normal(self.q_bias_mu, sigma_bias)\n",
    "        \n",
    "        return nw.log_prob(self.weight_sample).sum() + nb.log_prob(self.bias_sample).sum()\n",
    "    \n",
    "    def prior_log_pdf(self):\n",
    "        sigma_weight = rho_to_sigma(self.prior_weight_rho)\n",
    "        nw = torch.distributions.Normal(self.prior_weight_mu, sigma_weight)\n",
    "        \n",
    "        sigma_bias = rho_to_sigma(self.prior_bias_rho)\n",
    "        nb = torch.distributions.Normal(self.prior_bias_mu, sigma_bias)\n",
    "        \n",
    "        return nw.log_prob(self.weight_sample).sum() + nb.log_prob(self.bias_sample).sum()\n",
    "    \n",
    "    def lock_means(self):\n",
    "        self.q_weight_mu.requires_grad = False\n",
    "        self.q_bias_mu.requires_grad = False\n",
    "        \n",
    "    def lock_rhos(self):\n",
    "        self.q_weight_rho.requires_grad = False\n",
    "        self.q_bias_rho.requires_grad = False\n",
    "        \n",
    "    def unlock_means(self):\n",
    "        self.q_weight_mu.requires_grad = True\n",
    "        self.q_bias_mu.requires_grad = True\n",
    "        \n",
    "    def unlock_rhos(self):\n",
    "        self.q_weight_rho.requires_grad = True\n",
    "        self.q_bias_rho.requires_grad = True\n",
    "        \n",
    "    def set_parameters(self, w_sample, b_sample):\n",
    "        self.weight_sample = w_sample.detach()\n",
    "        self.bias_sample = b_sample.detach()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.normal_(self.q_weight_mu, mean=0.0, std=3.0)\n",
    "        torch.nn.init.constant_(self.q_weight_rho, -5.0)\n",
    "        torch.nn.init.normal_(self.q_bias_mu, mean=0.0, std=3.0)\n",
    "        torch.nn.init.constant_(self.q_bias_rho, -5.0)\n",
    "       \n",
    "    def forward(self, input):\n",
    "        return torch.nn.functional.linear(input, self.weight_sample, bias=self.bias_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, H, mu_init = None, sigma_init = None):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = ProbabilisticLinear(1, H)\n",
    "        self.linear2 = ProbabilisticLinear(H,1)\n",
    "        \n",
    "        self.registered_layers = []\n",
    "        self.registered_layers.append(self.linear1)\n",
    "        self.registered_layers.append(self.linear2)\n",
    "        \n",
    "        \n",
    "        self.nb_parameters = self.count_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x;\n",
    "        for k in range(len(self.registered_layers)-1):\n",
    "            out = torch.tanh(self.registered_layers[k](out))\n",
    "        out = self.registered_layers[-1](out)\n",
    "        return out\n",
    "    \n",
    "    def resample_parameters(self):\n",
    "        w_samples = []\n",
    "        b_samples = []\n",
    "        for k in range(len(self.registered_layers)):\n",
    "            self.registered_layers[k].generate_rand()\n",
    "            self.registered_layers[k].reparameterization()\n",
    "        \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def lock_means(self):\n",
    "        for k in range(len(self.registered_layers)):\n",
    "            self.registered_layers[k].lock_means()\n",
    "        \n",
    "    def lock_rhos(self):\n",
    "        for k in range(len(self.registered_layers)):\n",
    "            self.registered_layers[k].lock_rhos()\n",
    "        \n",
    "    def unlock_means(self):\n",
    "        for k in range(len(self.registered_layers)):\n",
    "            self.registered_layers[k].unlock_means()\n",
    "        \n",
    "    def unlock_rhos(self):\n",
    "        for k in range(len(self.registered_layers)):\n",
    "            self.registered_layers[k].unlock_rhos()\n",
    "            \n",
    "    def set_parameters(self, w_samples, b_samples):\n",
    "        for k in range(len(self.registered_layers)):\n",
    "            self.registered_layers[k].set_parameters(w_samples[k], b_samples[k])\n",
    "    \n",
    "    def q_log_pdf(self):\n",
    "        list_LQ = []\n",
    "        for k in range(len(self.registered_layers)):\n",
    "            list_LQ.append(self.registered_layers[k].q_log_pdf())\n",
    "        stack_LQ = torch.stack(list_LQ)\n",
    "        return torch.sum(stack_LQ)\n",
    "    \n",
    "    def prior_log_pdf(self):\n",
    "        list_LP = []\n",
    "        for k in range(len(self.registered_layers)):\n",
    "            list_LP.append(self.registered_layers[k].prior_log_pdf())\n",
    "        stack_LP = torch.stack(list_LP)\n",
    "        return torch.sum(stack_LP)\n",
    "        \n",
    "    def compute_elbo(self, x_data, y_data, sample_size):\n",
    "        L = []\n",
    "        for _ in range(sample_size):\n",
    "            self.resample_parameters()\n",
    "\n",
    "            LQ = self.q_log_pdf() \n",
    "            LP = self.prior_log_pdf() \n",
    "\n",
    "            y_pred = self.forward(x_data)\n",
    "            LL = log_norm(y_data, y_pred.t(), torch.tensor(sigma_noise)).sum()\n",
    "\n",
    "            L.append(LQ - LP - LL)\n",
    "        L = torch.stack(L)\n",
    "        L = torch.mean(L)\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(tensor):\n",
    "    return (tensor*tensor).sum().sqrt()\n",
    "L2_vect = np.vectorize(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(list_of_tensors):\n",
    "    m = len(list_of_tensors)\n",
    "    D = torch.diag(L2_vec(list_of_tensors))\n",
    "    for i in range(m):\n",
    "        for j in range(i+1, m):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, H, nComponents):\n",
    "        super(BoostingModel, self).__init__()\n",
    "        \n",
    "        self.H = H\n",
    "        self.components = [RegressionModel(H)]\n",
    "        self.mixture_probas = torch.tensor([1.])\n",
    "        self.nComponents = nComponents\n",
    "        self.current_nComponents = 1\n",
    "        self.current_component = self.components[0]\n",
    "        self.current_component_index = 0\n",
    "        self.current_parameters = {'weight': [layer.weight_sample for layer in self.current_component.registered_layers],\n",
    "                                   'bias': [layer.bias_sample for layer in self.current_component.registered_layers]}\n",
    "        self.current_proba_parameter = None\n",
    "        \n",
    "    def mu_distances(self):\n",
    "        \n",
    "        d = {'linear1': torch.tensor()}\n",
    "        \n",
    "        \n",
    "        \n",
    "    def refresh_current_parameters(self):\n",
    "        self.current_parameters = {'weight': [layer.weight_sample for layer in self.current_component.registered_layers],\n",
    "                                   'bias': [layer.bias_sample for layer in self.current_component.registered_layers]}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.current_component(x)\n",
    "        return out\n",
    "    \n",
    "    def resample_parameters_in_eval(self):\n",
    "        self.sample_component(last = True)\n",
    "        self.sample_parameters()\n",
    "        self.refresh_current_parameters()\n",
    "        \n",
    "    def resample_parameters_in_train(self):\n",
    "        self.sample_component(last = False)\n",
    "        self.sample_parameters()\n",
    "        self.refresh_current_parameters()\n",
    "    \n",
    "    def sample_component(self, last = False):\n",
    "        \n",
    "        if len(self.components) == 1:\n",
    "            self.current_component\n",
    "        \n",
    "        elif not last:\n",
    "            self.current_component = np.random.choice(self.components[:-1], p = self.mixture_probas.data.numpy())\n",
    "        else:\n",
    "            pi = sigmoid(self.current_proba_parameter.detach())\n",
    "            self.current_component = np.random.choice(self.components, p = torch.cat(((1-pi)*self.mixture_probas, pi.unsqueeze(0))).numpy())\n",
    "\n",
    "    def sample_parameters(self):\n",
    "        self.current_component.resample_parameters()\n",
    "        \n",
    "    def mixture_log_pdf(self):\n",
    "        pi = sigmoid(self.current_proba_parameter)\n",
    "        probs = torch.cat((self.mixture_probas*(1-pi), pi.unsqueeze(0)))\n",
    "        #print('probs', probs)\n",
    "        log_q = []\n",
    "        for i, component in enumerate(self.components):\n",
    "            component.set_parameters(self.current_parameters['weight'], self.current_parameters['bias'])\n",
    "         #   print(\"log_q_comp\", component.q_log_pdf())\n",
    "         #   print('prob', probs[i])\n",
    "            log_q.append(component.q_log_pdf())\n",
    "        log_q = torch.stack(log_q)\n",
    "        #print('GO')\n",
    "        #print('before anything', log_q)\n",
    "        #signs = log_q.detach().sign()\n",
    "        #log_q = log_q*signs\n",
    "        #print('after signs', log_q)\n",
    "        #log_q = log_q**probs\n",
    "        #print('after probs', log_q)\n",
    "        #log_q = log_q*signs\n",
    "        #log_q = torch.logsumexp(log_q, dim = -1)\n",
    "        #print('stacked log q', log_q)\n",
    "        maximum = log_q.max()\n",
    "        #print('maximum', maximum)\n",
    "        \"\"\"investigate gradient of max\"\"\"\n",
    "        log_q = log_q - maximum\n",
    "        log_q = torch.exp(log_q)\n",
    "        log_q = log_q*probs\n",
    "        log_q = torch.log(torch.sum(log_q)) + maximum\n",
    "        return(log_q)\n",
    "    \n",
    "    def compute_mixture_elbo(self, x_data, y_data, sample_size):\n",
    "        if type(self.current_proba_parameter) != type(None):\n",
    "            \n",
    "            pi_new = sigmoid(self.current_proba_parameter)\n",
    "            \n",
    "        \n",
    "            \"\"\"Compute expectancy w.r.t old mixture\"\"\"\n",
    "            L_old_mixture = []\n",
    "            for _ in range(sample_size):\n",
    "                self.resample_parameters_in_train()\n",
    "                LQ = self.mixture_log_pdf()\n",
    "                y_pred = self.forward(x_data)\n",
    "                LL = log_norm(y_data, y_pred.t(), torch.tensor(sigma_noise)).sum()\n",
    "                L_old_mixture.append(LQ - LL)\n",
    "            L_old_mixture = torch.stack(L_old_mixture)\n",
    "            L_old_mixture = torch.mean(L_old_mixture)\n",
    "            #print(L_old_mixture)\n",
    "\n",
    "            \"\"\"Compute expectancy w.r.t new component\"\"\"\n",
    "            L_new_component = []\n",
    "            for _ in range(sample_size):\n",
    "                self.current_component = self.components[-1]\n",
    "                self.sample_parameters()\n",
    "                self.refresh_current_parameters()\n",
    "                LQ = self.mixture_log_pdf()\n",
    "                self.sample_component(last = True)\n",
    "                y_pred = self.forward(x_data)\n",
    "                LL = log_norm(y_data, y_pred.t(), torch.tensor(sigma_noise)).sum()\n",
    "                L_new_component.append(LQ - LL)\n",
    "            L_new_component = torch.stack(L_new_component)\n",
    "            L_new_component = torch.mean(L_new_component)\n",
    "            #print(L_new_component)\n",
    "            L = (1 - pi_new)*L_old_mixture + pi_new*L_new_component\n",
    "            return L\n",
    "        else:\n",
    "            return(self.components[0].compute_elbo(x_data, y_data, sample_size))\n",
    "    \n",
    "    \n",
    "    def new_component(self, losses, epsilon, new_pi):\n",
    "        #print('std losses',torch.std(losses))\n",
    "        \n",
    "        if epsilon and len(self.components)<self.nComponents:\n",
    "        #if torch.std(losses) < epsilon and len(self.components)<self.nComponents:\n",
    "            print(\"WE GOT THERE !!!!\")\n",
    "            for component in self.components:\n",
    "                \"\"\"lock fonctionne ?\"\"\"\n",
    "                component.lock_means()\n",
    "                component.lock_rhos()\n",
    "            \n",
    "            self.components.append(RegressionModel(self.H))\n",
    "            if type(self.current_proba_parameter) != type(None):\n",
    "                self.current_proba_parameter.detach_()\n",
    "                pi = sigmoid(self.current_proba_parameter)\n",
    "                self.mixture_probas = torch.cat((self.mixture_probas*(1 - pi), pi.unsqueeze(0)))\n",
    "            self.current_proba_parameter = torch.tensor(float(new_pi))#, requires_grad = True)\n",
    "            print('NEW COMPONENT OK')\n",
    "            return 1\n",
    "#torch.std(losses) < epsilon\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- Underfit the first component\\n- Initialize with prefitted new components\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Underfit the first component\n",
    "- Initialize with prefitted new components\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoostingModel(50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.95,verbose=True)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-d42281618536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m#print(M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mliveloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mliveloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;31m#print('epoch', j, 'num_components', len(model.components), 'stop', stop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/livelossplot/generic_plot.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                       \u001b[0mskip_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_first\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                       \u001b[0mextra_plots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_plots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                       fig_path=self.fig_path)\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_extrema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 print_extrema(self.logs,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/livelossplot/core.py\u001b[0m in \u001b[0;36mdraw_plot\u001b[0;34m(logs, metrics, figsize, max_epoch, max_cols, series_fmt, metric2title, skip_first, extra_plots, fig_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axes_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;31m# add a layout box to this, for both the full axis, and the poss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# of the axis.  We need both because the axes may become smaller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axes_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"axes_locator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_axes_spines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# this call may differ for non-sep axes, e.g., polar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_gen_axes_spines\u001b[0;34m(self, locations, offset, units)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \"\"\"\n\u001b[1;32m    957\u001b[0m         return OrderedDict((side, mspines.Spine.linear_spine(self, side))\n\u001b[0;32m--> 958\u001b[0;31m                            for side in ['left', 'right', 'bottom', 'top'])\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \"\"\"\n\u001b[1;32m    957\u001b[0m         return OrderedDict((side, mspines.Spine.linear_spine(self, side))\n\u001b[0;32m--> 958\u001b[0;31m                            for side in ['left', 'right', 'bottom', 'top'])\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/spines.py\u001b[0m in \u001b[0;36mlinear_spine\u001b[0;34m(cls, axes, spine_type, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unable to make path for spine \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mspine_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspine_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axes.spines.{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspine_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/spines.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, axes, spine_type, path, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Behavior copied from mpatches.Ellipse:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Note: This cannot be calculated until this is added to an Axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_patch_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentityTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_smart_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "num_epoch = 300\n",
    "num_iterations = 100\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "M = int(.005/learning_rate)+1\n",
    "M=5\n",
    "for j in range(num_epoch):\n",
    "    new_comp_this_epoch = 0\n",
    "    logs = {}\n",
    "    losses = [None] * num_iterations\n",
    "    \n",
    "    for k in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        if new_comp_this_epoch:\n",
    "            print('before grad', model.current_proba_parameter)\n",
    "        loss = model.compute_mixture_elbo(x_data, y_data, M)\n",
    "        losses[k] = loss.detach()\n",
    "        loss.backward()\n",
    "        #if new_comp_this_epoch:\n",
    "        #    print('after grad', model.current_proba_parameter)\n",
    "        #print(model.components[0].linear1.weight_sample.grad)\n",
    "        gradients = model.components[-1].linear1.q_weight_mu.grad.mean()\n",
    "        \n",
    "        optimizer.step()\n",
    "        #print(model.current_proba_parameter)\n",
    "    if j>1:\n",
    "        new_comp = model.new_component(None, epsilon = (j%20 == 0), new_pi = 0)#np.log(1./(4*model.nComponents+3)))\n",
    "\n",
    "        #print('STOP', stop)\n",
    "        if new_comp:\n",
    "            model.current_nComponents += 1\n",
    "            print(\"J\", j, \"COMPONENTS\", len(model.components))\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            optimizer.add_param_group({\"params\": model.current_proba_parameter, 'lr': .05})\n",
    "\n",
    "        #print(model.current_proba_parameter)\n",
    "    #print(model.mixture_probas)\n",
    "    #print(torch.std(torch.stack(losses[k-4:k])))\n",
    "    def f(x):\n",
    "        if type(x) == type(None):\n",
    "            return(1.)\n",
    "        else:\n",
    "            return(sigmoid(x))\n",
    "    #f = lambda x: {type(None): 1}.get(type(x), sigmoid(x))\n",
    "    logs['expected_loss'] = torch.stack(losses).mean().detach().clone().numpy()\n",
    "    logs['learning rate'] = optimizer.param_groups[0]['lr']\n",
    "    logs['ncomponents'] = len(model.components)\n",
    "    logs['current_proba'] = f(model.current_proba_parameter)\n",
    "    logs['gradients_weights'] = gradients\n",
    "    #iilogs['proba_gradients'] = prob_grad\n",
    "    #if type(model.current_proba_parameter) != type(None):\n",
    "    #    logs['current_proba'] = model.current_proba_parameter.detach().data\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    M = int(.005/lr)+1\n",
    "    M=5\n",
    "    #print(M)\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "    #print('epoch', j, 'num_components', len(model.components), 'stop', stop)\n",
    "    \n",
    "    scheduler.step(logs['expected_loss'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.forward(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.mixture_log_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.compute_mixture_elbo(x_data, y_data, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1458.1899, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.9397e-04],\n",
       "        [ 1.0016e-03],\n",
       "        [-4.4646e-04],\n",
       "        [ 1.1627e-04],\n",
       "        [ 1.3744e-03],\n",
       "        [-2.2294e-03],\n",
       "        [ 1.4354e-03],\n",
       "        [ 2.0348e-03],\n",
       "        [-1.1218e-03],\n",
       "        [-2.4766e-03],\n",
       "        [-9.9182e-05],\n",
       "        [ 1.5330e-04],\n",
       "        [ 6.7857e-04],\n",
       "        [ 2.5956e-03],\n",
       "        [ 1.0789e-03],\n",
       "        [-1.7995e-03],\n",
       "        [ 1.5810e-03],\n",
       "        [-2.6595e-03],\n",
       "        [-3.1624e-05],\n",
       "        [ 1.7452e-03],\n",
       "        [-4.0419e-04],\n",
       "        [-2.3826e-03],\n",
       "        [-2.7879e-03],\n",
       "        [-8.4686e-04],\n",
       "        [-1.1913e-03],\n",
       "        [ 1.6628e-03],\n",
       "        [-7.1147e-04],\n",
       "        [-3.5753e-04],\n",
       "        [-2.3557e-03],\n",
       "        [ 1.4602e-03],\n",
       "        [ 2.8380e-03],\n",
       "        [-1.2802e-03],\n",
       "        [-3.9054e-04],\n",
       "        [-7.3778e-04],\n",
       "        [ 9.8283e-04],\n",
       "        [ 1.3812e-03],\n",
       "        [ 2.6370e-03],\n",
       "        [ 2.8750e-03],\n",
       "        [-8.3812e-04],\n",
       "        [-2.8533e-03],\n",
       "        [ 1.4253e-03],\n",
       "        [ 4.0553e-04],\n",
       "        [ 6.9835e-04],\n",
       "        [ 6.6568e-04],\n",
       "        [-7.2585e-04],\n",
       "        [ 5.9614e-05],\n",
       "        [ 1.5755e-03],\n",
       "        [-2.4700e-06],\n",
       "        [ 2.7901e-03],\n",
       "        [-1.1400e-04]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.components[-1].linear1.q_weight_mu.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.3178],\n",
       "        [ 6.7339],\n",
       "        [ 6.8020],\n",
       "        [ 6.4573],\n",
       "        [ 6.3460],\n",
       "        [ 6.1342],\n",
       "        [ 6.3708],\n",
       "        [ 6.9426],\n",
       "        [ 6.3006],\n",
       "        [ 6.2289],\n",
       "        [ 6.7864],\n",
       "        [ 5.9156],\n",
       "        [ 5.9571],\n",
       "        [ 6.6979],\n",
       "        [ 6.0999],\n",
       "        [ 6.5616],\n",
       "        [ 6.5451],\n",
       "        [ 6.9388],\n",
       "        [ 6.6847],\n",
       "        [ 6.2879],\n",
       "        [ 5.8126],\n",
       "        [ 6.6394],\n",
       "        [ 6.1699],\n",
       "        [ 6.4289],\n",
       "        [ 6.8629],\n",
       "        [ 5.8339],\n",
       "        [ 6.7764],\n",
       "        [ 5.7553],\n",
       "        [ 6.1235],\n",
       "        [ 6.8961],\n",
       "        [ 5.7552],\n",
       "        [ 6.7602],\n",
       "        [ 6.5350],\n",
       "        [ 6.3210],\n",
       "        [ 6.1398],\n",
       "        [ 6.1944],\n",
       "        [ 6.8754],\n",
       "        [ 6.7715],\n",
       "        [ 6.8968],\n",
       "        [ 6.6031],\n",
       "        [ 6.7859],\n",
       "        [ 6.8253],\n",
       "        [ 6.9049],\n",
       "        [ 6.9519],\n",
       "        [ 6.3608],\n",
       "        [ 6.8310],\n",
       "        [ 5.9391],\n",
       "        [ 6.3082],\n",
       "        [ 6.5931],\n",
       "        [ 6.0663],\n",
       "        [ 6.6965],\n",
       "        [ 6.7874],\n",
       "        [ 6.7944],\n",
       "        [ 5.7600],\n",
       "        [ 6.0281],\n",
       "        [ 6.2138],\n",
       "        [ 6.8243],\n",
       "        [ 6.2787],\n",
       "        [ 6.4158],\n",
       "        [ 5.8687],\n",
       "        [ 6.9209],\n",
       "        [ 5.8385],\n",
       "        [ 6.5858],\n",
       "        [ 5.8745],\n",
       "        [ 6.7207],\n",
       "        [ 6.5771],\n",
       "        [ 6.9602],\n",
       "        [ 6.8251],\n",
       "        [ 5.8438],\n",
       "        [ 6.3641],\n",
       "        [ 6.7763],\n",
       "        [ 6.4647],\n",
       "        [ 6.0595],\n",
       "        [ 6.7364],\n",
       "        [ 6.4300],\n",
       "        [ 6.8117],\n",
       "        [ 6.9329],\n",
       "        [ 6.3877],\n",
       "        [ 6.6071],\n",
       "        [ 6.7648],\n",
       "        [ 5.9290],\n",
       "        [ 6.8355],\n",
       "        [ 6.4717],\n",
       "        [ 5.9692],\n",
       "        [ 6.2131],\n",
       "        [ 6.8533],\n",
       "        [ 5.9403],\n",
       "        [ 6.1181],\n",
       "        [ 5.9504],\n",
       "        [ 6.4080],\n",
       "        [ 5.7331],\n",
       "        [ 6.6991],\n",
       "        [ 6.6197],\n",
       "        [ 5.8457],\n",
       "        [ 6.0487],\n",
       "        [ 5.7904],\n",
       "        [ 6.4717],\n",
       "        [ 6.8112],\n",
       "        [ 6.8004],\n",
       "        [ 5.8843],\n",
       "        [-7.5722],\n",
       "        [ 4.8540],\n",
       "        [-2.9890],\n",
       "        [ 4.9728],\n",
       "        [-4.2362],\n",
       "        [-4.0963],\n",
       "        [-6.0928],\n",
       "        [-3.9891],\n",
       "        [ 3.4894],\n",
       "        [-5.9062],\n",
       "        [ 4.8035],\n",
       "        [-1.3767],\n",
       "        [ 0.5110],\n",
       "        [-1.9846],\n",
       "        [ 0.2799],\n",
       "        [-2.1465],\n",
       "        [-2.7170],\n",
       "        [ 3.8350],\n",
       "        [ 1.6333],\n",
       "        [-3.3658],\n",
       "        [-2.9420],\n",
       "        [-6.6130],\n",
       "        [-2.1349],\n",
       "        [-5.8240],\n",
       "        [-6.5926],\n",
       "        [-5.8282],\n",
       "        [-5.1499],\n",
       "        [-7.5352],\n",
       "        [-0.7138],\n",
       "        [ 1.4064],\n",
       "        [ 1.7400],\n",
       "        [-7.4351],\n",
       "        [ 1.0400],\n",
       "        [-3.4284],\n",
       "        [-3.1227],\n",
       "        [-6.8737],\n",
       "        [-0.2820],\n",
       "        [-6.7123],\n",
       "        [ 1.1329],\n",
       "        [ 4.4523],\n",
       "        [-7.3497],\n",
       "        [ 1.6039],\n",
       "        [-5.1656],\n",
       "        [ 0.0542],\n",
       "        [-0.0807],\n",
       "        [-4.8337],\n",
       "        [ 4.3895],\n",
       "        [-7.2604],\n",
       "        [-5.7576],\n",
       "        [-3.8108],\n",
       "        [-7.2614],\n",
       "        [-7.1307],\n",
       "        [-6.6301],\n",
       "        [-2.9432],\n",
       "        [-6.7754],\n",
       "        [-7.4213],\n",
       "        [-7.5572],\n",
       "        [-6.9162],\n",
       "        [-1.2876],\n",
       "        [ 2.1578],\n",
       "        [-4.4430],\n",
       "        [-7.5890],\n",
       "        [-7.3740],\n",
       "        [-4.7176],\n",
       "        [-6.9331],\n",
       "        [ 2.2079],\n",
       "        [ 2.7537],\n",
       "        [ 2.3792],\n",
       "        [ 1.4512],\n",
       "        [-2.0374],\n",
       "        [-4.5902],\n",
       "        [-7.5458],\n",
       "        [-0.9512],\n",
       "        [-4.8243],\n",
       "        [-3.1156],\n",
       "        [-0.7239],\n",
       "        [ 4.2930],\n",
       "        [-0.5225],\n",
       "        [ 2.0163],\n",
       "        [-7.1603],\n",
       "        [-5.5506],\n",
       "        [-7.5800],\n",
       "        [-5.7434],\n",
       "        [-3.8186],\n",
       "        [-7.0724],\n",
       "        [ 4.0803],\n",
       "        [-0.7420],\n",
       "        [-0.5236],\n",
       "        [-3.2599],\n",
       "        [-0.0631],\n",
       "        [ 2.4883],\n",
       "        [-4.0643],\n",
       "        [ 4.6929],\n",
       "        [ 1.9307],\n",
       "        [-7.3469],\n",
       "        [-3.9418],\n",
       "        [-1.8428],\n",
       "        [-5.3351],\n",
       "        [ 2.7942],\n",
       "        [-7.2681]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-52a0569421b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "std losses tensor(225065.0938)\n",
      "None\n",
      "std losses tensor(81373.9531)\n",
      "None\n",
      "std losses tensor(57236.8359)\n",
      "None\n",
      "std losses tensor(80391.8516)\n",
      "None\n",
      "std losses tensor(60620.0703)\n",
      "None\n",
      "std losses tensor(24250.9844)\n",
      "None\n",
      "std losses tensor(34824.5703)\n",
      "None\n",
      "std losses tensor(55225.5625)\n",
      "None\n",
      "std losses tensor(59821.3945)\n",
      "None\n",
      "std losses tensor(48727.1094)\n",
      "None\n",
      "std losses tensor(31881.7969)\n",
      "None\n",
      "std losses tensor(13890.7715)\n",
      "None\n",
      "std losses tensor(7090.0781)\n",
      "None\n",
      "std losses tensor(13719.4160)\n",
      "None\n",
      "std losses tensor(16131.4512)\n",
      "None\n",
      "std losses tensor(13211.7627)\n",
      "None\n",
      "std losses tensor(7068.8350)\n",
      "None\n",
      "std losses tensor(3132.5833)\n",
      "None\n",
      "std losses tensor(8619.9785)\n",
      "None\n",
      "std losses tensor(13056.0273)\n",
      "None\n",
      "std losses tensor(14447.5996)\n",
      "None\n",
      "std losses tensor(11914.5498)\n",
      "None\n",
      "std losses tensor(7902.5571)\n",
      "None\n",
      "std losses tensor(3698.8779)\n",
      "None\n",
      "std losses tensor(1941.7659)\n",
      "None\n",
      "std losses tensor(3839.4480)\n",
      "None\n",
      "std losses tensor(4812.2295)\n",
      "None\n",
      "std losses tensor(4358.5103)\n",
      "None\n",
      "std losses tensor(2672.3843)\n",
      "None\n",
      "std losses tensor(1298.6182)\n",
      "None\n",
      "std losses tensor(2816.6975)\n",
      "None\n",
      "std losses tensor(4008.0491)\n",
      "None\n",
      "std losses tensor(4219.7314)\n",
      "None\n",
      "std losses tensor(3511.3953)\n",
      "None\n",
      "std losses tensor(2287.5615)\n",
      "None\n",
      "std losses tensor(947.9753)\n",
      "None\n",
      "std losses tensor(878.3491)\n",
      "None\n",
      "std losses tensor(1440.2571)\n",
      "None\n",
      "std losses tensor(1573.3521)\n",
      "None\n",
      "std losses tensor(1127.5764)\n",
      "None\n",
      "std losses tensor(571.4711)\n",
      "None\n",
      "std losses tensor(571.9999)\n",
      "None\n",
      "std losses tensor(1148.5566)\n",
      "None\n",
      "std losses tensor(1500.4185)\n",
      "None\n",
      "std losses tensor(1323.9514)\n",
      "None\n",
      "std losses tensor(872.6108)\n",
      "None\n",
      "std losses tensor(336.9813)\n",
      "None\n",
      "std losses tensor(202.7768)\n",
      "None\n",
      "std losses tensor(473.4794)\n",
      "None\n",
      "std losses tensor(543.8279)\n",
      "None\n",
      "std losses tensor(446.1845)\n",
      "None\n",
      "std losses tensor(181.8386)\n",
      "None\n",
      "std losses tensor(214.9554)\n",
      "None\n",
      "std losses tensor(407.2226)\n",
      "None\n",
      "std losses tensor(478.7425)\n",
      "None\n",
      "std losses tensor(418.3707)\n",
      "None\n",
      "std losses tensor(290.6862)\n",
      "None\n",
      "std losses tensor(148.8168)\n",
      "None\n",
      "std losses tensor(74.4147)\n",
      "None\n",
      "std losses tensor(140.7549)\n",
      "None\n",
      "std losses tensor(128.7328)\n",
      "None\n",
      "std losses tensor(184.2617)\n",
      "None\n",
      "std losses tensor(142.2190)\n",
      "None\n",
      "std losses tensor(162.9513)\n",
      "None\n",
      "std losses tensor(223.3829)\n",
      "None\n",
      "std losses tensor(142.3433)\n",
      "None\n",
      "std losses tensor(122.1443)\n",
      "None\n",
      "std losses tensor(60.0739)\n",
      "None\n",
      "std losses tensor(43.8517)\n",
      "WE GOT THERE !!!!\n",
      "NEW COMPONENT OK\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(457.4285)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32253060., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5743e+02, -3.2253e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(457.4285, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32253518.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(456.7354, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(462.3545)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32211436., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6235e+02, -3.2211e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(462.3545, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32211898.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(461.6613, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(443.7991)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32165858., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4380e+02, -3.2166e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(443.7991, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32166302.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(443.1059, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(448.5361)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32280562., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4854e+02, -3.2281e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(448.5361, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32281010.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(447.8429, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(464.9698)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32149104., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6497e+02, -3.2149e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(464.9698, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32149568.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(464.2766, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(456.2870)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32238296., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5629e+02, -3.2238e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(456.2870, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32238752.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(455.5938, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(449.7424)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32224728., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4974e+02, -3.2225e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(449.7424, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32225178.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(449.0493, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(457.6411)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32142784., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5764e+02, -3.2143e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(457.6411, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32143242.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(456.9479, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(454.8359)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32288028., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5484e+02, -3.2288e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(454.8359, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32288482.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(454.1428, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(448.6812)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32200804., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4868e+02, -3.2201e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(448.6812, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32201252.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(447.9881, grad_fn=<AddBackward0>)\n",
      "tensor(-517.4592, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29114160.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(539.2814, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9114e+07,  5.3928e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(539.2814, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29114700.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(538.5883, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_q_comp tensor(-29103700.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(549.2662, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9104e+07,  5.4927e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(549.2662, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29104250.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(548.5731, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29095512.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(549.9595, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9096e+07,  5.4996e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(549.9595, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29096062.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(549.2663, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29101044.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(542.8679, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9101e+07,  5.4287e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(542.8679, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29101586.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(542.1747, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29101764.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(536.5632, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9102e+07,  5.3656e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(536.5632, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29102300.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(535.8701, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29105288.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(539.1368, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9105e+07,  5.3914e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(539.1368, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29105828.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(538.4437, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29090528.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(539.6976, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9091e+07,  5.3970e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(539.6976, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29091068.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(539.0044, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29117664.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(545.3580, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9118e+07,  5.4536e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(545.3580, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29118210.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(544.6649, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29098504.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(539.5941, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9099e+07,  5.3959e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(539.5941, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29099044.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(538.9009, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29102610.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(531.1414, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9103e+07,  5.3114e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(531.1414, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29103142.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(530.4482, grad_fn=<AddBackward0>)\n",
      "tensor(-5726060.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(50.8078)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(460.5234)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32002792., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6052e+02, -3.2003e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(460.5234, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32003252.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(459.8302, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(458.3763)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32305588., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5838e+02, -3.2306e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(458.3763, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32306046.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(457.6831, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(469.1922)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32068820., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6919e+02, -3.2069e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(469.1922, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32069290.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(468.4991, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(464.0227)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32123534., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6402e+02, -3.2124e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(464.0227, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32123998.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(463.3296, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(456.3348)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32198064., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5633e+02, -3.2198e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(456.3348, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32198520.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(455.6416, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(466.5638)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32309396., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6656e+02, -3.2309e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(466.5638, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32309862.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(465.8707, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(466.5969)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32142560., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6660e+02, -3.2143e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(466.5969, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32143026.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(465.9037, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(461.9584)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32140188., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6196e+02, -3.2140e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(461.9584, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32140650.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(461.2653, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(456.3724)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32003062., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5637e+02, -3.2003e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(456.3724, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32003518.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(455.6793, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(445.9048)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32166434., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4590e+02, -3.2166e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(445.9048, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32166880.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(445.2116, grad_fn=<AddBackward0>)\n",
      "tensor(-624.7360, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29011014.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(537.9434, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9011e+07,  5.3794e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(537.9434, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29011552.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(537.2502, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28987562.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(502.4563, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8988e+07,  5.0246e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(502.4563, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28988064.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(501.7632, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29016100.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(538.5676, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9016e+07,  5.3857e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(538.5676, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29016638.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(537.8745, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29006448.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(547.4010, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9006e+07,  5.4740e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(547.4010, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29006996.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(546.7078, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29018204.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(543.7957, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9018e+07,  5.4380e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(543.7957, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29018748.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(543.1025, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29007238.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(515.6066, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9007e+07,  5.1561e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(515.6066, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29007754.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(514.9135, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29018306.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(531.0607, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9018e+07,  5.3106e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(531.0607, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29018838.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(530.3675, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29022420.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(549.1718, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9022e+07,  5.4917e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(549.1718, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29022970.,         0.], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(548.4786, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29001872.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(535.7921, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9002e+07,  5.3579e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(535.7921, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29002408.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(535.0989, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-29002170.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(550.2764, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.9002e+07,  5.5028e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(550.2764, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-29002720.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(549.5832, grad_fn=<AddBackward0>)\n",
      "tensor(-5748844., grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(1782842.7500)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(442.7985)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32081394., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4280e+02, -3.2081e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(442.7985, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32081836.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(442.1053, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(467.4574)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32155560., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6746e+02, -3.2156e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(467.4574, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32156028.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(466.7643, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(456.1662)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32239770., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5617e+02, -3.2240e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(456.1662, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32240226.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(455.4731, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(463.7530)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32128542., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6375e+02, -3.2129e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(463.7530, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32129006.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(463.0598, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(456.6781)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32188150., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5668e+02, -3.2188e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(456.6781, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32188606.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(455.9849, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(447.0153)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32203164., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4702e+02, -3.2203e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(447.0153, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32203612.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(446.3222, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(443.7403)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32107644., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4374e+02, -3.2108e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(443.7403, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32108088.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(443.0471, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(438.9699)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32358716., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.3897e+02, -3.2359e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(438.9699, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32359154.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(438.2767, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(470.7192)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32069224., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.7072e+02, -3.2069e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(470.7192, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32069694.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(470.0260, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(455.1873)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32209716., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5519e+02, -3.2210e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(455.1873, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32210172.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(454.4942, grad_fn=<AddBackward0>)\n",
      "tensor(-618.4310, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28941952.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(535.5001, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8942e+07,  5.3550e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(535.5001, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28942488.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(534.8069, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28929842.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(534.6874, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8930e+07,  5.3469e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(534.6874, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28930376.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(533.9943, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28938708.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(539.4563, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8939e+07,  5.3946e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(539.4563, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28939248.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(538.7631, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28934420.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(544.9434, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8934e+07,  5.4494e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(544.9434, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28934964.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(544.2502, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28960772.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(546.6584, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8961e+07,  5.4666e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(546.6584, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28961318.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(545.9653, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28925982.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(546.0196, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8926e+07,  5.4602e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(546.0196, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28926528.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(545.3264, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28928790.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(553.5504, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8929e+07,  5.5355e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(553.5504, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28929344.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(552.8572, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28961578.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(522.3958, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8962e+07,  5.2240e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(522.3958, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28962100.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(521.7026, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28926742.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(534.6296, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8927e+07,  5.3463e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(534.6296, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28927276.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(533.9364, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28922184.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(530.3864, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8922e+07,  5.3039e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(530.3864, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28922714.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(529.6932, grad_fn=<AddBackward0>)\n",
      "tensor(-5745039., grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(2062779.5000)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(456.6094)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32114612., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5661e+02, -3.2115e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(456.6094, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32115068.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(455.9163, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(448.2742)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32210312., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4827e+02, -3.2210e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(448.2742, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32210760.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(447.5810, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(454.5466)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32285512., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5455e+02, -3.2286e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(454.5466, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32285966.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(453.8535, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(449.2651)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32170880., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4927e+02, -3.2171e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(449.2651, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32171330.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(448.5720, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(456.0239)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32163092., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5602e+02, -3.2163e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(456.0239, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32163548.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(455.3307, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(458.5819)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32195436., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5858e+02, -3.2195e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(458.5819, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32195894.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(457.8887, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(456.5376)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32135782., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5654e+02, -3.2136e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(456.5376, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32136238.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(455.8445, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(453.8273)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32182672., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5383e+02, -3.2183e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(453.8273, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32183126.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(453.1341, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(446.7407)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32089972., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4674e+02, -3.2090e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(446.7407, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32090418.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(446.0475, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(458.4803)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32252048., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5848e+02, -3.2252e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(458.4803, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32252506.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(457.7871, grad_fn=<AddBackward0>)\n",
      "tensor(-756.0316, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28839748.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(540.5568, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8840e+07,  5.4056e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(540.5568, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28840288.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(539.8636, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28861040.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(537.8871, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8861e+07,  5.3789e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(537.8871, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28861578.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(537.1940, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28849808.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(543.9586, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8850e+07,  5.4396e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(543.9586, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28850352.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(543.2654, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28848038.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(543.7388, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8848e+07,  5.4374e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(543.7388, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28848582.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(543.0457, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28870648.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(556.8904, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8871e+07,  5.5689e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(556.8904, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28871204.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(556.1972, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28860786.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(553.2496, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8861e+07,  5.5325e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(553.2496, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28861340.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(552.5565, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28863464.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(544.9350, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8863e+07,  5.4493e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(544.9350, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28864008.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(544.2418, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28868056.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(534.3700, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8868e+07,  5.3437e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(534.3700, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28868590.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(533.6768, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28865248.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(536.9254, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8865e+07,  5.3693e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(536.9254, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28865784.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(536.2322, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28871202.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(545.2234, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8871e+07,  5.4522e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(545.2234, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28871748.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(544.5303, grad_fn=<AddBackward0>)\n",
      "tensor(-5745057., grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(1787213.5000)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(453.9080)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32043616., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5391e+02, -3.2044e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(453.9080, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32044070.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(453.2148, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(452.0803)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32111840., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5208e+02, -3.2112e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(452.0803, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32112292.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(451.3871, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(455.7390)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32138864., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5574e+02, -3.2139e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(455.7390, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32139320.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(455.0459, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(457.0767)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32235804., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5708e+02, -3.2236e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(457.0767, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32236262.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(456.3835, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(445.7130)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32045264., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4571e+02, -3.2045e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(445.7130, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32045710.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(445.0198, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(448.8474)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32181388., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4885e+02, -3.2181e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(448.8474, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32181836.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(448.1543, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(443.4263)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32276232., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4343e+02, -3.2276e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(443.4263, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32276676.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(442.7332, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(461.1670)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32205208., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6117e+02, -3.2205e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(461.1670, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32205670.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(460.4739, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(444.8182)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32149544., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4482e+02, -3.2150e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(444.8182, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32149988.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(444.1250, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(453.3965)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32119456., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5340e+02, -3.2119e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(453.3965, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32119910.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(452.7034, grad_fn=<AddBackward0>)\n",
      "tensor(-733.8171, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28786342.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(553.6165, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8786e+07,  5.5362e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(553.6165, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28786896.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(552.9233, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28809936.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(542.2006, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8810e+07,  5.4220e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(542.2006, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28810478.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(541.5074, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28804692.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(540.7252, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8805e+07,  5.4073e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(540.7252, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28805232.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(540.0320, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28800436.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(538.0101, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8800e+07,  5.3801e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(538.0101, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28800974.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(537.3169, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28800926.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(541.1519, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8801e+07,  5.4115e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(541.1519, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28801468.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(540.4587, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28804416.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(540.1994, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8804e+07,  5.4020e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(540.1994, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28804956.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(539.5062, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28796264.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(549.4603, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8796e+07,  5.4946e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(549.4603, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28796814.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(548.7671, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28790924.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(542.6469, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8791e+07,  5.4265e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(542.6469, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28791466.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(541.9537, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28788636.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(541.5477, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8789e+07,  5.4155e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(541.5477, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28789178.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(540.8546, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28794888.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(527.5782, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8795e+07,  5.2758e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(527.5782, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28795416.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(526.8850, grad_fn=<AddBackward0>)\n",
      "tensor(-5742266.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(6426.7715)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(464.3524)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32032180., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6435e+02, -3.2032e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(464.3524, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32032644.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(463.6593, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(462.8074)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32291764., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6281e+02, -3.2292e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(462.8074, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32292226.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(462.1143, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(451.7801)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32123080., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5178e+02, -3.2123e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(451.7801, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32123532.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(451.0869, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(441.3173)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32070104., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4132e+02, -3.2070e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(441.3173, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32070546.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(440.6242, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(458.8118)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32219376., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5881e+02, -3.2219e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(458.8118, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32219834.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(458.1187, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(466.5422)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32250100., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6654e+02, -3.2250e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(466.5422, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32250566.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(465.8491, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(451.2011)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32163356., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5120e+02, -3.2163e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(451.2011, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32163808.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(450.5080, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(441.7833)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32132180., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4178e+02, -3.2132e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(441.7833, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32132622.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(441.0902, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(464.9588)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32213968., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6496e+02, -3.2214e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(464.9588, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32214432.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(464.2657, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(449.8842)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32227608., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4988e+02, -3.2228e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(449.8842, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32228058.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(449.1910, grad_fn=<AddBackward0>)\n",
      "tensor(-906.4712, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28742816.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(540.3594, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8743e+07,  5.4036e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(540.3594, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28743356.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(539.6663, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28749252.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(542.3602, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8749e+07,  5.4236e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(542.3602, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28749794.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(541.6670, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28728050.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(529.8734, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8728e+07,  5.2987e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(529.8734, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28728580.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(529.1802, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28741360.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(540.8218, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8741e+07,  5.4082e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(540.8218, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28741900.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(540.1286, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28729490.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(557.5424, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8729e+07,  5.5754e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(557.5424, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28730048.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(556.8492, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28724086.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(542.3326, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8724e+07,  5.4233e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(542.3326, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28724628.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(541.6394, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28721730.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(533.6560, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8722e+07,  5.3366e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(533.6560, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28722264.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(532.9628, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28736726.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(554.2792, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8737e+07,  5.5428e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(554.2792, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28737280.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(553.5861, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28728136.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(532.9903, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8728e+07,  5.3299e+02], grad_fn=<StackBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum tensor(532.9903, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28728668.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(532.2971, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28736436.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(536.0091, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8736e+07,  5.3601e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(536.0091, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28736972.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(535.3159, grad_fn=<AddBackward0>)\n",
      "tensor(-5748931., grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(1664.6460)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(452.9833)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32062844., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5298e+02, -3.2063e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(452.9833, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32063296.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(452.2902, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(455.1229)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32152468., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5512e+02, -3.2152e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(455.1229, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32152924.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(454.4298, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(449.1399)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32100752., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4914e+02, -3.2101e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(449.1399, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32101202.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(448.4467, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(439.3102)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32209624., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.3931e+02, -3.2210e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(439.3102, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32210064.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(438.6171, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(466.9320)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32090078., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6693e+02, -3.2090e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(466.9320, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32090544.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(466.2389, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(461.2793)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32165608., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6128e+02, -3.2166e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(461.2793, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32166070.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(460.5862, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(455.4480)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32261514., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5545e+02, -3.2262e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(455.4480, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32261970.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(454.7548, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(438.2523)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-31964196., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.3825e+02, -3.1964e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(438.2523, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -31964634.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(437.5591, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(452.8670)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32207790., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5287e+02, -3.2208e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(452.8670, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32208242.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(452.1738, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(464.4956)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32218380., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6450e+02, -3.2218e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(464.4956, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32218844.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(463.8025, grad_fn=<AddBackward0>)\n",
      "tensor(-757.4828, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28680166.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(542.7258, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8680e+07,  5.4273e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(542.7258, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28680708.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(542.0326, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28685856.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(532.2216, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8686e+07,  5.3222e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(532.2216, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28686388.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(531.5284, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28700188.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(530.0091, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8700e+07,  5.3001e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(530.0091, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28700718.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(529.3159, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28689538.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(545.8021, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8690e+07,  5.4580e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(545.8021, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28690084.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(545.1089, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28691012.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(541.7819, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8691e+07,  5.4178e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(541.7819, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28691554.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(541.0887, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28667756.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(529.1888, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8668e+07,  5.2919e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(529.1888, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28668286.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(528.4956, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28674902.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(536.3518, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8675e+07,  5.3635e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(536.3518, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28675438.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(535.6586, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28692780.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(540.7630, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8693e+07,  5.4076e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(540.7630, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28693320.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(540.0698, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28683184.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(555.4778, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8683e+07,  5.5548e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(555.4778, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28683740.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(554.7846, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28674998.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(538.6337, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8675e+07,  5.3863e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(538.6337, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28675536.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(537.9405, grad_fn=<AddBackward0>)\n",
      "tensor(-5756854., grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(1735.2369)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(447.2997)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32117928., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4730e+02, -3.2118e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(447.2997, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32118376.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(446.6066, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(448.1914)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-31947040., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4819e+02, -3.1947e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(448.1914, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -31947488.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(447.4982, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(449.1830)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32246408., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked log q tensor([ 4.4918e+02, -3.2246e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(449.1830, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32246858.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(448.4898, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(437.4698)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32193912., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.3747e+02, -3.2194e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(437.4698, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32194350.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(436.7766, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(450.3424)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32212040., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5034e+02, -3.2212e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(450.3424, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32212490.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(449.6493, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(458.0331)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32173528., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5803e+02, -3.2174e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(458.0331, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32173986.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(457.3399, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(446.5500)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32084716., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4655e+02, -3.2085e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(446.5500, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32085162.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(445.8568, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(448.8288)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32045552., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4883e+02, -3.2046e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(448.8288, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32046000.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(448.1357, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(458.8538)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32089700., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5885e+02, -3.2090e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(458.8538, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32090158.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(458.1607, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(465.1218)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32338056., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6512e+02, -3.2338e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(465.1218, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32338522.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(464.4286, grad_fn=<AddBackward0>)\n",
      "tensor(-909.4574, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28647188.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(548.3009, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8647e+07,  5.4830e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(548.3009, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28647736.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(547.6077, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28639476.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(545.0371, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8639e+07,  5.4504e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(545.0371, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28640022.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(544.3439, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28640748.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(541.6453, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8641e+07,  5.4165e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(541.6453, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28641290.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(540.9521, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28643218.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(537.5789, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8643e+07,  5.3758e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(537.5789, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28643756.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(536.8857, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28638968.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(540.3613, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8639e+07,  5.4036e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(540.3613, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28639508.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(539.6682, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28631064.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(539.7731, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8631e+07,  5.3977e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(539.7731, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28631604.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(539.0800, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28635252.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(532.9127, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8635e+07,  5.3291e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(532.9127, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28635784.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(532.2195, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28654994.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(543.2921, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8655e+07,  5.4329e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(543.2921, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28655538.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(542.5989, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28625240.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(545.9516, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8625e+07,  5.4595e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(545.9516, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28625786.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(545.2584, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28643962.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(541.5779, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8644e+07,  5.4158e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(541.5779, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28644504.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(540.8847, grad_fn=<AddBackward0>)\n",
      "tensor(-5750964., grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(3949.6707)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(462.9883)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32098620., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6299e+02, -3.2099e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(462.9883, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32099082.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(462.2952, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(433.9666)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32197480., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.3397e+02, -3.2197e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(433.9666, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32197914.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(433.2735, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(442.2463)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32311564., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4225e+02, -3.2312e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(442.2463, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32312006.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(441.5531, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(450.3234)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32077680., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5032e+02, -3.2078e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(450.3234, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32078130.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(449.6303, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(446.6085)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32131948., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4661e+02, -3.2132e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(446.6085, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32132394.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(445.9153, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(450.5017)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32241952., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5050e+02, -3.2242e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(450.5017, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32242402.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(449.8086, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(459.1959)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32205916., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5920e+02, -3.2206e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(459.1959, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32206376.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(458.5027, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(458.5860)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32229820., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5859e+02, -3.2230e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(458.5860, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32230278.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(457.8929, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(452.0580)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32389060., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5206e+02, -3.2389e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(452.0580, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32389512.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(451.3649, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(448.9829)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32311244., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4898e+02, -3.2311e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(448.9829, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32311692.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(448.2897, grad_fn=<AddBackward0>)\n",
      "tensor(-851.3859, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28589170.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(548.7192, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8589e+07,  5.4872e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(548.7192, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28589718.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(548.0261, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28590480.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(540.0510, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8590e+07,  5.4005e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(540.0510, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28591020.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(539.3578, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28589016.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(519.3516, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8589e+07,  5.1935e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(519.3516, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_q before exp tensor([-28589536.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(518.6584, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28598000.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(545.3545, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8598e+07,  5.4535e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(545.3545, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28598546.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(544.6613, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28575828.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(536.7795, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8576e+07,  5.3678e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(536.7795, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28576364.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(536.0863, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28589318.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(538.1005, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8589e+07,  5.3810e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(538.1005, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28589856.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(537.4073, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28581116.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(546.9958, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8581e+07,  5.4700e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(546.9958, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28581662.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(546.3027, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28589154.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(535.1464, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8589e+07,  5.3515e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(535.1464, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28589690.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(534.4532, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28586716.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(550.1013, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8587e+07,  5.5010e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(550.1013, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28587266.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(549.4081, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28591458.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(534.0045, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8591e+07,  5.3400e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(534.0045, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28591992.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(533.3113, grad_fn=<AddBackward0>)\n",
      "tensor(-5748693., grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(3750.4812)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(428.9845)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32333692., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.2898e+02, -3.2334e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(428.9845, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32334120.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(428.2913, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(440.4726)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32131294., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4047e+02, -3.2131e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(440.4726, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32131734.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(439.7794, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(458.1347)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-31998848., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5813e+02, -3.1999e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(458.1347, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -31999306.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(457.4415, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(449.0082)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32080562., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4901e+02, -3.2081e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(449.0082, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32081012.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(448.3151, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(448.7546)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32317920., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4875e+02, -3.2318e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(448.7546, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32318368.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(448.0615, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(461.2485)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32021822., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6125e+02, -3.2022e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(461.2485, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32022284.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(460.5554, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(460.2865)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32088980., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.6029e+02, -3.2089e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(460.2865, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32089440.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(459.5934, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(450.7401)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32275288., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5074e+02, -3.2275e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(450.7401, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32275738.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(450.0470, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(435.2596)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32152200., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.3526e+02, -3.2152e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(435.2596, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32152636.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(434.5664, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(451.9518)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32250882., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5195e+02, -3.2251e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(451.9518, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32251334.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(451.2586, grad_fn=<AddBackward0>)\n",
      "tensor(-1126.7493, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28540124.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(549.7564, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8540e+07,  5.4976e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(549.7564, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28540674.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(549.0632, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28544838.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(554.2534, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8545e+07,  5.5425e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(554.2534, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28545392.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(553.5602, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28548470.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(547.1847, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8548e+07,  5.4718e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(547.1847, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28549018.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(546.4915, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28565388.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(550.7361, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8565e+07,  5.5074e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(550.7361, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28565938.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(550.0430, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28554660.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(542.0420, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8555e+07,  5.4204e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(542.0420, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28555202.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(541.3488, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28553884.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(539.3981, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8554e+07,  5.3940e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(539.3981, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28554424.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(538.7049, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28531102.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(547.7806, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8531e+07,  5.4778e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(547.7806, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28531650.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(547.0875, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28564438.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(539.0538, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8564e+07,  5.3905e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(539.0538, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28564978.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(538.3607, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28550650.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(544.5170, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8551e+07,  5.4452e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(544.5170, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28551194.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(543.8238, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28546724.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(539.4625, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8547e+07,  5.3946e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(539.4625, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28547264.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(538.7693, grad_fn=<AddBackward0>)\n",
      "tensor(-5766239.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "std losses tensor(2344.0044)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(459.5050)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32248584., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5951e+02, -3.2249e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(459.5050, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32249044.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(458.8119, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(453.5767)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32245938., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5358e+02, -3.2246e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(453.5767, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32246392.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(452.8835, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(445.9860)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32170048., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4599e+02, -3.2170e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(445.9860, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32170494.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(445.2928, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(437.5806)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32148480., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.3758e+02, -3.2148e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(437.5806, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32148918.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(436.8875, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(440.0322)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32080384., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4003e+02, -3.2080e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(440.0322, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32080824.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(439.3390, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(440.5504)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32208700., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.4055e+02, -3.2209e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(440.5504, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32209140.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(439.8572, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(452.2173)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32213628., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5222e+02, -3.2214e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(452.2173, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32214080.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(451.5241, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(453.0833)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32114824., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5308e+02, -3.2115e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(453.0833, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32115278.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(452.3901, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(457.2805)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32255948., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5728e+02, -3.2256e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(457.2805, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32256406.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(456.5873, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(451.9727)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(-32068814., grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([ 4.5197e+02, -3.2069e+07], grad_fn=<StackBackward>)\n",
      "maximum tensor(451.9727, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([        0., -32069266.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([1., 0.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.5000, 0.0000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(451.2796, grad_fn=<AddBackward0>)\n",
      "tensor(-1023.9196, grad_fn=<MeanBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28513710.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(536.4562, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8514e+07,  5.3646e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(536.4562, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28514246.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(535.7631, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28514686.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(541.2756, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8515e+07,  5.4128e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(541.2756, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28515228.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(540.5825, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28509540.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(545.7259, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8510e+07,  5.4573e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(545.7259, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28510086.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(545.0327, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28513218.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(558.8877, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8513e+07,  5.5889e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(558.8877, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28513776.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(558.1945, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28519480.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(536.1606, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8519e+07,  5.3616e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(536.1606, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28520016.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(535.4674, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28512528.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(563.9849, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8513e+07,  5.6398e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(563.9849, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28513092.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(563.2917, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28517884.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(542.4854, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8518e+07,  5.4249e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(542.4854, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28518426.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(541.7922, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28498832.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(527.5534, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8499e+07,  5.2755e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(527.5534, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28499360.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(526.8602, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28531984.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "log_q_comp tensor(548.1760, grad_fn=<SumBackward0>)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n",
      "stacked log q tensor([-2.8532e+07,  5.4818e+02], grad_fn=<StackBackward>)\n",
      "maximum tensor(548.1760, grad_fn=<MaxBackward1>)\n",
      "log_q before exp tensor([-28532532.,         0.], grad_fn=<SubBackward0>)\n",
      "log_q after exp tensor([0., 1.], grad_fn=<ExpBackward>)\n",
      "log_q ponderated tensor([0.0000, 0.5000], grad_fn=<MulBackward0>)\n",
      "log_q tensor(547.4828, grad_fn=<AddBackward0>)\n",
      "probs tensor([0.5000, 0.5000], grad_fn=<CatBackward>)\n",
      "log_q_comp tensor(-28502906.)\n",
      "prob tensor(0.5000, grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-381-969752a04d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mixture_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-378-7cdc77df8fdd>\u001b[0m in \u001b[0;36mcompute_mixture_elbo\u001b[0;34m(self, x_data, y_data, sample_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh_current_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mLQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixture_log_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-378-7cdc77df8fdd>\u001b[0m in \u001b[0;36mmixture_log_pdf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mcomponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_q_comp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_log_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mlog_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_log_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-4a4311806e9a>\u001b[0m in \u001b[0;36mq_log_pdf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlist_LQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mlist_LQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistered_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_log_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mstack_LQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_LQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_LQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-1f3f2896fdc8>\u001b[0m in \u001b[0;36mq_log_pdf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_bias_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprior_log_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlog_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_scale\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "num_epoch = 100\n",
    "num_iterations = 100\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "M = int(.005/learning_rate)+1\n",
    "M=10\n",
    "for j in range(num_epoch):\n",
    "    logs = {}\n",
    "    losses = [None] * num_iterations\n",
    "    \n",
    "    for k in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.compute_mixture_elbo(x_data, y_data, M)\n",
    "        losses[k] = loss.detach()\n",
    "        loss.backward()\n",
    "        #print(model.components[0].linear1.weight_sample.grad)\n",
    "        optimizer.step()\n",
    "        print(model.current_proba_parameter)\n",
    "        if k>4:\n",
    "            stop = model.new_component(torch.stack(losses[k-4:k]), epsilon = 50, new_pi = .5)\n",
    "            \n",
    "            #print('STOP', stop)\n",
    "            if not stop:\n",
    "                print('new_component')\n",
    "                break\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #print(model.mixture_probas)\n",
    "    #print(torch.std(torch.stack(losses[k-4:k])))\n",
    "    logs['expected_loss'] = torch.stack(losses).mean().detach().clone().numpy()\n",
    "    logs['learning rate'] = optimizer.param_groups[0]['lr']\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    M = int(.005/lr)+1\n",
    "    M=10\n",
    "    #print(M)\n",
    "    liveloss.update(logs)\n",
    "    #liveloss.draw()\n",
    "    #print('epoch', j, 'num_components', len(model.components), 'stop', stop)\n",
    "    \n",
    "    scheduler.step(logs['expected_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionModel(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test = torch.linspace(-2.0, 2.0).unsqueeze(1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11.7, 8.27)\n",
    "\n",
    "plt.scatter(x_data.cpu(), y_data.cpu())\n",
    "for _ in range(1000):\n",
    "    model.resample_parameters_in_eval()\n",
    "\n",
    "    y_test = model.forward(x_test)\n",
    "    plt.plot(x_test.detach().cpu().numpy(), y_test.detach().cpu().numpy(), alpha=0.05, linewidth=1, color='lightblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BoostingModel' object has no attribute 'resample_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5b61898fb1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    589\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 591\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BoostingModel' object has no attribute 'resample_parameters'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test = torch.linspace(-2.0, 2.0).unsqueeze(1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11.7, 8.27)\n",
    "\n",
    "plt.scatter(x_data.cpu(), y_data.cpu())\n",
    "for _ in range(1000):\n",
    "    model.resample_parameters()\n",
    "\n",
    "    y_test = model.forward(x_test)\n",
    "    plt.plot(x_test.detach().cpu().numpy(), y_test.detach().cpu().numpy(), alpha=0.05, linewidth=1, color='lightblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Users/cortax/Anaconda3/Library/bin/graphviz/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_dot(model(x_data), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionModel(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.95,verbose=True)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mixture_probas.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-6beaa7bcc148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mliveloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mliveloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expected_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/livelossplot/generic_plot.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                       \u001b[0mskip_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_first\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                       \u001b[0mextra_plots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_plots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                       fig_path=self.fig_path)\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_extrema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 print_extrema(self.logs,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/livelossplot/core.py\u001b[0m in \u001b[0;36mdraw_plot\u001b[0;34m(logs, metrics, figsize, max_epoch, max_cols, series_fmt, metric2title, skip_first, extra_plots, fig_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mextra_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfig_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mtight_layout\u001b[0;34m(pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   1367\u001b[0m         labels) will fit into. Default is (0, 0, 1, 1).\n\u001b[1;32m   1368\u001b[0m     \"\"\"\n\u001b[0;32m-> 1369\u001b[0;31m     \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mtight_layout\u001b[0;34m(self, renderer, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         kwargs = get_tight_layout_figure(\n\u001b[1;32m   2474\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplotspec_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2475\u001b[0;31m             pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/tight_layout.py\u001b[0m in \u001b[0;36mget_tight_layout_figure\u001b[0;34m(fig, axes_list, subplotspec_list, renderer, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                      \u001b[0msubplot_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                                      \u001b[0max_bbox_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max_bbox_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                                      pad=pad, h_pad=h_pad, w_pad=w_pad)\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;31m# kwargs can be none if tight_layout fails...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/tight_layout.py\u001b[0m in \u001b[0;36mauto_adjust_subplotpars\u001b[0;34m(fig, renderer, nrows_ncols, num1num2_list, subplot_list, ax_bbox_list, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         tight_bbox_raw = union([ax.get_tightbbox(renderer) for ax in subplots\n\u001b[0m\u001b[1;32m    112\u001b[0m                                 if ax.get_visible()])\n\u001b[1;32m    113\u001b[0m         tight_bbox = TransformedBbox(tight_bbox_raw,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/tight_layout.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         tight_bbox_raw = union([ax.get_tightbbox(renderer) for ax in subplots\n\u001b[0;32m--> 112\u001b[0;31m                                 if ax.get_visible()])\n\u001b[0m\u001b[1;32m    113\u001b[0m         tight_bbox = TransformedBbox(tight_bbox_raw,\n\u001b[1;32m    114\u001b[0m                                      fig.transFigure.inverted())\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   4354\u001b[0m                 \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_yaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_title_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4357\u001b[0m         \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_window_extent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_title_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2552\u001b[0m                     \u001b[0;31m# this happens for an empty bb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2554\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_window_extent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2555\u001b[0m                 y = self.transAxes.inverted().transform(\n\u001b[1;32m   2556\u001b[0m                         (0., top))[1]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get window extent w/o renderer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclean_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 w, h, d = renderer.get_text_width_height_descent(\n\u001b[0;32m--> 298\u001b[0;31m                     clean_line, self._fontproperties, ismath=ismath)\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hinting_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_agg_font\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_width_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# width and height of unrotated string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_get_agg_font\u001b[0;34m(self, prop)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mGet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfont\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mefficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \"\"\"\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindfont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_font\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36mfindfont\u001b[0;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing)\u001b[0m\n\u001b[1;32m   1236\u001b[0m         return self._findfont_cached(\n\u001b[1;32m   1237\u001b[0m             \u001b[0mprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallback_to_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebuild_if_missing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m             rc_params)\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    665\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stretch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m              self.get_file())\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "num_epoch = 1000\n",
    "num_iterations = 100\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "M = int(.005/learning_rate)+1\n",
    "for j in range(num_epoch):\n",
    "    logs = {}\n",
    "    losses = [None] * num_iterations\n",
    "    \n",
    "    for k in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.compute_elbo(x_data, y_data, M)\n",
    "        losses[k] = loss\n",
    "        loss.backward()\n",
    "        #print(model.linear1.weight_sample.grad)\n",
    "        optimizer.step()\n",
    "    \n",
    "    logs['expected_loss'] = torch.stack(losses).mean().detach().clone().numpy()\n",
    "    logs['learning rate'] = optimizer.param_groups[0]['lr']\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    M = int(.005/lr)+1\n",
    "    print(M)\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "    \n",
    "    scheduler.step(logs['expected_loss'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionModel(\n",
       "  (linear1): ProbabilisticLinear()\n",
       "  (linear2): ProbabilisticLinear()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test = torch.linspace(-2.0, 2.0).unsqueeze(1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11.7, 8.27)\n",
    "\n",
    "plt.scatter(x_data.cpu(), y_data.cpu())\n",
    "for _ in range(1000):\n",
    "    (L1_weight_epsilon, L1_bias_epsilon) = model.linear1.generate_rand()\n",
    "    (L1_weight_sample, L1_bias_sample) = model.linear1.reparameterization()\n",
    "    \n",
    "    y_test = model.forward(x_test)\n",
    "    plt.plot(x_test.detach().cpu().numpy(), y_test.detach().cpu().numpy(), alpha=0.05, linewidth=1, color='lightblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
