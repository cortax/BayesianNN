{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from autograd.misc.optimizers import adam\n",
    "\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_variational_inference(logprob, D, num_samples):\n",
    "    \"\"\"Implements http://arxiv.org/abs/1401.0118, and uses the\n",
    "    local reparameterization trick from http://arxiv.org/abs/1506.02557\"\"\"\n",
    "\n",
    "    def unpack_params(params):\n",
    "        # Variational dist is a diagonal Gaussian.\n",
    "        mean, log_std = params[:D], params[D:]\n",
    "        return mean, log_std\n",
    "\n",
    "    def gaussian_entropy(log_std):\n",
    "        return 0.5 * D * (1.0 + np.log(2*np.pi)) + np.sum(log_std)\n",
    "\n",
    "    rs = npr.RandomState(0)\n",
    "    def variational_objective(params, t):\n",
    "        \"\"\"Provides a stochastic estimate of the variational lower bound.\"\"\"\n",
    "        mean, log_std = unpack_params(params)\n",
    "        samples = rs.randn(num_samples, D) * np.exp(log_std) + mean\n",
    "        lower_bound = gaussian_entropy(log_std) + np.mean(logprob(samples, t))\n",
    "        return -lower_bound\n",
    "\n",
    "    gradient = grad(variational_objective)\n",
    "\n",
    "    return variational_objective, gradient, unpack_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn_funs(layer_sizes, L2_reg, noise_variance, nonlinearity=np.tanh):\n",
    "    \"\"\"These functions implement a standard multi-layer perceptron,\n",
    "    vectorized over both training examples and weight samples.\"\"\"\n",
    "    shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
    "    num_weights = sum((m+1)*n for m, n in shapes)\n",
    "\n",
    "    def unpack_layers(weights):\n",
    "        num_weight_sets = len(weights)\n",
    "        for m, n in shapes:\n",
    "            yield weights[:, :m*n]     .reshape((num_weight_sets, m, n)),\\\n",
    "                  weights[:, m*n:m*n+n].reshape((num_weight_sets, 1, n))\n",
    "            weights = weights[:, (m+1)*n:]\n",
    "\n",
    "    def predictions(weights, inputs):\n",
    "        \"\"\"weights is shape (num_weight_samples x num_weights)\n",
    "           inputs  is shape (num_datapoints x D)\"\"\"\n",
    "        inputs = np.expand_dims(inputs, 0)\n",
    "        for W, b in unpack_layers(weights):\n",
    "            outputs = np.einsum('mnd,mdo->mno', inputs, W) + b\n",
    "            inputs = nonlinearity(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def logprob(weights, inputs, targets):\n",
    "        log_prior = -L2_reg * np.sum(weights**2, axis=1)\n",
    "        preds = predictions(weights, inputs)\n",
    "        log_lik = -np.sum((preds - targets)**2, axis=1)[:, 0] / noise_variance\n",
    "        return log_prior + log_lik\n",
    "\n",
    "    return num_weights, predictions, logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toy_dataset(n_data=40, noise_std=0.1):\n",
    "    D = 1\n",
    "    rs = npr.RandomState(0)\n",
    "    inputs  = np.concatenate([np.linspace(0, 2, num=n_data/2),\n",
    "                              np.linspace(6, 8, num=n_data/2)])\n",
    "    targets = np.cos(inputs) + rs.randn(n_data) * noise_std\n",
    "    inputs = (inputs - 4.0) / 4.0\n",
    "    inputs  = inputs.reshape((len(inputs), D))\n",
    "    targets = targets.reshape((len(targets), D))\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify inference problem by its unnormalized log-posterior.\n",
    "rbf = lambda x: np.exp(-x**2)\n",
    "relu = lambda x: np.maximum(x, 0.)\n",
    "num_weights, predictions, logprob = \\\n",
    "    make_nn_funs(layer_sizes=[1, 20, 20, 1], L2_reg=0.1,\n",
    "                 noise_variance=0.01, nonlinearity=rbf)\n",
    "\n",
    "inputs, targets = build_toy_dataset()\n",
    "log_posterior = lambda weights, t: logprob(weights, inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build variational objective.\n",
    "objective, gradient, unpack_params = \\\n",
    "    black_box_variational_inference(log_posterior, num_weights,\n",
    "                                    num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(params, t, g):\n",
    "    print(\"Iteration {} lower bound {}\".format(t, -objective(params, t)))\n",
    "\n",
    "    # Sample functions from posterior.\n",
    "    rs = npr.RandomState(0)\n",
    "    mean, log_std = unpack_params(params)\n",
    "    #rs = npr.RandomState(0)\n",
    "    sample_weights = rs.randn(10, num_weights) * np.exp(log_std) + mean\n",
    "    plot_inputs = np.linspace(-8, 8, num=400)\n",
    "    outputs = predictions(sample_weights, np.expand_dims(plot_inputs, 1))\n",
    "\n",
    "    # Plot data and functions.\n",
    "    plt.cla()\n",
    "    ax.plot(inputs.ravel(), targets.ravel(), 'bx')\n",
    "    ax.plot(plot_inputs, outputs[:, :, 0].T)\n",
    "    ax.set_ylim([-2, 3])\n",
    "    plt.draw()\n",
    "    plt.pause(1.0/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variational parameters\n",
    "rs = npr.RandomState(0)\n",
    "init_mean    = rs.randn(num_weights)\n",
    "init_log_std = -5 * np.ones(num_weights)\n",
    "init_var_params = np.concatenate([init_mean, init_log_std])\n",
    "\n",
    "print(\"Optimizing variational parameters...\")\n",
    "variational_params = adam(gradient, init_var_params,\n",
    "                          step_size=0.1, num_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
