{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = 0.1\n",
    "(x_data, y_data) = torch.load('foong_data.pt')\n",
    "#x_data = x_data.to(device)\n",
    "#y_data = y_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_norm(x, mu, std):\n",
    "    \"\"\"Compute the log pdf of x,\n",
    "    under a normal distribution with mean mu and standard deviation std.\"\"\"\n",
    "    return -0.5 * torch.log(2*np.pi*std**2) -(0.5 * (1/(std**2))* (x-mu)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho_to_sigma(rho):\n",
    "    sigma = torch.log(1 + torch.exp(rho))\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_to_rho(sigma):\n",
    "    rho = torch.log(torch.exp(sigma) - 1)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticLinear(nn.Module):\n",
    "    __constants__ = ['bias', 'in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(ProbabilisticLinear, self).__init__()\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        \n",
    "        self.weight = torch.Tensor(out_features, in_features)\n",
    "        self.bias = torch.Tensor(out_features, in_features)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "            self.bias_rho = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_rho', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def generate_rand(self):\n",
    "        weight_epsilon = torch.randn(size=self.weight_mu.size())\n",
    "        bias_epsilon = torch.randn(size=self.bias_mu.size())\n",
    "        return (weight_epsilon, bias_epsilon)\n",
    "    \n",
    "    def reparameterization(self, weight_epsilon, bias_epsilon):\n",
    "        sigma = self.softplus(self.weight_rho)\n",
    "        weight_sample = weight_epsilon.mul(sigma).add(self.weight_mu)\n",
    "        sigma = self.softplus(self.bias_rho)\n",
    "        bias_sample = bias_epsilon.mul(sigma).add(self.bias_mu)\n",
    "        return (weight_sample, bias_sample)\n",
    "\n",
    "    def log_pdf(self, weight_sample, bias_sample):\n",
    "        sigmaw = self.softplus(self.weight_rho)\n",
    "        nw = torch.distributions.Normal(self.weight_mu, sigmaw)\n",
    "        sigmab = self.softplus(self.bias_rho)\n",
    "        nb = torch.distributions.Normal(self.bias_mu, sigmab)\n",
    "        return nw.log_prob(weight_sample).sum() + nb.log_prob(bias_sample).sum()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight_mu, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.weight_rho, a=math.sqrt(5))\n",
    "        if self.bias_mu is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight_mu)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias_mu, -bound, bound)\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight_rho)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias_rho, -bound, bound)\n",
    "    \n",
    "    def set_parameters(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return torch.nn.functional.linear(input, self.weight, self.bias)\n",
    "\n",
    "    #def extra_repr(self):\n",
    "    #    return 'in_features={}, out_features={}, bias={}'.format(\n",
    "    #        self.in_features, self.out_features, self.bias is not None\n",
    "    #    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        \n",
    "        #The parameters of variation distribution q(w)\n",
    "        self.q_linear1 = ProbabilisticLinear(1, H)\n",
    "        self.q_linear2 = ProbabilisticLinear(H,1)\n",
    "        \n",
    "        #The prior distribution p(w)\n",
    "        mu = torch.tensor(0.0)\n",
    "        rho = sigma_to_rho(torch.tensor(1.0))\n",
    "        \n",
    "        self.prior_linear1 = ProbabilisticLinear(1, H)\n",
    "        nn.init.constant_(self.prior_linear1.weight_mu, mu)\n",
    "        nn.init.constant_(self.prior_linear1.weight_rho, rho)\n",
    "        nn.init.constant_(self.prior_linear1.bias_mu, mu)\n",
    "        nn.init.constant_(self.prior_linear1.bias_rho, rho)\n",
    "        \n",
    "        self.prior_linear1.weight_mu.requires_grad = False\n",
    "        self.prior_linear1.weight_rho.requires_grad = False\n",
    "        self.prior_linear1.bias_mu.requires_grad = False\n",
    "        self.prior_linear1.bias_rho.requires_grad = False\n",
    "        \n",
    "        self.prior_linear2 = ProbabilisticLinear(H,1)\n",
    "        nn.init.constant_(self.prior_linear2.weight_mu, mu)\n",
    "        nn.init.constant_(self.prior_linear2.weight_rho, rho)\n",
    "        nn.init.constant_(self.prior_linear2.bias_mu, mu)\n",
    "        nn.init.constant_(self.prior_linear2.bias_rho, rho)\n",
    "        \n",
    "        self.prior_linear2.weight_mu.requires_grad = False\n",
    "        self.prior_linear2.weight_rho.requires_grad = False\n",
    "        self.prior_linear2.bias_mu.requires_grad = False\n",
    "        self.prior_linear2.bias_rho.requires_grad = False\n",
    "        \n",
    "        self.nb_parameters = self.count_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.tanh(self.q_linear1(x))\n",
    "        out = self.q_linear2(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "          \n",
    "    def generate_rand_(self):\n",
    "        self.q_linear1.weight_epsilon = torch.randn(size=self.q_linear1.weight_mu.size())\n",
    "        self.q_linear1.bias_epsilon = torch.randn(size=self.q_linear1.bias_mu.size())\n",
    "        self.q_linear2.weight_epsilon = torch.randn(size=self.q_linear2.weight_mu.size())\n",
    "        self.q_linear2.bias_epsilon = torch.randn(size=self.q_linear2.bias_mu.size())\n",
    "    \n",
    "    def reparam(self):\n",
    "        sigma = self.softplus(self.q_linear1.weight_rho)\n",
    "        self.q_linear1.weight_sample = self.q_linear1.weight_epsilon.mul(sigma).add(self.q_linear1.weight_mu)\n",
    "        sigma = self.softplus(self.q_linear1.bias_rho)\n",
    "        self.q_linear1.bias_sample = self.q_linear1.bias_epsilon.mul(sigma).add(self.q_linear1.bias_mu)\n",
    "        sigma = self.softplus(self.q_linear2.weight_rho)\n",
    "        self.q_linear2.weight_sample = self.q_linear2.weight_epsilon.mul(sigma).add(self.q_linear2.weight_mu)\n",
    "        sigma = self.softplus(self.q_linear2.bias_rho)\n",
    "        self.q_linear2.bias_sample = self.q_linear2.bias_epsilon.mul(sigma).add(self.q_linear2.bias_mu)\n",
    "        \n",
    "    def compute_elbo(self, x_data, y_data):        \n",
    "        (q1_weight_epsilon, q1_bias_epsilon) = self.q_linear1.generate_rand()\n",
    "        (q1_weight_sample, q1_bias_sample) = self.q_linear1.reparameterization(q1_weight_epsilon, q1_bias_epsilon)\n",
    "        \n",
    "        (q2_weight_epsilon, q2_bias_epsilon) = self.q_linear2.generate_rand()\n",
    "        (q2_weight_sample, q2_bias_sample) = self.q_linear2.reparameterization(q2_weight_epsilon, q2_bias_epsilon)\n",
    "        \n",
    "        LQ = self.q_linear1.log_pdf(q1_weight_sample, q1_bias_sample) + self.q_linear2.log_pdf(q2_weight_sample, q2_bias_sample)\n",
    "        \n",
    "        LP = self.prior_linear1.log_pdf(q1_weight_sample, q1_bias_sample) + self.prior_linear2.log_pdf(q2_weight_sample, q2_bias_sample)\n",
    "        \n",
    "        self.q_linear1.set_parameters(q1_weight_sample, q1_bias_sample)\n",
    "        self.q_linear2.set_parameters(q2_weight_sample, q2_bias_sample)\n",
    "        y_pred = self.forward(x_data)\n",
    "        n = torch.distributions.Normal(y_pred, sigma_noise)\n",
    "        LL = n.log_prob(y_data).sum()\n",
    "        \n",
    "        loss = LQ - LP - LL\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionModel(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11699395., grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_elbo(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = torch.tensor(10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1000):\n",
    "    lst_grad_mu = []\n",
    "    lst_grad_rho = []\n",
    "    lst_v = []\n",
    "    for j in range(0,20):\n",
    "        epsilon = torch.distributions.normal.Normal(torch.tensor(0.0) , torch.tensor(1.0)).sample(torch.Size([2,]))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            w = theta[0:2].clone().detach() + torch.log(1 + torch.exp(theta[2:4].clone().detach()))*epsilon\n",
    "        w.requires_grad = True\n",
    "\n",
    "        v = f(w, theta)   \n",
    "        v.backward()\n",
    "        lst_v.append(v.data)\n",
    "        v = torch.tensor([])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            grad_mu = w.grad + theta.grad[0:2]\n",
    "            grad_rho = w.grad*epsilon/(1 + torch.exp(-theta[2:4])) + -theta.grad[2:4]\n",
    "\n",
    "\n",
    "        lst_grad_mu.append(grad_mu)\n",
    "        lst_grad_rho.append(grad_rho)\n",
    "\n",
    "        theta.grad.zero_()\n",
    "        w.grad.zero_()\n",
    "        v.detach()\n",
    "    grad_mu = torch.stack(lst_grad_mu).mean(dim=0)\n",
    "    grad_rho = torch.stack(lst_grad_rho).mean(dim=0)\n",
    "    \n",
    "    print(torch.stack(lst_v).mean())\n",
    "    #print(grad_mu)\n",
    "    #print(grad_rho)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        theta[0:2] -= alpha*grad_mu\n",
    "        theta[2:4] -= alpha*grad_rho\n",
    "    \n",
    "    print(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
