{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HiddenBlock(nn.Module):\n",
    "    def __init__(self, hidden_input, hidden_output, dropout=0.2, activation='ReLu', bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = nn.Linear(hidden_input, hidden_output, bias)\n",
    "        if activation == 'ReLu':\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        self.hidden_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, debug_prints=False):\n",
    "        hidden = self.hidden_layer(x)\n",
    "        acti = self.activation(hidden)\n",
    "        drop = self.hidden_dropout(acti)\n",
    "\n",
    "        if debug_prints:\n",
    "            print('Hidden', hidden)\n",
    "            print('Activation', acti)\n",
    "            print('Hidden Dropout', drop, '\\n')\n",
    "            \n",
    "        return drop\n",
    "\n",
    "class MultilayerPerceptronNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes,\n",
    "                    activation = 'ReLu',\n",
    "                    target_type_string='Regression',\n",
    "                    input_dropout=0, hidden_dropout=0,\n",
    "                    bias=True):\n",
    "        super().__init__()\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "\n",
    "        hidden_blocs = []\n",
    "        num_hidden = len(hidden_sizes)\n",
    "        for i in range(num_hidden):\n",
    "            in_size = input_size if i==0 else hidden_sizes[i-1]\n",
    "            out_size = hidden_sizes[i]\n",
    "            hidden_blocs += [HiddenBlock(in_size, out_size, hidden_dropout, activation, bias)]\n",
    "        self.hidden_blocks = nn.Sequential(*hidden_blocs)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size, bias)\n",
    "\n",
    "        self.target_type_string = target_type_string\n",
    "        if target_type_string=='Regression':\n",
    "            self.loss_function = nn.MSELoss()\n",
    "        elif target_type_string=='Classification':\n",
    "            self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, debug_prints=False):\n",
    "        in_drop = self.input_dropout(x)\n",
    "        if debug_prints:\n",
    "            print('Input', x)\n",
    "            print('Input Dropout', in_drop, '\\n')\n",
    "\n",
    "        hid = self.hidden_blocks(in_drop)\n",
    "        \n",
    "        out = self.output_layer(hid)\n",
    "        if debug_prints:\n",
    "            print('Output', out, '\\n')\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 12\n",
    "output_size = 12\n",
    "hidden_sizes = [10, 8, 6, 4, 6, 8, 10]\n",
    "mlpTest = MultilayerPerceptronReLuNet(input_size, output_size, hidden_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n"
     ]
    }
   ],
   "source": [
    "in_data = []\n",
    "for i in range(input_size):\n",
    "    in_data += [i]\n",
    "in_data = torch.Tensor(in_data)\n",
    "print(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n",
      "Input Dropout tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]) \n",
      "\n",
      "Hidden tensor([ 1.4212, -3.7230, -5.2845, -3.2397, -1.6919, -3.1640,  1.1619,  0.3054,\n",
      "         4.1130,  4.2298], grad_fn=<AddBackward0>)\n",
      "Activation tensor([1.4212, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1619, 0.3054, 4.1130,\n",
      "        4.2298], grad_fn=<ThresholdBackward0>)\n",
      "Hidden Dropout tensor([1.4212, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1619, 0.3054, 4.1130,\n",
      "        4.2298], grad_fn=<ThresholdBackward0>) \n",
      "\n",
      "Hidden tensor([-1.3746, -0.2098,  1.5144, -0.3680, -1.1193, -1.4083, -1.9301, -0.4666],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Activation tensor([0.0000, 0.0000, 1.5144, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       grad_fn=<ThresholdBackward0>)\n",
      "Hidden Dropout tensor([0.0000, 0.0000, 1.5144, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       grad_fn=<ThresholdBackward0>) \n",
      "\n",
      "Hidden tensor([-0.1393, -0.0147,  0.7593, -0.1031,  0.6299,  0.3549],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Activation tensor([0.0000, 0.0000, 0.7593, 0.0000, 0.6299, 0.3549],\n",
      "       grad_fn=<ThresholdBackward0>)\n",
      "Hidden Dropout tensor([0.0000, 0.0000, 0.7593, 0.0000, 0.6299, 0.3549],\n",
      "       grad_fn=<ThresholdBackward0>) \n",
      "\n",
      "Hidden tensor([-0.2352, -0.0987, -0.1344,  0.4554], grad_fn=<AddBackward0>)\n",
      "Activation tensor([0.0000, 0.0000, 0.0000, 0.4554], grad_fn=<ThresholdBackward0>)\n",
      "Hidden Dropout tensor([0.0000, 0.0000, 0.0000, 0.4554], grad_fn=<ThresholdBackward0>) \n",
      "\n",
      "Hidden tensor([ 0.3660, -0.4169,  0.3869,  0.1947,  0.2460,  0.0094],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Activation tensor([0.3660, 0.0000, 0.3869, 0.1947, 0.2460, 0.0094],\n",
      "       grad_fn=<ThresholdBackward0>)\n",
      "Hidden Dropout tensor([0.3660, 0.0000, 0.3869, 0.1947, 0.2460, 0.0094],\n",
      "       grad_fn=<ThresholdBackward0>) \n",
      "\n",
      "Hidden tensor([ 0.0776,  0.1755,  0.4411,  0.0710, -0.1028, -0.1038,  0.1505,  0.3028],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Activation tensor([0.0776, 0.1755, 0.4411, 0.0710, 0.0000, 0.0000, 0.1505, 0.3028],\n",
      "       grad_fn=<ThresholdBackward0>)\n",
      "Hidden Dropout tensor([0.0776, 0.1755, 0.4411, 0.0710, 0.0000, 0.0000, 0.1505, 0.3028],\n",
      "       grad_fn=<ThresholdBackward0>) \n",
      "\n",
      "Hidden tensor([ 0.2038,  0.2283,  0.2195, -0.3318, -0.2640,  0.0033, -0.1951, -0.0481,\n",
      "         0.3116, -0.0370], grad_fn=<AddBackward0>)\n",
      "Activation tensor([0.2038, 0.2283, 0.2195, 0.0000, 0.0000, 0.0033, 0.0000, 0.0000, 0.3116,\n",
      "        0.0000], grad_fn=<ThresholdBackward0>)\n",
      "Hidden Dropout tensor([0.2038, 0.2283, 0.2195, 0.0000, 0.0000, 0.0033, 0.0000, 0.0000, 0.3116,\n",
      "        0.0000], grad_fn=<ThresholdBackward0>) \n",
      "\n",
      "Output tensor([-0.4962,  0.1690, -0.0160, -0.1945, -0.1669, -0.0843, -0.0096, -0.0207,\n",
      "         0.0404, -0.1655,  0.2052, -0.0154], grad_fn=<AddBackward0>) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.4962,  0.1690, -0.0160, -0.1945, -0.1669, -0.0843, -0.0096, -0.0207,\n",
       "         0.0404, -0.1655,  0.2052, -0.0154], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpTest(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
