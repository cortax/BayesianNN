{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class GatedRecurrentUnitHnNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, \n",
    "                        window_size, hidden_size, num_layers, \n",
    "                        target_type_string='Regression',\n",
    "                        bias=True, batch_first=True, \n",
    "                        bidirectional=False,\n",
    "                        dropout_hidden=0, dropout_Hn=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=bias,\n",
    "                            batch_first=batch_first,\n",
    "                            dropout=dropout_hidden, \n",
    "                            bidirectional=bidirectional)\n",
    "        self.hn_dropout_layer = nn.Dropout(dropout_Hn)\n",
    "        self.predict_layer = nn.Linear(hidden_size * num_layers * (bidirectional + 1), output_size) #(hidden*layers*1, 1)\n",
    "        \n",
    "        self.target_type_string = target_type_string\n",
    "        if target_type_string=='Regression':\n",
    "            self.loss_function = nn.MSELoss()\n",
    "        elif target_type_string=='Classification':\n",
    "            self.loss_function = nn.CrossEntropyLoss()\n",
    "             \n",
    "    def forward(self, input):\n",
    "        #print('batch:', input)\n",
    "        #print(input.shape)\n",
    "        #print(input[0])\n",
    "        #print(input[0][0])\n",
    "        #print(input[0][0][0])\n",
    "        #print('input:', input)\n",
    "\n",
    "        gru_out, h_n = self.gru(input) #shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        print('hn:', h_n)\n",
    "        print('gru_out', gru_out)\n",
    "        print(gru_out==h_n)\n",
    "        #print('hn size:', h_n.size(0), h_n.size(1), h_n.size(2))\n",
    "        \n",
    "        list_to_cat = []\n",
    "        for layers in h_n[:]:\n",
    "            list_to_cat.append(layers)\n",
    "        h_n_concat = torch.cat(list_to_cat, dim=1) #shape: (batch, hidden_size * num_layers * num_directions)\n",
    "        print('hn concat:', h_n_concat)\n",
    "        #print('hn concat size:', h_n_concat.size(0), h_n_concat.size(1))\n",
    "        \n",
    "        h_n_with_dropout = self.hn_dropout_layer(h_n_concat)\n",
    "        predict = self.predict_layer(h_n_with_dropout) #predict est vertical\n",
    "        #print('predict:', predict)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = 1\n",
    "window_size = 1\n",
    "hidden_size = 8\n",
    "num_layers = 1\n",
    "target_type_string='Regression'\n",
    "bias=True\n",
    "batch_first=True\n",
    "bidirectional=False\n",
    "dropout_hidden=0\n",
    "dropout_Hn=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GatedRecurrentUnitHnNet(input_size, output_size, \n",
    "                        window_size, hidden_size, num_layers, \n",
    "                        target_type_string='Regression',\n",
    "                        bias=True, batch_first=True, \n",
    "                        bidirectional=False,\n",
    "                        dropout_hidden=0, dropout_Hn=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[[7.9018e+04, 1.2779e+01]],\n",
    "        [[5.2380e+04, 1.9846e+01]],\n",
    "        [[5.9300e+04, 1.0390e+01]],\n",
    "        [[6.4618e+04, 7.1564e+00]],\n",
    "        [[6.2786e+04, 6.4031e+00]],\n",
    "        [[6.0496e+04, 6.3149e+00]],\n",
    "        [[6.6420e+04, 1.0902e+01]],\n",
    "        [[8.4788e+04, 1.6613e+01]],\n",
    "        [[6.5484e+04, 9.7487e+00]],\n",
    "        [[5.1308e+04, 2.7077e+01]],\n",
    "        [[3.3682e+04, 1.7220e+01]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn: tensor([[[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "         [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]]], grad_fn=<StackBackward>)\n",
      "gru_out tensor([[[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n",
      "hn concat: tensor([[ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  1., -1.,  1.,  0.,  1.]], grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8084],\n",
       "        [0.8084],\n",
       "        [0.8084],\n",
       "        [0.8084],\n",
       "        [0.8084],\n",
       "        [0.8084],\n",
       "        [0.8084],\n",
       "        [0.8084],\n",
       "        [0.8084],\n",
       "        [0.8084],\n",
       "        [0.8084]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru(torch.Tensor(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
