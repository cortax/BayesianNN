{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)\n",
    "        return self.linear(y1[:, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def data_generator(N, seq_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_length: Length of the adding problem data\n",
    "        N: # of data in the set\n",
    "    \"\"\"\n",
    "    X_num = torch.rand([N, 1, seq_length])\n",
    "    X_mask = torch.zeros([N, 1, seq_length])\n",
    "    Y = torch.zeros([N, 1])\n",
    "    for i in range(N):\n",
    "        positions = np.random.choice(seq_length, size=2, replace=False)\n",
    "        X_mask[i, 0, positions[0]] = 1\n",
    "        X_mask[i, 0, positions[1]] = 1\n",
    "        Y[i,0] = X_num[i, 0, positions[0]] + X_num[i, 0, positions[1]]\n",
    "    X = torch.cat((X_num, X_mask), dim=1)\n",
    "    return Variable(X), Variable(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 2\n",
    "n_classes = 1\n",
    "batch_size = 8\n",
    "seq_length = 400\n",
    "epochs = 10\n",
    "#num_hidden: number of hidden units per layer\n",
    "num_hidden = 10\n",
    "#levels: Number of stacked \"Temporal Blocks\" in the TCN\n",
    "levels = 6\n",
    "\n",
    "#print(\"Producing data...\")\n",
    "X_train, Y_train = data_generator(50000, seq_length)\n",
    "X_test, Y_test = data_generator(1000, seq_length)\n",
    "\n",
    "\n",
    "# Note: We use a very simple setting here (assuming all levels have the same # of channels.\n",
    "channel_sizes = [num_hidden] * levels\n",
    "kernel_size = 7\n",
    "dropout = 0.0\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3828e-01, 7.6006e-01, 9.1701e-01, 9.5113e-01, 2.8325e-01, 7.6695e-01,\n",
       "        9.7632e-01, 4.1888e-01, 1.9247e-01, 7.1049e-01, 6.8221e-01, 9.9924e-02,\n",
       "        2.2611e-01, 4.0765e-01, 3.8856e-01, 8.1870e-01, 8.1219e-01, 6.2296e-01,\n",
       "        6.1935e-01, 6.6132e-02, 8.5097e-02, 2.9459e-01, 2.9862e-01, 2.1198e-01,\n",
       "        5.9707e-01, 3.8256e-01, 9.1999e-01, 3.4878e-01, 6.3482e-01, 8.5936e-01,\n",
       "        1.2417e-02, 1.0475e-01, 7.4702e-01, 9.7629e-01, 6.7929e-01, 6.9704e-01,\n",
       "        3.5785e-01, 2.1712e-01, 3.3499e-01, 2.3734e-01, 5.8855e-01, 1.9085e-01,\n",
       "        2.3255e-01, 7.0426e-01, 9.4318e-01, 7.6312e-01, 1.8672e-01, 5.1846e-02,\n",
       "        1.3478e-01, 4.3276e-01, 3.3906e-01, 1.1481e-01, 8.3152e-01, 3.4553e-01,\n",
       "        5.3452e-01, 5.2529e-01, 4.9701e-01, 2.1752e-01, 5.1216e-01, 1.8565e-01,\n",
       "        2.5466e-01, 4.1591e-01, 8.1981e-01, 9.4530e-01, 4.6204e-01, 9.7778e-01,\n",
       "        3.6878e-01, 2.7689e-01, 8.7917e-01, 3.0063e-01, 4.0216e-01, 2.3362e-01,\n",
       "        3.0507e-01, 9.3912e-01, 5.0923e-01, 4.0762e-01, 3.1581e-02, 6.1660e-01,\n",
       "        9.5069e-01, 8.6364e-01, 3.0118e-01, 9.7943e-01, 5.2368e-01, 3.7464e-01,\n",
       "        7.4183e-01, 3.9020e-01, 9.7572e-01, 5.1156e-01, 2.6023e-01, 5.3082e-01,\n",
       "        2.1840e-01, 5.5629e-01, 8.5219e-01, 4.1021e-01, 5.8737e-01, 3.9460e-01,\n",
       "        8.4682e-01, 1.2002e-01, 4.0332e-01, 2.7310e-01, 7.3661e-01, 5.9672e-01,\n",
       "        4.4905e-01, 4.8805e-01, 5.9628e-04, 9.6492e-01, 4.6931e-01, 3.0604e-01,\n",
       "        5.1862e-01, 4.0056e-01, 1.6768e-01, 7.5199e-02, 5.5613e-01, 3.6999e-01,\n",
       "        4.6306e-01, 4.4969e-01, 3.3564e-01, 4.9822e-02, 2.3780e-03, 2.2575e-02,\n",
       "        9.9100e-01, 4.2393e-01, 4.0707e-01, 4.9740e-01, 5.4932e-02, 9.2427e-01,\n",
       "        1.6691e-01, 7.9618e-01, 5.8315e-01, 1.9980e-01, 9.8206e-01, 8.6734e-01,\n",
       "        3.2979e-01, 7.6489e-01, 7.2818e-01, 6.5702e-02, 5.9508e-01, 3.6273e-01,\n",
       "        6.8268e-01, 8.4180e-01, 5.2959e-01, 1.7814e-02, 8.7322e-01, 2.7231e-01,\n",
       "        2.8263e-01, 6.7402e-01, 2.5287e-01, 6.9333e-01, 3.6464e-01, 3.1700e-01,\n",
       "        1.8766e-01, 9.0486e-01, 9.5791e-01, 9.6708e-01, 5.1489e-01, 1.4477e-01,\n",
       "        7.9753e-01, 9.9221e-01, 3.6327e-01, 1.7999e-01, 2.4557e-01, 6.9448e-01,\n",
       "        2.9331e-01, 8.1882e-01, 3.3076e-01, 4.2856e-01, 6.7076e-01, 4.2159e-01,\n",
       "        8.3338e-01, 2.0746e-01, 5.7536e-01, 5.0677e-01, 7.3058e-02, 3.3253e-03,\n",
       "        7.8099e-01, 2.5704e-01, 9.4559e-01, 7.6955e-01, 3.2412e-01, 5.7644e-01,\n",
       "        6.7881e-01, 9.0762e-01, 5.1648e-01, 1.1726e-01, 9.3174e-01, 7.5471e-01,\n",
       "        3.2978e-01, 4.7539e-01, 1.6606e-01, 8.6910e-01, 3.9280e-01, 3.8107e-01,\n",
       "        1.5863e-01, 5.7958e-01, 9.1374e-01, 7.7031e-01, 7.8616e-01, 6.6639e-01,\n",
       "        3.4095e-01, 4.7638e-01, 5.3411e-01, 4.2954e-01, 7.7158e-01, 8.7727e-01,\n",
       "        7.9949e-01, 9.5265e-01, 7.6270e-01, 9.2765e-02, 8.8068e-01, 6.7875e-01,\n",
       "        8.8027e-01, 6.7392e-01, 3.4494e-01, 9.1823e-01, 7.6007e-02, 2.4042e-01,\n",
       "        3.1152e-01, 5.1259e-01, 7.1830e-01, 9.8031e-01, 2.1682e-01, 5.1583e-01,\n",
       "        4.4178e-01, 3.9507e-01, 4.8485e-01, 9.3250e-01, 4.6344e-01, 3.4120e-01,\n",
       "        9.8198e-01, 3.0246e-01, 8.0901e-02, 1.3652e-01, 1.0609e-01, 1.7439e-01,\n",
       "        7.7936e-01, 8.9562e-01, 6.5291e-01, 7.8996e-01, 3.9544e-01, 4.4841e-01,\n",
       "        1.7259e-01, 9.0221e-01, 1.4373e-01, 3.8505e-02, 8.2573e-01, 9.5840e-01,\n",
       "        1.4103e-01, 2.0808e-01, 7.5774e-01, 6.4082e-01, 6.1628e-01, 9.9438e-01,\n",
       "        5.5561e-02, 4.5494e-01, 6.0288e-01, 6.4338e-01, 3.6795e-01, 3.8324e-01,\n",
       "        9.1023e-01, 1.9682e-01, 9.8863e-01, 9.4470e-01, 2.9872e-01, 9.9713e-01,\n",
       "        9.0799e-01, 4.1383e-01, 7.5726e-01, 3.2968e-01, 1.8637e-01, 4.9135e-01,\n",
       "        9.8801e-01, 8.3317e-01, 2.0299e-01, 6.5756e-01, 8.2868e-01, 3.1206e-02,\n",
       "        3.3419e-01, 6.7071e-01, 7.0206e-01, 9.2776e-01, 7.0093e-01, 5.5726e-01,\n",
       "        8.5523e-01, 2.8460e-02, 5.1893e-01, 2.9317e-01, 6.5574e-01, 4.3988e-01,\n",
       "        6.5259e-01, 1.3988e-01, 3.1896e-01, 8.2460e-02, 1.6356e-01, 3.8401e-01,\n",
       "        9.1856e-01, 4.7691e-01, 2.6895e-01, 9.8436e-01, 2.1738e-01, 1.9217e-01,\n",
       "        7.0662e-01, 9.1670e-01, 7.7664e-01, 6.4674e-01, 2.6158e-01, 5.8899e-01,\n",
       "        5.6300e-02, 5.7757e-01, 8.3625e-02, 4.5200e-01, 5.7204e-01, 7.5364e-01,\n",
       "        2.0972e-01, 2.8212e-01, 2.2321e-01, 6.1650e-01, 4.2762e-01, 7.1381e-01,\n",
       "        9.8275e-01, 1.5142e-01, 6.0792e-01, 8.2689e-02, 9.8657e-01, 2.5139e-01,\n",
       "        1.2069e-01, 4.9686e-01, 1.8215e-01, 8.6904e-01, 6.1375e-01, 2.3034e-01,\n",
       "        5.4728e-01, 8.7508e-01, 8.1331e-01, 6.0032e-02, 4.0300e-01, 3.1712e-01,\n",
       "        7.8869e-01, 8.2403e-03, 8.3024e-01, 4.4530e-01, 1.0339e-01, 2.6026e-01,\n",
       "        8.4991e-01, 8.2387e-01, 5.8764e-01, 2.4362e-01, 2.9369e-01, 5.0491e-02,\n",
       "        3.5114e-01, 3.3216e-01, 4.8465e-02, 6.2415e-01, 9.9667e-01, 6.4924e-01,\n",
       "        2.3808e-01, 5.3612e-01, 6.7768e-01, 9.0405e-01, 1.2044e-01, 7.0943e-01,\n",
       "        2.2039e-01, 8.8153e-01, 4.4922e-02, 6.9068e-01, 6.2863e-01, 4.8010e-01,\n",
       "        6.0328e-01, 5.0947e-01, 7.2076e-01, 2.1598e-01, 6.9658e-01, 8.8700e-01,\n",
       "        8.1253e-01, 9.5340e-01, 8.0395e-01, 5.3658e-01, 8.2414e-01, 3.4041e-01,\n",
       "        9.1096e-01, 1.0633e-01, 6.0655e-01, 3.0241e-01, 9.9086e-01, 9.4990e-02,\n",
       "        7.9194e-01, 3.0737e-01, 1.1949e-01, 3.4900e-01, 4.0494e-01, 7.0933e-01,\n",
       "        6.0634e-01, 6.3814e-01, 9.8015e-01, 2.2699e-01, 7.0940e-01, 8.1047e-01,\n",
       "        6.8277e-01, 1.9583e-02, 5.5717e-01, 7.1990e-01])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, X_train.size(0), batch_size):\n",
    "        if i + batch_size > X_train.size(0):\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+batch_size)], Y_train[i:(i+batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 400])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4691e-01, 4.7932e-01, 3.4841e-01, 3.0986e-01, 7.0181e-01, 4.7097e-01,\n",
       "        2.2430e-01, 3.1750e-01, 1.4830e-01, 8.2008e-01, 3.5697e-01, 5.8757e-01,\n",
       "        6.6105e-01, 3.1613e-01, 9.1187e-01, 3.5923e-01, 3.5367e-01, 1.8791e-01,\n",
       "        1.0985e-01, 2.8470e-01, 3.2126e-01, 9.9816e-01, 1.9188e-01, 1.9611e-01,\n",
       "        4.4079e-01, 2.6091e-01, 3.4290e-01, 9.5968e-01, 1.5774e-01, 5.4944e-01,\n",
       "        4.2977e-01, 1.9353e-01, 2.5661e-01, 9.4326e-01, 3.1973e-01, 5.3713e-01,\n",
       "        2.1968e-01, 7.4206e-01, 9.4140e-01, 1.9690e-01, 3.8755e-01, 8.2674e-01,\n",
       "        4.4132e-01, 6.9913e-01, 8.3288e-01, 9.9932e-01, 7.7595e-01, 4.4082e-01,\n",
       "        5.0792e-02, 4.7113e-01, 9.5398e-01, 3.0008e-01, 9.9670e-01, 5.0500e-01,\n",
       "        9.7430e-01, 3.6835e-01, 3.2493e-01, 9.2898e-01, 3.5834e-01, 9.5333e-01,\n",
       "        1.2156e-01, 3.4994e-01, 3.1817e-01, 7.2535e-03, 7.5396e-01, 9.2259e-01,\n",
       "        6.3666e-01, 5.3627e-01, 8.9480e-01, 2.9077e-01, 7.1576e-01, 8.9599e-01,\n",
       "        9.9244e-01, 2.2213e-01, 9.0209e-01, 2.1446e-01, 4.8594e-02, 1.5256e-01,\n",
       "        6.9673e-01, 9.8375e-01, 6.1807e-01, 5.2849e-02, 1.3680e-01, 8.1658e-01,\n",
       "        1.7644e-01, 1.4879e-01, 2.7731e-01, 1.4258e-02, 9.4653e-02, 6.7939e-01,\n",
       "        8.9017e-01, 3.7801e-01, 9.6930e-01, 4.3977e-01, 8.2233e-01, 6.8754e-01,\n",
       "        5.8954e-01, 6.8026e-01, 7.7478e-01, 6.9986e-01, 2.1838e-01, 1.0646e-01,\n",
       "        2.6482e-01, 1.8278e-01, 1.2259e-01, 2.9845e-01, 9.5805e-01, 9.9490e-01,\n",
       "        2.2018e-01, 8.3813e-02, 3.6458e-01, 7.6035e-01, 1.2584e-01, 6.8898e-01,\n",
       "        3.8394e-01, 1.0524e-01, 5.9562e-01, 6.7433e-01, 6.0147e-01, 3.4124e-01,\n",
       "        9.0546e-01, 2.2196e-01, 9.3245e-01, 9.0193e-01, 9.4290e-01, 1.8964e-01,\n",
       "        5.3345e-01, 8.9357e-01, 7.2630e-01, 4.2152e-01, 4.7571e-01, 1.6437e-01,\n",
       "        6.7871e-01, 9.0389e-01, 2.0220e-01, 5.9413e-01, 7.4508e-01, 6.4664e-01,\n",
       "        6.4079e-01, 8.7204e-01, 9.7055e-01, 8.4706e-01, 4.1522e-01, 7.1282e-01,\n",
       "        7.8921e-01, 1.5560e-01, 9.0208e-01, 4.2246e-01, 4.2001e-01, 5.7886e-01,\n",
       "        4.1725e-01, 5.7627e-01, 7.2730e-01, 2.5238e-01, 5.8383e-01, 3.7871e-02,\n",
       "        3.8342e-01, 6.4429e-01, 3.2827e-01, 1.9577e-01, 5.4534e-01, 1.3688e-01,\n",
       "        1.7066e-01, 6.2990e-01, 7.5123e-01, 6.1746e-01, 2.5817e-01, 5.4714e-02,\n",
       "        5.5545e-01, 3.9432e-01, 9.1933e-01, 6.3217e-01, 2.3653e-01, 7.3689e-01,\n",
       "        9.8178e-02, 3.1956e-03, 4.8664e-01, 6.2932e-02, 4.2081e-01, 7.8298e-01,\n",
       "        7.3403e-01, 2.0542e-01, 8.3159e-01, 4.4200e-01, 9.1609e-02, 9.3580e-01,\n",
       "        7.1124e-01, 5.6895e-01, 3.0989e-01, 2.1321e-01, 6.9940e-01, 5.2822e-04,\n",
       "        1.6702e-01, 8.7893e-01, 3.3530e-01, 9.4490e-01, 2.6102e-01, 6.8668e-01,\n",
       "        3.7327e-02, 9.0728e-01, 8.2198e-01, 8.7286e-01, 7.8543e-01, 8.4959e-01,\n",
       "        3.5576e-01, 7.3944e-01, 2.7672e-01, 2.8062e-02, 6.8707e-01, 7.7726e-01,\n",
       "        3.3881e-01, 5.5182e-01, 2.8712e-01, 2.0090e-01, 4.8210e-01, 7.4548e-01,\n",
       "        9.1765e-01, 2.3802e-01, 4.6162e-01, 4.7775e-01, 9.0445e-01, 1.6226e-01,\n",
       "        5.2658e-01, 4.6813e-01, 9.4751e-01, 2.8154e-01, 8.7975e-01, 7.3227e-01,\n",
       "        2.0103e-02, 9.4255e-01, 4.9371e-01, 5.1563e-01, 2.4376e-01, 5.9990e-01,\n",
       "        6.1208e-01, 3.7962e-01, 4.0907e-01, 6.2902e-01, 6.8339e-01, 1.4884e-01,\n",
       "        2.0608e-01, 7.8444e-01, 7.3620e-01, 6.8179e-01, 7.4561e-01, 4.6749e-01,\n",
       "        3.9985e-01, 9.4229e-01, 2.6980e-01, 6.7576e-01, 8.7396e-01, 2.6642e-01,\n",
       "        1.8244e-01, 8.5486e-01, 6.1401e-02, 8.6430e-01, 6.1764e-01, 7.7064e-01,\n",
       "        1.3360e-01, 8.6585e-01, 5.1895e-01, 8.6715e-01, 7.2375e-01, 5.6906e-01,\n",
       "        2.9626e-02, 2.1931e-01, 9.1030e-01, 6.5166e-02, 1.2985e-01, 4.9979e-01,\n",
       "        9.7176e-01, 4.6770e-01, 2.1666e-01, 1.2089e-02, 5.7924e-01, 4.8177e-01,\n",
       "        2.5928e-01, 3.2124e-01, 6.5288e-02, 7.0004e-01, 9.8627e-01, 1.1763e-01,\n",
       "        7.5642e-01, 5.4923e-01, 7.4721e-02, 7.2248e-01, 9.7229e-01, 8.3308e-01,\n",
       "        2.4507e-01, 3.1949e-01, 9.5658e-02, 8.0207e-01, 2.7176e-01, 3.8466e-02,\n",
       "        2.9295e-01, 6.6178e-01, 5.8117e-01, 5.7114e-01, 3.5759e-01, 5.4612e-01,\n",
       "        9.1543e-01, 1.9323e-02, 8.1867e-01, 2.6698e-01, 9.0374e-01, 1.0623e-01,\n",
       "        6.1244e-01, 6.5790e-01, 9.7082e-01, 2.4377e-01, 1.2291e-01, 3.6192e-01,\n",
       "        4.4750e-01, 7.5934e-01, 8.1414e-02, 2.8531e-01, 5.7469e-01, 7.2555e-01,\n",
       "        5.3239e-01, 6.1080e-01, 3.6399e-01, 2.9000e-01, 7.2577e-01, 6.1025e-01,\n",
       "        7.6164e-01, 3.2991e-01, 4.0761e-01, 9.2442e-01, 9.9389e-01, 3.6508e-01,\n",
       "        8.4697e-01, 2.0166e-01, 9.9701e-01, 4.4878e-01, 6.0559e-01, 8.1978e-01,\n",
       "        1.8070e-01, 1.2583e-01, 7.8031e-01, 5.0547e-01, 7.3314e-01, 2.3797e-01,\n",
       "        3.4551e-01, 5.4212e-01, 6.4840e-02, 6.1266e-01, 7.3999e-01, 9.0692e-01,\n",
       "        6.0543e-01, 4.3847e-01, 6.8520e-01, 3.3047e-02, 1.5917e-01, 2.8034e-01,\n",
       "        2.4220e-01, 4.0387e-01, 3.4128e-01, 5.3525e-01, 5.0949e-01, 2.4659e-01,\n",
       "        8.0876e-01, 9.0272e-01, 1.6099e-02, 5.4834e-01, 8.8635e-01, 4.3005e-01,\n",
       "        6.8255e-02, 1.3712e-01, 3.2587e-01, 3.2758e-01, 2.0346e-01, 3.8425e-01,\n",
       "        9.0522e-01, 5.1750e-01, 8.3139e-01, 2.6135e-01, 8.9650e-02, 6.4455e-01,\n",
       "        4.0885e-01, 4.9543e-01, 6.0094e-01, 6.1272e-02, 5.5124e-03, 4.8843e-01,\n",
       "        5.0594e-01, 6.9843e-01, 8.1642e-01, 4.5248e-01, 6.1863e-01, 1.6408e-01,\n",
       "        2.7343e-01, 9.6729e-01, 7.9929e-01, 6.2869e-01, 5.6309e-01, 8.3430e-01,\n",
       "        3.4213e-01, 8.4006e-02, 3.3696e-01, 5.3035e-01])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1103],\n",
       "        [0.1104],\n",
       "        [0.1103],\n",
       "        [0.1114],\n",
       "        [0.1098],\n",
       "        [0.1111],\n",
       "        [0.1105],\n",
       "        [0.1111]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input x must be of size \n",
    "model.eval()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_hidden: number of hidden units per layer\n",
    "num_hidden = 10\n",
    "#levels: Number of stacked \"Temporal Blocks\" in the TCN\n",
    "levels = 6\n",
    "\n",
    "#channel_sizes = [num_hidden] * levels\n",
    "channel_sizes = [10, 8, 14, 12, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 8, 14, 12, 19]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(channel_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 10 1\n",
      "1 10 10 2\n",
      "2 10 10 4\n",
      "3 10 10 8\n",
      "4 10 10 16\n",
      "5 10 10 32\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "for i in range(len(channel_sizes)):\n",
    "    in_channels = num_inputs if i == 0 else channel_sizes[i-1]\n",
    "    out_channels = channel_sizes[i]\n",
    "    dilation_size = 2 ** i\n",
    "    print(i, in_channels, out_channels, dilation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
